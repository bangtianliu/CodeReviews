# Detailed Code Reviews

**Total PRs: 148**

**Generated:** 2026-01-19 23:45:33

---

## [PR #23201](https://github.com/iree-org/iree/pull/23201): Bump actions/cache from 5.0.1 to 5.0.2 in the github-actions group

### Review Summary

**APPROVED** (2026-01-20)


---


## [PR #23200](https://github.com/iree-org/iree/pull/23200): [GPU][Codegen] Skip dimension expansion for ops with non projected permutation maps

### Review Summary

**APPROVED** (2026-01-19)

This seems like a pragmatic solution


---


## [PR #23198](https://github.com/iree-org/iree/pull/23198): [LinalgExt] Add verifier check to disallow index_base in explicit-index mode

### Review Summary

**APPROVED** (2026-01-19)


---


## [PR #23193](https://github.com/iree-org/iree/pull/23193): [LinalgExt] Extend arg_compare tiling interface for explicit-index mode

### Review Summary

**COMMENTED** (2026-01-19)

**APPROVED** (2026-01-19)


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1664`

```diff
@@ -1646,18 +1653,30 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   Value cmpResult = regionMap.lookup(srcBlock.getTerminator()->getOperand(0));
   Value selectedValue = arith::SelectOp::create(b, loc, cmpResult,
                                                 candidateValue, bestValueSoFar);
-  Value indexOffset = ivs[reductionDim];
-  if (getIndexBase()) {
-    indexOffset = arith::AddIOp::create(b, loc, getIndexBase(), indexOffset);
-  }
-  Value castedIndex = indexOffset;
-  if (castedIndex.getType() != bestIndexSoFar.getType()) {
-    castedIndex = arith::IndexCastOp::create(b, loc, bestIndexSoFar.getType(),
-                                             castedIndex);
+
+  Value candidateIndex;
+  if (hasExplicitIndexInput()) {
+    // Explicit-index mode: load from input.
+    // The verifier ensures input and output index types match, so no cast
+    // needed.
+    candidateIndex = memref::LoadOp::create(b, loc, getInputIndex(), ivs);
+  } else {
+    // Implicit-index mode: compute from iteration variable.
```

**Comment:**
```suggestion
    // Implicit-index mode: compute from induction variable.
```

? If that's not it, maybe it's worth renaming this function argument.

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1671`

```diff
@@ -1646,18 +1653,30 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   Value cmpResult = regionMap.lookup(srcBlock.getTerminator()->getOperand(0));
   Value selectedValue = arith::SelectOp::create(b, loc, cmpResult,
                                                 candidateValue, bestValueSoFar);
-  Value indexOffset = ivs[reductionDim];
-  if (getIndexBase()) {
-    indexOffset = arith::AddIOp::create(b, loc, getIndexBase(), indexOffset);
-  }
-  Value castedIndex = indexOffset;
-  if (castedIndex.getType() != bestIndexSoFar.getType()) {
-    castedIndex = arith::IndexCastOp::create(b, loc, bestIndexSoFar.getType(),
-                                             castedIndex);
+
+  Value candidateIndex;
+  if (hasExplicitIndexInput()) {
+    // Explicit-index mode: load from input.
+    // The verifier ensures input and output index types match, so no cast
+    // needed.
+    candidateIndex = memref::LoadOp::create(b, loc, getInputIndex(), ivs);
+  } else {
+    // Implicit-index mode: compute from iteration variable.
+    Value indexOffset = ivs[reductionDim];
+    if (getIndexBase()) {
+      indexOffset = arith::AddIOp::create(b, loc, getIndexBase(), indexOffset);
+    }
+    candidateIndex = indexOffset;
+
+    // Cast if iteration variable type doesn't match output index type.
```

**Comment:**
also here

---


---


## [PR #23190](https://github.com/iree-org/iree/pull/23190): [CI] Skip onnx_ops hip rdna3 tests

### Review Summary

**APPROVED** (2026-01-19)


---


## [PR #23188](https://github.com/iree-org/iree/pull/23188): [docs] Add policy for AI tool use

### Review Summary

**COMMENTED** (2026-01-19)


### Code Comments

**File:** `docs/website/docs/developers/general/contributing.md:347`

```diff
@@ -332,6 +332,29 @@ Dependency updates | `integrates/*` | `integrates/llvm-20240501`
 Branches that do not meet these guidelines may be deleted, especially if
 they [appear to be stale](https://github.com/iree-org/iree/branches/stale).
 
+### :octicons-hubot-16: AI tool use
+
+We adopt the [LLVM AI Tool Use Policy](https://llvm.org/docs/AIToolPolicy.html)
+and require all AI-assisted contributions (Pull Requests, issues, design
+proposals) to be reviewed and understood by the author(s).
+
+The key points include:
+
+1. "Contributors must read and review all LLM-generated code or text before they
+   ask other project members to review it."
+2. "Contributors are expected to be transparent and label contributions that
+   contain substantial amounts of tool-generated content." This can be done with
+   the `Assisted-by: tool-name` trailer in PR descriptions.
```

**Comment:**
I can add this as an alternative; `Assisted-by: ` was supposed to be an an example, not a strict requirement.

---


---


## [PR #23184](https://github.com/iree-org/iree/pull/23184): Propagate binding correlation information to LLVM codegen for mutable binding fusion

### Review Summary

**COMMENTED** (2026-01-20)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/ConvertToLLVM.cpp:351`

```diff
@@ -349,13 +341,58 @@ class ConvertFunc : public ConvertOpToLLVMPattern<func::FuncOp> {
 
     // Set argument attributes.
     Attribute unit = rewriter.getUnitAttr();
+
+    // Build a map from HAL binding index to correlation group and noalias
+    // bindings. This allows us to determine which bindings are in the same
+    // correlation group (i.e., point to the same resource but with different
+    // offsets) and which bindings point to different resources (noalias).
+    DenseMap<int64_t, SmallVector<int64_t>> bindingCorrelationMap;
+    DenseMap<int64_t, SmallVector<int64_t>> bindingNoaliasMap;
+    for (auto subspan : subspans) {
```

**Comment:**
spell out the type

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/ConvertToLLVM.cpp:390`

```diff
@@ -349,13 +341,58 @@ class ConvertFunc : public ConvertOpToLLVMPattern<func::FuncOp> {
 
     // Set argument attributes.
     Attribute unit = rewriter.getUnitAttr();
+
+    // Build a map from HAL binding index to correlation group and noalias
+    // bindings. This allows us to determine which bindings are in the same
+    // correlation group (i.e., point to the same resource but with different
+    // offsets) and which bindings point to different resources (noalias).
+    DenseMap<int64_t, SmallVector<int64_t>> bindingCorrelationMap;
+    DenseMap<int64_t, SmallVector<int64_t>> bindingNoaliasMap;
+    for (auto subspan : subspans) {
+      int64_t binding = subspan.getBinding().getSExtValue();
+      // Try to read correlation information from the original function.
+      // Note: This assumes the correlation attribute was preserved through
+      // Stream â†’ HAL conversion. If not, we'll need to preserve it explicitly.
+      if (auto correlationAttr = funcOp.getArgAttrOfType<ArrayAttr>(
+              binding, "stream.binding_correlation")) {
+        SmallVector<int64_t> correlatedBindings;
+        for (IntegerAttr intAttr : correlationAttr.getAsRange<IntegerAttr>()) {
+          correlatedBindings.push_back(intAttr.getInt());
+        }
+        if (!correlatedBindings.empty()) {
+          bindingCorrelationMap[binding] = std::move(correlatedBindings);
+        }
+      }
+      // Read noalias information (bindings pointing to different resources).
+      if (auto noaliasAttr = funcOp.getArgAttrOfType<ArrayAttr>(
+              binding, "stream.binding_noalias")) {
+        SmallVector<int64_t> noaliasBindings;
+        for (IntegerAttr intAttr : noaliasAttr.getAsRange<IntegerAttr>()) {
+          noaliasBindings.push_back(intAttr.getInt());
+        }
+        if (!noaliasBindings.empty()) {
+          bindingNoaliasMap[binding] = std::move(noaliasBindings);
+        }
+      }
+    }
+
     for (auto [idx, info] : llvm::enumerate(bindingsInfo)) {
       // As a convention with HAL all the kernel argument pointers are 16Bytes
       // aligned.
       newFuncOp.setArgAttr(idx, LLVM::LLVMDialect::getAlignAttrName(),
                            rewriter.getI32IntegerAttr(16));
+
+      // Check if this binding has correlation or noalias information.
+      // - Correlation: bindings in the same group point to the same resource
+      //   but with different offsets (noalias).
+      // - Noalias: bindings pointing to different resources (noalias).
+      auto correlationIt = bindingCorrelationMap.find(idx);
+      auto noaliasIt = bindingNoaliasMap.find(idx);
```

**Comment:**
These seem unused

---

**File:** `compiler/src/iree/compiler/Dialect/Stream/Transforms/FuseDispatchBindings.cpp:193`

```diff
@@ -191,15 +190,50 @@ static void updateExecutableSignature(IREE::Stream::ExecutableOp executableOp,
   SmallVector<BlockArgument> newBindingArgs;
   auto bindingType = IREE::Stream::BindingType::get(funcOp.getContext());
   auto offsetType = IndexType::get(funcOp.getContext());
-  for (auto &binding : bindings) {
+  for (auto binding : llvm::enumerate(bindings)) {
```

**Comment:**
Use structured bindings for the index and the value

---

**File:** `compiler/src/iree/compiler/Dialect/Stream/Transforms/FuseDispatchBindings.cpp:195`

```diff
@@ -191,15 +190,50 @@ static void updateExecutableSignature(IREE::Stream::ExecutableOp executableOp,
   SmallVector<BlockArgument> newBindingArgs;
   auto bindingType = IREE::Stream::BindingType::get(funcOp.getContext());
   auto offsetType = IndexType::get(funcOp.getContext());
-  for (auto &binding : bindings) {
+  for (auto binding : llvm::enumerate(bindings)) {
     SmallVector<Location> locs;
-    for (unsigned oldIdx : binding.correlationMap.set_bits()) {
+    for (unsigned oldIdx : binding.value().correlationMap.set_bits()) {
```

**Comment:**
```suggestion
    for (unsigned oldIdx : binding->correlationMap.set_bits()) {
```

---

**File:** `compiler/src/iree/compiler/Dialect/Stream/Transforms/FuseDispatchBindings.cpp:213`

```diff
@@ -191,15 +190,50 @@ static void updateExecutableSignature(IREE::Stream::ExecutableOp executableOp,
   SmallVector<BlockArgument> newBindingArgs;
   auto bindingType = IREE::Stream::BindingType::get(funcOp.getContext());
   auto offsetType = IndexType::get(funcOp.getContext());
-  for (auto &binding : bindings) {
+  for (auto binding : llvm::enumerate(bindings)) {
     SmallVector<Location> locs;
-    for (unsigned oldIdx : binding.correlationMap.set_bits()) {
+    for (unsigned oldIdx : binding.value().correlationMap.set_bits()) {
       locs.push_back(oldBindingArgs[oldIdx].getLoc());
     }
     auto loc = FusedLoc::get(funcOp.getContext(), locs);
     auto bindingArg =
         entryBlock.insertArgument(newBindingArgs.size(), bindingType, loc);
     newBindingArgs.push_back(bindingArg);
+
+    // Store correlation information as an attribute on the binding argument.
+    // This tells us which other bindings are in the same correlation group
+    // (i.e., point to the same resource but with different offsets, so they
+    // don't alias each other).
+    //
+    // For bindings in different correlation groups (pointing to different
+    // resources), we also store this information so that LLVM codegen can
+    // infer that they don't alias each other (noalias).
+    SmallVector<Attribute> correlatedIndices;
+    SmallVector<Attribute> noaliasIndices;
+    for (auto otherBinding : llvm::enumerate(bindings)) {
```

**Comment:**
use structured bindings for the index and the value

---

**File:** `compiler/src/iree/compiler/Dialect/Stream/Transforms/FuseDispatchBindings.cpp:222`

```diff
@@ -191,15 +190,50 @@ static void updateExecutableSignature(IREE::Stream::ExecutableOp executableOp,
   SmallVector<BlockArgument> newBindingArgs;
   auto bindingType = IREE::Stream::BindingType::get(funcOp.getContext());
   auto offsetType = IndexType::get(funcOp.getContext());
-  for (auto &binding : bindings) {
+  for (auto binding : llvm::enumerate(bindings)) {
     SmallVector<Location> locs;
-    for (unsigned oldIdx : binding.correlationMap.set_bits()) {
+    for (unsigned oldIdx : binding.value().correlationMap.set_bits()) {
       locs.push_back(oldBindingArgs[oldIdx].getLoc());
     }
     auto loc = FusedLoc::get(funcOp.getContext(), locs);
     auto bindingArg =
         entryBlock.insertArgument(newBindingArgs.size(), bindingType, loc);
     newBindingArgs.push_back(bindingArg);
+
+    // Store correlation information as an attribute on the binding argument.
+    // This tells us which other bindings are in the same correlation group
+    // (i.e., point to the same resource but with different offsets, so they
+    // don't alias each other).
+    //
+    // For bindings in different correlation groups (pointing to different
+    // resources), we also store this information so that LLVM codegen can
+    // infer that they don't alias each other (noalias).
+    SmallVector<Attribute> correlatedIndices;
+    SmallVector<Attribute> noaliasIndices;
+    for (auto otherBinding : llvm::enumerate(bindings)) {
+      if (binding.index() == otherBinding.index()) {
+        continue;
+      }
+
+      if (binding.value().correlationMap ==
+          otherBinding.value().correlationMap) {
+        // Same correlation group: same resource, different offsets (noalias)
+        correlatedIndices.push_back(IntegerAttr::get(
+            IntegerType::get(funcOp.getContext(), 32), otherBinding.index()));
```

**Comment:**
create a `Builder` and do something like `b.getI32Attr(...);`

---

**File:** `compiler/src/iree/compiler/Dialect/Stream/Transforms/FuseDispatchBindings.cpp:226`

```diff
@@ -191,15 +190,50 @@ static void updateExecutableSignature(IREE::Stream::ExecutableOp executableOp,
   SmallVector<BlockArgument> newBindingArgs;
   auto bindingType = IREE::Stream::BindingType::get(funcOp.getContext());
   auto offsetType = IndexType::get(funcOp.getContext());
-  for (auto &binding : bindings) {
+  for (auto binding : llvm::enumerate(bindings)) {
     SmallVector<Location> locs;
-    for (unsigned oldIdx : binding.correlationMap.set_bits()) {
+    for (unsigned oldIdx : binding.value().correlationMap.set_bits()) {
       locs.push_back(oldBindingArgs[oldIdx].getLoc());
     }
     auto loc = FusedLoc::get(funcOp.getContext(), locs);
     auto bindingArg =
         entryBlock.insertArgument(newBindingArgs.size(), bindingType, loc);
     newBindingArgs.push_back(bindingArg);
+
+    // Store correlation information as an attribute on the binding argument.
+    // This tells us which other bindings are in the same correlation group
+    // (i.e., point to the same resource but with different offsets, so they
+    // don't alias each other).
+    //
+    // For bindings in different correlation groups (pointing to different
+    // resources), we also store this information so that LLVM codegen can
+    // infer that they don't alias each other (noalias).
+    SmallVector<Attribute> correlatedIndices;
+    SmallVector<Attribute> noaliasIndices;
+    for (auto otherBinding : llvm::enumerate(bindings)) {
+      if (binding.index() == otherBinding.index()) {
+        continue;
+      }
+
+      if (binding.value().correlationMap ==
+          otherBinding.value().correlationMap) {
+        // Same correlation group: same resource, different offsets (noalias)
+        correlatedIndices.push_back(IntegerAttr::get(
+            IntegerType::get(funcOp.getContext(), 32), otherBinding.index()));
+      } else {
+        // Different correlation groups: different resources (noalias)
+        noaliasIndices.push_back(IntegerAttr::get(
+            IntegerType::get(funcOp.getContext(), 32), otherBinding.index()));
```

**Comment:**
Also here

---

**File:** `compiler/src/iree/compiler/Dialect/Stream/Transforms/FuseDispatchBindings.cpp:220`

```diff
@@ -191,15 +190,50 @@ static void updateExecutableSignature(IREE::Stream::ExecutableOp executableOp,
   SmallVector<BlockArgument> newBindingArgs;
   auto bindingType = IREE::Stream::BindingType::get(funcOp.getContext());
   auto offsetType = IndexType::get(funcOp.getContext());
-  for (auto &binding : bindings) {
+  for (auto binding : llvm::enumerate(bindings)) {
     SmallVector<Location> locs;
-    for (unsigned oldIdx : binding.correlationMap.set_bits()) {
+    for (unsigned oldIdx : binding.value().correlationMap.set_bits()) {
       locs.push_back(oldBindingArgs[oldIdx].getLoc());
     }
     auto loc = FusedLoc::get(funcOp.getContext(), locs);
     auto bindingArg =
         entryBlock.insertArgument(newBindingArgs.size(), bindingType, loc);
     newBindingArgs.push_back(bindingArg);
+
+    // Store correlation information as an attribute on the binding argument.
+    // This tells us which other bindings are in the same correlation group
+    // (i.e., point to the same resource but with different offsets, so they
+    // don't alias each other).
+    //
+    // For bindings in different correlation groups (pointing to different
+    // resources), we also store this information so that LLVM codegen can
+    // infer that they don't alias each other (noalias).
+    SmallVector<Attribute> correlatedIndices;
+    SmallVector<Attribute> noaliasIndices;
+    for (auto otherBinding : llvm::enumerate(bindings)) {
+      if (binding.index() == otherBinding.index()) {
+        continue;
+      }
+
+      if (binding.value().correlationMap ==
+          otherBinding.value().correlationMap) {
+        // Same correlation group: same resource, different offsets (noalias)
```

**Comment:**
```suggestion
        // Same correlation group: same resource, different offsets (noalias).
```

---

**File:** `compiler/src/iree/compiler/Dialect/Stream/Transforms/FuseDispatchBindings.cpp:224`

```diff
@@ -191,15 +190,50 @@ static void updateExecutableSignature(IREE::Stream::ExecutableOp executableOp,
   SmallVector<BlockArgument> newBindingArgs;
   auto bindingType = IREE::Stream::BindingType::get(funcOp.getContext());
   auto offsetType = IndexType::get(funcOp.getContext());
-  for (auto &binding : bindings) {
+  for (auto binding : llvm::enumerate(bindings)) {
     SmallVector<Location> locs;
-    for (unsigned oldIdx : binding.correlationMap.set_bits()) {
+    for (unsigned oldIdx : binding.value().correlationMap.set_bits()) {
       locs.push_back(oldBindingArgs[oldIdx].getLoc());
     }
     auto loc = FusedLoc::get(funcOp.getContext(), locs);
     auto bindingArg =
         entryBlock.insertArgument(newBindingArgs.size(), bindingType, loc);
     newBindingArgs.push_back(bindingArg);
+
+    // Store correlation information as an attribute on the binding argument.
+    // This tells us which other bindings are in the same correlation group
+    // (i.e., point to the same resource but with different offsets, so they
+    // don't alias each other).
+    //
+    // For bindings in different correlation groups (pointing to different
+    // resources), we also store this information so that LLVM codegen can
+    // infer that they don't alias each other (noalias).
+    SmallVector<Attribute> correlatedIndices;
+    SmallVector<Attribute> noaliasIndices;
+    for (auto otherBinding : llvm::enumerate(bindings)) {
+      if (binding.index() == otherBinding.index()) {
+        continue;
+      }
+
+      if (binding.value().correlationMap ==
+          otherBinding.value().correlationMap) {
+        // Same correlation group: same resource, different offsets (noalias)
+        correlatedIndices.push_back(IntegerAttr::get(
+            IntegerType::get(funcOp.getContext(), 32), otherBinding.index()));
+      } else {
+        // Different correlation groups: different resources (noalias)
```

**Comment:**
```suggestion
        // Different correlation groups: different resources (noalias).
```

---


---


## [PR #23176](https://github.com/iree-org/iree/pull/23176): [SPIRV][Codegen] Refine consumer compatibility checks for SPIRVSubgroupReduce

### Review Summary

**APPROVED** (2026-01-17)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/SPIRV/test/config_default_reduction.mlir:243`

```diff
@@ -201,11 +206,45 @@ func.func @reduction_with_distributable_elementwise_consumer(
   return %epilogue : tensor<512x12xf32>
 }
 
-//  CHECK-DAG: #[[CONFIG1:.+]] = #iree_codegen.lowering_config<tile_sizes = {{\[}}[], [128]{{\]}}>
-//  CHECK-DAG: #[[CONFIG2:.+]] = #iree_codegen.lowering_config<tile_sizes = {{\[}}[1], [0, 128]{{\]}}>
+func.func @fail_reduction_with_nondistributable_consumer(
```

**Comment:**
I'd put this in a separate split so that the checks are more local to the code under test

---

**File:** `compiler/src/iree/compiler/Codegen/SPIRV/KernelConfig.cpp:1134`

```diff
@@ -1145,49 +1105,100 @@ static bool canDistributeShape(ArrayRef<int64_t> shape, int64_t groupSize) {
 //   - d1=3: 8 % 3 != 0 and 3 % 8 != 0 -> failure
 // Since the consumer fails distribution, the reduction is also blocked and
 // the entire chain of operations stays inside the region.
-static bool hasIncompatibleConsumer(linalg::LinalgOp reductionOp,
-                                    int64_t groupSize) {
+//
+// Example: consumer indexing map permutes dims in a way that
+// breaks producer-defined distribution.
+//
+//   %red = linalg.generic {
+//     indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2, d1)>,
+//                      affine_map<(d0, d1, d2) -> (d0, d1)>],
+//     iterator_types = ["parallel", "parallel", "reduction"]
+//   } ins(%in : tensor<16x64x74xf32>)
+//     outs(%init : tensor<16x74xf32>) { ... } -> tensor<16x74xf32>
+//
+//   %consumer = linalg.generic {
+//     indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2, d1)>,
+//                      affine_map<(d0, d1, d2) -> (d0, d2)>,
+//                      affine_map<(d0, d1, d2) -> (d0, d2, d1)>],
+//     iterator_types = ["parallel", "parallel", "parallel"]
+//   } ins(%in, %red : tensor<16x64x74xf32>, tensor<16x74xf32>)
+//     outs(%out : tensor<16x74x64xf32>) { ... }
+//
+// Producer tiling is defined in the producer iteration space (par: d0=16,d1=74;
+// red: d2=64). Consumer indexing maps can reassociate %red such that a
+// producer reduction dim becomes a consumer indexing dim; those dims must
+// satisfy the groupSize distribution constraint directly. Here that extent is
+// 74, which fails for group size = 32, so we bail out of the reduction
+// pipeline.
+static bool isConsumerCompatible(linalg::LinalgOp consumerOp, Value result,
+                                 SmallVector<unsigned> reductionDims,
```

**Comment:**
Passing a vector by value causes copies (and allocations). Pass this as ArrayRef instead.

---


---


## [PR #23175](https://github.com/iree-org/iree/pull/23175): [Codegen][GPU] Enable swizzling for scaled matmuls

### Review Summary

**COMMENTED** (2026-01-16)

What's the speedup after we land this?

**COMMENTED** (2026-01-16)

**COMMENTED** (2026-01-16)

**COMMENTED** (2026-01-16)

**COMMENTED** (2026-01-16)

**COMMENTED** (2026-01-16)

**CHANGES_REQUESTED** (2026-01-16)

**COMMENTED** (2026-01-18)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/ConfigUtils.cpp:926`

```diff
@@ -914,8 +914,28 @@ getMatmulOrIGEMMLoweringConfigAndWorkgroupSize(
     // TODO(#22119): We don't use global load DMA for scaled matmuls, because
     // compilation doesn't support it. Once this is fixed, we should use global
     // load DMA here when possible.
-    promotionArray = {};
     promotionList.append({2, 3});
+    // The row width is the number of elements across which we want to swizzle
+    // groups over.To avoid bank conflicts, we want to set the row width to be
+    // the number of elements that fill all the cache lines.
+    auto defaultConfigAttr = IREE::GPU::DerivedThreadConfigAttr::get(context);
+    int64_t lhsBitwidth = lhsElemType.getIntOrFloatBitWidth();
+    int64_t rhsBitwidth = rhsElemType.getIntOrFloatBitWidth();
+    int64_t lhsRowWidth = kCacheLineSizeBits / lhsBitwidth;
+    int64_t rhsRowWidth = kCacheLineSizeBits / rhsBitwidth;
+    int64_t accessWidth = schedule->kSizes.back();
```

**Comment:**
What is the unit? I thought that kSizes has the number of elements, not bits?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/ConfigUtils.cpp:979`

```diff
@@ -956,6 +976,7 @@ getMatmulOrIGEMMLoweringConfigAndWorkgroupSize(
   }
   auto configDict = DictionaryAttr::get(context, attrs);
   auto loweringConfig = IREE::GPU::LoweringConfigAttr::get(context, configDict);
+  llvm::errs() << "loweringConfig: " << loweringConfig << "\n";
```

**Comment:**
drop this debug print

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/ConfigUtils.cpp:924`

```diff
@@ -914,8 +914,28 @@ getMatmulOrIGEMMLoweringConfigAndWorkgroupSize(
     // TODO(#22119): We don't use global load DMA for scaled matmuls, because
     // compilation doesn't support it. Once this is fixed, we should use global
     // load DMA here when possible.
-    promotionArray = {};
     promotionList.append({2, 3});
+    // The row width is the number of elements across which we want to swizzle
+    // groups over.To avoid bank conflicts, we want to set the row width to be
+    // the number of elements that fill all the cache lines.
+    auto defaultConfigAttr = IREE::GPU::DerivedThreadConfigAttr::get(context);
+    int64_t lhsBitwidth = lhsElemType.getIntOrFloatBitWidth();
+    int64_t rhsBitwidth = rhsElemType.getIntOrFloatBitWidth();
```

**Comment:**
Are we always accessing a single element from lhs and rhs?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/ConfigUtils.cpp:925`

```diff
@@ -914,8 +914,28 @@ getMatmulOrIGEMMLoweringConfigAndWorkgroupSize(
     // TODO(#22119): We don't use global load DMA for scaled matmuls, because
     // compilation doesn't support it. Once this is fixed, we should use global
     // load DMA here when possible.
-    promotionArray = {};
     promotionList.append({2, 3});
+    // The row width is the number of elements across which we want to swizzle
+    // groups over.To avoid bank conflicts, we want to set the row width to be
+    // the number of elements that fill all the cache lines.
+    auto defaultConfigAttr = IREE::GPU::DerivedThreadConfigAttr::get(context);
+    int64_t lhsBitwidth = lhsElemType.getIntOrFloatBitWidth();
+    int64_t rhsBitwidth = rhsElemType.getIntOrFloatBitWidth();
+    int64_t lhsRowWidth = kCacheLineSizeBits / lhsBitwidth;
+    int64_t rhsRowWidth = kCacheLineSizeBits / rhsBitwidth;
```

**Comment:**
LDS != cache. This seems like the wrong constant to use even if the value will end up being the same.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/ConfigUtils.cpp:926`

```diff
@@ -914,8 +914,28 @@ getMatmulOrIGEMMLoweringConfigAndWorkgroupSize(
     // TODO(#22119): We don't use global load DMA for scaled matmuls, because
     // compilation doesn't support it. Once this is fixed, we should use global
     // load DMA here when possible.
-    promotionArray = {};
     promotionList.append({2, 3});
+    // The row width is the number of elements across which we want to swizzle
+    // groups over.To avoid bank conflicts, we want to set the row width to be
+    // the number of elements that fill all the cache lines.
+    auto defaultConfigAttr = IREE::GPU::DerivedThreadConfigAttr::get(context);
+    int64_t lhsBitwidth = lhsElemType.getIntOrFloatBitWidth();
+    int64_t rhsBitwidth = rhsElemType.getIntOrFloatBitWidth();
+    int64_t lhsRowWidth = kCacheLineSizeBits / lhsBitwidth;
+    int64_t rhsRowWidth = kCacheLineSizeBits / rhsBitwidth;
+    int64_t accessWidth = schedule->kSizes.back();
```

**Comment:**
I don't understand your reply. Can you update variable names to clearly indicate if the unit is bits or elements?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/ConfigUtils.cpp:925`

```diff
@@ -914,8 +914,28 @@ getMatmulOrIGEMMLoweringConfigAndWorkgroupSize(
     // TODO(#22119): We don't use global load DMA for scaled matmuls, because
     // compilation doesn't support it. Once this is fixed, we should use global
     // load DMA here when possible.
-    promotionArray = {};
     promotionList.append({2, 3});
+    // The row width is the number of elements across which we want to swizzle
+    // groups over.To avoid bank conflicts, we want to set the row width to be
+    // the number of elements that fill all the cache lines.
+    auto defaultConfigAttr = IREE::GPU::DerivedThreadConfigAttr::get(context);
+    int64_t lhsBitwidth = lhsElemType.getIntOrFloatBitWidth();
+    int64_t rhsBitwidth = rhsElemType.getIntOrFloatBitWidth();
+    int64_t lhsRowWidth = kCacheLineSizeBits / lhsBitwidth;
+    int64_t rhsRowWidth = kCacheLineSizeBits / rhsBitwidth;
```

**Comment:**
It shouldn't be a constant, it should be based on the target env. We have something similar already added to pad encoding.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/ConfigUtils.cpp:919`

```diff
@@ -914,8 +914,28 @@ getMatmulOrIGEMMLoweringConfigAndWorkgroupSize(
     // TODO(#22119): We don't use global load DMA for scaled matmuls, because
     // compilation doesn't support it. Once this is fixed, we should use global
     // load DMA here when possible.
-    promotionArray = {};
     promotionList.append({2, 3});
+    // The row width is the number of elements across which we want to swizzle
```

**Comment:**
If it's elements call the related variables something like `numRowElems` and `numAccessElems`. To me width is usually means bits/bytes.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/ConfigUtils.cpp:47`

```diff
@@ -44,6 +44,7 @@ namespace mlir::iree_compiler::IREE::GPU {
 
 constexpr int64_t kCacheLineSizeBits = 128 * 8;
 constexpr int64_t kPreferredCopyNumBits = 128;
+constexpr int64_t kLDSBankWidthBits = 32 * 4 * 8;
```

**Comment:**
This needs to be a target env attribute. For example mi300 has 32 banks while mi355 has 64 banks.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/ConfigUtils.cpp:939`

```diff
@@ -914,8 +915,28 @@ getMatmulOrIGEMMLoweringConfigAndWorkgroupSize(
     // TODO(#22119): We don't use global load DMA for scaled matmuls, because
     // compilation doesn't support it. Once this is fixed, we should use global
     // load DMA here when possible.
-    promotionArray = {};
     promotionList.append({2, 3});
+    // The row width is the number of elements across which we want to swizzle
+    // groups over.To avoid bank conflicts, we want to set the row width to be
+    // the number of elements that fill all the cache lines.
+    auto defaultConfigAttr = IREE::GPU::DerivedThreadConfigAttr::get(context);
+    int64_t lhsBitwidth = lhsElemType.getIntOrFloatBitWidth();
+    int64_t rhsBitwidth = rhsElemType.getIntOrFloatBitWidth();
+    int64_t lhsNumRowElems = kLDSBankWidthBits / lhsBitwidth;
+    int64_t rhsNumRowElems = kLDSBankWidthBits / rhsBitwidth;
+    int64_t numAccessElems = schedule->kSizes.back();
+    auto lhsSwizzleAttr = IREE::Codegen::XORShuffleAttr::get(
+        context, lhsNumRowElems, numAccessElems, /*row_stride=*/int64_t(0),
+        /*per_phase=*/int64_t(0));
+    auto rhsSwizzleAttr = IREE::Codegen::XORShuffleAttr::get(
+        context, rhsNumRowElems, numAccessElems, /*row_stride=*/int64_t(0),
+        /*per_phase=*/int64_t(0));
+    Attribute lhsSwizzleOperand = IREE::GPU::SwizzleOperandAttr::get(
+        context, defaultConfigAttr, lhsSwizzleAttr);
+    Attribute rhsSwizzleOperand = IREE::GPU::SwizzleOperandAttr::get(
+        context, defaultConfigAttr, rhsSwizzleAttr);
+    promotionArray = {lhsSwizzleOperand, rhsSwizzleOperand, defaultConfigAttr,
+                      defaultConfigAttr};
```

**Comment:**
We can land bindings separately to keep this PR small

---


---


## [PR #23170](https://github.com/iree-org/iree/pull/23170): [Codegen] fixes typo in assert statement

### Review Summary

**APPROVED** (2026-01-16)


---


## [PR #23169](https://github.com/iree-org/iree/pull/23169): [Codegen] Fix compiler errors in LinkTuningSpecsPass

### Review Summary

**COMMENTED** (2026-01-16)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:413`

```diff
@@ -409,12 +409,8 @@ static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
   SmallVector<Attribute> mergedActions;
 
   for (ForeachMatchOp foreachMatchOp : foreachMatchOps) {
-    ArrayAttr matchers = foreachMatchOp.getMatchers();
-    ArrayAttr actions = foreachMatchOp.getActions();
-    for (auto [matcher, action] : llvm::zip_equal(matchers, actions)) {
-      mergedMatchers.push_back(cast<SymbolRefAttr>(matcher));
-      mergedActions.push_back(cast<SymbolRefAttr>(action));
-    }
+    llvm::append_range(mergedMatchers, foreachMatchOp.getMatchers());
+    llvm::append_range(mergedActions, foreachMatchOp.getActions());
```

**Comment:**
I don't understand. There's no difference AFAICT.

---


---


## [PR #23168](https://github.com/iree-org/iree/pull/23168): [CI] Update torch_ops config file and run tests when config files are modified

### Review Summary

**APPROVED** (2026-01-16)


---


## [PR #23167](https://github.com/iree-org/iree/pull/23167): [Transforms] fix const member initializes fo HoistIntoGlobalsPass

### Review Summary

**APPROVED** (2026-01-18)


---


## [PR #23153](https://github.com/iree-org/iree/pull/23153): [LinalgExt] Extend arg_compare to support both value and index provided ( explicit-index mode)

### Review Summary

**COMMENTED** (2026-01-16)

**APPROVED** (2026-01-17)


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1254`

```diff
@@ -1247,37 +1247,70 @@ TopkOp::reifyResultShapes(OpBuilder &b,
 LogicalResult ArgCompareOp::verify() {
   Operation *op = getOperation();
 
-  unsigned numInputVals = llvm::size(getInputs());
-  if (numInputVals != 1) {
-    return op->emitOpError(
-               "expected exactly one tensor input operand, but got ")
-           << numInputVals;
+  // The operation supports two modes based on the number of inputs:
+  // - Implicit-index mode (1 input): Computes indices from iteration variables.
+  // - Explicit-index mode (2 inputs): Uses pre-existing (value, index) pairs.
+  unsigned numInputs = llvm::size(getInputs());
+  if (numInputs < 1 || numInputs > 2) {
```

**Comment:**
```suggestion
  if (numInputs == 1 || numInputs == 2) {
```

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1253`

```diff
@@ -1247,37 +1247,70 @@ TopkOp::reifyResultShapes(OpBuilder &b,
 LogicalResult ArgCompareOp::verify() {
   Operation *op = getOperation();
 
-  unsigned numInputVals = llvm::size(getInputs());
-  if (numInputVals != 1) {
-    return op->emitOpError(
-               "expected exactly one tensor input operand, but got ")
-           << numInputVals;
+  // The operation supports two modes based on the number of inputs:
+  // - Implicit-index mode (1 input): Computes indices from iteration variables.
+  // - Explicit-index mode (2 inputs): Uses pre-existing (value, index) pairs.
+  unsigned numInputs = llvm::size(getInputs());
```

**Comment:**
Maybe add this as an extra class declaration instead?

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1274`

```diff
@@ -1247,37 +1247,70 @@ TopkOp::reifyResultShapes(OpBuilder &b,
 LogicalResult ArgCompareOp::verify() {
   Operation *op = getOperation();
 
-  unsigned numInputVals = llvm::size(getInputs());
-  if (numInputVals != 1) {
-    return op->emitOpError(
-               "expected exactly one tensor input operand, but got ")
-           << numInputVals;
+  // The operation supports two modes based on the number of inputs:
+  // - Implicit-index mode (1 input): Computes indices from iteration variables.
+  // - Explicit-index mode (2 inputs): Uses pre-existing (value, index) pairs.
+  unsigned numInputs = llvm::size(getInputs());
+  if (numInputs < 1 || numInputs > 2) {
+    return op->emitOpError("expected 1 or 2 input operands, but got ")
+           << numInputs;
   }
 
+  bool isExplicitIndexMode = numInputs == 2;
+  ShapedType inputValueType = getInputType();
+  Type inputElemType = inputValueType.getElementType();
+
   unsigned numOutputs = getNumDpsInits();
   if (numOutputs != 2) {
     return op->emitOpError(
                "expected two output operands (value and index), but got ")
            << numOutputs;
   }
 
-  uint64_t dim = getDimension();
-  int64_t rank = getInputRank();
-  if (dim >= rank) {
-    return op->emitOpError("reduction dimension exceeds or equals input rank. ")
-           << "got dimension: " << dim << ", but input rank is: " << rank;
-  }
-
-  ShapedType inputType = getInputType();
   auto outputValueType = getOutputValueType();
   auto outputIndexType = getOutputIndexType();
 
-  if (inputType.getElementType() != outputValueType.getElementType()) {
+  if (isExplicitIndexMode) {
+    ShapedType inputIndexType = cast<ShapedType>(getInputIndex().getType());
```

**Comment:**
Also here: maybe add this as a helper in ODS?

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1279`

```diff
@@ -1247,37 +1247,70 @@ TopkOp::reifyResultShapes(OpBuilder &b,
 LogicalResult ArgCompareOp::verify() {
   Operation *op = getOperation();
 
-  unsigned numInputVals = llvm::size(getInputs());
-  if (numInputVals != 1) {
-    return op->emitOpError(
-               "expected exactly one tensor input operand, but got ")
-           << numInputVals;
+  // The operation supports two modes based on the number of inputs:
+  // - Implicit-index mode (1 input): Computes indices from iteration variables.
+  // - Explicit-index mode (2 inputs): Uses pre-existing (value, index) pairs.
+  unsigned numInputs = llvm::size(getInputs());
+  if (numInputs < 1 || numInputs > 2) {
+    return op->emitOpError("expected 1 or 2 input operands, but got ")
+           << numInputs;
   }
 
+  bool isExplicitIndexMode = numInputs == 2;
+  ShapedType inputValueType = getInputType();
+  Type inputElemType = inputValueType.getElementType();
+
   unsigned numOutputs = getNumDpsInits();
   if (numOutputs != 2) {
     return op->emitOpError(
                "expected two output operands (value and index), but got ")
            << numOutputs;
   }
 
-  uint64_t dim = getDimension();
-  int64_t rank = getInputRank();
-  if (dim >= rank) {
-    return op->emitOpError("reduction dimension exceeds or equals input rank. ")
-           << "got dimension: " << dim << ", but input rank is: " << rank;
-  }
-
-  ShapedType inputType = getInputType();
   auto outputValueType = getOutputValueType();
   auto outputIndexType = getOutputIndexType();
 
-  if (inputType.getElementType() != outputValueType.getElementType()) {
+  if (isExplicitIndexMode) {
+    ShapedType inputIndexType = cast<ShapedType>(getInputIndex().getType());
+
+    if (failed(verifyCompatibleShape(inputValueType, inputIndexType))) {
+      return op->emitOpError(
+                 "explicit-index mode: value and index inputs must have same "
+                 "shape. ")
```

**Comment:**
does compatible == the same, or do we allow for some wiggle room?

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1293`

```diff
@@ -1247,37 +1247,70 @@ TopkOp::reifyResultShapes(OpBuilder &b,
 LogicalResult ArgCompareOp::verify() {
   Operation *op = getOperation();
 
-  unsigned numInputVals = llvm::size(getInputs());
-  if (numInputVals != 1) {
-    return op->emitOpError(
-               "expected exactly one tensor input operand, but got ")
-           << numInputVals;
+  // The operation supports two modes based on the number of inputs:
+  // - Implicit-index mode (1 input): Computes indices from iteration variables.
+  // - Explicit-index mode (2 inputs): Uses pre-existing (value, index) pairs.
+  unsigned numInputs = llvm::size(getInputs());
+  if (numInputs < 1 || numInputs > 2) {
+    return op->emitOpError("expected 1 or 2 input operands, but got ")
+           << numInputs;
   }
 
+  bool isExplicitIndexMode = numInputs == 2;
+  ShapedType inputValueType = getInputType();
+  Type inputElemType = inputValueType.getElementType();
+
   unsigned numOutputs = getNumDpsInits();
   if (numOutputs != 2) {
     return op->emitOpError(
                "expected two output operands (value and index), but got ")
            << numOutputs;
   }
 
-  uint64_t dim = getDimension();
-  int64_t rank = getInputRank();
-  if (dim >= rank) {
-    return op->emitOpError("reduction dimension exceeds or equals input rank. ")
-           << "got dimension: " << dim << ", but input rank is: " << rank;
-  }
-
-  ShapedType inputType = getInputType();
   auto outputValueType = getOutputValueType();
   auto outputIndexType = getOutputIndexType();
 
-  if (inputType.getElementType() != outputValueType.getElementType()) {
+  if (isExplicitIndexMode) {
+    ShapedType inputIndexType = cast<ShapedType>(getInputIndex().getType());
+
+    if (failed(verifyCompatibleShape(inputValueType, inputIndexType))) {
+      return op->emitOpError(
+                 "explicit-index mode: value and index inputs must have same "
+                 "shape. ")
+             << "Value shape: "
+             << llvm::interleaved_array(inputValueType.getShape())
+             << ", index shape: "
+             << llvm::interleaved_array(inputIndexType.getShape());
+    }
+
+    if (!isa<IntegerType, IndexType>(inputIndexType.getElementType())) {
+      return op->emitOpError(
+                 "explicit-index mode: index input must have integer or index "
+                 "element type, but got ")
+             << inputIndexType.getElementType();
+    }
+
+    if (inputIndexType.getElementType() != outputIndexType.getElementType()) {
```

**Comment:**
Can we define these through ODS?

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1308`

```diff
@@ -1247,37 +1247,70 @@ TopkOp::reifyResultShapes(OpBuilder &b,
 LogicalResult ArgCompareOp::verify() {
   Operation *op = getOperation();
 
-  unsigned numInputVals = llvm::size(getInputs());
-  if (numInputVals != 1) {
-    return op->emitOpError(
-               "expected exactly one tensor input operand, but got ")
-           << numInputVals;
+  // The operation supports two modes based on the number of inputs:
+  // - Implicit-index mode (1 input): Computes indices from iteration variables.
+  // - Explicit-index mode (2 inputs): Uses pre-existing (value, index) pairs.
+  unsigned numInputs = llvm::size(getInputs());
+  if (numInputs < 1 || numInputs > 2) {
+    return op->emitOpError("expected 1 or 2 input operands, but got ")
+           << numInputs;
   }
 
+  bool isExplicitIndexMode = numInputs == 2;
+  ShapedType inputValueType = getInputType();
+  Type inputElemType = inputValueType.getElementType();
+
   unsigned numOutputs = getNumDpsInits();
   if (numOutputs != 2) {
     return op->emitOpError(
                "expected two output operands (value and index), but got ")
            << numOutputs;
   }
 
-  uint64_t dim = getDimension();
-  int64_t rank = getInputRank();
-  if (dim >= rank) {
-    return op->emitOpError("reduction dimension exceeds or equals input rank. ")
-           << "got dimension: " << dim << ", but input rank is: " << rank;
-  }
-
-  ShapedType inputType = getInputType();
   auto outputValueType = getOutputValueType();
   auto outputIndexType = getOutputIndexType();
 
-  if (inputType.getElementType() != outputValueType.getElementType()) {
+  if (isExplicitIndexMode) {
+    ShapedType inputIndexType = cast<ShapedType>(getInputIndex().getType());
+
+    if (failed(verifyCompatibleShape(inputValueType, inputIndexType))) {
+      return op->emitOpError(
+                 "explicit-index mode: value and index inputs must have same "
+                 "shape. ")
+             << "Value shape: "
+             << llvm::interleaved_array(inputValueType.getShape())
+             << ", index shape: "
+             << llvm::interleaved_array(inputIndexType.getShape());
+    }
+
+    if (!isa<IntegerType, IndexType>(inputIndexType.getElementType())) {
+      return op->emitOpError(
+                 "explicit-index mode: index input must have integer or index "
+                 "element type, but got ")
+             << inputIndexType.getElementType();
+    }
+
+    if (inputIndexType.getElementType() != outputIndexType.getElementType()) {
+      return op->emitOpError(
+                 "explicit-index mode: input and output index element types "
+                 "must match. ")
+             << "Input index type: " << inputIndexType.getElementType()
+             << ", output index type: " << outputIndexType.getElementType();
+    }
+  }
+
+  if (inputValueType.getElementType() != outputValueType.getElementType()) {
     return op->emitOpError("input and output value element types must match. ")
-           << "Input type: " << inputType.getElementType()
+           << "Input type: " << inputValueType.getElementType()
            << ", output value type: " << outputValueType.getElementType();
   }
 
+  if (!isa<IntegerType, IndexType>(outputIndexType.getElementType())) {
```

**Comment:**
Also here

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1259`

```diff
@@ -1247,37 +1247,70 @@ TopkOp::reifyResultShapes(OpBuilder &b,
 LogicalResult ArgCompareOp::verify() {
   Operation *op = getOperation();
 
-  unsigned numInputVals = llvm::size(getInputs());
-  if (numInputVals != 1) {
-    return op->emitOpError(
-               "expected exactly one tensor input operand, but got ")
-           << numInputVals;
+  // The operation supports two modes based on the number of inputs:
+  // - Implicit-index mode (1 input): Computes indices from iteration variables.
+  // - Explicit-index mode (2 inputs): Uses pre-existing (value, index) pairs.
+  unsigned numInputs = llvm::size(getInputs());
+  if (numInputs < 1 || numInputs > 2) {
+    return op->emitOpError("expected 1 or 2 input operands, but got ")
+           << numInputs;
   }
 
+  bool isExplicitIndexMode = numInputs == 2;
```

**Comment:**
You already have a helper for this: `hasExplicitIndexInput`

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.td:859`

```diff
@@ -820,6 +846,19 @@ def IREELinalgExt_ArgCompareOp : IREELinalgExt_Op<"arg_compare", [
       return getDpsInputOperand(0)->get();
     }
 
+    // Returns true when the operation is in explicit-index mode, which occurs
+    // when a second input operand provides pre-computed indices.
+    bool hasExplicitIndexInput() {
+      return getInputs().size() == 2;
+    }
+
+    // Returns the index input operand. This method should only be called when
+    // the operation is in explicit-index mode.
+    Value getInputIndex() {
+      assert(hasExplicitIndexInput() && "Only valid in explicit-index mode");
+      return getDpsInputOperand(1)->get();
```

**Comment:**
alternatively you could return nullptr when there's none and not assert

---


---


## [PR #23152](https://github.com/iree-org/iree/pull/23152): [GPU] Inject index hints during MMA lane distribution

### Review Summary

**COMMENTED** (2026-01-17)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.cpp:802`

```diff
@@ -782,6 +784,59 @@ MMAAttr::buildUnderlyingOperations(OpBuilder &builder, Location loc,
   return failure();
 }
 
+/// Creates index_hint ops wrapping delinearized lane ID values.
+/// The `delinearizedLaneId` values come from delinearizing the lane ID using
+/// `basis`, with the innermost/fastest-varying dimension last.
+///
+/// Non-final indices get lane_constant hints (uniform across lane groups).
+/// The final index gets lane_increment hint (increments within lane group).
+/// The group size is derived from the innermost basis element.
+/// Indices with a unit basis are ignored, and given a lane_constant hint.
+static SmallVector<Value>
+createTransposeLoadIndexHint(OpBuilder &builder, Location loc,
+                             ValueRange delinearizedLaneId,
+                             ArrayRef<int64_t> basis) {
+  // Need at least 2 dimensions for transpose load pattern.
+  if (delinearizedLaneId.size() < 2) {
+    return SmallVector<Value>(delinearizedLaneId.begin(),
+                              delinearizedLaneId.end());
```

**Comment:**
prefer `llvm::to_vector` or `llvm::to_vector_of<T>`

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.cpp:815`

```diff
@@ -782,6 +784,59 @@ MMAAttr::buildUnderlyingOperations(OpBuilder &builder, Location loc,
   return failure();
 }
 
+/// Creates index_hint ops wrapping delinearized lane ID values.
+/// The `delinearizedLaneId` values come from delinearizing the lane ID using
+/// `basis`, with the innermost/fastest-varying dimension last.
+///
+/// Non-final indices get lane_constant hints (uniform across lane groups).
+/// The final index gets lane_increment hint (increments within lane group).
+/// The group size is derived from the innermost basis element.
+/// Indices with a unit basis are ignored, and given a lane_constant hint.
+static SmallVector<Value>
+createTransposeLoadIndexHint(OpBuilder &builder, Location loc,
+                             ValueRange delinearizedLaneId,
+                             ArrayRef<int64_t> basis) {
+  // Need at least 2 dimensions for transpose load pattern.
+  if (delinearizedLaneId.size() < 2) {
+    return SmallVector<Value>(delinearizedLaneId.begin(),
+                              delinearizedLaneId.end());
+  }
+
+  // Find the index of the innermost non-unit (> 1) basis element.
+  // This determines which result gets the lane-increment hint.
+  // Size-1 dimensions produce constant 0 outputs regardless of lane ID,
+  // so they don't contribute to the meaningful group structure.
+  int64_t groupSize = 1;
+  size_t incrementResultIdx = delinearizedLaneId.size() - 1;
+  // The delinearized indices could have N or N + 1 results, and the basis
+  // elements are aligned with the last N results, so iterate backwards
+  // together.
+  for (size_t i = 1; i <= basis.size(); ++i) {
+    groupSize = basis[basis.size() - i];
```

**Comment:**
Do not recalculate the end
```suggestion
  for (size_t i = 1, e = basis.size(); i <= e; ++i) {
    groupSize = basis[e - i];
```

---


---


## [PR #23151](https://github.com/iree-org/iree/pull/23151): [Codegen] Extend small float emulation to support f4E2M1FN types.

### Review Summary

**APPROVED** (2026-01-19)


---


## [PR #23149](https://github.com/iree-org/iree/pull/23149): [bazel][NFC] Add `# keep sorted` to enforce_glob calls in BUILD.bazel files.

### Review Summary

**APPROVED** (2026-01-15)

I wish there was some automatic way to make it the default


---


## [PR #23138](https://github.com/iree-org/iree/pull/23138): Integrate torch-mlir@ac33bab4

### Review Summary

**APPROVED** (2026-01-15)


---


## [PR #23135](https://github.com/iree-org/iree/pull/23135): [CI] Update iree-test-suite

### Review Summary

**APPROVED** (2026-01-15)


---


## [PR #23119](https://github.com/iree-org/iree/pull/23119): [CPU] Add fp8 emulation support for CPU backend

### Review Summary

**COMMENTED** (2026-01-15)

**COMMENTED** (2026-01-15)

**COMMENTED** (2026-01-17)

**COMMENTED** (2026-01-17)

**COMMENTED** (2026-01-17)

**APPROVED** (2026-01-19)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:46`

```diff
@@ -30,6 +36,632 @@ namespace mlir::iree_compiler {
 
 namespace {
 
+/// SFINAE (i.e., substitution failure is not an error) helper to detect if
+/// T::get(MLIRContext*) is valid.
+template <typename T, typename = void>
+struct HasContextGet : std::false_type {};
+template <typename T>
+struct HasContextGet<
+    T, std::void_t<decltype(T::get(std::declval<MLIRContext *>()))>>
+    : std::true_type {};
```

**Comment:**
use llvm::is_detected instead

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:50`

```diff
@@ -30,6 +36,632 @@ namespace mlir::iree_compiler {
 
 namespace {
 
+/// SFINAE (i.e., substitution failure is not an error) helper to detect if
+/// T::get(MLIRContext*) is valid.
+template <typename T, typename = void>
+struct HasContextGet : std::false_type {};
+template <typename T>
+struct HasContextGet<
+    T, std::void_t<decltype(T::get(std::declval<MLIRContext *>()))>>
+    : std::true_type {};
+
+/// Helpers to append types to a vector if they are f8 types.
+template <typename T>
+void maybeAppendType(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
```

**Comment:**
```suggestion
static void maybeAppendType(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:59`

```diff
@@ -30,6 +36,632 @@ namespace mlir::iree_compiler {
 
 namespace {
 
+/// SFINAE (i.e., substitution failure is not an error) helper to detect if
+/// T::get(MLIRContext*) is valid.
+template <typename T, typename = void>
+struct HasContextGet : std::false_type {};
+template <typename T>
+struct HasContextGet<
+    T, std::void_t<decltype(T::get(std::declval<MLIRContext *>()))>>
+    : std::true_type {};
+
+/// Helpers to append types to a vector if they are f8 types.
+template <typename T>
+void maybeAppendType(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  if constexpr (HasContextGet<T>::value) {
+    Type t = T::get(ctx);
+    if (isa<FloatType>(t) && t.getIntOrFloatBitWidth() == 8) {
+      types.push_back(t);
+    }
+  }
+}
+template <typename... Ts>
+void appendTypesIf(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
```

**Comment:**
```suggestion
static void appendTypesIf(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:209`

```diff
@@ -30,6 +36,632 @@ namespace mlir::iree_compiler {
 
 namespace {
 
+/// SFINAE (i.e., substitution failure is not an error) helper to detect if
+/// T::get(MLIRContext*) is valid.
+template <typename T, typename = void>
+struct HasContextGet : std::false_type {};
+template <typename T>
+struct HasContextGet<
+    T, std::void_t<decltype(T::get(std::declval<MLIRContext *>()))>>
+    : std::true_type {};
+
+/// Helpers to append types to a vector if they are f8 types.
+template <typename T>
+void maybeAppendType(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  if constexpr (HasContextGet<T>::value) {
+    Type t = T::get(ctx);
+    if (isa<FloatType>(t) && t.getIntOrFloatBitWidth() == 8) {
+      types.push_back(t);
+    }
+  }
+}
+template <typename... Ts>
+void appendTypesIf(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  (maybeAppendType<Ts>(ctx, types), ...);
+}
+
+//===----------------------------------------------------------------------===//
+// Floating-point format constants and type info
+//===----------------------------------------------------------------------===//
+//
+// This follows the same approach as IREE's
+// runtime/src/iree/base/internal/math.h for floating-point conversions. The
+// compiler emits equivalent logic using MLIR arith ops instead of C control
+// flow.
+//
+// IEEE 754 floating-point format: [sign | exponent | mantissa]
+//   - Sign: 1 bit (0 = positive, 1 = negative)
+//   - Exponent: biased (stored = actual + bias)
+//   - Mantissa: fractional bits with implicit leading 1 for normal values
+//
+// Special values:
+//   - Zero: exp=0, mantissa=0 (signed zero if format supports it)
+//   - Denormal: exp=0, mantissa!=0, value = mantissa * 2^(1-bias-mantissa_bits)
+//   - Inf: exp=max, mantissa=0 (IEEE types only)
+//   - NaN: exp=max, mantissa!=0 (IEEE), or special encoding (FNUZ: 0x80)
+
+// F32 format constants (IEEE 754 binary32).
+constexpr int kF32MantBits = 23;
+constexpr int kF32Bias = 127;
+constexpr int kF32MantMask = (1 << kF32MantBits) - 1;
+constexpr int kF32MaxExp = 0xFF; // All exponent bits set (255).
+
+/// Parameters describing a floating-point format for conversion.
+/// Derived from APFloat semantics at runtime to support all MLIR float types.
+struct FloatTypeInfo {
+  unsigned expBits;  // Number of exponent bits.
+  unsigned mantBits; // Number of mantissa bits (excluding implicit leading 1).
+  int bias;          // Exponent bias: stored_exp = actual_exp + bias.
+  bool hasInf;       // Format supports infinity (exp=max, mantissa=0).
+  bool hasNan;       // Format supports NaN.
+  bool hasNegZero;   // Format supports negative zero.
+  bool nanAsNegZero; // NaN encoded as 0x80 (FNUZ types).
+  unsigned signMask; // Bit mask for sign bit.
+  unsigned expMask;  // Bit mask for exponent field.
+  unsigned mantMask; // Bit mask for mantissa field.
+  unsigned nanEncoding; // Bit pattern for canonical NaN.
+  unsigned infEncoding; // Bit pattern for +Inf (0 if no Inf).
+  unsigned maxFinite;   // Bit pattern for max finite value.
+};
+
+/// Returns format info for a float type by querying APFloat semantics.
+static FloatTypeInfo getFloatTypeInfo(Type type) {
+  auto floatType = cast<FloatType>(type);
+  const llvm::fltSemantics &sem = floatType.getFloatSemantics();
+  FloatTypeInfo info;
+
+  // Derive format parameters from APFloat semantics.
+  info.mantBits = llvm::APFloat::semanticsPrecision(sem) - 1;
+  unsigned totalBits = llvm::APFloat::semanticsSizeInBits(sem);
+  info.expBits = totalBits - 1 - info.mantBits;
+
+  // Compute masks.
+  info.signMask = 1u << (info.expBits + info.mantBits);
+  info.mantMask = (1u << info.mantBits) - 1;
+  info.expMask = ((1u << info.expBits) - 1) << info.mantBits;
+
+  // Bias from APFloat semantics.
+  int minExp = llvm::APFloat::semanticsMinExponent(sem);
+  info.bias = 1 - minExp;
+
+  info.hasInf = llvm::APFloat::semanticsHasInf(sem);
+  info.hasNan = llvm::APFloat::semanticsHasNaN(sem);
+
+  // Check for FNUZ types where NaN is encoded as negative zero (0x80).
+  // These types have no negative zero and no infinity.
+  llvm::APFloat negZero = llvm::APFloat::getZero(sem, /*Negative=*/true);
+  info.hasNegZero = negZero.isZero() && negZero.isNegative();
+  info.nanAsNegZero = !info.hasNegZero && !info.hasInf && info.hasNan;
+
+  // Compute NaN and Inf encodings.
+  unsigned maxExpCode = (1u << info.expBits) - 1;
+  if (info.nanAsNegZero) {
+    // FNUZ types: NaN = 0x80 (sign bit set, all else zero).
+    info.nanEncoding = info.signMask;
+  } else {
+    // IEEE and FN types: NaN = all exp bits + some mantissa bits.
+    info.nanEncoding = info.expMask | info.mantMask;
+  }
+
+  // Inf encoding: only meaningful for types with infinity.
+  info.infEncoding = info.hasInf ? info.expMask : 0;
+
+  // Max finite value: for types with infinity, it's exp=(max-1), mantissa=all
+  // 1s. For types without infinity, max exp is valid, so exp=max, mantissa=all
+  // 1s (except for FN types where max mantissa is NaN).
+  if (info.hasInf) {
+    info.maxFinite = ((maxExpCode - 1) << info.mantBits) | info.mantMask;
+  } else if (info.hasNan && !info.nanAsNegZero) {
+    // FN types: max exp is valid but max mantissa is NaN.
+    info.maxFinite = info.expMask | (info.mantMask - 1);
+  } else {
+    // FNUZ or no-NaN types: all bit patterns except NaN are valid.
+    info.maxFinite = info.expMask | info.mantMask;
+  }
+
+  return info;
+}
+
+//===----------------------------------------------------------------------===//
+// Helper for float emulation patterns
+//===----------------------------------------------------------------------===//
+
+/// Helper class for emulating float conversions using integer bit manipulation.
+/// Handles both scalar and vector types uniformly.
+class FloatEmulationHelper {
+public:
+  FloatEmulationHelper(RewriterBase &rewriter, Location loc, Type type)
+      : rewriter(rewriter), loc(loc), vecType(dyn_cast<VectorType>(type)) {
+    Type i32ScalarType = rewriter.getI32Type();
+    Type i8ScalarType = rewriter.getI8Type();
+    Type f32ScalarType = rewriter.getF32Type();
+    i32Type = vecType ? VectorType::get(vecType.getShape(), i32ScalarType)
+                      : i32ScalarType;
+    i8Type = vecType ? VectorType::get(vecType.getShape(), i8ScalarType)
+                     : i8ScalarType;
+    f32Type = vecType ? VectorType::get(vecType.getShape(), f32ScalarType)
+                      : f32ScalarType;
+  }
+
+  /// Creates an i32 constant, splatted if working with vectors.
+  Value createI32Const(int64_t value) {
+    auto attr = rewriter.getIntegerAttr(rewriter.getI32Type(), value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(i32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, attr);
+  }
+
+  /// Creates an f32 constant, splatted if working with vectors.
+  Value createF32Const(float value) {
+    auto attr = rewriter.getF32FloatAttr(value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(f32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, attr);
+  }
+
+  /// Extracts sign, exponent, and mantissa from an f32 value (as i32 bits).
+  /// F32 format: 1 sign bit, 8 exponent bits, 23 mantissa bits.
+  /// Returns {sign, biasedExp, mantissa}.
+  std::tuple<Value, Value, Value> extractF32Fields(Value i32Val) {
```

**Comment:**
Can you use a struct instead and name the fields so that we don't have to guess which value corresponds to which component?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:354`

```diff
@@ -30,6 +36,632 @@ namespace mlir::iree_compiler {
 
 namespace {
 
+/// SFINAE (i.e., substitution failure is not an error) helper to detect if
+/// T::get(MLIRContext*) is valid.
+template <typename T, typename = void>
+struct HasContextGet : std::false_type {};
+template <typename T>
+struct HasContextGet<
+    T, std::void_t<decltype(T::get(std::declval<MLIRContext *>()))>>
+    : std::true_type {};
+
+/// Helpers to append types to a vector if they are f8 types.
+template <typename T>
+void maybeAppendType(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  if constexpr (HasContextGet<T>::value) {
+    Type t = T::get(ctx);
+    if (isa<FloatType>(t) && t.getIntOrFloatBitWidth() == 8) {
+      types.push_back(t);
+    }
+  }
+}
+template <typename... Ts>
+void appendTypesIf(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  (maybeAppendType<Ts>(ctx, types), ...);
+}
+
+//===----------------------------------------------------------------------===//
+// Floating-point format constants and type info
+//===----------------------------------------------------------------------===//
+//
+// This follows the same approach as IREE's
+// runtime/src/iree/base/internal/math.h for floating-point conversions. The
+// compiler emits equivalent logic using MLIR arith ops instead of C control
+// flow.
+//
+// IEEE 754 floating-point format: [sign | exponent | mantissa]
+//   - Sign: 1 bit (0 = positive, 1 = negative)
+//   - Exponent: biased (stored = actual + bias)
+//   - Mantissa: fractional bits with implicit leading 1 for normal values
+//
+// Special values:
+//   - Zero: exp=0, mantissa=0 (signed zero if format supports it)
+//   - Denormal: exp=0, mantissa!=0, value = mantissa * 2^(1-bias-mantissa_bits)
+//   - Inf: exp=max, mantissa=0 (IEEE types only)
+//   - NaN: exp=max, mantissa!=0 (IEEE), or special encoding (FNUZ: 0x80)
+
+// F32 format constants (IEEE 754 binary32).
+constexpr int kF32MantBits = 23;
+constexpr int kF32Bias = 127;
+constexpr int kF32MantMask = (1 << kF32MantBits) - 1;
+constexpr int kF32MaxExp = 0xFF; // All exponent bits set (255).
+
+/// Parameters describing a floating-point format for conversion.
+/// Derived from APFloat semantics at runtime to support all MLIR float types.
+struct FloatTypeInfo {
+  unsigned expBits;  // Number of exponent bits.
+  unsigned mantBits; // Number of mantissa bits (excluding implicit leading 1).
+  int bias;          // Exponent bias: stored_exp = actual_exp + bias.
+  bool hasInf;       // Format supports infinity (exp=max, mantissa=0).
+  bool hasNan;       // Format supports NaN.
+  bool hasNegZero;   // Format supports negative zero.
+  bool nanAsNegZero; // NaN encoded as 0x80 (FNUZ types).
+  unsigned signMask; // Bit mask for sign bit.
+  unsigned expMask;  // Bit mask for exponent field.
+  unsigned mantMask; // Bit mask for mantissa field.
+  unsigned nanEncoding; // Bit pattern for canonical NaN.
+  unsigned infEncoding; // Bit pattern for +Inf (0 if no Inf).
+  unsigned maxFinite;   // Bit pattern for max finite value.
+};
+
+/// Returns format info for a float type by querying APFloat semantics.
+static FloatTypeInfo getFloatTypeInfo(Type type) {
+  auto floatType = cast<FloatType>(type);
+  const llvm::fltSemantics &sem = floatType.getFloatSemantics();
+  FloatTypeInfo info;
+
+  // Derive format parameters from APFloat semantics.
+  info.mantBits = llvm::APFloat::semanticsPrecision(sem) - 1;
+  unsigned totalBits = llvm::APFloat::semanticsSizeInBits(sem);
+  info.expBits = totalBits - 1 - info.mantBits;
+
+  // Compute masks.
+  info.signMask = 1u << (info.expBits + info.mantBits);
+  info.mantMask = (1u << info.mantBits) - 1;
+  info.expMask = ((1u << info.expBits) - 1) << info.mantBits;
+
+  // Bias from APFloat semantics.
+  int minExp = llvm::APFloat::semanticsMinExponent(sem);
+  info.bias = 1 - minExp;
+
+  info.hasInf = llvm::APFloat::semanticsHasInf(sem);
+  info.hasNan = llvm::APFloat::semanticsHasNaN(sem);
+
+  // Check for FNUZ types where NaN is encoded as negative zero (0x80).
+  // These types have no negative zero and no infinity.
+  llvm::APFloat negZero = llvm::APFloat::getZero(sem, /*Negative=*/true);
+  info.hasNegZero = negZero.isZero() && negZero.isNegative();
+  info.nanAsNegZero = !info.hasNegZero && !info.hasInf && info.hasNan;
+
+  // Compute NaN and Inf encodings.
+  unsigned maxExpCode = (1u << info.expBits) - 1;
+  if (info.nanAsNegZero) {
+    // FNUZ types: NaN = 0x80 (sign bit set, all else zero).
+    info.nanEncoding = info.signMask;
+  } else {
+    // IEEE and FN types: NaN = all exp bits + some mantissa bits.
+    info.nanEncoding = info.expMask | info.mantMask;
+  }
+
+  // Inf encoding: only meaningful for types with infinity.
+  info.infEncoding = info.hasInf ? info.expMask : 0;
+
+  // Max finite value: for types with infinity, it's exp=(max-1), mantissa=all
+  // 1s. For types without infinity, max exp is valid, so exp=max, mantissa=all
+  // 1s (except for FN types where max mantissa is NaN).
+  if (info.hasInf) {
+    info.maxFinite = ((maxExpCode - 1) << info.mantBits) | info.mantMask;
+  } else if (info.hasNan && !info.nanAsNegZero) {
+    // FN types: max exp is valid but max mantissa is NaN.
+    info.maxFinite = info.expMask | (info.mantMask - 1);
+  } else {
+    // FNUZ or no-NaN types: all bit patterns except NaN are valid.
+    info.maxFinite = info.expMask | info.mantMask;
+  }
+
+  return info;
+}
+
+//===----------------------------------------------------------------------===//
+// Helper for float emulation patterns
+//===----------------------------------------------------------------------===//
+
+/// Helper class for emulating float conversions using integer bit manipulation.
+/// Handles both scalar and vector types uniformly.
+class FloatEmulationHelper {
+public:
+  FloatEmulationHelper(RewriterBase &rewriter, Location loc, Type type)
+      : rewriter(rewriter), loc(loc), vecType(dyn_cast<VectorType>(type)) {
+    Type i32ScalarType = rewriter.getI32Type();
+    Type i8ScalarType = rewriter.getI8Type();
+    Type f32ScalarType = rewriter.getF32Type();
+    i32Type = vecType ? VectorType::get(vecType.getShape(), i32ScalarType)
+                      : i32ScalarType;
+    i8Type = vecType ? VectorType::get(vecType.getShape(), i8ScalarType)
+                     : i8ScalarType;
+    f32Type = vecType ? VectorType::get(vecType.getShape(), f32ScalarType)
+                      : f32ScalarType;
+  }
+
+  /// Creates an i32 constant, splatted if working with vectors.
+  Value createI32Const(int64_t value) {
+    auto attr = rewriter.getIntegerAttr(rewriter.getI32Type(), value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(i32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, attr);
+  }
+
+  /// Creates an f32 constant, splatted if working with vectors.
+  Value createF32Const(float value) {
+    auto attr = rewriter.getF32FloatAttr(value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(f32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, attr);
+  }
+
+  /// Extracts sign, exponent, and mantissa from an f32 value (as i32 bits).
+  /// F32 format: 1 sign bit, 8 exponent bits, 23 mantissa bits.
+  /// Returns {sign, biasedExp, mantissa}.
+  std::tuple<Value, Value, Value> extractF32Fields(Value i32Val) {
+    Value cMantBits = createI32Const(kF32MantBits);
+    Value cExpMask = createI32Const(kF32MaxExp);
+    Value cMantMask = createI32Const(kF32MantMask);
+    Value cSignShift = createI32Const(31);
+
+    Value sign = arith::ShRUIOp::create(rewriter, loc, i32Val, cSignShift);
+    Value biasedExp = arith::AndIOp::create(
+        rewriter, loc, arith::ShRUIOp::create(rewriter, loc, i32Val, cMantBits),
+        cExpMask);
+    Value mantissa = arith::AndIOp::create(rewriter, loc, i32Val, cMantMask);
+
+    return {sign, biasedExp, mantissa};
+  }
+
+  /// Adds a bias to input for round-to-nearest-even before right-shifting.
+  /// This matches runtime/src/iree/base/internal/math.h's bias_to_nearest_even:
+  ///   even_bit = 1 << shift_amount
+  ///   odd_bit = even_bit >> 1
+  ///   bias = (input & even_bit) ? odd_bit : (odd_bit - 1)
+  ///   return input + bias
+  ///
+  /// The caller should right-shift the result by shift_amount after this call.
+  Value biasForRoundToNearestEven(Value input, Value shiftAmount) {
+    Value c0 = createI32Const(0);
+    Value c1 = createI32Const(1);
+    Value evenBit = arith::ShLIOp::create(rewriter, loc, c1, shiftAmount);
+    Value oddBit = arith::ShRUIOp::create(rewriter, loc, evenBit, c1);
+    Value oddBitMinus1 = arith::SubIOp::create(rewriter, loc, oddBit, c1);
+    Value hasEvenBit = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::ne,
+        arith::AndIOp::create(rewriter, loc, input, evenBit), c0);
+    Value bias = arith::SelectOp::create(rewriter, loc, hasEvenBit, oddBit,
+                                         oddBitMinus1);
+    return arith::AddIOp::create(rewriter, loc, input, bias);
+  }
+
+  Type getI32Type() const { return i32Type; }
+  Type getI8Type() const { return i8Type; }
+  Type getF32Type() const { return f32Type; }
+
+private:
+  RewriterBase &rewriter;
+  Location loc;
+  VectorType vecType;
+  Type i32Type;
+  Type i8Type;
+  Type f32Type;
+};
+
+//===----------------------------------------------------------------------===//
+// TruncF to small float emulation pattern
+//===----------------------------------------------------------------------===//
+
+/// Emulates arith.truncf from f32 to fp8 using integer bit manipulation.
+/// This implementation follows IREE's runtime/src/iree/base/internal/math.h.
+///
+/// Features:
+///   - Round-to-nearest-even (IEEE 754 default rounding mode).
+///   - Proper denormal/subnormal generation for underflow cases.
+///   - Correct handling of all fp8 format variants (IEEE, FN, FNUZ).
+///
+/// The conversion handles three categories of fp8 formats:
+///
+/// 1. FNUZ types (f8E5M2FNUZ, f8E4M3FNUZ): No Inf, no negative zero.
+///    - NaN is encoded as 0x80 (sign=1, exp=0, mantissa=0).
+///    - Overflow produces NaN; zero is always positive.
+///
+/// 2. IEEE types (f8E5M2): Has Inf and negative zero.
+///    - Standard IEEE-like encoding with Inf at max exponent.
+///
+/// 3. FN types (f8E4M3FN): No Inf, but has negative zero.
+///    - Max exponent values are valid finite numbers (except NaN encoding).
+struct TruncFToFP8 final : public OpRewritePattern<arith::TruncFOp> {
```

**Comment:**
```suggestion
struct TruncFToFP8 final : OpRewritePattern<arith::TruncFOp> {
```
public is the default for structs

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:574`

```diff
@@ -30,6 +36,632 @@ namespace mlir::iree_compiler {
 
 namespace {
 
+/// SFINAE (i.e., substitution failure is not an error) helper to detect if
+/// T::get(MLIRContext*) is valid.
+template <typename T, typename = void>
+struct HasContextGet : std::false_type {};
+template <typename T>
+struct HasContextGet<
+    T, std::void_t<decltype(T::get(std::declval<MLIRContext *>()))>>
+    : std::true_type {};
+
+/// Helpers to append types to a vector if they are f8 types.
+template <typename T>
+void maybeAppendType(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  if constexpr (HasContextGet<T>::value) {
+    Type t = T::get(ctx);
+    if (isa<FloatType>(t) && t.getIntOrFloatBitWidth() == 8) {
+      types.push_back(t);
+    }
+  }
+}
+template <typename... Ts>
+void appendTypesIf(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  (maybeAppendType<Ts>(ctx, types), ...);
+}
+
+//===----------------------------------------------------------------------===//
+// Floating-point format constants and type info
+//===----------------------------------------------------------------------===//
+//
+// This follows the same approach as IREE's
+// runtime/src/iree/base/internal/math.h for floating-point conversions. The
+// compiler emits equivalent logic using MLIR arith ops instead of C control
+// flow.
+//
+// IEEE 754 floating-point format: [sign | exponent | mantissa]
+//   - Sign: 1 bit (0 = positive, 1 = negative)
+//   - Exponent: biased (stored = actual + bias)
+//   - Mantissa: fractional bits with implicit leading 1 for normal values
+//
+// Special values:
+//   - Zero: exp=0, mantissa=0 (signed zero if format supports it)
+//   - Denormal: exp=0, mantissa!=0, value = mantissa * 2^(1-bias-mantissa_bits)
+//   - Inf: exp=max, mantissa=0 (IEEE types only)
+//   - NaN: exp=max, mantissa!=0 (IEEE), or special encoding (FNUZ: 0x80)
+
+// F32 format constants (IEEE 754 binary32).
+constexpr int kF32MantBits = 23;
+constexpr int kF32Bias = 127;
+constexpr int kF32MantMask = (1 << kF32MantBits) - 1;
+constexpr int kF32MaxExp = 0xFF; // All exponent bits set (255).
+
+/// Parameters describing a floating-point format for conversion.
+/// Derived from APFloat semantics at runtime to support all MLIR float types.
+struct FloatTypeInfo {
+  unsigned expBits;  // Number of exponent bits.
+  unsigned mantBits; // Number of mantissa bits (excluding implicit leading 1).
+  int bias;          // Exponent bias: stored_exp = actual_exp + bias.
+  bool hasInf;       // Format supports infinity (exp=max, mantissa=0).
+  bool hasNan;       // Format supports NaN.
+  bool hasNegZero;   // Format supports negative zero.
+  bool nanAsNegZero; // NaN encoded as 0x80 (FNUZ types).
+  unsigned signMask; // Bit mask for sign bit.
+  unsigned expMask;  // Bit mask for exponent field.
+  unsigned mantMask; // Bit mask for mantissa field.
+  unsigned nanEncoding; // Bit pattern for canonical NaN.
+  unsigned infEncoding; // Bit pattern for +Inf (0 if no Inf).
+  unsigned maxFinite;   // Bit pattern for max finite value.
+};
+
+/// Returns format info for a float type by querying APFloat semantics.
+static FloatTypeInfo getFloatTypeInfo(Type type) {
+  auto floatType = cast<FloatType>(type);
+  const llvm::fltSemantics &sem = floatType.getFloatSemantics();
+  FloatTypeInfo info;
+
+  // Derive format parameters from APFloat semantics.
+  info.mantBits = llvm::APFloat::semanticsPrecision(sem) - 1;
+  unsigned totalBits = llvm::APFloat::semanticsSizeInBits(sem);
+  info.expBits = totalBits - 1 - info.mantBits;
+
+  // Compute masks.
+  info.signMask = 1u << (info.expBits + info.mantBits);
+  info.mantMask = (1u << info.mantBits) - 1;
+  info.expMask = ((1u << info.expBits) - 1) << info.mantBits;
+
+  // Bias from APFloat semantics.
+  int minExp = llvm::APFloat::semanticsMinExponent(sem);
+  info.bias = 1 - minExp;
+
+  info.hasInf = llvm::APFloat::semanticsHasInf(sem);
+  info.hasNan = llvm::APFloat::semanticsHasNaN(sem);
+
+  // Check for FNUZ types where NaN is encoded as negative zero (0x80).
+  // These types have no negative zero and no infinity.
+  llvm::APFloat negZero = llvm::APFloat::getZero(sem, /*Negative=*/true);
+  info.hasNegZero = negZero.isZero() && negZero.isNegative();
+  info.nanAsNegZero = !info.hasNegZero && !info.hasInf && info.hasNan;
+
+  // Compute NaN and Inf encodings.
+  unsigned maxExpCode = (1u << info.expBits) - 1;
+  if (info.nanAsNegZero) {
+    // FNUZ types: NaN = 0x80 (sign bit set, all else zero).
+    info.nanEncoding = info.signMask;
+  } else {
+    // IEEE and FN types: NaN = all exp bits + some mantissa bits.
+    info.nanEncoding = info.expMask | info.mantMask;
+  }
+
+  // Inf encoding: only meaningful for types with infinity.
+  info.infEncoding = info.hasInf ? info.expMask : 0;
+
+  // Max finite value: for types with infinity, it's exp=(max-1), mantissa=all
+  // 1s. For types without infinity, max exp is valid, so exp=max, mantissa=all
+  // 1s (except for FN types where max mantissa is NaN).
+  if (info.hasInf) {
+    info.maxFinite = ((maxExpCode - 1) << info.mantBits) | info.mantMask;
+  } else if (info.hasNan && !info.nanAsNegZero) {
+    // FN types: max exp is valid but max mantissa is NaN.
+    info.maxFinite = info.expMask | (info.mantMask - 1);
+  } else {
+    // FNUZ or no-NaN types: all bit patterns except NaN are valid.
+    info.maxFinite = info.expMask | info.mantMask;
+  }
+
+  return info;
+}
+
+//===----------------------------------------------------------------------===//
+// Helper for float emulation patterns
+//===----------------------------------------------------------------------===//
+
+/// Helper class for emulating float conversions using integer bit manipulation.
+/// Handles both scalar and vector types uniformly.
+class FloatEmulationHelper {
+public:
+  FloatEmulationHelper(RewriterBase &rewriter, Location loc, Type type)
+      : rewriter(rewriter), loc(loc), vecType(dyn_cast<VectorType>(type)) {
+    Type i32ScalarType = rewriter.getI32Type();
+    Type i8ScalarType = rewriter.getI8Type();
+    Type f32ScalarType = rewriter.getF32Type();
+    i32Type = vecType ? VectorType::get(vecType.getShape(), i32ScalarType)
+                      : i32ScalarType;
+    i8Type = vecType ? VectorType::get(vecType.getShape(), i8ScalarType)
+                     : i8ScalarType;
+    f32Type = vecType ? VectorType::get(vecType.getShape(), f32ScalarType)
+                      : f32ScalarType;
+  }
+
+  /// Creates an i32 constant, splatted if working with vectors.
+  Value createI32Const(int64_t value) {
+    auto attr = rewriter.getIntegerAttr(rewriter.getI32Type(), value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(i32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, attr);
+  }
+
+  /// Creates an f32 constant, splatted if working with vectors.
+  Value createF32Const(float value) {
+    auto attr = rewriter.getF32FloatAttr(value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(f32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, attr);
+  }
+
+  /// Extracts sign, exponent, and mantissa from an f32 value (as i32 bits).
+  /// F32 format: 1 sign bit, 8 exponent bits, 23 mantissa bits.
+  /// Returns {sign, biasedExp, mantissa}.
+  std::tuple<Value, Value, Value> extractF32Fields(Value i32Val) {
+    Value cMantBits = createI32Const(kF32MantBits);
+    Value cExpMask = createI32Const(kF32MaxExp);
+    Value cMantMask = createI32Const(kF32MantMask);
+    Value cSignShift = createI32Const(31);
+
+    Value sign = arith::ShRUIOp::create(rewriter, loc, i32Val, cSignShift);
+    Value biasedExp = arith::AndIOp::create(
+        rewriter, loc, arith::ShRUIOp::create(rewriter, loc, i32Val, cMantBits),
+        cExpMask);
+    Value mantissa = arith::AndIOp::create(rewriter, loc, i32Val, cMantMask);
+
+    return {sign, biasedExp, mantissa};
+  }
+
+  /// Adds a bias to input for round-to-nearest-even before right-shifting.
+  /// This matches runtime/src/iree/base/internal/math.h's bias_to_nearest_even:
+  ///   even_bit = 1 << shift_amount
+  ///   odd_bit = even_bit >> 1
+  ///   bias = (input & even_bit) ? odd_bit : (odd_bit - 1)
+  ///   return input + bias
+  ///
+  /// The caller should right-shift the result by shift_amount after this call.
+  Value biasForRoundToNearestEven(Value input, Value shiftAmount) {
+    Value c0 = createI32Const(0);
+    Value c1 = createI32Const(1);
+    Value evenBit = arith::ShLIOp::create(rewriter, loc, c1, shiftAmount);
+    Value oddBit = arith::ShRUIOp::create(rewriter, loc, evenBit, c1);
+    Value oddBitMinus1 = arith::SubIOp::create(rewriter, loc, oddBit, c1);
+    Value hasEvenBit = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::ne,
+        arith::AndIOp::create(rewriter, loc, input, evenBit), c0);
+    Value bias = arith::SelectOp::create(rewriter, loc, hasEvenBit, oddBit,
+                                         oddBitMinus1);
+    return arith::AddIOp::create(rewriter, loc, input, bias);
+  }
+
+  Type getI32Type() const { return i32Type; }
+  Type getI8Type() const { return i8Type; }
+  Type getF32Type() const { return f32Type; }
+
+private:
+  RewriterBase &rewriter;
+  Location loc;
+  VectorType vecType;
+  Type i32Type;
+  Type i8Type;
+  Type f32Type;
+};
+
+//===----------------------------------------------------------------------===//
+// TruncF to small float emulation pattern
+//===----------------------------------------------------------------------===//
+
+/// Emulates arith.truncf from f32 to fp8 using integer bit manipulation.
+/// This implementation follows IREE's runtime/src/iree/base/internal/math.h.
+///
+/// Features:
+///   - Round-to-nearest-even (IEEE 754 default rounding mode).
+///   - Proper denormal/subnormal generation for underflow cases.
+///   - Correct handling of all fp8 format variants (IEEE, FN, FNUZ).
+///
+/// The conversion handles three categories of fp8 formats:
+///
+/// 1. FNUZ types (f8E5M2FNUZ, f8E4M3FNUZ): No Inf, no negative zero.
+///    - NaN is encoded as 0x80 (sign=1, exp=0, mantissa=0).
+///    - Overflow produces NaN; zero is always positive.
+///
+/// 2. IEEE types (f8E5M2): Has Inf and negative zero.
+///    - Standard IEEE-like encoding with Inf at max exponent.
+///
+/// 3. FN types (f8E4M3FN): No Inf, but has negative zero.
+///    - Max exponent values are valid finite numbers (except NaN encoding).
+struct TruncFToFP8 final : public OpRewritePattern<arith::TruncFOp> {
+  using Base::Base;
+
+  LogicalResult matchAndRewrite(arith::TruncFOp op,
+                                PatternRewriter &rewriter) const override {
+    Type resultType = op.getResult().getType();
+    Type inputType = op.getIn().getType();
+    Type resultElemType = getElementTypeOrSelf(resultType);
+    Type inputElemType = getElementTypeOrSelf(inputType);
+
+    // TODO(#23105): handle other fp types, e.g., fp4.
+    if (!isa<Float32Type>(inputElemType) ||
+        resultElemType.getIntOrFloatBitWidth() != 8) {
+      return failure();
+    }
+
+    FloatTypeInfo dstInfo = getFloatTypeInfo(resultElemType);
+    Location loc = op.getLoc();
+    FloatEmulationHelper helper(rewriter, loc, resultType);
+
+    // Constants for destination format.
+    int dstMaxBiasedExp = (1 << dstInfo.expBits) - 1;
+    // Max exponent for normal values (excludes Inf/NaN encoding if applicable).
+    int dstMaxNormalBiasedExp =
+        dstInfo.hasInf ? dstMaxBiasedExp - 1 : dstMaxBiasedExp;
+    int mantShift = kF32MantBits - dstInfo.mantBits;
+
+    // Create constants.
+    Value c0 = helper.createI32Const(0);
+    Value c1 = helper.createI32Const(1);
+    Value cF32MantBits = helper.createI32Const(kF32MantBits);
+    Value cF32MantMask = helper.createI32Const(kF32MantMask);
+    Value cF32MaxExp = helper.createI32Const(kF32MaxExp);
+    Value cF32Bias = helper.createI32Const(kF32Bias);
+    Value cDstBias = helper.createI32Const(dstInfo.bias);
+    Value cDstMantBits = helper.createI32Const(dstInfo.mantBits);
+    Value cDstExpShift = cDstMantBits;
+    Value cDstSignShift =
+        helper.createI32Const(dstInfo.expBits + dstInfo.mantBits);
+    Value cDstMantMask = helper.createI32Const(dstInfo.mantMask);
+    Value cDstMaxNormalExp = helper.createI32Const(dstMaxNormalBiasedExp);
+    Value cMantShift = helper.createI32Const(mantShift);
+    Value cNaN = helper.createI32Const(dstInfo.nanEncoding);
+    Value cDstSignMask = helper.createI32Const(dstInfo.signMask);
+
+    // Bitcast f32 to i32 and extract fields.
+    Value i32Val = arith::BitcastOp::create(rewriter, loc, helper.getI32Type(),
+                                            op.getIn());
+    auto [f32Sign, f32Exp, f32Mant] = helper.extractF32Fields(i32Val);
+
+    // Compute destination sign.
+    Value dstSign =
+        arith::ShLIOp::create(rewriter, loc, f32Sign, cDstSignShift);
+
+    // Check for NaN/Inf in source.
+    Value srcIsNanOrInf = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::eq, f32Exp, cF32MaxExp);
+    Value srcIsNan = arith::AndIOp::create(
+        rewriter, loc, srcIsNanOrInf,
+        arith::CmpIOp::create(rewriter, loc, arith::CmpIPredicate::ne, f32Mant,
+                              c0));
+    Value srcIsInf = arith::AndIOp::create(
+        rewriter, loc, srcIsNanOrInf,
+        arith::CmpIOp::create(rewriter, loc, arith::CmpIPredicate::eq, f32Mant,
+                              c0));
+
+    // Check for zero or subnormal in source (exp == 0).
+    Value srcExpIsZero = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::eq, f32Exp, c0);
+
+    // Compute arithmetic exponent (unbiased).
+    Value arithmeticExp =
+        arith::SubIOp::create(rewriter, loc, f32Exp, cF32Bias);
+
+    // Check overflow: arithmetic_exp > max_normal_exp - dst_bias + dst_bias
+    // Simplified: biased_dst_exp > max_normal_exp
+    Value biasedDstExp =
+        arith::AddIOp::create(rewriter, loc, arithmeticExp, cDstBias);
+    Value isOverflow =
+        arith::CmpIOp::create(rewriter, loc, arith::CmpIPredicate::sgt,
+                              biasedDstExp, cDstMaxNormalExp);
+
+    // Check underflow: biased_dst_exp <= 0 means subnormal or zero.
+    Value isUnderflow = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::sle, biasedDstExp, c0);
+
+    // Check if rounding caused mantissa overflow (carry into exponent).
+    Value biasedMant = helper.biasForRoundToNearestEven(f32Mant, cMantShift);
+    Value mantOverflowed = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::ugt, biasedMant, cF32MantMask);
+    biasedDstExp = arith::SelectOp::create(
+        rewriter, loc, mantOverflowed,
+        arith::AddIOp::create(rewriter, loc, biasedDstExp, c1), biasedDstExp);
+    biasedMant =
+        arith::SelectOp::create(rewriter, loc, mantOverflowed, c0, biasedMant);
+
+    // Re-check overflow after rounding increment.
+    isOverflow = arith::OrIOp::create(
+        rewriter, loc, isOverflow,
+        arith::CmpIOp::create(rewriter, loc, arith::CmpIPredicate::sgt,
+                              biasedDstExp, cDstMaxNormalExp));
+
+    // Shift mantissa to destination width.
+    Value dstMant =
+        arith::ShRUIOp::create(rewriter, loc, biasedMant, cMantShift);
+    dstMant = arith::AndIOp::create(rewriter, loc, dstMant, cDstMantMask);
+
+    // Pack normal result.
+    Value dstExp =
+        arith::ShLIOp::create(rewriter, loc, biasedDstExp, cDstExpShift);
+    Value normalResult = arith::OrIOp::create(
+        rewriter, loc, arith::OrIOp::create(rewriter, loc, dstSign, dstExp),
+        dstMant);
+
+    // Underflow case: generate subnormal or zero.
+    // For subnormals, dst_exp = 0 and mantissa encodes the value.
+    // shift_amount = f32_mant_bits - dst_mant_bits - arithmetic_exp + (1 -
+    // dst_bias)
+    Value dstSubnormalExp = helper.createI32Const(1 - dstInfo.bias);
+    Value shiftAmount = arith::SubIOp::create(
+        rewriter, loc,
+        arith::SubIOp::create(rewriter, loc, cF32MantBits, cDstMantBits),
+        arith::SubIOp::create(rewriter, loc, arithmeticExp, dstSubnormalExp));
+
+    // Add implicit leading 1 to f32 mantissa for the shift.
+    Value cImplicitBit = helper.createI32Const(1 << kF32MantBits);
+    Value effectiveMant =
+        arith::OrIOp::create(rewriter, loc, f32Mant, cImplicitBit);
+
+    // Compute round-to-nearest-even for subnormal.
+    Value subnormalMantRounded =
+        helper.biasForRoundToNearestEven(effectiveMant, shiftAmount);
+
+    // Shift to get subnormal mantissa.
+    Value subnormalMant = arith::ShRUIOp::create(
+        rewriter, loc, subnormalMantRounded, shiftAmount);
+    subnormalMant =
+        arith::AndIOp::create(rewriter, loc, subnormalMant, cDstMantMask);
+
+    // Subnormal result has exp=0.
+    Value subnormalResult =
+        arith::OrIOp::create(rewriter, loc, dstSign, subnormalMant);
+
+    // Check if shift is too large (complete underflow to zero).
+    Value shiftTooLarge = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::sgt, shiftAmount,
+        helper.createI32Const(kF32MantBits + 1));
+    subnormalResult = arith::SelectOp::create(
+        rewriter, loc, shiftTooLarge,
+        dstInfo.nanAsNegZero ? c0 : dstSign, // Zero (signed if supported)
+        subnormalResult);
+
+    // Select cascade for final result.
+    //
+    // Unlike runtime code which uses early returns, SSA form requires computing
+    // all paths and selecting between them. The ORDER of selects matters:
+    // later selects override earlier ones. We order from lowest to highest
+    // priority so the final select (NaN) always wins.
+    //
+    // Priority (lowest to highest):
+    //   1. Normal/subnormal computation (base case)
+    //   2. Source zero/subnormal -> zero
+    //   3. Negative zero correction (FNUZ only, must be before NaN handling.)
+    //   4. Overflow -> Inf or NaN
+    //   5. Source Inf -> Inf or NaN
+    //   6. Source NaN -> NaN (highest priority, always wins)
+    Value result = arith::SelectOp::create(rewriter, loc, isUnderflow,
+                                           subnormalResult, normalResult);
+
+    // F32 subnormals (exp=0) become zero in fp8 (much smaller than fp8 min).
+    Value zeroResult = dstInfo.nanAsNegZero ? c0 : dstSign;
+    result = arith::SelectOp::create(rewriter, loc, srcExpIsZero, zeroResult,
+                                     result);
+
+    // FNUZ: Negative zero (0x80) must become positive zero (0x00).
+    // CRITICAL: This must happen BEFORE NaN/Inf/overflow handling.
+    // For FNUZ types, 0x80 is the NaN encoding, not negative zero.
+    // If we did this after, we would incorrectly convert NaN to zero.
+    if (dstInfo.nanAsNegZero) {
+      Value resultIsNegZero = arith::CmpIOp::create(
+          rewriter, loc, arith::CmpIPredicate::eq, result, cDstSignMask);
+      result =
+          arith::SelectOp::create(rewriter, loc, resultIsNegZero, c0, result);
+    }
+
+    // Overflow and source Inf both map to the same result:
+    // Inf (IEEE) or NaN (FN/FNUZ) or saturate to max finite (no Inf/NaN).
+    Value infOrOverflowResult;
+    if (dstInfo.hasInf) {
+      infOrOverflowResult = arith::OrIOp::create(
+          rewriter, loc, dstSign, helper.createI32Const(dstInfo.infEncoding));
+    } else if (dstInfo.hasNan) {
+      infOrOverflowResult = cNaN;
+    } else {
+      // No Inf, no NaN: saturate to max finite.
+      infOrOverflowResult = arith::OrIOp::create(
+          rewriter, loc, dstSign, helper.createI32Const(dstInfo.maxFinite));
+    }
+    result = arith::SelectOp::create(rewriter, loc, isOverflow,
+                                     infOrOverflowResult, result);
+    result = arith::SelectOp::create(rewriter, loc, srcIsInf,
+                                     infOrOverflowResult, result);
+
+    // Handle source NaN last so it takes precedence.
+    Value nanResult;
+    if (dstInfo.hasNan) {
+      nanResult = cNaN;
+    } else {
+      nanResult = c0; // No NaN encoding: convert to +0.
+    }
+    result =
+        arith::SelectOp::create(rewriter, loc, srcIsNan, nanResult, result);
+
+    // Truncate to i8 and bitcast to fp8.
+    result = arith::TruncIOp::create(rewriter, loc, helper.getI8Type(), result);
+    result = arith::BitcastOp::create(rewriter, loc, resultType, result);
+    rewriter.replaceOp(op, result);
+    return success();
+  }
+};
+
+//===----------------------------------------------------------------------===//
+// ExtF from small float emulation pattern
+//===----------------------------------------------------------------------===//
+
+/// Emulates arith.extf from fp8 to f32 using integer bit manipulation.
+/// This implementation follows IREE's runtime/src/iree/base/internal/math.h.
+///
+/// For normal values: adjust exponent bias and shift mantissa.
+///
+/// For denormals (exp=0, mantissa!=0):
+///   value = mantissa * 2^(1 - bias - mantissa_bits)
+/// We implement this using uitofp + mulf with a precomputed scale factor,
+/// which is simpler and more general than enumerating all possible values.
+struct ExtFFromFP8 final : public OpRewritePattern<arith::ExtFOp> {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:354`

```diff
@@ -30,6 +36,632 @@ namespace mlir::iree_compiler {
 
 namespace {
 
+/// SFINAE (i.e., substitution failure is not an error) helper to detect if
+/// T::get(MLIRContext*) is valid.
+template <typename T, typename = void>
+struct HasContextGet : std::false_type {};
+template <typename T>
+struct HasContextGet<
+    T, std::void_t<decltype(T::get(std::declval<MLIRContext *>()))>>
+    : std::true_type {};
+
+/// Helpers to append types to a vector if they are f8 types.
+template <typename T>
+void maybeAppendType(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  if constexpr (HasContextGet<T>::value) {
+    Type t = T::get(ctx);
+    if (isa<FloatType>(t) && t.getIntOrFloatBitWidth() == 8) {
+      types.push_back(t);
+    }
+  }
+}
+template <typename... Ts>
+void appendTypesIf(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  (maybeAppendType<Ts>(ctx, types), ...);
+}
+
+//===----------------------------------------------------------------------===//
+// Floating-point format constants and type info
+//===----------------------------------------------------------------------===//
+//
+// This follows the same approach as IREE's
+// runtime/src/iree/base/internal/math.h for floating-point conversions. The
+// compiler emits equivalent logic using MLIR arith ops instead of C control
+// flow.
+//
+// IEEE 754 floating-point format: [sign | exponent | mantissa]
+//   - Sign: 1 bit (0 = positive, 1 = negative)
+//   - Exponent: biased (stored = actual + bias)
+//   - Mantissa: fractional bits with implicit leading 1 for normal values
+//
+// Special values:
+//   - Zero: exp=0, mantissa=0 (signed zero if format supports it)
+//   - Denormal: exp=0, mantissa!=0, value = mantissa * 2^(1-bias-mantissa_bits)
+//   - Inf: exp=max, mantissa=0 (IEEE types only)
+//   - NaN: exp=max, mantissa!=0 (IEEE), or special encoding (FNUZ: 0x80)
+
+// F32 format constants (IEEE 754 binary32).
+constexpr int kF32MantBits = 23;
+constexpr int kF32Bias = 127;
+constexpr int kF32MantMask = (1 << kF32MantBits) - 1;
+constexpr int kF32MaxExp = 0xFF; // All exponent bits set (255).
+
+/// Parameters describing a floating-point format for conversion.
+/// Derived from APFloat semantics at runtime to support all MLIR float types.
+struct FloatTypeInfo {
+  unsigned expBits;  // Number of exponent bits.
+  unsigned mantBits; // Number of mantissa bits (excluding implicit leading 1).
+  int bias;          // Exponent bias: stored_exp = actual_exp + bias.
+  bool hasInf;       // Format supports infinity (exp=max, mantissa=0).
+  bool hasNan;       // Format supports NaN.
+  bool hasNegZero;   // Format supports negative zero.
+  bool nanAsNegZero; // NaN encoded as 0x80 (FNUZ types).
+  unsigned signMask; // Bit mask for sign bit.
+  unsigned expMask;  // Bit mask for exponent field.
+  unsigned mantMask; // Bit mask for mantissa field.
+  unsigned nanEncoding; // Bit pattern for canonical NaN.
+  unsigned infEncoding; // Bit pattern for +Inf (0 if no Inf).
+  unsigned maxFinite;   // Bit pattern for max finite value.
+};
+
+/// Returns format info for a float type by querying APFloat semantics.
+static FloatTypeInfo getFloatTypeInfo(Type type) {
+  auto floatType = cast<FloatType>(type);
+  const llvm::fltSemantics &sem = floatType.getFloatSemantics();
+  FloatTypeInfo info;
+
+  // Derive format parameters from APFloat semantics.
+  info.mantBits = llvm::APFloat::semanticsPrecision(sem) - 1;
+  unsigned totalBits = llvm::APFloat::semanticsSizeInBits(sem);
+  info.expBits = totalBits - 1 - info.mantBits;
+
+  // Compute masks.
+  info.signMask = 1u << (info.expBits + info.mantBits);
+  info.mantMask = (1u << info.mantBits) - 1;
+  info.expMask = ((1u << info.expBits) - 1) << info.mantBits;
+
+  // Bias from APFloat semantics.
+  int minExp = llvm::APFloat::semanticsMinExponent(sem);
+  info.bias = 1 - minExp;
+
+  info.hasInf = llvm::APFloat::semanticsHasInf(sem);
+  info.hasNan = llvm::APFloat::semanticsHasNaN(sem);
+
+  // Check for FNUZ types where NaN is encoded as negative zero (0x80).
+  // These types have no negative zero and no infinity.
+  llvm::APFloat negZero = llvm::APFloat::getZero(sem, /*Negative=*/true);
+  info.hasNegZero = negZero.isZero() && negZero.isNegative();
+  info.nanAsNegZero = !info.hasNegZero && !info.hasInf && info.hasNan;
+
+  // Compute NaN and Inf encodings.
+  unsigned maxExpCode = (1u << info.expBits) - 1;
+  if (info.nanAsNegZero) {
+    // FNUZ types: NaN = 0x80 (sign bit set, all else zero).
+    info.nanEncoding = info.signMask;
+  } else {
+    // IEEE and FN types: NaN = all exp bits + some mantissa bits.
+    info.nanEncoding = info.expMask | info.mantMask;
+  }
+
+  // Inf encoding: only meaningful for types with infinity.
+  info.infEncoding = info.hasInf ? info.expMask : 0;
+
+  // Max finite value: for types with infinity, it's exp=(max-1), mantissa=all
+  // 1s. For types without infinity, max exp is valid, so exp=max, mantissa=all
+  // 1s (except for FN types where max mantissa is NaN).
+  if (info.hasInf) {
+    info.maxFinite = ((maxExpCode - 1) << info.mantBits) | info.mantMask;
+  } else if (info.hasNan && !info.nanAsNegZero) {
+    // FN types: max exp is valid but max mantissa is NaN.
+    info.maxFinite = info.expMask | (info.mantMask - 1);
+  } else {
+    // FNUZ or no-NaN types: all bit patterns except NaN are valid.
+    info.maxFinite = info.expMask | info.mantMask;
+  }
+
+  return info;
+}
+
+//===----------------------------------------------------------------------===//
+// Helper for float emulation patterns
+//===----------------------------------------------------------------------===//
+
+/// Helper class for emulating float conversions using integer bit manipulation.
+/// Handles both scalar and vector types uniformly.
+class FloatEmulationHelper {
+public:
+  FloatEmulationHelper(RewriterBase &rewriter, Location loc, Type type)
+      : rewriter(rewriter), loc(loc), vecType(dyn_cast<VectorType>(type)) {
+    Type i32ScalarType = rewriter.getI32Type();
+    Type i8ScalarType = rewriter.getI8Type();
+    Type f32ScalarType = rewriter.getF32Type();
+    i32Type = vecType ? VectorType::get(vecType.getShape(), i32ScalarType)
+                      : i32ScalarType;
+    i8Type = vecType ? VectorType::get(vecType.getShape(), i8ScalarType)
+                     : i8ScalarType;
+    f32Type = vecType ? VectorType::get(vecType.getShape(), f32ScalarType)
+                      : f32ScalarType;
+  }
+
+  /// Creates an i32 constant, splatted if working with vectors.
+  Value createI32Const(int64_t value) {
+    auto attr = rewriter.getIntegerAttr(rewriter.getI32Type(), value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(i32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, attr);
+  }
+
+  /// Creates an f32 constant, splatted if working with vectors.
+  Value createF32Const(float value) {
+    auto attr = rewriter.getF32FloatAttr(value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(f32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, attr);
+  }
+
+  /// Extracts sign, exponent, and mantissa from an f32 value (as i32 bits).
+  /// F32 format: 1 sign bit, 8 exponent bits, 23 mantissa bits.
+  /// Returns {sign, biasedExp, mantissa}.
+  std::tuple<Value, Value, Value> extractF32Fields(Value i32Val) {
+    Value cMantBits = createI32Const(kF32MantBits);
+    Value cExpMask = createI32Const(kF32MaxExp);
+    Value cMantMask = createI32Const(kF32MantMask);
+    Value cSignShift = createI32Const(31);
+
+    Value sign = arith::ShRUIOp::create(rewriter, loc, i32Val, cSignShift);
+    Value biasedExp = arith::AndIOp::create(
+        rewriter, loc, arith::ShRUIOp::create(rewriter, loc, i32Val, cMantBits),
+        cExpMask);
+    Value mantissa = arith::AndIOp::create(rewriter, loc, i32Val, cMantMask);
+
+    return {sign, biasedExp, mantissa};
+  }
+
+  /// Adds a bias to input for round-to-nearest-even before right-shifting.
+  /// This matches runtime/src/iree/base/internal/math.h's bias_to_nearest_even:
+  ///   even_bit = 1 << shift_amount
+  ///   odd_bit = even_bit >> 1
+  ///   bias = (input & even_bit) ? odd_bit : (odd_bit - 1)
+  ///   return input + bias
+  ///
+  /// The caller should right-shift the result by shift_amount after this call.
+  Value biasForRoundToNearestEven(Value input, Value shiftAmount) {
+    Value c0 = createI32Const(0);
+    Value c1 = createI32Const(1);
+    Value evenBit = arith::ShLIOp::create(rewriter, loc, c1, shiftAmount);
+    Value oddBit = arith::ShRUIOp::create(rewriter, loc, evenBit, c1);
+    Value oddBitMinus1 = arith::SubIOp::create(rewriter, loc, oddBit, c1);
+    Value hasEvenBit = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::ne,
+        arith::AndIOp::create(rewriter, loc, input, evenBit), c0);
+    Value bias = arith::SelectOp::create(rewriter, loc, hasEvenBit, oddBit,
+                                         oddBitMinus1);
+    return arith::AddIOp::create(rewriter, loc, input, bias);
+  }
+
+  Type getI32Type() const { return i32Type; }
+  Type getI8Type() const { return i8Type; }
+  Type getF32Type() const { return f32Type; }
+
+private:
+  RewriterBase &rewriter;
+  Location loc;
+  VectorType vecType;
+  Type i32Type;
+  Type i8Type;
+  Type f32Type;
+};
+
+//===----------------------------------------------------------------------===//
+// TruncF to small float emulation pattern
+//===----------------------------------------------------------------------===//
+
+/// Emulates arith.truncf from f32 to fp8 using integer bit manipulation.
+/// This implementation follows IREE's runtime/src/iree/base/internal/math.h.
+///
+/// Features:
+///   - Round-to-nearest-even (IEEE 754 default rounding mode).
+///   - Proper denormal/subnormal generation for underflow cases.
+///   - Correct handling of all fp8 format variants (IEEE, FN, FNUZ).
+///
+/// The conversion handles three categories of fp8 formats:
+///
+/// 1. FNUZ types (f8E5M2FNUZ, f8E4M3FNUZ): No Inf, no negative zero.
+///    - NaN is encoded as 0x80 (sign=1, exp=0, mantissa=0).
+///    - Overflow produces NaN; zero is always positive.
+///
+/// 2. IEEE types (f8E5M2): Has Inf and negative zero.
+///    - Standard IEEE-like encoding with Inf at max exponent.
+///
+/// 3. FN types (f8E4M3FN): No Inf, but has negative zero.
+///    - Max exponent values are valid finite numbers (except NaN encoding).
+struct TruncFToFP8 final : public OpRewritePattern<arith::TruncFOp> {
```

**Comment:**
not done

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:574`

```diff
@@ -30,6 +36,632 @@ namespace mlir::iree_compiler {
 
 namespace {
 
+/// SFINAE (i.e., substitution failure is not an error) helper to detect if
+/// T::get(MLIRContext*) is valid.
+template <typename T, typename = void>
+struct HasContextGet : std::false_type {};
+template <typename T>
+struct HasContextGet<
+    T, std::void_t<decltype(T::get(std::declval<MLIRContext *>()))>>
+    : std::true_type {};
+
+/// Helpers to append types to a vector if they are f8 types.
+template <typename T>
+void maybeAppendType(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  if constexpr (HasContextGet<T>::value) {
+    Type t = T::get(ctx);
+    if (isa<FloatType>(t) && t.getIntOrFloatBitWidth() == 8) {
+      types.push_back(t);
+    }
+  }
+}
+template <typename... Ts>
+void appendTypesIf(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  (maybeAppendType<Ts>(ctx, types), ...);
+}
+
+//===----------------------------------------------------------------------===//
+// Floating-point format constants and type info
+//===----------------------------------------------------------------------===//
+//
+// This follows the same approach as IREE's
+// runtime/src/iree/base/internal/math.h for floating-point conversions. The
+// compiler emits equivalent logic using MLIR arith ops instead of C control
+// flow.
+//
+// IEEE 754 floating-point format: [sign | exponent | mantissa]
+//   - Sign: 1 bit (0 = positive, 1 = negative)
+//   - Exponent: biased (stored = actual + bias)
+//   - Mantissa: fractional bits with implicit leading 1 for normal values
+//
+// Special values:
+//   - Zero: exp=0, mantissa=0 (signed zero if format supports it)
+//   - Denormal: exp=0, mantissa!=0, value = mantissa * 2^(1-bias-mantissa_bits)
+//   - Inf: exp=max, mantissa=0 (IEEE types only)
+//   - NaN: exp=max, mantissa!=0 (IEEE), or special encoding (FNUZ: 0x80)
+
+// F32 format constants (IEEE 754 binary32).
+constexpr int kF32MantBits = 23;
+constexpr int kF32Bias = 127;
+constexpr int kF32MantMask = (1 << kF32MantBits) - 1;
+constexpr int kF32MaxExp = 0xFF; // All exponent bits set (255).
+
+/// Parameters describing a floating-point format for conversion.
+/// Derived from APFloat semantics at runtime to support all MLIR float types.
+struct FloatTypeInfo {
+  unsigned expBits;  // Number of exponent bits.
+  unsigned mantBits; // Number of mantissa bits (excluding implicit leading 1).
+  int bias;          // Exponent bias: stored_exp = actual_exp + bias.
+  bool hasInf;       // Format supports infinity (exp=max, mantissa=0).
+  bool hasNan;       // Format supports NaN.
+  bool hasNegZero;   // Format supports negative zero.
+  bool nanAsNegZero; // NaN encoded as 0x80 (FNUZ types).
+  unsigned signMask; // Bit mask for sign bit.
+  unsigned expMask;  // Bit mask for exponent field.
+  unsigned mantMask; // Bit mask for mantissa field.
+  unsigned nanEncoding; // Bit pattern for canonical NaN.
+  unsigned infEncoding; // Bit pattern for +Inf (0 if no Inf).
+  unsigned maxFinite;   // Bit pattern for max finite value.
+};
+
+/// Returns format info for a float type by querying APFloat semantics.
+static FloatTypeInfo getFloatTypeInfo(Type type) {
+  auto floatType = cast<FloatType>(type);
+  const llvm::fltSemantics &sem = floatType.getFloatSemantics();
+  FloatTypeInfo info;
+
+  // Derive format parameters from APFloat semantics.
+  info.mantBits = llvm::APFloat::semanticsPrecision(sem) - 1;
+  unsigned totalBits = llvm::APFloat::semanticsSizeInBits(sem);
+  info.expBits = totalBits - 1 - info.mantBits;
+
+  // Compute masks.
+  info.signMask = 1u << (info.expBits + info.mantBits);
+  info.mantMask = (1u << info.mantBits) - 1;
+  info.expMask = ((1u << info.expBits) - 1) << info.mantBits;
+
+  // Bias from APFloat semantics.
+  int minExp = llvm::APFloat::semanticsMinExponent(sem);
+  info.bias = 1 - minExp;
+
+  info.hasInf = llvm::APFloat::semanticsHasInf(sem);
+  info.hasNan = llvm::APFloat::semanticsHasNaN(sem);
+
+  // Check for FNUZ types where NaN is encoded as negative zero (0x80).
+  // These types have no negative zero and no infinity.
+  llvm::APFloat negZero = llvm::APFloat::getZero(sem, /*Negative=*/true);
+  info.hasNegZero = negZero.isZero() && negZero.isNegative();
+  info.nanAsNegZero = !info.hasNegZero && !info.hasInf && info.hasNan;
+
+  // Compute NaN and Inf encodings.
+  unsigned maxExpCode = (1u << info.expBits) - 1;
+  if (info.nanAsNegZero) {
+    // FNUZ types: NaN = 0x80 (sign bit set, all else zero).
+    info.nanEncoding = info.signMask;
+  } else {
+    // IEEE and FN types: NaN = all exp bits + some mantissa bits.
+    info.nanEncoding = info.expMask | info.mantMask;
+  }
+
+  // Inf encoding: only meaningful for types with infinity.
+  info.infEncoding = info.hasInf ? info.expMask : 0;
+
+  // Max finite value: for types with infinity, it's exp=(max-1), mantissa=all
+  // 1s. For types without infinity, max exp is valid, so exp=max, mantissa=all
+  // 1s (except for FN types where max mantissa is NaN).
+  if (info.hasInf) {
+    info.maxFinite = ((maxExpCode - 1) << info.mantBits) | info.mantMask;
+  } else if (info.hasNan && !info.nanAsNegZero) {
+    // FN types: max exp is valid but max mantissa is NaN.
+    info.maxFinite = info.expMask | (info.mantMask - 1);
+  } else {
+    // FNUZ or no-NaN types: all bit patterns except NaN are valid.
+    info.maxFinite = info.expMask | info.mantMask;
+  }
+
+  return info;
+}
+
+//===----------------------------------------------------------------------===//
+// Helper for float emulation patterns
+//===----------------------------------------------------------------------===//
+
+/// Helper class for emulating float conversions using integer bit manipulation.
+/// Handles both scalar and vector types uniformly.
+class FloatEmulationHelper {
+public:
+  FloatEmulationHelper(RewriterBase &rewriter, Location loc, Type type)
+      : rewriter(rewriter), loc(loc), vecType(dyn_cast<VectorType>(type)) {
+    Type i32ScalarType = rewriter.getI32Type();
+    Type i8ScalarType = rewriter.getI8Type();
+    Type f32ScalarType = rewriter.getF32Type();
+    i32Type = vecType ? VectorType::get(vecType.getShape(), i32ScalarType)
+                      : i32ScalarType;
+    i8Type = vecType ? VectorType::get(vecType.getShape(), i8ScalarType)
+                     : i8ScalarType;
+    f32Type = vecType ? VectorType::get(vecType.getShape(), f32ScalarType)
+                      : f32ScalarType;
+  }
+
+  /// Creates an i32 constant, splatted if working with vectors.
+  Value createI32Const(int64_t value) {
+    auto attr = rewriter.getIntegerAttr(rewriter.getI32Type(), value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(i32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, attr);
+  }
+
+  /// Creates an f32 constant, splatted if working with vectors.
+  Value createF32Const(float value) {
+    auto attr = rewriter.getF32FloatAttr(value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(f32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, attr);
+  }
+
+  /// Extracts sign, exponent, and mantissa from an f32 value (as i32 bits).
+  /// F32 format: 1 sign bit, 8 exponent bits, 23 mantissa bits.
+  /// Returns {sign, biasedExp, mantissa}.
+  std::tuple<Value, Value, Value> extractF32Fields(Value i32Val) {
+    Value cMantBits = createI32Const(kF32MantBits);
+    Value cExpMask = createI32Const(kF32MaxExp);
+    Value cMantMask = createI32Const(kF32MantMask);
+    Value cSignShift = createI32Const(31);
+
+    Value sign = arith::ShRUIOp::create(rewriter, loc, i32Val, cSignShift);
+    Value biasedExp = arith::AndIOp::create(
+        rewriter, loc, arith::ShRUIOp::create(rewriter, loc, i32Val, cMantBits),
+        cExpMask);
+    Value mantissa = arith::AndIOp::create(rewriter, loc, i32Val, cMantMask);
+
+    return {sign, biasedExp, mantissa};
+  }
+
+  /// Adds a bias to input for round-to-nearest-even before right-shifting.
+  /// This matches runtime/src/iree/base/internal/math.h's bias_to_nearest_even:
+  ///   even_bit = 1 << shift_amount
+  ///   odd_bit = even_bit >> 1
+  ///   bias = (input & even_bit) ? odd_bit : (odd_bit - 1)
+  ///   return input + bias
+  ///
+  /// The caller should right-shift the result by shift_amount after this call.
+  Value biasForRoundToNearestEven(Value input, Value shiftAmount) {
+    Value c0 = createI32Const(0);
+    Value c1 = createI32Const(1);
+    Value evenBit = arith::ShLIOp::create(rewriter, loc, c1, shiftAmount);
+    Value oddBit = arith::ShRUIOp::create(rewriter, loc, evenBit, c1);
+    Value oddBitMinus1 = arith::SubIOp::create(rewriter, loc, oddBit, c1);
+    Value hasEvenBit = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::ne,
+        arith::AndIOp::create(rewriter, loc, input, evenBit), c0);
+    Value bias = arith::SelectOp::create(rewriter, loc, hasEvenBit, oddBit,
+                                         oddBitMinus1);
+    return arith::AddIOp::create(rewriter, loc, input, bias);
+  }
+
+  Type getI32Type() const { return i32Type; }
+  Type getI8Type() const { return i8Type; }
+  Type getF32Type() const { return f32Type; }
+
+private:
+  RewriterBase &rewriter;
+  Location loc;
+  VectorType vecType;
+  Type i32Type;
+  Type i8Type;
+  Type f32Type;
+};
+
+//===----------------------------------------------------------------------===//
+// TruncF to small float emulation pattern
+//===----------------------------------------------------------------------===//
+
+/// Emulates arith.truncf from f32 to fp8 using integer bit manipulation.
+/// This implementation follows IREE's runtime/src/iree/base/internal/math.h.
+///
+/// Features:
+///   - Round-to-nearest-even (IEEE 754 default rounding mode).
+///   - Proper denormal/subnormal generation for underflow cases.
+///   - Correct handling of all fp8 format variants (IEEE, FN, FNUZ).
+///
+/// The conversion handles three categories of fp8 formats:
+///
+/// 1. FNUZ types (f8E5M2FNUZ, f8E4M3FNUZ): No Inf, no negative zero.
+///    - NaN is encoded as 0x80 (sign=1, exp=0, mantissa=0).
+///    - Overflow produces NaN; zero is always positive.
+///
+/// 2. IEEE types (f8E5M2): Has Inf and negative zero.
+///    - Standard IEEE-like encoding with Inf at max exponent.
+///
+/// 3. FN types (f8E4M3FN): No Inf, but has negative zero.
+///    - Max exponent values are valid finite numbers (except NaN encoding).
+struct TruncFToFP8 final : public OpRewritePattern<arith::TruncFOp> {
+  using Base::Base;
+
+  LogicalResult matchAndRewrite(arith::TruncFOp op,
+                                PatternRewriter &rewriter) const override {
+    Type resultType = op.getResult().getType();
+    Type inputType = op.getIn().getType();
+    Type resultElemType = getElementTypeOrSelf(resultType);
+    Type inputElemType = getElementTypeOrSelf(inputType);
+
+    // TODO(#23105): handle other fp types, e.g., fp4.
+    if (!isa<Float32Type>(inputElemType) ||
+        resultElemType.getIntOrFloatBitWidth() != 8) {
+      return failure();
+    }
+
+    FloatTypeInfo dstInfo = getFloatTypeInfo(resultElemType);
+    Location loc = op.getLoc();
+    FloatEmulationHelper helper(rewriter, loc, resultType);
+
+    // Constants for destination format.
+    int dstMaxBiasedExp = (1 << dstInfo.expBits) - 1;
+    // Max exponent for normal values (excludes Inf/NaN encoding if applicable).
+    int dstMaxNormalBiasedExp =
+        dstInfo.hasInf ? dstMaxBiasedExp - 1 : dstMaxBiasedExp;
+    int mantShift = kF32MantBits - dstInfo.mantBits;
+
+    // Create constants.
+    Value c0 = helper.createI32Const(0);
+    Value c1 = helper.createI32Const(1);
+    Value cF32MantBits = helper.createI32Const(kF32MantBits);
+    Value cF32MantMask = helper.createI32Const(kF32MantMask);
+    Value cF32MaxExp = helper.createI32Const(kF32MaxExp);
+    Value cF32Bias = helper.createI32Const(kF32Bias);
+    Value cDstBias = helper.createI32Const(dstInfo.bias);
+    Value cDstMantBits = helper.createI32Const(dstInfo.mantBits);
+    Value cDstExpShift = cDstMantBits;
+    Value cDstSignShift =
+        helper.createI32Const(dstInfo.expBits + dstInfo.mantBits);
+    Value cDstMantMask = helper.createI32Const(dstInfo.mantMask);
+    Value cDstMaxNormalExp = helper.createI32Const(dstMaxNormalBiasedExp);
+    Value cMantShift = helper.createI32Const(mantShift);
+    Value cNaN = helper.createI32Const(dstInfo.nanEncoding);
+    Value cDstSignMask = helper.createI32Const(dstInfo.signMask);
+
+    // Bitcast f32 to i32 and extract fields.
+    Value i32Val = arith::BitcastOp::create(rewriter, loc, helper.getI32Type(),
+                                            op.getIn());
+    auto [f32Sign, f32Exp, f32Mant] = helper.extractF32Fields(i32Val);
+
+    // Compute destination sign.
+    Value dstSign =
+        arith::ShLIOp::create(rewriter, loc, f32Sign, cDstSignShift);
+
+    // Check for NaN/Inf in source.
+    Value srcIsNanOrInf = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::eq, f32Exp, cF32MaxExp);
+    Value srcIsNan = arith::AndIOp::create(
+        rewriter, loc, srcIsNanOrInf,
+        arith::CmpIOp::create(rewriter, loc, arith::CmpIPredicate::ne, f32Mant,
+                              c0));
+    Value srcIsInf = arith::AndIOp::create(
+        rewriter, loc, srcIsNanOrInf,
+        arith::CmpIOp::create(rewriter, loc, arith::CmpIPredicate::eq, f32Mant,
+                              c0));
+
+    // Check for zero or subnormal in source (exp == 0).
+    Value srcExpIsZero = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::eq, f32Exp, c0);
+
+    // Compute arithmetic exponent (unbiased).
+    Value arithmeticExp =
+        arith::SubIOp::create(rewriter, loc, f32Exp, cF32Bias);
+
+    // Check overflow: arithmetic_exp > max_normal_exp - dst_bias + dst_bias
+    // Simplified: biased_dst_exp > max_normal_exp
+    Value biasedDstExp =
+        arith::AddIOp::create(rewriter, loc, arithmeticExp, cDstBias);
+    Value isOverflow =
+        arith::CmpIOp::create(rewriter, loc, arith::CmpIPredicate::sgt,
+                              biasedDstExp, cDstMaxNormalExp);
+
+    // Check underflow: biased_dst_exp <= 0 means subnormal or zero.
+    Value isUnderflow = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::sle, biasedDstExp, c0);
+
+    // Check if rounding caused mantissa overflow (carry into exponent).
+    Value biasedMant = helper.biasForRoundToNearestEven(f32Mant, cMantShift);
+    Value mantOverflowed = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::ugt, biasedMant, cF32MantMask);
+    biasedDstExp = arith::SelectOp::create(
+        rewriter, loc, mantOverflowed,
+        arith::AddIOp::create(rewriter, loc, biasedDstExp, c1), biasedDstExp);
+    biasedMant =
+        arith::SelectOp::create(rewriter, loc, mantOverflowed, c0, biasedMant);
+
+    // Re-check overflow after rounding increment.
+    isOverflow = arith::OrIOp::create(
+        rewriter, loc, isOverflow,
+        arith::CmpIOp::create(rewriter, loc, arith::CmpIPredicate::sgt,
+                              biasedDstExp, cDstMaxNormalExp));
+
+    // Shift mantissa to destination width.
+    Value dstMant =
+        arith::ShRUIOp::create(rewriter, loc, biasedMant, cMantShift);
+    dstMant = arith::AndIOp::create(rewriter, loc, dstMant, cDstMantMask);
+
+    // Pack normal result.
+    Value dstExp =
+        arith::ShLIOp::create(rewriter, loc, biasedDstExp, cDstExpShift);
+    Value normalResult = arith::OrIOp::create(
+        rewriter, loc, arith::OrIOp::create(rewriter, loc, dstSign, dstExp),
+        dstMant);
+
+    // Underflow case: generate subnormal or zero.
+    // For subnormals, dst_exp = 0 and mantissa encodes the value.
+    // shift_amount = f32_mant_bits - dst_mant_bits - arithmetic_exp + (1 -
+    // dst_bias)
+    Value dstSubnormalExp = helper.createI32Const(1 - dstInfo.bias);
+    Value shiftAmount = arith::SubIOp::create(
+        rewriter, loc,
+        arith::SubIOp::create(rewriter, loc, cF32MantBits, cDstMantBits),
+        arith::SubIOp::create(rewriter, loc, arithmeticExp, dstSubnormalExp));
+
+    // Add implicit leading 1 to f32 mantissa for the shift.
+    Value cImplicitBit = helper.createI32Const(1 << kF32MantBits);
+    Value effectiveMant =
+        arith::OrIOp::create(rewriter, loc, f32Mant, cImplicitBit);
+
+    // Compute round-to-nearest-even for subnormal.
+    Value subnormalMantRounded =
+        helper.biasForRoundToNearestEven(effectiveMant, shiftAmount);
+
+    // Shift to get subnormal mantissa.
+    Value subnormalMant = arith::ShRUIOp::create(
+        rewriter, loc, subnormalMantRounded, shiftAmount);
+    subnormalMant =
+        arith::AndIOp::create(rewriter, loc, subnormalMant, cDstMantMask);
+
+    // Subnormal result has exp=0.
+    Value subnormalResult =
+        arith::OrIOp::create(rewriter, loc, dstSign, subnormalMant);
+
+    // Check if shift is too large (complete underflow to zero).
+    Value shiftTooLarge = arith::CmpIOp::create(
+        rewriter, loc, arith::CmpIPredicate::sgt, shiftAmount,
+        helper.createI32Const(kF32MantBits + 1));
+    subnormalResult = arith::SelectOp::create(
+        rewriter, loc, shiftTooLarge,
+        dstInfo.nanAsNegZero ? c0 : dstSign, // Zero (signed if supported)
+        subnormalResult);
+
+    // Select cascade for final result.
+    //
+    // Unlike runtime code which uses early returns, SSA form requires computing
+    // all paths and selecting between them. The ORDER of selects matters:
+    // later selects override earlier ones. We order from lowest to highest
+    // priority so the final select (NaN) always wins.
+    //
+    // Priority (lowest to highest):
+    //   1. Normal/subnormal computation (base case)
+    //   2. Source zero/subnormal -> zero
+    //   3. Negative zero correction (FNUZ only, must be before NaN handling.)
+    //   4. Overflow -> Inf or NaN
+    //   5. Source Inf -> Inf or NaN
+    //   6. Source NaN -> NaN (highest priority, always wins)
+    Value result = arith::SelectOp::create(rewriter, loc, isUnderflow,
+                                           subnormalResult, normalResult);
+
+    // F32 subnormals (exp=0) become zero in fp8 (much smaller than fp8 min).
+    Value zeroResult = dstInfo.nanAsNegZero ? c0 : dstSign;
+    result = arith::SelectOp::create(rewriter, loc, srcExpIsZero, zeroResult,
+                                     result);
+
+    // FNUZ: Negative zero (0x80) must become positive zero (0x00).
+    // CRITICAL: This must happen BEFORE NaN/Inf/overflow handling.
+    // For FNUZ types, 0x80 is the NaN encoding, not negative zero.
+    // If we did this after, we would incorrectly convert NaN to zero.
+    if (dstInfo.nanAsNegZero) {
+      Value resultIsNegZero = arith::CmpIOp::create(
+          rewriter, loc, arith::CmpIPredicate::eq, result, cDstSignMask);
+      result =
+          arith::SelectOp::create(rewriter, loc, resultIsNegZero, c0, result);
+    }
+
+    // Overflow and source Inf both map to the same result:
+    // Inf (IEEE) or NaN (FN/FNUZ) or saturate to max finite (no Inf/NaN).
+    Value infOrOverflowResult;
+    if (dstInfo.hasInf) {
+      infOrOverflowResult = arith::OrIOp::create(
+          rewriter, loc, dstSign, helper.createI32Const(dstInfo.infEncoding));
+    } else if (dstInfo.hasNan) {
+      infOrOverflowResult = cNaN;
+    } else {
+      // No Inf, no NaN: saturate to max finite.
+      infOrOverflowResult = arith::OrIOp::create(
+          rewriter, loc, dstSign, helper.createI32Const(dstInfo.maxFinite));
+    }
+    result = arith::SelectOp::create(rewriter, loc, isOverflow,
+                                     infOrOverflowResult, result);
+    result = arith::SelectOp::create(rewriter, loc, srcIsInf,
+                                     infOrOverflowResult, result);
+
+    // Handle source NaN last so it takes precedence.
+    Value nanResult;
+    if (dstInfo.hasNan) {
+      nanResult = cNaN;
+    } else {
+      nanResult = c0; // No NaN encoding: convert to +0.
+    }
+    result =
+        arith::SelectOp::create(rewriter, loc, srcIsNan, nanResult, result);
+
+    // Truncate to i8 and bitcast to fp8.
+    result = arith::TruncIOp::create(rewriter, loc, helper.getI8Type(), result);
+    result = arith::BitcastOp::create(rewriter, loc, resultType, result);
+    rewriter.replaceOp(op, result);
+    return success();
+  }
+};
+
+//===----------------------------------------------------------------------===//
+// ExtF from small float emulation pattern
+//===----------------------------------------------------------------------===//
+
+/// Emulates arith.extf from fp8 to f32 using integer bit manipulation.
+/// This implementation follows IREE's runtime/src/iree/base/internal/math.h.
+///
+/// For normal values: adjust exponent bias and shift mantissa.
+///
+/// For denormals (exp=0, mantissa!=0):
+///   value = mantissa * 2^(1 - bias - mantissa_bits)
+/// We implement this using uitofp + mulf with a precomputed scale factor,
+/// which is simpler and more general than enumerating all possible values.
+struct ExtFFromFP8 final : public OpRewritePattern<arith::ExtFOp> {
```

**Comment:**
not done

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:812`

```diff
@@ -97,16 +800,31 @@ void ConvertUnsupportedFloatArithPass::runOnOperation() {
     return signalPassFailure();
   }
 
-  TypeConverter converter;
-  arith::populateEmulateUnsupportedFloatsConversions(converter, sourceTypes,
-                                                     targetType);
-  RewritePatternSet patterns(context);
-  arith::populateEmulateUnsupportedFloatsPatterns(patterns, converter);
-  ConversionTarget target(*context);
-  arith::populateEmulateUnsupportedFloatsLegality(target, converter);
+  // Apply the standard float emulation patterns. This inserts extf/truncf pairs
+  // around unsupported float operations.
+  {
+    TypeConverter converter;
+    arith::populateEmulateUnsupportedFloatsConversions(converter, sourceTypes,
+                                                       targetType);
+    RewritePatternSet patterns(context);
+    arith::populateEmulateUnsupportedFloatsPatterns(patterns, converter);
+    ConversionTarget target(*context);
+    arith::populateEmulateUnsupportedFloatsLegality(target, converter);
```

**Comment:**
Is it valid to mix conversion and rewrite patterns? I remember it used to not always work a few years ago, but maybe Matthias fixed this

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:825`

```diff
@@ -97,16 +800,31 @@ void ConvertUnsupportedFloatArithPass::runOnOperation() {
     return signalPassFailure();
   }
 
-  TypeConverter converter;
-  arith::populateEmulateUnsupportedFloatsConversions(converter, sourceTypes,
-                                                     targetType);
-  RewritePatternSet patterns(context);
-  arith::populateEmulateUnsupportedFloatsPatterns(patterns, converter);
-  ConversionTarget target(*context);
-  arith::populateEmulateUnsupportedFloatsLegality(target, converter);
+  // Apply the standard float emulation patterns. This inserts extf/truncf pairs
+  // around unsupported float operations.
+  {
+    TypeConverter converter;
+    arith::populateEmulateUnsupportedFloatsConversions(converter, sourceTypes,
+                                                       targetType);
+    RewritePatternSet patterns(context);
+    arith::populateEmulateUnsupportedFloatsPatterns(patterns, converter);
+    ConversionTarget target(*context);
+    arith::populateEmulateUnsupportedFloatsLegality(target, converter);
+
+    if (failed(applyPartialConversion(funcOp, target, std::move(patterns)))) {
+      return signalPassFailure();
+    }
+  }
 
-  if (failed(applyPartialConversion(funcOp, target, std::move(patterns)))) {
-    signalPassFailure();
+  // For CPU, emulate extf/truncf to/from small float types using integer ops.
+  // This is needed because LLVM doesn't support fpext/fptrunc for fp8 types;
+  // the fp8 types eventually get lowered to i8 in LLVM IR.
+  if (isCPU) {
+    RewritePatternSet emulationPatterns(context);
+    emulationPatterns.add<TruncFToFP8, ExtFFromFP8>(context);
+    if (failed(applyPatternsGreedily(funcOp, std::move(emulationPatterns)))) {
```

**Comment:**
I think you can use the walk pattern rewrite driver if you don't have to rewrite newly created ops

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:139`

```diff
@@ -30,6 +35,683 @@ namespace mlir::iree_compiler {
 
 namespace {
 
+/// Detector for T::get(MLIRContext*) used by llvm::is_detected.
+template <typename T>
+using hasContextGet = decltype(T::get(std::declval<MLIRContext *>()));
+
+/// Helpers to append types to a vector if they are f8 types.
+template <typename T>
+static void maybeAppendType(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  if constexpr (llvm::is_detected<hasContextGet, T>::value) {
+    Type t = T::get(ctx);
+    if (isa<FloatType>(t) && t.getIntOrFloatBitWidth() == 8) {
+      types.push_back(t);
+    }
+  }
+}
+template <typename... Ts>
+static void appendTypesIf(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  (maybeAppendType<Ts>(ctx, types), ...);
+}
+
+//===----------------------------------------------------------------------===//
+// Helper for float emulation patterns
+//===----------------------------------------------------------------------===//
+//
+// This follows the same approach as IREE's
+// runtime/src/iree/base/internal/math.h for floating-point conversions. The
+// compiler emits equivalent logic using MLIR arith ops instead of C control
+// flow.
+//
+// IEEE 754 floating-point format: [sign | exponent | mantissa]
+//   - Sign: 1 bit (0 = positive, 1 = negative)
+//   - Exponent: biased (stored = actual + bias)
+//   - Mantissa: fractional bits with implicit leading 1 for normal values
+//
+// Special values:
+//   - Zero: exp=0, mantissa=0 (signed zero if format supports it)
+//   - Denormal: exp=0, mantissa!=0, value = mantissa * 2^(1-bias-mantissa_bits)
+//   - Inf: exp=max, mantissa=0 (IEEE types only)
+//   - NaN: exp=max, mantissa!=0 (IEEE), or special encoding (FNUZ: 0x80)
+
+// F32 format constants (IEEE 754 binary32).
+constexpr int kF32MantBits = 23;
+constexpr int kF32Bias = 127;
+
+/// Extracted components from an f32 value stored as i32 bits.
+struct F32Fields {
+  Value sign;      // Sign bit (0 or 1) shifted to bit 0.
+  Value biasedExp; // Biased exponent (8 bits).
+  Value mantissa;  // Mantissa (23 bits, no implicit leading 1).
+};
+
+/// Helper class for emulating small float (e.g., fp8) conversions to/from f32
+/// using integer bit manipulation. Handles both scalar and vector types.
+///
+/// Takes a small float type (scalar or vector), queries its semantics via
+/// APFloat, and provides methods that return Value constants for the format
+/// parameters.
+class FloatEmulationHelper {
+public:
+  FloatEmulationHelper(RewriterBase &rewriter, Location loc,
+                       Type smallFloatType)
+      : rewriter(rewriter), loc(loc),
+        vecType(dyn_cast<VectorType>(smallFloatType)),
+        sem(cast<FloatType>(getElementTypeOrSelf(smallFloatType))
+                .getFloatSemantics()) {
+    // Setup scalar and vector types for i32, i8, f32.
+    Type i32Scalar = rewriter.getI32Type();
+    Type i8Scalar = rewriter.getI8Type();
+    Type f32Scalar = rewriter.getF32Type();
+    i32Type =
+        vecType ? VectorType::get(vecType.getShape(), i32Scalar) : i32Scalar;
+    i8Type = vecType ? VectorType::get(vecType.getShape(), i8Scalar) : i8Scalar;
+    f32Type =
+        vecType ? VectorType::get(vecType.getShape(), f32Scalar) : f32Scalar;
+
+    // Derive format parameters from APFloat semantics.
+    smallMantBits = llvm::APFloat::semanticsPrecision(sem) - 1;
+    int totalBits = llvm::APFloat::semanticsSizeInBits(sem);
+    smallExpBits = totalBits - 1 - smallMantBits;
+    smallBias = 1 - llvm::APFloat::semanticsMinExponent(sem);
+
+    // Query format capabilities.
+    smallHasInf = llvm::APFloat::semanticsHasInf(sem);
+    smallHasNan = llvm::APFloat::semanticsHasNaN(sem);
+
+    // Check for FNUZ types where NaN is encoded as negative zero (0x80).
+    llvm::APFloat negZero = llvm::APFloat::getZero(sem, /*Negative=*/true);
+    smallHasNegZero = negZero.isZero() && negZero.isNegative();
+    smallNanIsNegZero = !smallHasNegZero && !smallHasInf && smallHasNan;
+  }
+
+  //===--------------------------------------------------------------------===//
+  // Generic constant creation
+  //===--------------------------------------------------------------------===//
+
+  /// Creates an i32 constant, splatted if working with vectors.
+  Value createI32Const(int64_t value) {
+    auto attr = rewriter.getIntegerAttr(rewriter.getI32Type(), value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(i32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, attr);
```

**Comment:**
Do we ever fold constants? What would these fold to?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertUnsupportedFloatArithPass.cpp:149`

```diff
@@ -30,6 +35,683 @@ namespace mlir::iree_compiler {
 
 namespace {
 
+/// Detector for T::get(MLIRContext*) used by llvm::is_detected.
+template <typename T>
+using hasContextGet = decltype(T::get(std::declval<MLIRContext *>()));
+
+/// Helpers to append types to a vector if they are f8 types.
+template <typename T>
+static void maybeAppendType(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  if constexpr (llvm::is_detected<hasContextGet, T>::value) {
+    Type t = T::get(ctx);
+    if (isa<FloatType>(t) && t.getIntOrFloatBitWidth() == 8) {
+      types.push_back(t);
+    }
+  }
+}
+template <typename... Ts>
+static void appendTypesIf(MLIRContext *ctx, SmallVectorImpl<Type> &types) {
+  (maybeAppendType<Ts>(ctx, types), ...);
+}
+
+//===----------------------------------------------------------------------===//
+// Helper for float emulation patterns
+//===----------------------------------------------------------------------===//
+//
+// This follows the same approach as IREE's
+// runtime/src/iree/base/internal/math.h for floating-point conversions. The
+// compiler emits equivalent logic using MLIR arith ops instead of C control
+// flow.
+//
+// IEEE 754 floating-point format: [sign | exponent | mantissa]
+//   - Sign: 1 bit (0 = positive, 1 = negative)
+//   - Exponent: biased (stored = actual + bias)
+//   - Mantissa: fractional bits with implicit leading 1 for normal values
+//
+// Special values:
+//   - Zero: exp=0, mantissa=0 (signed zero if format supports it)
+//   - Denormal: exp=0, mantissa!=0, value = mantissa * 2^(1-bias-mantissa_bits)
+//   - Inf: exp=max, mantissa=0 (IEEE types only)
+//   - NaN: exp=max, mantissa!=0 (IEEE), or special encoding (FNUZ: 0x80)
+
+// F32 format constants (IEEE 754 binary32).
+constexpr int kF32MantBits = 23;
+constexpr int kF32Bias = 127;
+
+/// Extracted components from an f32 value stored as i32 bits.
+struct F32Fields {
+  Value sign;      // Sign bit (0 or 1) shifted to bit 0.
+  Value biasedExp; // Biased exponent (8 bits).
+  Value mantissa;  // Mantissa (23 bits, no implicit leading 1).
+};
+
+/// Helper class for emulating small float (e.g., fp8) conversions to/from f32
+/// using integer bit manipulation. Handles both scalar and vector types.
+///
+/// Takes a small float type (scalar or vector), queries its semantics via
+/// APFloat, and provides methods that return Value constants for the format
+/// parameters.
+class FloatEmulationHelper {
+public:
+  FloatEmulationHelper(RewriterBase &rewriter, Location loc,
+                       Type smallFloatType)
+      : rewriter(rewriter), loc(loc),
+        vecType(dyn_cast<VectorType>(smallFloatType)),
+        sem(cast<FloatType>(getElementTypeOrSelf(smallFloatType))
+                .getFloatSemantics()) {
+    // Setup scalar and vector types for i32, i8, f32.
+    Type i32Scalar = rewriter.getI32Type();
+    Type i8Scalar = rewriter.getI8Type();
+    Type f32Scalar = rewriter.getF32Type();
+    i32Type =
+        vecType ? VectorType::get(vecType.getShape(), i32Scalar) : i32Scalar;
+    i8Type = vecType ? VectorType::get(vecType.getShape(), i8Scalar) : i8Scalar;
+    f32Type =
+        vecType ? VectorType::get(vecType.getShape(), f32Scalar) : f32Scalar;
+
+    // Derive format parameters from APFloat semantics.
+    smallMantBits = llvm::APFloat::semanticsPrecision(sem) - 1;
+    int totalBits = llvm::APFloat::semanticsSizeInBits(sem);
+    smallExpBits = totalBits - 1 - smallMantBits;
+    smallBias = 1 - llvm::APFloat::semanticsMinExponent(sem);
+
+    // Query format capabilities.
+    smallHasInf = llvm::APFloat::semanticsHasInf(sem);
+    smallHasNan = llvm::APFloat::semanticsHasNaN(sem);
+
+    // Check for FNUZ types where NaN is encoded as negative zero (0x80).
+    llvm::APFloat negZero = llvm::APFloat::getZero(sem, /*Negative=*/true);
+    smallHasNegZero = negZero.isZero() && negZero.isNegative();
+    smallNanIsNegZero = !smallHasNegZero && !smallHasInf && smallHasNan;
+  }
+
+  //===--------------------------------------------------------------------===//
+  // Generic constant creation
+  //===--------------------------------------------------------------------===//
+
+  /// Creates an i32 constant, splatted if working with vectors.
+  Value createI32Const(int64_t value) {
+    auto attr = rewriter.getIntegerAttr(rewriter.getI32Type(), value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(i32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, i32Type, attr);
+  }
+
+  /// Creates an f32 constant, splatted if working with vectors.
+  Value createF32Const(float value) {
+    auto attr = rewriter.getF32FloatAttr(value);
+    if (vecType) {
+      auto splatAttr = SplatElementsAttr::get(cast<ShapedType>(f32Type), attr);
+      return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, splatAttr);
+    }
+    return rewriter.createOrFold<arith::ConstantOp>(loc, f32Type, attr);
```

**Comment:**
Also here

---


---


## [PR #23116](https://github.com/iree-org/iree/pull/23116): [LLVMGPU] Handle mixed-precision matmuls by returning nullopt

### Review Summary

**APPROVED** (2026-01-13)


---


## [PR #23114](https://github.com/iree-org/iree/pull/23114): Integrate llvm-project@18c5225d64 [ours 8ba507b29e]

### Review Summary

**APPROVED** (2026-01-13)


---


## [PR #23112](https://github.com/iree-org/iree/pull/23112): Update iree-test-suites

### Review Summary

**APPROVED** (2026-01-13)


---


## [PR #23111](https://github.com/iree-org/iree/pull/23111): [GPU] Account for scale operands in shared memory calculation

### Review Summary

**COMMENTED** (2026-01-13)

Can we test this?

**APPROVED** (2026-01-13)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/ConfigUtils.cpp:780`

```diff
@@ -773,24 +773,24 @@ getMatmulOrIGEMMLoweringConfigAndWorkgroupSize(
   assert((operands.size() == 3 || scaled) && "expected 3 operands");
   assert((operands.size() == 5 || !scaled) && "expected 5 operands");
 
-  Value lhs = operands[0];
-  Value rhs = operands[1];
-
-  Value init = operands[2];
+  Type lhsElemType = getElementTypeOrSelf(operands[0]);
+  Type rhsElemType = getElementTypeOrSelf(operands[1]);
+  Type initElemType = getElementTypeOrSelf(operands[2]);
+  Type lhsScaleType = nullptr;
+  Type rhsScaleType = nullptr;
```

**Comment:**
```suggestion
  Type lhsScaleType;
  Type rhsScaleType;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUHeuristics.cpp:724`

```diff
@@ -694,7 +715,15 @@ FailureOr<GPUMMASchedule> deduceMMASchedule(
       LDBG() << "Available Shared Memory: " << sharedMemLimitInBytes << " bytes"
              << "Predicted Shared Memory Used by Schedule: " << sharedMemoryUsed
              << " bytes";
-      return isAligned && sharedMemoryUsed <= sharedMemLimitInBytes;
+
+      bool isValid = isAligned && sharedMemoryUsed <= sharedMemLimitInBytes;
+      if (isValid) {
+        // Only emit remark for the shared memory usage of the valid schedule.
+        remark::analysis(loc, remark::RemarkOpts::name("SharedMemoryUsage")
+                                  .category("deduceMMASchedule"))
+            << std::to_string(sharedMemoryUsed);
```

**Comment:**
This is great, thanks! I wonder if we could further break down the calculation and print what is the size of each sub-allocation, but fine to leave as-is for this PR

---


---


## [PR #23110](https://github.com/iree-org/iree/pull/23110): [Codegen][GPU] Fix lane offset handling in coalesced DMA lowering

### Review Summary

**COMMENTED** (2026-01-19)

**COMMENTED** (2026-01-19)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/AMDGPULowerCoalescedDMAToGatherLDS.cpp:402`

```diff
@@ -366,29 +370,39 @@ struct LowerCoalescedGatherDMAPattern final
           int64_t linearOffset =
               segment.startOffset + transferIdx * segment.elementsPerTransfer;
 
-          // Build dimension offsets: outer dims use static offsets,
-          // linear dims use delinearized offsets.
-          SmallVector<Value> dimOffsets;
-
-          // Add outer dimension offsets (constant values from iteration).
+          // Build outer dimension offsets (constant values from iteration).
+          // These are the same for both source and destination.
+          SmallVector<Value> outerDimOffsets;
           for (int64_t i = 0; i < numOuterDims; ++i) {
-            dimOffsets.push_back(
+            outerDimOffsets.push_back(
                 arith::ConstantIndexOp::create(rewriter, loc, outerOffsets[i]));
           }
 
-          // Delinearize the linear offset into the trailing dimensions.
           Value linearOffsetVal =
               arith::ConstantIndexOp::create(rewriter, loc, linearOffset);
-          SmallVector<OpFoldResult> basis =
-              getAsIndexOpFoldResult(rewriter.getContext(), linearBasis);
-          auto delinearize = affine::AffineDelinearizeIndexOp::create(
+
+          // Source indices: add lane offset before delinearization (divergent).
+          Value srcLinearOffset =
+              arith::AddIOp::create(rewriter, loc, linearOffsetVal, laneOffset);
+          auto srcDelinearize = affine::AffineDelinearizeIndexOp::create(
+              rewriter, loc, srcLinearOffset, basis, /*hasOuterBound=*/true);
+
+          SmallVector<Value> srcDimOffsets(outerDimOffsets);
+          for (Value v : srcDelinearize.getResults()) {
+            srcDimOffsets.push_back(v);
+          }
+
+          // Destination indices: no lane offset (subgroup-uniform).
+          auto dstDelinearize = affine::AffineDelinearizeIndexOp::create(
               rewriter, loc, linearOffsetVal, basis, /*hasOuterBound=*/true);
-          for (Value v : delinearize.getResults()) {
-            dimOffsets.push_back(v);
+
+          SmallVector<Value> dstDimOffsets(outerDimOffsets);
+          for (Value v : dstDelinearize.getResults()) {
+            dstDimOffsets.push_back(v);
           }
```

**Comment:**
```suggestion
          auto dstDimOffsets = llvm::to_vector_of<Value>(dstDelinearize.getResults());
}
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/AMDGPULowerCoalescedDMAToGatherLDS.cpp:391`

```diff
@@ -366,29 +370,39 @@ struct LowerCoalescedGatherDMAPattern final
           int64_t linearOffset =
               segment.startOffset + transferIdx * segment.elementsPerTransfer;
 
-          // Build dimension offsets: outer dims use static offsets,
-          // linear dims use delinearized offsets.
-          SmallVector<Value> dimOffsets;
-
-          // Add outer dimension offsets (constant values from iteration).
+          // Build outer dimension offsets (constant values from iteration).
+          // These are the same for both source and destination.
+          SmallVector<Value> outerDimOffsets;
           for (int64_t i = 0; i < numOuterDims; ++i) {
-            dimOffsets.push_back(
+            outerDimOffsets.push_back(
                 arith::ConstantIndexOp::create(rewriter, loc, outerOffsets[i]));
           }
 
-          // Delinearize the linear offset into the trailing dimensions.
           Value linearOffsetVal =
               arith::ConstantIndexOp::create(rewriter, loc, linearOffset);
-          SmallVector<OpFoldResult> basis =
-              getAsIndexOpFoldResult(rewriter.getContext(), linearBasis);
-          auto delinearize = affine::AffineDelinearizeIndexOp::create(
+
+          // Source indices: add lane offset before delinearization (divergent).
+          Value srcLinearOffset =
+              arith::AddIOp::create(rewriter, loc, linearOffsetVal, laneOffset);
+          auto srcDelinearize = affine::AffineDelinearizeIndexOp::create(
+              rewriter, loc, srcLinearOffset, basis, /*hasOuterBound=*/true);
+
+          SmallVector<Value> srcDimOffsets(outerDimOffsets);
+          for (Value v : srcDelinearize.getResults()) {
```

**Comment:**
just do `auto srcDimOffsets = llvm::to_vector_of<Value>(...);`

---


---


## [PR #23104](https://github.com/iree-org/iree/pull/23104): [NFC] Add tablegen_compile_commands.yml to .gitignore

### Review Summary

**APPROVED** (2026-01-13)


---


## [PR #23102](https://github.com/iree-org/iree/pull/23102): [LinalgExt] Add OuterReduction tiling strategy for ArgCompareOp

### Review Summary

**COMMENTED** (2026-01-20)

**COMMENTED** (2026-01-20)


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1782`

```diff
@@ -1742,83 +1743,161 @@ FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
   SmallVector<Operation *> slices;
   SmallVector<Value> tiledOperands;
 
-  // Extract a slice of the input operand.
   SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
   Operation *inputSlice =
       getSlice(b, loc, getInputValue(), offsets, sizes, strides);
   tiledOperands.push_back(inputSlice->getResult(0));
   slices.push_back(inputSlice);
 
-  // Extract slices of the init operands (partial results). For split-reduction,
-  // slice along the reduction dimension to get extra parallelism.
-  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
-  for (int64_t i = 0; i < rank; ++i) {
-    if (i == reductionDim) {
-      initOffsets.push_back(splitReductionIvs[0]);
-      initSizes.push_back(b.getIndexAttr(1));
-    } else {
-      // For non-reduction dimensions, use the same offsets/sizes as input.
-      initOffsets.push_back(offsets[i]);
-      initSizes.push_back(sizes[i]);
+  // The two strategies handle partial results differently. For arg_compare on
+  // tensor<64x4096xf32> with reduction dim=1 and tile_size=128 (32 chunks):
+  // - OuterReduction: Partial results are tensor<64x128xf32>,
+  // tensor<64x128xi32>
+  //   (tile shape). Uses scf.for iter_args to accumulate across iterations.
+  // - OuterParallel: Partial results are tensor<64x32xf32>, tensor<64x32xi32>
+  //   (one slot per chunk). Uses scf.forall with parallel_insert_slice.
+  Value initValResult, initIdxResult;
+  if (strategy == ReductionTilingStrategy::PartialReductionOuterReduction) {
+    initValResult = init[0];
+    initIdxResult = init[1];
+  } else {
+    SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+    for (int64_t i = 0; i < rank; ++i) {
+      if (i == reductionDim) {
+        initOffsets.push_back(splitReductionIvs[0]);
+        initSizes.push_back(b.getIndexAttr(1));
+      } else {
+        initOffsets.push_back(offsets[i]);
+        initSizes.push_back(sizes[i]);
+      }
+      initStrides.push_back(b.getIndexAttr(1));
     }
-    initStrides.push_back(b.getIndexAttr(1));
-  }
 
-  SmallVector<int64_t> resultValShape, resultIdxShape;
-  for (int64_t i = 0; i < rank; ++i) {
-    if (i == reductionDim) {
-      continue;
+    SmallVector<int64_t> resultShape;
+    for (int64_t i = 0; i < rank; ++i) {
+      if (i == reductionDim) {
+        continue;
+      }
+      if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+        resultShape.push_back(cast<IntegerAttr>(sizeAttr).getInt());
```

**Comment:**
You can also use matchers to get the constant value directly (`m_Constant`)

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1815`

```diff
@@ -1742,83 +1743,161 @@ FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
   SmallVector<Operation *> slices;
   SmallVector<Value> tiledOperands;
 
-  // Extract a slice of the input operand.
   SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
   Operation *inputSlice =
       getSlice(b, loc, getInputValue(), offsets, sizes, strides);
   tiledOperands.push_back(inputSlice->getResult(0));
   slices.push_back(inputSlice);
 
-  // Extract slices of the init operands (partial results). For split-reduction,
-  // slice along the reduction dimension to get extra parallelism.
-  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
-  for (int64_t i = 0; i < rank; ++i) {
-    if (i == reductionDim) {
-      initOffsets.push_back(splitReductionIvs[0]);
-      initSizes.push_back(b.getIndexAttr(1));
-    } else {
-      // For non-reduction dimensions, use the same offsets/sizes as input.
-      initOffsets.push_back(offsets[i]);
-      initSizes.push_back(sizes[i]);
+  // The two strategies handle partial results differently. For arg_compare on
+  // tensor<64x4096xf32> with reduction dim=1 and tile_size=128 (32 chunks):
+  // - OuterReduction: Partial results are tensor<64x128xf32>,
+  // tensor<64x128xi32>
+  //   (tile shape). Uses scf.for iter_args to accumulate across iterations.
+  // - OuterParallel: Partial results are tensor<64x32xf32>, tensor<64x32xi32>
+  //   (one slot per chunk). Uses scf.forall with parallel_insert_slice.
+  Value initValResult, initIdxResult;
+  if (strategy == ReductionTilingStrategy::PartialReductionOuterReduction) {
+    initValResult = init[0];
+    initIdxResult = init[1];
+  } else {
+    SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+    for (int64_t i = 0; i < rank; ++i) {
+      if (i == reductionDim) {
+        initOffsets.push_back(splitReductionIvs[0]);
+        initSizes.push_back(b.getIndexAttr(1));
+      } else {
+        initOffsets.push_back(offsets[i]);
+        initSizes.push_back(sizes[i]);
+      }
+      initStrides.push_back(b.getIndexAttr(1));
     }
-    initStrides.push_back(b.getIndexAttr(1));
-  }
 
-  SmallVector<int64_t> resultValShape, resultIdxShape;
-  for (int64_t i = 0; i < rank; ++i) {
-    if (i == reductionDim) {
-      continue;
+    SmallVector<int64_t> resultShape;
+    for (int64_t i = 0; i < rank; ++i) {
+      if (i == reductionDim) {
+        continue;
+      }
+      if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+        resultShape.push_back(cast<IntegerAttr>(sizeAttr).getInt());
+      } else {
+        resultShape.push_back(ShapedType::kDynamic);
+      }
     }
 
-    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
-      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
-      resultValShape.push_back(size);
-      resultIdxShape.push_back(size);
-      continue;
-    }
+    auto sliceValResultType =
+        cast<RankedTensorType>(getOutputValueType().clone(resultShape));
+    auto sliceIdxResultType =
+        cast<RankedTensorType>(getOutputIndexType().clone(resultShape));
+
+    auto initValSlice =
+        tensor::ExtractSliceOp::create(b, loc, sliceValResultType, init[0],
+                                       initOffsets, initSizes, initStrides);
+    initValResult = initValSlice.getResult();
+    slices.push_back(initValSlice);
+
+    auto initIdxSlice =
+        tensor::ExtractSliceOp::create(b, loc, sliceIdxResultType, init[1],
+                                       initOffsets, initSizes, initStrides);
+    initIdxResult = initIdxSlice.getResult();
+    slices.push_back(initIdxSlice);
+  }
+
+  // OuterReduction requires a different IR structure than OuterParallel. We
+  // cannot use a tiled ArgCompareOp because each iteration must compare the
+  // new tile against the accumulated result, not perform an independent
+  // reduction. Instead, we emit a linalg.generic that applies the comparator
+  // elementwise and updates both value and index accumulators.
+  if (strategy == ReductionTilingStrategy::PartialReductionOuterReduction) {
+    Value tileInput = tiledOperands[0];
+
+    // All operands use identity maps: 1 input + 2 outputs (value, index).
+    AffineMap identityMap =
```

**Comment:**
```suggestion
    auto identityMap =
```
See https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1722`

```diff
@@ -1722,9 +1722,10 @@ FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
     ArrayRef<OpFoldResult> sizes,
```

**Comment:**
Any change we can break this up? This function is getting really long and complicated

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1981`

```diff
@@ -1882,17 +1972,23 @@ LogicalResult ArgCompareOp::getPartialResultTilePosition(
     SmallVector<OpFoldResult> &resultOffsets,
     SmallVector<OpFoldResult> &resultSizes) {
   int64_t reductionDim = getDimension();
-  int64_t inputRank = getInputRank();
   resultOffsets.clear();
   resultSizes.clear();
-  for (int64_t i = 0; i < inputRank; ++i) {
+
+  bool isOuterReduction =
+      tilingStrategy == ReductionTilingStrategy::PartialReductionOuterReduction;
+
+  for (int64_t i = 0, e = getInputRank(); i < e; ++i) {
```

**Comment:**
You can do `for (auto [i, size, offset] : llvm::enumerate(sizes, offsets))` 

---


---


## [PR #23091](https://github.com/iree-org/iree/pull/23091): [Codegen] Fix shared memory overflow for accumulating scaled matmuls

### Review Summary

**APPROVED** (2026-01-12)

**COMMENTED** (2026-01-12)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Utils/Utils.cpp:2235`

```diff
@@ -2218,4 +2219,77 @@ std::optional<SmallVector<int64_t>> getCopyTileSizes(linalg::CopyOp copyOp) {
   return tileSizes;
 }
 
+//===----------------------------------------------------------------------===//
+// Utility functions for accumulating operations
+//===----------------------------------------------------------------------===//
+
+// Get the `iree_tensor_ext.dispatch.tensor.load` that this value is
+// populated with. This could potentially walk the use-def chain to get the load
+// operation, but for now it just returns the load op if that is the defining
+// operation for `v`.
+template <typename LoadOpTy>
+static std::optional<LoadOpTy> getLoadOp(Value v) {
+  auto loadOp = v.getDefiningOp<LoadOpTy>();
+  if (loadOp) {
+    return loadOp;
+  }
```

**Comment:**
```suggestion
  if (auto loadOp = v.getDefiningOp<LoadOpTy>()) {
    return loadOp;
  }
```

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/Utils.cpp:2251`

```diff
@@ -2218,4 +2219,77 @@ std::optional<SmallVector<int64_t>> getCopyTileSizes(linalg::CopyOp copyOp) {
   return tileSizes;
 }
 
+//===----------------------------------------------------------------------===//
+// Utility functions for accumulating operations
+//===----------------------------------------------------------------------===//
+
+// Get the `iree_tensor_ext.dispatch.tensor.load` that this value is
+// populated with. This could potentially walk the use-def chain to get the load
+// operation, but for now it just returns the load op if that is the defining
+// operation for `v`.
+template <typename LoadOpTy>
+static std::optional<LoadOpTy> getLoadOp(Value v) {
+  auto loadOp = v.getDefiningOp<LoadOpTy>();
+  if (loadOp) {
+    return loadOp;
+  }
+  return std::nullopt;
+}
+
+// Get the `iree_tensor_ext.dispatch.tensor.store` that this value is
+// populated writes to. This could potentially walk the use-def chain of DPS
+// init operands to get the store operation, but for now it just returns the
+// store op if the result has a single use and that use is the store op.
+template <typename StoreOpTy>
+static std::optional<StoreOpTy> getStoreOp(Value v) {
+  if (v.getNumUses() != 1) {
+    return std::nullopt;
+  }
+  auto storeOp = dyn_cast<StoreOpTy>(*(v.getUsers().begin()));
+  if (storeOp) {
+    return storeOp;
+  }
```

**Comment:**
```suggestion
  if (auto storeOp = dyn_cast<StoreOpTy>(*(v.getUsers().begin()))) {
    return storeOp;
  }
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUHeuristics.cpp:684`

```diff
@@ -681,6 +681,11 @@ FailureOr<GPUMMASchedule> deduceMMASchedule(
                              transposedLhs, transposedRhs);
       int64_t sharedMemoryUsed = calculateOperandsSharedMemoryUsedInBytes(
           schedule, lhsBitwidth, rhsBitwidth, problem.numHorizontallyFusedOps);
+      // Add accumulator/result memory when it uses shared memory (LDS):
```

**Comment:**
I appreciate them FWIW

---


---


## [PR #23085](https://github.com/iree-org/iree/pull/23085): [Codegen][MXFP4] Add attribute to lowering_config for XOR swizzle support.

### Review Summary

**COMMENTED** (2026-01-14)

**APPROVED** (2026-01-15)

**COMMENTED** (2026-01-15)

**APPROVED** (2026-01-15)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.td:142`

```diff
@@ -131,6 +131,43 @@ def IREEGPU_PromoteWithCacheSwizzle :
   );
 }
 
+def IREEGPU_SwizzleOperand :
+    AttrDef<IREEGPU_Dialect, "SwizzleOperand", [
+      DeclareAttrInterfaceMethods<IREEGPU_PromotionAttr, [
+        "promoteOperand",
+      ]>
+    ]> {
+  let mnemonic = "swizzle_operand";
+  let summary = [{
+    Indicate promotion of an operand with setting a xor swizzle value.
```

**Comment:**
nit: I'm not good at English, but shouldn't this be 'an xor`?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.td:145`

```diff
@@ -131,6 +131,43 @@ def IREEGPU_PromoteWithCacheSwizzle :
   );
 }
 
+def IREEGPU_SwizzleOperand :
+    AttrDef<IREEGPU_Dialect, "SwizzleOperand", [
+      DeclareAttrInterfaceMethods<IREEGPU_PromotionAttr, [
+        "promoteOperand",
+      ]>
+    ]> {
+  let mnemonic = "swizzle_operand";
+  let summary = [{
+    Indicate promotion of an operand with setting a xor swizzle value.
+  }];
+  let description = [{
+    When promoting, this will create a swizzle hint op on the alloc associated
```

**Comment:**
Can you explain what kind of promotion this is talking about and what this will ultimately achieve in terms of the generated code?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.td:148`

```diff
@@ -131,6 +131,43 @@ def IREEGPU_PromoteWithCacheSwizzle :
   );
 }
 
+def IREEGPU_SwizzleOperand :
+    AttrDef<IREEGPU_Dialect, "SwizzleOperand", [
+      DeclareAttrInterfaceMethods<IREEGPU_PromotionAttr, [
+        "promoteOperand",
+      ]>
+    ]> {
+  let mnemonic = "swizzle_operand";
+  let summary = [{
+    Indicate promotion of an operand with setting a xor swizzle value.
+  }];
+  let description = [{
+    When promoting, this will create a swizzle hint op on the alloc associated
+    with the input operand. For example,
+
+    ```
```

**Comment:**
```suggestion
    ```mlir
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.td:155`

```diff
@@ -131,6 +131,43 @@ def IREEGPU_PromoteWithCacheSwizzle :
   );
 }
 
+def IREEGPU_SwizzleOperand :
+    AttrDef<IREEGPU_Dialect, "SwizzleOperand", [
+      DeclareAttrInterfaceMethods<IREEGPU_PromotionAttr, [
+        "promoteOperand",
+      ]>
+    ]> {
+  let mnemonic = "swizzle_operand";
+  let summary = [{
+    Indicate promotion of an operand with setting a xor swizzle value.
+  }];
+  let description = [{
+    When promoting, this will create a swizzle hint op on the alloc associated
+    with the input operand. For example,
+
+    ```
+    %0 = tensor_ext.dispatch.tensor.load : tensor<?x4096xf16>
+    %1 = linalg.matmul ins(%0, ...)
+    ```
+
+    Becomes with `#iree_gpu.swizzle_operand<#iree_gpu.use_global_load_dma>`
+
+    ```
```

**Comment:**
```suggestion
    ```mlir
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.td:167`

```diff
@@ -131,6 +131,43 @@ def IREEGPU_PromoteWithCacheSwizzle :
   );
 }
 
+def IREEGPU_SwizzleOperand :
+    AttrDef<IREEGPU_Dialect, "SwizzleOperand", [
+      DeclareAttrInterfaceMethods<IREEGPU_PromotionAttr, [
+        "promoteOperand",
+      ]>
+    ]> {
+  let mnemonic = "swizzle_operand";
+  let summary = [{
+    Indicate promotion of an operand with setting a xor swizzle value.
+  }];
+  let description = [{
+    When promoting, this will create a swizzle hint op on the alloc associated
+    with the input operand. For example,
+
+    ```
+    %0 = tensor_ext.dispatch.tensor.load : tensor<?x4096xf16>
+    %1 = linalg.matmul ins(%0, ...)
+    ```
+
+    Becomes with `#iree_gpu.swizzle_operand<#iree_gpu.use_global_load_dma>`
+
+    ```
+    %0 = tensor_ext.dispatch.tensor.load : tensor<?x4096xf16>
+    %1 = tensor.empty()
+    %2 = swizzle_hint_op %1 xor_shuffle(8192, 32)
+    %3 = linalg.copy lowering_config = #iree_gpu.use_global_load_dma ins(%0) outs(%1)
+    %4 = linalg.matmul ins(%3, ...)
+    ```
+  }];
+  let assemblyFormat = "`<` struct(params) `>`";
+  let parameters = (ins
+    "Attribute":$copy_config,
+    AttrParameter<"int64_t", "">:$row_width,
+    AttrParameter<"int64_t", "">:$access_width
```

**Comment:**
Can we confine these integers to some reasonable range?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/PromotionImpls.cpp:65`

```diff
@@ -8,52 +8,65 @@
 #include "iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUInterfaces.h"
 
 #include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenAttrs.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
 #include "iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUOps.h"
 #include "iree/compiler/Dialect/LinalgExt/IR/LinalgExtInterfaces.h"
 #include "iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.h"
+#include "llvm/Support/DebugLog.h"
 #include "mlir/Dialect/Affine/IR/AffineOps.h"
 #include "mlir/Dialect/Arith/Utils/Utils.h"
 #include "mlir/Dialect/Linalg/IR/Linalg.h"
 #include "mlir/Dialect/Tensor/IR/Tensor.h"
 #include "mlir/IR/BuiltinTypes.h"
 #include "mlir/Interfaces/TilingInterface.h"
 
+#define DEBUG_TYPE "iree-codegen-promotion-utils"
+
 namespace mlir::iree_compiler::IREE::GPU {
 
 /// Helper to insert copy with the specified attr.
 Value promoteValue(OpBuilder &builder, Location loc, Value v, Attribute attr) {
   auto tensorType = cast<RankedTensorType>(v.getType());
   SmallVector<OpFoldResult> mixedSizes = tensor::getMixedSizes(builder, loc, v);
-
   Value empty = tensor::EmptyOp::create(builder, loc, mixedSizes,
                                         tensorType.getElementType());
   auto copy = linalg::CopyOp::create(builder, loc, v, empty);
   setLoweringConfig(copy, attr);
   return copy.getResult(0);
 }
 
-/// Inserts a `linalg.copy` directly before the given operation on the
-/// specified operand, for example with operand index = 1:
-///
-///   %2 = linalg.matmul ins(%0, %1)
-///
-/// becomes
-///
-///   %empty = tensor.empty()
-///   %copy = linalg.copy %1 to %empty {
-///     lowering_config = #iree_gpu.{derived_thread_config|use_global_dma}}
-///   linalg.matmul ins(%0, %copy)
-///
-/// If the producer is already a tilable op, the producer is just annotated with
-/// the underlying attribute.
-/// Additionally we can also promote results so in above example we will
-/// generate for index = 2 :
-///   %out_buffer = bufferization.alloc_tensor
-///   %copy1 = linalg.copy %2 to %out_buffer
-///   %copy2 = linalg.copy %copy1 to %empty {
-///     lowering_config = #iree_gpu.derived_thread_config}
-Value defaultPromotionImpl(OpBuilder &builder, OpOperand &operand,
-                           Attribute attr) {
+// Helper to insert a swizzle hint op and flatten the associated alloc.
+Value swizzlePromoteValue(OpBuilder &builder, Location loc, Value v,
+                          Attribute attr, int64_t rowWidth,
+                          int64_t accessWidth) {
+  auto tensorType = cast<RankedTensorType>(v.getType());
+  SmallVector<OpFoldResult> mixedSizes = tensor::getMixedSizes(builder, loc, v);
+  if (!tensorType.hasStaticShape()) {
+    LDBG() << "Cannot create swizzle_hint op for non static shapes";
+    Value empty = tensor::EmptyOp::create(builder, loc, mixedSizes,
+                                          tensorType.getElementType());
+    auto copy = linalg::CopyOp::create(builder, loc, v, empty);
+    setLoweringConfig(copy, attr);
+    return copy.getResult(0);
+  }
+  int64_t numElements = tensorType.getNumElements();
+  Value empty = tensor::EmptyOp::create(builder, loc, {numElements},
+                                        tensorType.getElementType());
+  IREE::Codegen::SwizzleAttrInterface swizzle =
+      IREE::Codegen::XORShuffleAttr::get(builder.getContext(), rowWidth,
+                                         accessWidth, int64_t(), int64_t());
+  Value swizzled =
+      IREE::Codegen::SwizzleHintOp::create(builder, loc, empty, swizzle);
+  Value expanded = tensor::ExpandShapeOp::create(
+      builder, loc, tensorType, swizzled,
+      {llvm::to_vector(llvm::seq(tensorType.getRank()))});
+  auto copy = linalg::CopyOp::create(builder, loc, v, expanded);
+  setLoweringConfig(copy, attr);
+  return copy.getResult(0);
```

**Comment:**
Should we remove the early exit and unify these two code paths?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/PromotionImpls.cpp:57`

```diff
@@ -8,52 +8,65 @@
 #include "iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUInterfaces.h"
 
 #include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenAttrs.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
 #include "iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUOps.h"
 #include "iree/compiler/Dialect/LinalgExt/IR/LinalgExtInterfaces.h"
 #include "iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.h"
+#include "llvm/Support/DebugLog.h"
 #include "mlir/Dialect/Affine/IR/AffineOps.h"
 #include "mlir/Dialect/Arith/Utils/Utils.h"
 #include "mlir/Dialect/Linalg/IR/Linalg.h"
 #include "mlir/Dialect/Tensor/IR/Tensor.h"
 #include "mlir/IR/BuiltinTypes.h"
 #include "mlir/Interfaces/TilingInterface.h"
 
+#define DEBUG_TYPE "iree-codegen-promotion-utils"
+
 namespace mlir::iree_compiler::IREE::GPU {
 
 /// Helper to insert copy with the specified attr.
 Value promoteValue(OpBuilder &builder, Location loc, Value v, Attribute attr) {
   auto tensorType = cast<RankedTensorType>(v.getType());
   SmallVector<OpFoldResult> mixedSizes = tensor::getMixedSizes(builder, loc, v);
-
   Value empty = tensor::EmptyOp::create(builder, loc, mixedSizes,
                                         tensorType.getElementType());
   auto copy = linalg::CopyOp::create(builder, loc, v, empty);
   setLoweringConfig(copy, attr);
   return copy.getResult(0);
 }
 
-/// Inserts a `linalg.copy` directly before the given operation on the
-/// specified operand, for example with operand index = 1:
-///
-///   %2 = linalg.matmul ins(%0, %1)
-///
-/// becomes
-///
-///   %empty = tensor.empty()
-///   %copy = linalg.copy %1 to %empty {
-///     lowering_config = #iree_gpu.{derived_thread_config|use_global_dma}}
-///   linalg.matmul ins(%0, %copy)
-///
-/// If the producer is already a tilable op, the producer is just annotated with
-/// the underlying attribute.
-/// Additionally we can also promote results so in above example we will
-/// generate for index = 2 :
-///   %out_buffer = bufferization.alloc_tensor
-///   %copy1 = linalg.copy %2 to %out_buffer
-///   %copy2 = linalg.copy %copy1 to %empty {
-///     lowering_config = #iree_gpu.derived_thread_config}
-Value defaultPromotionImpl(OpBuilder &builder, OpOperand &operand,
-                           Attribute attr) {
+// Helper to insert a swizzle hint op and flatten the associated alloc.
+Value swizzlePromoteValue(OpBuilder &builder, Location loc, Value v,
+                          Attribute attr, int64_t rowWidth,
+                          int64_t accessWidth) {
+  auto tensorType = cast<RankedTensorType>(v.getType());
+  SmallVector<OpFoldResult> mixedSizes = tensor::getMixedSizes(builder, loc, v);
+  if (!tensorType.hasStaticShape()) {
+    LDBG() << "Cannot create swizzle_hint op for non static shapes";
+    Value empty = tensor::EmptyOp::create(builder, loc, mixedSizes,
+                                          tensorType.getElementType());
+    auto copy = linalg::CopyOp::create(builder, loc, v, empty);
+    setLoweringConfig(copy, attr);
+    return copy.getResult(0);
+  }
+  int64_t numElements = tensorType.getNumElements();
+  Value empty = tensor::EmptyOp::create(builder, loc, {numElements},
+                                        tensorType.getElementType());
+  IREE::Codegen::SwizzleAttrInterface swizzle =
+      IREE::Codegen::XORShuffleAttr::get(builder.getContext(), rowWidth,
+                                         accessWidth, int64_t(), int64_t());
```

**Comment:**
```suggestion
                                         accessWidth, int64_t(0), int64_t(0));
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/PromotionImpls.cpp:57`

```diff
@@ -8,52 +8,65 @@
 #include "iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUInterfaces.h"
 
 #include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenAttrs.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
 #include "iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUOps.h"
 #include "iree/compiler/Dialect/LinalgExt/IR/LinalgExtInterfaces.h"
 #include "iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.h"
+#include "llvm/Support/DebugLog.h"
 #include "mlir/Dialect/Affine/IR/AffineOps.h"
 #include "mlir/Dialect/Arith/Utils/Utils.h"
 #include "mlir/Dialect/Linalg/IR/Linalg.h"
 #include "mlir/Dialect/Tensor/IR/Tensor.h"
 #include "mlir/IR/BuiltinTypes.h"
 #include "mlir/Interfaces/TilingInterface.h"
 
+#define DEBUG_TYPE "iree-codegen-promotion-utils"
+
 namespace mlir::iree_compiler::IREE::GPU {
 
 /// Helper to insert copy with the specified attr.
 Value promoteValue(OpBuilder &builder, Location loc, Value v, Attribute attr) {
   auto tensorType = cast<RankedTensorType>(v.getType());
   SmallVector<OpFoldResult> mixedSizes = tensor::getMixedSizes(builder, loc, v);
-
   Value empty = tensor::EmptyOp::create(builder, loc, mixedSizes,
                                         tensorType.getElementType());
   auto copy = linalg::CopyOp::create(builder, loc, v, empty);
   setLoweringConfig(copy, attr);
   return copy.getResult(0);
 }
 
-/// Inserts a `linalg.copy` directly before the given operation on the
-/// specified operand, for example with operand index = 1:
-///
-///   %2 = linalg.matmul ins(%0, %1)
-///
-/// becomes
-///
-///   %empty = tensor.empty()
-///   %copy = linalg.copy %1 to %empty {
-///     lowering_config = #iree_gpu.{derived_thread_config|use_global_dma}}
-///   linalg.matmul ins(%0, %copy)
-///
-/// If the producer is already a tilable op, the producer is just annotated with
-/// the underlying attribute.
-/// Additionally we can also promote results so in above example we will
-/// generate for index = 2 :
-///   %out_buffer = bufferization.alloc_tensor
-///   %copy1 = linalg.copy %2 to %out_buffer
-///   %copy2 = linalg.copy %copy1 to %empty {
-///     lowering_config = #iree_gpu.derived_thread_config}
-Value defaultPromotionImpl(OpBuilder &builder, OpOperand &operand,
-                           Attribute attr) {
+// Helper to insert a swizzle hint op and flatten the associated alloc.
+Value swizzlePromoteValue(OpBuilder &builder, Location loc, Value v,
+                          Attribute attr, int64_t rowWidth,
+                          int64_t accessWidth) {
+  auto tensorType = cast<RankedTensorType>(v.getType());
+  SmallVector<OpFoldResult> mixedSizes = tensor::getMixedSizes(builder, loc, v);
+  if (!tensorType.hasStaticShape()) {
+    LDBG() << "Cannot create swizzle_hint op for non static shapes";
+    Value empty = tensor::EmptyOp::create(builder, loc, mixedSizes,
+                                          tensorType.getElementType());
+    auto copy = linalg::CopyOp::create(builder, loc, v, empty);
+    setLoweringConfig(copy, attr);
+    return copy.getResult(0);
+  }
+  int64_t numElements = tensorType.getNumElements();
+  Value empty = tensor::EmptyOp::create(builder, loc, {numElements},
+                                        tensorType.getElementType());
+  IREE::Codegen::SwizzleAttrInterface swizzle =
+      IREE::Codegen::XORShuffleAttr::get(builder.getContext(), rowWidth,
+                                         accessWidth, int64_t(), int64_t());
```

**Comment:**
Can you also add inline comments with argument names for these?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/PromotionImpls.cpp:119`

```diff
@@ -78,11 +88,65 @@ Value defaultPromotionImpl(OpBuilder &builder, OpOperand &operand,
   if (!tensorType) {
     return operand.get();
   }
+  return std::nullopt;
+}
 
+/// Inserts a `linalg.copy` directly before the given operation on the
+/// specified operand, for example with operand index = 1:
+///
+///   %2 = linalg.matmul ins(%0, %1)
+///
+/// becomes
+///
+///   %empty = tensor.empty()
+///   %copy = linalg.copy %1 to %empty {
+///     lowering_config = #iree_gpu.{derived_thread_config|use_global_dma}}
+///   linalg.matmul ins(%0, %copy)
+///
+/// If the producer is already a tilable op, the producer is just annotated with
+/// the underlying attribute.
+/// Additionally we can also promote results so in above example we will
+/// generate for index = 2 :
+///   %out_buffer = bufferization.alloc_tensor
+///   %copy1 = linalg.copy %2 to %out_buffer
+///   %copy2 = linalg.copy %copy1 to %empty {
+///     lowering_config = #iree_gpu.derived_thread_config}
```

**Comment:**
Can you surround IR with code blocks? Also in the function below.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/PromotionImpls.cpp:97`

```diff
@@ -94,23 +94,29 @@ std::optional<Value> promotionImpl(OpBuilder &builder, OpOperand &operand,
 /// Inserts a `linalg.copy` directly before the given operation on the
 /// specified operand, for example with operand index = 1:
 ///
+///```mlir
```

**Comment:**
Missing space before code block. Also elsewhere.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.td:167`

```diff
@@ -131,6 +131,43 @@ def IREEGPU_PromoteWithCacheSwizzle :
   );
 }
 
+def IREEGPU_SwizzleOperand :
+    AttrDef<IREEGPU_Dialect, "SwizzleOperand", [
+      DeclareAttrInterfaceMethods<IREEGPU_PromotionAttr, [
+        "promoteOperand",
+      ]>
+    ]> {
+  let mnemonic = "swizzle_operand";
+  let summary = [{
+    Indicate promotion of an operand with setting a xor swizzle value.
+  }];
+  let description = [{
+    When promoting, this will create a swizzle hint op on the alloc associated
+    with the input operand. For example,
+
+    ```
+    %0 = tensor_ext.dispatch.tensor.load : tensor<?x4096xf16>
+    %1 = linalg.matmul ins(%0, ...)
+    ```
+
+    Becomes with `#iree_gpu.swizzle_operand<#iree_gpu.use_global_load_dma>`
+
+    ```
+    %0 = tensor_ext.dispatch.tensor.load : tensor<?x4096xf16>
+    %1 = tensor.empty()
+    %2 = swizzle_hint_op %1 xor_shuffle(8192, 32)
+    %3 = linalg.copy lowering_config = #iree_gpu.use_global_load_dma ins(%0) outs(%1)
+    %4 = linalg.matmul ins(%3, ...)
+    ```
+  }];
+  let assemblyFormat = "`<` struct(params) `>`";
+  let parameters = (ins
+    "Attribute":$copy_config,
+    AttrParameter<"int64_t", "">:$row_width,
+    AttrParameter<"int64_t", "">:$access_width
```

**Comment:**
It can be done independently, no?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/PromotionImpls.cpp:145`

```diff
@@ -78,11 +88,72 @@ Value defaultPromotionImpl(OpBuilder &builder, OpOperand &operand,
   if (!tensorType) {
     return operand.get();
   }
+  return std::nullopt;
+}
 
+/// Inserts a `linalg.copy` directly before the given operation on the
+/// specified operand, for example with operand index = 1:
+///
+/// ```mlir
+///   %2 = linalg.matmul ins(%0, %1)
+/// ```
+///
+/// becomes
+///
+/// ```mlir
+///   %empty = tensor.empty()
+///   %copy = linalg.copy %1 to %empty {
+///     lowering_config = #iree_gpu.{derived_thread_config|use_global_dma}}
+///   linalg.matmul ins(%0, %copy)
+/// ```
+///
+/// If the producer is already a tilable op, the producer is just annotated with
+/// the underlying attribute.
+/// Additionally we can also promote results so in above example we will
+/// generate for index = 2 :
+///
+/// ```mlir
+///   %out_buffer = bufferization.alloc_tensor
+///   %copy1 = linalg.copy %2 to %out_buffer
+///   %copy2 = linalg.copy %copy1 to %empty {
+///     lowering_config = #iree_gpu.derived_thread_config}
+/// ```
+Value defaultPromotionImpl(OpBuilder &builder, OpOperand &operand,
+                           Attribute attr) {
+  std::optional<Value> promotedValue = promotionImpl(builder, operand, attr);
+  if (promotedValue.has_value()) {
+    return promotedValue.value();
+  }
   return promoteValue(builder, operand.getOwner()->getLoc(), operand.get(),
                       attr);
 }
 
+/// Inserts a `linalg.copy` directly before the given operation on the
+/// specified operand, similar to the defaultPromotionImpl.
+/// The difference is this also assigns a `iree_codegen.swizzle_hint` op
+/// to the generated `tensor.empty` op.
+/// For example:
+///
+///   %2 = linalg.matmul ins(%0, %1)
+///
+/// becomes
+///
+///   %empty = tensor.empty()
+///   %swizzle = iree_codegen.swizzle_hint %empty[...]
+///   %copy = linalg.copy %1 to %swizzle {
+///     lowering_config = #iree_gpu.{derived_thread_config|use_global_dma}}
+///   linalg.matmul ins(%0, %copy)
```

**Comment:**
code blocks please

---


---


## [PR #23084](https://github.com/iree-org/iree/pull/23084): [Codegen][MXFP4] Add folding patterns for tensor.empty op that can bypass SwizzleHintOps

### Review Summary

**COMMENTED** (2026-01-14)

Can you also add tests for the changes in gpu apply tiling level?

**COMMENTED** (2026-01-14)

**COMMENTED** (2026-01-15)

**COMMENTED** (2026-01-15)

**COMMENTED** (2026-01-15)

**COMMENTED** (2026-01-15)

**COMMENTED** (2026-01-15)

**APPROVED** (2026-01-15)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUApplyTilingLevel.cpp:64`

```diff
@@ -60,8 +61,8 @@ getTiledOps(Operation *funcOp, IREE::GPU::TilingLevel tilingLevel) {
 }
 
 void GPUApplyTilingLevelPass::runOnOperation() {
-  FunctionOpInterface funcOp = getOperation();
 
+  FunctionOpInterface funcOp = getOperation();
```

**Comment:**
nit: Superfluous empty line

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:134`

```diff
@@ -119,6 +125,15 @@ static FailureOr<Value> createSharedAllocDestination(RewriterBase &rewriter,
       empty.getDynamicSizes(),
       /*copy=*/Value(), /*size_hint=*/Value(),
       /*memory_space=*/sharedMemoryAddrSpace);
+  // If the original `tensor.empty` has a swizzle hint, apply it to the new
+  // allocation.
+  if (auto swizzleHintOp =
+          dyn_cast<IREE::Codegen::SwizzleHintOp>(*empty->getUsers().begin())) {
```

**Comment:**
How do you know that swizzle will be the first user?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:133`

```diff
@@ -119,6 +125,15 @@ static FailureOr<Value> createSharedAllocDestination(RewriterBase &rewriter,
       empty.getDynamicSizes(),
       /*copy=*/Value(), /*size_hint=*/Value(),
       /*memory_space=*/sharedMemoryAddrSpace);
+  // If the original `tensor.empty` has a swizzle hint, apply it to the new
+  // allocation.
+  if (auto swizzleHintOp =
+          dyn_cast<IREE::Codegen::SwizzleHintOp>(*empty->getUsers().begin())) {
+    auto newSwizzle = IREE::Codegen::SwizzleHintOp::create(
+        rewriter, empty->getLoc(), allocTensor.getResult(),
```

**Comment:**
nit: put the location in a local variable and reuse

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2094`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
```

**Comment:**
```suggestion
struct FoldSwizzleHintOpWithExtractSliceOp final
    : OpRewritePattern<tensor::ExtractSliceOp> {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2107`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
```

**Comment:**
This is not a dependent type, you can access members directly

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2103`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
```

**Comment:**
Push this down closer to its first use, define variables when you need them

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2114`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+
+    // Check for tensor.empty source.
+    auto emptyOp =
+        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2110`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+
+    // Check for tensor.empty source.
```

**Comment:**
This is obvious from the logic, I'd drop this comment

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2098`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
```

**Comment:**
```suggestion
      : OpRewritePattern(ctx, benefit),
```
I think you don't need it here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2128`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+
+    // Check for tensor.empty source.
+    auto emptyOp =
+        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
+    if (!emptyOp) {
+      return failure();
+    }
+
+    // Check for single use.
+    if (foldSingleUseOnly && !llvm::hasSingleElement(emptyOp->getUses())) {
+      return failure();
+    }
+
+    // Create new tensor.empty op. tensor.extract_slice may be rank-reducing;
+    // its dynamic sizes must be preserved as well as its result type.
+    auto tensorType = RankedTensorType::get(sliceOp.getType().getShape(),
+                                            sliceOp.getType().getElementType(),
+                                            sliceOp.getType().getEncoding());
```

**Comment:**
nit: put slice type in a local variable

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2120`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+
+    // Check for tensor.empty source.
+    auto emptyOp =
+        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
+    if (!emptyOp) {
+      return failure();
+    }
+
+    // Check for single use.
+    if (foldSingleUseOnly && !llvm::hasSingleElement(emptyOp->getUses())) {
```

**Comment:**
`emptyOp->getNumUses() != 1`

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2133`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+
+    // Check for tensor.empty source.
+    auto emptyOp =
+        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
+    if (!emptyOp) {
+      return failure();
+    }
+
+    // Check for single use.
+    if (foldSingleUseOnly && !llvm::hasSingleElement(emptyOp->getUses())) {
+      return failure();
+    }
+
+    // Create new tensor.empty op. tensor.extract_slice may be rank-reducing;
+    // its dynamic sizes must be preserved as well as its result type.
+    auto tensorType = RankedTensorType::get(sliceOp.getType().getShape(),
+                                            sliceOp.getType().getElementType(),
+                                            sliceOp.getType().getEncoding());
+    auto newEmptyOp =
+        tensor::EmptyOp::create(rewriter, loc, tensorType, sliceOp.getSizes());
+    auto newSwizzleHintOp = IREE::Codegen::SwizzleHintOp::create(
+        rewriter, loc, newEmptyOp, swizzleHintOp.getSwizzle());
+    rewriter.replaceOp(sliceOp, newSwizzleHintOp.getResult());
```

**Comment:**
use `replaceOpWithNewOp`

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2142`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+
+    // Check for tensor.empty source.
+    auto emptyOp =
+        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
+    if (!emptyOp) {
+      return failure();
+    }
+
+    // Check for single use.
+    if (foldSingleUseOnly && !llvm::hasSingleElement(emptyOp->getUses())) {
+      return failure();
+    }
+
+    // Create new tensor.empty op. tensor.extract_slice may be rank-reducing;
+    // its dynamic sizes must be preserved as well as its result type.
+    auto tensorType = RankedTensorType::get(sliceOp.getType().getShape(),
+                                            sliceOp.getType().getElementType(),
+                                            sliceOp.getType().getEncoding());
+    auto newEmptyOp =
+        tensor::EmptyOp::create(rewriter, loc, tensorType, sliceOp.getSizes());
+    auto newSwizzleHintOp = IREE::Codegen::SwizzleHintOp::create(
+        rewriter, loc, newEmptyOp, swizzleHintOp.getSwizzle());
+    rewriter.replaceOp(sliceOp, newSwizzleHintOp.getResult());
+    return success();
+  }
+
+private:
+  bool foldSingleUseOnly = false;
+};
+
+template <typename ReshapeOp>
+struct FoldSwizzleHintOpWithReshapeOp : public OpRewritePattern<ReshapeOp> {
```

**Comment:**
```suggestion
struct FoldSwizzleHintOpWithReshapeOp final : OpRewritePattern<ReshapeOp> {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2165`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+
+    // Check for tensor.empty source.
+    auto emptyOp =
+        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
+    if (!emptyOp) {
+      return failure();
+    }
+
+    // Check for single use.
+    if (foldSingleUseOnly && !llvm::hasSingleElement(emptyOp->getUses())) {
+      return failure();
+    }
+
+    // Create new tensor.empty op. tensor.extract_slice may be rank-reducing;
+    // its dynamic sizes must be preserved as well as its result type.
+    auto tensorType = RankedTensorType::get(sliceOp.getType().getShape(),
+                                            sliceOp.getType().getElementType(),
+                                            sliceOp.getType().getEncoding());
+    auto newEmptyOp =
+        tensor::EmptyOp::create(rewriter, loc, tensorType, sliceOp.getSizes());
+    auto newSwizzleHintOp = IREE::Codegen::SwizzleHintOp::create(
+        rewriter, loc, newEmptyOp, swizzleHintOp.getSwizzle());
+    rewriter.replaceOp(sliceOp, newSwizzleHintOp.getResult());
+    return success();
+  }
+
+private:
+  bool foldSingleUseOnly = false;
+};
+
+template <typename ReshapeOp>
+struct FoldSwizzleHintOpWithReshapeOp : public OpRewritePattern<ReshapeOp> {
+  FoldSwizzleHintOpWithReshapeOp(MLIRContext *ctx, PatternBenefit benefit = 1,
+                                 bool foldSingleUseOnly = false)
+      : OpRewritePattern<ReshapeOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(ReshapeOp reshapeOp,
+                                PatternRewriter &rewriter) const override {
+    // Check for tensor.empty source.
+    auto swizzleHintOp =
+        reshapeOp.getSrc()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+    auto emptyOp =
+        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
+    if (!emptyOp) {
+      return failure();
+    }
+
+    // Check for single use.
+    if (foldSingleUseOnly &&
+        !llvm::hasSingleElement(swizzleHintOp->getUses())) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2178`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+
+    // Check for tensor.empty source.
+    auto emptyOp =
+        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
+    if (!emptyOp) {
+      return failure();
+    }
+
+    // Check for single use.
+    if (foldSingleUseOnly && !llvm::hasSingleElement(emptyOp->getUses())) {
+      return failure();
+    }
+
+    // Create new tensor.empty op. tensor.extract_slice may be rank-reducing;
+    // its dynamic sizes must be preserved as well as its result type.
+    auto tensorType = RankedTensorType::get(sliceOp.getType().getShape(),
+                                            sliceOp.getType().getElementType(),
+                                            sliceOp.getType().getEncoding());
+    auto newEmptyOp =
+        tensor::EmptyOp::create(rewriter, loc, tensorType, sliceOp.getSizes());
+    auto newSwizzleHintOp = IREE::Codegen::SwizzleHintOp::create(
+        rewriter, loc, newEmptyOp, swizzleHintOp.getSwizzle());
+    rewriter.replaceOp(sliceOp, newSwizzleHintOp.getResult());
+    return success();
+  }
+
+private:
+  bool foldSingleUseOnly = false;
+};
+
+template <typename ReshapeOp>
+struct FoldSwizzleHintOpWithReshapeOp : public OpRewritePattern<ReshapeOp> {
+  FoldSwizzleHintOpWithReshapeOp(MLIRContext *ctx, PatternBenefit benefit = 1,
+                                 bool foldSingleUseOnly = false)
+      : OpRewritePattern<ReshapeOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(ReshapeOp reshapeOp,
+                                PatternRewriter &rewriter) const override {
+    // Check for tensor.empty source.
+    auto swizzleHintOp =
+        reshapeOp.getSrc()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+    auto emptyOp =
+        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
+    if (!emptyOp) {
+      return failure();
+    }
+
+    // Check for single use.
+    if (foldSingleUseOnly &&
+        !llvm::hasSingleElement(swizzleHintOp->getUses())) {
+      return failure();
+    }
+
+    // Reify result shape.
+    Location loc = reshapeOp.getLoc();
+    ReifiedRankedShapedTypeDims resultShapes;
+    if (failed(reifyResultShapes(rewriter, reshapeOp, resultShapes)) ||
+        !llvm::hasSingleElement(resultShapes)) {
+      return failure();
+    }
+
+    // Create new tensor.empty op.
+    // TODO: Do not drop tensor type encoding.
```

**Comment:**
Can you handle this here?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2204`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+
+    // Check for tensor.empty source.
+    auto emptyOp =
+        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
+    if (!emptyOp) {
+      return failure();
+    }
+
+    // Check for single use.
+    if (foldSingleUseOnly && !llvm::hasSingleElement(emptyOp->getUses())) {
+      return failure();
+    }
+
+    // Create new tensor.empty op. tensor.extract_slice may be rank-reducing;
+    // its dynamic sizes must be preserved as well as its result type.
+    auto tensorType = RankedTensorType::get(sliceOp.getType().getShape(),
+                                            sliceOp.getType().getElementType(),
+                                            sliceOp.getType().getEncoding());
+    auto newEmptyOp =
+        tensor::EmptyOp::create(rewriter, loc, tensorType, sliceOp.getSizes());
+    auto newSwizzleHintOp = IREE::Codegen::SwizzleHintOp::create(
+        rewriter, loc, newEmptyOp, swizzleHintOp.getSwizzle());
+    rewriter.replaceOp(sliceOp, newSwizzleHintOp.getResult());
+    return success();
+  }
+
+private:
+  bool foldSingleUseOnly = false;
+};
+
+template <typename ReshapeOp>
+struct FoldSwizzleHintOpWithReshapeOp : public OpRewritePattern<ReshapeOp> {
+  FoldSwizzleHintOpWithReshapeOp(MLIRContext *ctx, PatternBenefit benefit = 1,
+                                 bool foldSingleUseOnly = false)
+      : OpRewritePattern<ReshapeOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(ReshapeOp reshapeOp,
+                                PatternRewriter &rewriter) const override {
+    // Check for tensor.empty source.
+    auto swizzleHintOp =
+        reshapeOp.getSrc()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+    auto emptyOp =
+        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
+    if (!emptyOp) {
+      return failure();
+    }
+
+    // Check for single use.
+    if (foldSingleUseOnly &&
+        !llvm::hasSingleElement(swizzleHintOp->getUses())) {
+      return failure();
+    }
+
+    // Reify result shape.
+    Location loc = reshapeOp.getLoc();
+    ReifiedRankedShapedTypeDims resultShapes;
+    if (failed(reifyResultShapes(rewriter, reshapeOp, resultShapes)) ||
+        !llvm::hasSingleElement(resultShapes)) {
+      return failure();
+    }
+
+    // Create new tensor.empty op.
+    // TODO: Do not drop tensor type encoding.
+    Value emptyTensor =
+        tensor::EmptyOp::create(rewriter, loc, resultShapes[0],
+                                reshapeOp.getResultType().getElementType());
+    Value newSwizzleHintOp = IREE::Codegen::SwizzleHintOp::create(
+        rewriter, loc, emptyTensor, swizzleHintOp.getSwizzle());
+    if (newSwizzleHintOp.getType() != reshapeOp.getResultType()) {
+      rewriter.replaceOpWithNewOp<tensor::CastOp>(
+          reshapeOp, reshapeOp.getResultType(), newSwizzleHintOp);
+    } else {
+      rewriter.replaceOp(reshapeOp, newSwizzleHintOp);
+    }
+    return success();
+  }
+
+private:
+  bool foldSingleUseOnly = false;
+};
+
+} // namespace
+
+void populateFoldSwizzleHintOpPatterns(RewritePatternSet &patterns) {
+  patterns.insert<FoldSwizzleHintOpWithReshapeOp<tensor::ExpandShapeOp>>(
+      patterns.getContext());
+  patterns.insert<FoldSwizzleHintOpWithReshapeOp<tensor::CollapseShapeOp>>(
+      patterns.getContext());
+  patterns.insert<FoldSwizzleHintOpWithExtractSliceOp>(patterns.getContext());
```

**Comment:**
```suggestion
  patterns.add<FoldSwizzleHintOpWithReshapeOp<tensor::ExpandShapeOp>, FoldSwizzleHintOpWithReshapeOp<tensor::CollapseShapeOp>, FoldSwizzleHintOpWithExtractSliceOp>(
      patterns.getContext());
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/test/gpu_fold_swizzle_hint_ops.mlir:17`

```diff
@@ -0,0 +1,126 @@
+// RUN: iree-opt --split-input-file --mlir-print-local-scope --pass-pipeline="builtin.module(func.func(iree-codegen-gpu-apply-tiling-level, canonicalize, cse))" %s | FileCheck %s
+
+// Test: tensor.extract_slice of swizzle_hint(tensor.empty) should fold
+// to swizzle_hint(tensor.empty) with the sliced shape.
+func.func @fold_extract_slice_of_swizzle_hint() -> tensor<16x32xf32> {
+  %empty = tensor.empty() : tensor<64x64xf32>
+  %swizzle = iree_codegen.swizzle_hint %empty[#iree_codegen.rotate_rows<64, 4>] : tensor<64x64xf32>
+  %slice = tensor.extract_slice %swizzle[0, 0] [16, 32] [1, 1] : tensor<64x64xf32> to tensor<16x32xf32>
+  return %slice : tensor<16x32xf32>
+}
+
+// CHECK-LABEL: func.func @fold_extract_slice_of_swizzle_hint
+//       CHECK:   %[[EMPTY:.+]] = tensor.empty() : tensor<16x32xf32>
+//       CHECK:   %[[SWIZZLE:.+]] = iree_codegen.swizzle_hint %[[EMPTY]][#iree_codegen.rotate_rows<64, 4>] : tensor<16x32xf32>
+//       CHECK:   return %[[SWIZZLE]]
+
+// -----
```

**Comment:**
nit: I don't think we splits in these tests

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:134`

```diff
@@ -119,6 +125,15 @@ static FailureOr<Value> createSharedAllocDestination(RewriterBase &rewriter,
       empty.getDynamicSizes(),
       /*copy=*/Value(), /*size_hint=*/Value(),
       /*memory_space=*/sharedMemoryAddrSpace);
+  // If the original `tensor.empty` has a swizzle hint, apply it to the new
+  // allocation.
+  if (auto swizzleHintOp =
+          dyn_cast<IREE::Codegen::SwizzleHintOp>(*empty->getUsers().begin())) {
```

**Comment:**
Let's add a check or an assertion at the very least.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2127`

```diff
@@ -2090,47 +2094,44 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
 // proven stable, we could consider upstreaming this extension.
 
 namespace {
-struct FoldSwizzleHintOpWithExtractSliceOp
-    : public OpRewritePattern<tensor::ExtractSliceOp> {
+struct FoldSwizzleHintOpWithExtractSliceOp final
+    : OpRewritePattern<tensor::ExtractSliceOp> {
   FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
                                       PatternBenefit benefit = 1,
                                       bool foldSingleUseOnly = false)
-      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
-        foldSingleUseOnly(foldSingleUseOnly) {}
+      : OpRewritePattern(ctx, benefit), foldSingleUseOnly(foldSingleUseOnly) {}
 
   LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
                                 PatternRewriter &rewriter) const override {
-    Location loc = sliceOp.getLoc();
     // Check for swizzle_hint op source.
     auto swizzleHintOp =
-        sliceOp.getSource()
-            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+        sliceOp.getSource().getDefiningOp<IREE::Codegen::SwizzleHintOp>();
     if (!swizzleHintOp) {
       return failure();
     }
 
     // Check for tensor.empty source.
-    auto emptyOp =
-        swizzleHintOp.getOperand().template getDefiningOp<tensor::EmptyOp>();
+    auto emptyOp = swizzleHintOp.getOperand().getDefiningOp<tensor::EmptyOp>();
     if (!emptyOp) {
       return failure();
     }
 
     // Check for single use.
-    if (foldSingleUseOnly && !llvm::hasSingleElement(emptyOp->getUses())) {
+    if (foldSingleUseOnly && !emptyOp->hasOneUse()) {
       return failure();
     }
 
     // Create new tensor.empty op. tensor.extract_slice may be rank-reducing;
     // its dynamic sizes must be preserved as well as its result type.
-    auto tensorType = RankedTensorType::get(sliceOp.getType().getShape(),
-                                            sliceOp.getType().getElementType(),
-                                            sliceOp.getType().getEncoding());
+    Location loc = sliceOp.getLoc();
+    RankedTensorType sliceType = cast<RankedTensorType>(sliceOp.getType());
```

**Comment:**
`auto sliceType` 

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2170`

```diff
@@ -2175,10 +2173,10 @@ struct FoldSwizzleHintOpWithReshapeOp : public OpRewritePattern<ReshapeOp> {
     }
 
     // Create new tensor.empty op.
-    // TODO: Do not drop tensor type encoding.
     Value emptyTensor =
         tensor::EmptyOp::create(rewriter, loc, resultShapes[0],
-                                reshapeOp.getResultType().getElementType());
+                                reshapeOp.getResultType().getElementType(),
+                                reshapeOp.getResultType().getEncoding());
```

**Comment:**
Can we have a testcase for this?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2101`

```diff
@@ -2065,4 +2084,120 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp final
+    : OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
```

**Comment:**
Should we remove this flag? We don't have any tests that exercise this behavior anyway

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2107`

```diff
@@ -2065,4 +2080,128 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp
+    : public OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1,
+                                      bool foldSingleUseOnly = false)
+      : OpRewritePattern<tensor::ExtractSliceOp>(ctx, benefit),
+        foldSingleUseOnly(foldSingleUseOnly) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    Location loc = sliceOp.getLoc();
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource()
+            .template getDefiningOp<IREE::Codegen::SwizzleHintOp>();
```

**Comment:**
The other instances have the op type as the template argument. Here we know the type of `sliceOp` so we don't need it.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2102`

```diff
@@ -2065,4 +2085,112 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp final
+    : OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1)
+      : OpRewritePattern(ctx, benefit) {}
```

**Comment:**
```suggestion
  using Base::Base;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/Transforms/Transforms.cpp:2142`

```diff
@@ -2065,4 +2085,112 @@ void populateIREEGPULowerValueBarrierPatterns(RewritePatternSet &patterns) {
   patterns.add<LowerValueBarrierPattern>(patterns.getContext());
 }
 
+//===----------------------------------------------------------------------===//
+// SwizzleHintOp Fold Patterns
+//===----------------------------------------------------------------------===//
+
+// The following patterns are adapted from the populateFoldTensorEmptyPatterns
+// in upstream llvm-project. The main change is to add support for folding with
+// swizzle_hint ops from IREE. Once swizzle_hint ops are more widely used and
+// proven stable, we could consider upstreaming this extension.
+
+namespace {
+struct FoldSwizzleHintOpWithExtractSliceOp final
+    : OpRewritePattern<tensor::ExtractSliceOp> {
+  FoldSwizzleHintOpWithExtractSliceOp(MLIRContext *ctx,
+                                      PatternBenefit benefit = 1)
+      : OpRewritePattern(ctx, benefit) {}
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp sliceOp,
+                                PatternRewriter &rewriter) const override {
+    // Check for swizzle_hint op source.
+    auto swizzleHintOp =
+        sliceOp.getSource().getDefiningOp<IREE::Codegen::SwizzleHintOp>();
+    if (!swizzleHintOp) {
+      return failure();
+    }
+
+    // Check for tensor.empty source.
+    auto emptyOp = swizzleHintOp.getOperand().getDefiningOp<tensor::EmptyOp>();
+    if (!emptyOp) {
+      return failure();
+    }
+
+    // Check for single use.
+    if (!emptyOp->hasOneUse()) {
+      return failure();
+    }
+
+    // Create new tensor.empty op. tensor.extract_slice may be rank-reducing;
+    // its dynamic sizes must be preserved as well as its result type.
+    Location loc = sliceOp.getLoc();
+    auto sliceType = cast<RankedTensorType>(sliceOp.getType());
+    auto tensorType =
+        RankedTensorType::get(sliceType.getShape(), sliceType.getElementType(),
+                              sliceType.getEncoding());
+    auto newEmptyOp =
+        tensor::EmptyOp::create(rewriter, loc, tensorType, sliceOp.getSizes());
+    rewriter.replaceOpWithNewOp<IREE::Codegen::SwizzleHintOp>(
+        sliceOp, newEmptyOp, swizzleHintOp.getSwizzle());
+    return success();
+  }
+};
+
+template <typename ReshapeOp>
+struct FoldSwizzleHintOpWithReshapeOp final : OpRewritePattern<ReshapeOp> {
+  FoldSwizzleHintOpWithReshapeOp(MLIRContext *ctx, PatternBenefit benefit = 1)
+      : OpRewritePattern<ReshapeOp>(ctx, benefit) {}
```

**Comment:**
```suggestion
  using OpRewritePattern<ReshapeOp>::OpRewritePattern;
```

---


---


## [PR #23083](https://github.com/iree-org/iree/pull/23083): [Codegen][MXFP4] Adding SwizzleHintOp alloc flattening pass

### Review Summary

**COMMENTED** (2026-01-14)

**COMMENTED** (2026-01-14)

**COMMENTED** (2026-01-15)

**COMMENTED** (2026-01-15)

**APPROVED** (2026-01-15)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/FlattenSwizzleHintAllocs.cpp:1`

```diff
@@ -0,0 +1,71 @@
+// Copyright 2025 The IREE Authors
```

**Comment:**
```suggestion
// Copyright 2026 The IREE Authors
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/FlattenSwizzleHintAllocs.cpp:20`

```diff
@@ -0,0 +1,71 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/IR/PatternMatch.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_FLATTENSWIZZLEHINTALLOCSPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct FlattenSwizzleHintAllocsPass final
+    : public impl::FlattenSwizzleHintAllocsPassBase<
```

**Comment:**
```suggestion
struct FlattenSwizzleHintAllocsPass final
    : impl::FlattenSwizzleHintAllocsPassBase<
```
nit: with struct, `public` is already the default access specifier, this is redundant

---

**File:** `compiler/src/iree/compiler/Codegen/Common/FlattenSwizzleHintAllocs.cpp:45`

```diff
@@ -0,0 +1,71 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/IR/PatternMatch.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_FLATTENSWIZZLEHINTALLOCSPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct FlattenSwizzleHintAllocsPass final
+    : public impl::FlattenSwizzleHintAllocsPassBase<
+          FlattenSwizzleHintAllocsPass> {
+  using Base::Base;
+  void runOnOperation() override;
+};
+} // namespace
+
+static void flattenSwizzleHintAllocs(RewriterBase &rewriter,
+                                     IREE::Codegen::SwizzleHintOp hintOp) {
```

**Comment:**
Can you add a documentation comment explaining what this does? IR before and after would help.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/FlattenSwizzleHintAllocs.cpp:36`

```diff
@@ -0,0 +1,71 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/IR/PatternMatch.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_FLATTENSWIZZLEHINTALLOCSPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct FlattenSwizzleHintAllocsPass final
+    : public impl::FlattenSwizzleHintAllocsPassBase<
+          FlattenSwizzleHintAllocsPass> {
+  using Base::Base;
+  void runOnOperation() override;
+};
+} // namespace
+
+static void flattenSwizzleHintAllocs(RewriterBase &rewriter,
+                                     IREE::Codegen::SwizzleHintOp hintOp) {
+  auto allocOp = hintOp.getOperand().getDefiningOp<memref::AllocOp>();
+  if (!allocOp) {
+    return;
+  }
+  if (!allocOp->hasOneUse()) {
+    return;
+  }
+  auto resultType = allocOp.getType();
```

**Comment:**
Can you spell out the type here? I don't know if it's `ShapedType` or `MemRefType` or something else.
See https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---

**File:** `compiler/src/iree/compiler/Codegen/Common/FlattenSwizzleHintAllocs.cpp:40`

```diff
@@ -0,0 +1,71 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/IR/PatternMatch.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_FLATTENSWIZZLEHINTALLOCSPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct FlattenSwizzleHintAllocsPass final
+    : public impl::FlattenSwizzleHintAllocsPassBase<
+          FlattenSwizzleHintAllocsPass> {
+  using Base::Base;
+  void runOnOperation() override;
+};
+} // namespace
+
+static void flattenSwizzleHintAllocs(RewriterBase &rewriter,
+                                     IREE::Codegen::SwizzleHintOp hintOp) {
+  auto allocOp = hintOp.getOperand().getDefiningOp<memref::AllocOp>();
+  if (!allocOp) {
+    return;
+  }
+  if (!allocOp->hasOneUse()) {
+    return;
+  }
+  auto resultType = allocOp.getType();
+  if (resultType.getRank() == 1) {
+    return;
+  }
+  auto newResultShape = SmallVector<int64_t>({resultType.getNumElements()});
```

**Comment:**
```suggestion
 SmallVector<int64_t> newResultShape = {resultType.getNumElements()};
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/FlattenSwizzleHintAllocs.cpp:42`

```diff
@@ -0,0 +1,71 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/IR/PatternMatch.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_FLATTENSWIZZLEHINTALLOCSPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct FlattenSwizzleHintAllocsPass final
+    : public impl::FlattenSwizzleHintAllocsPassBase<
+          FlattenSwizzleHintAllocsPass> {
+  using Base::Base;
+  void runOnOperation() override;
+};
+} // namespace
+
+static void flattenSwizzleHintAllocs(RewriterBase &rewriter,
+                                     IREE::Codegen::SwizzleHintOp hintOp) {
+  auto allocOp = hintOp.getOperand().getDefiningOp<memref::AllocOp>();
+  if (!allocOp) {
+    return;
+  }
+  if (!allocOp->hasOneUse()) {
+    return;
+  }
+  auto resultType = allocOp.getType();
+  if (resultType.getRank() == 1) {
+    return;
+  }
+  auto newResultShape = SmallVector<int64_t>({resultType.getNumElements()});
+  MemRefType newResultType =
+      MemRefType::get(newResultShape, resultType.getElementType(), AffineMap(),
```

**Comment:**
```suggestion
  auto newResultType =
      MemRefType::get(newResultShape, resultType.getElementType(), AffineMap(),
```
see https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ResolveSwizzleHints.cpp:162`

```diff
@@ -157,6 +157,10 @@ static void swizzleGatherToLDS(RewriterBase &rewriter,
   });
 }
 
+static bool verifyFlatSwizzleHintOp(IREE::Codegen::SwizzleHintOp hintOp) {
+  return cast<MemRefType>(hintOp.getOperand().getType()).getRank() == 1;
+}
```

**Comment:**
Use `LogicalResult` for the return type. With `bool` it's not clear if `true` means success or failure. I'd also produce the error in the verifier

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/test/flatten_swizzle_hint_allocs.mlir:4`

```diff
@@ -0,0 +1,106 @@
+// RUN: iree-opt --allow-unregistered-dialect --pass-pipeline="builtin.module(func.func(iree-codegen-flatten-swizzle-hint-allocs))" \
+// RUN:   --split-input-file --mlir-print-local-scope %s | FileCheck %s
+
+// Test: 2D alloc with swizzle hint should be flattened to 1D
```

**Comment:**
```suggestion
// Test: 2D alloc with swizzle hint should be flattened to 1D.
```
See https://llvm.org/docs/CodingStandards.html#commenting

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/test/flatten_swizzle_hint_allocs.mlir:21`

```diff
@@ -0,0 +1,106 @@
+// RUN: iree-opt --allow-unregistered-dialect --pass-pipeline="builtin.module(func.func(iree-codegen-flatten-swizzle-hint-allocs))" \
+// RUN:   --split-input-file --mlir-print-local-scope %s | FileCheck %s
+
+// Test: 2D alloc with swizzle hint should be flattened to 1D
+func.func @flatten_2d_alloc() {
+  %alloc = memref.alloc() : memref<32x64xf32, #gpu.address_space<workgroup>>
+  %0 = iree_codegen.swizzle_hint %alloc[#iree_codegen.rotate_rows<64, 4>] : memref<32x64xf32, #gpu.address_space<workgroup>>
+  "test.use"(%0) : (memref<32x64xf32, #gpu.address_space<workgroup>>) -> ()
+  return
+}
+
+// CHECK-LABEL: func @flatten_2d_alloc
+//       CHECK:   %[[ALLOC1D:.+]] = memref.alloc() : memref<2048xf32, #gpu.address_space<workgroup>>
+//       CHECK:   %[[HINT:.+]] = iree_codegen.swizzle_hint %[[ALLOC1D]][#iree_codegen.rotate_rows<64, 4>] : memref<2048xf32, #gpu.address_space<workgroup>>
+//       CHECK:   %[[EXPAND:.+]] = memref.expand_shape %[[HINT]] {{\[\[}}0, 1{{\]\]}} output_shape [32, 64] : memref<2048xf32, #gpu.address_space<workgroup>> into memref<32x64xf32, #gpu.address_space<workgroup>>
+//       CHECK:   "test.use"(%[[EXPAND]])
+//   CHECK-NOT:   memref.alloc() : memref<32x64xf32
+//   CHECK-NOT:   iree_codegen.swizzle_hint {{.*}} : memref<32x64xf32
+//       CHECK:   return
+
+// -----
```

**Comment:**
Do we need to use splits? I think they mainly help if you have module-level attributes or if you want to check diagnostics

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/test/flatten_swizzle_hint_allocs.mlir:4`

```diff
@@ -0,0 +1,106 @@
+// RUN: iree-opt --allow-unregistered-dialect --pass-pipeline="builtin.module(func.func(iree-codegen-flatten-swizzle-hint-allocs))" \
+// RUN:   --split-input-file --mlir-print-local-scope %s | FileCheck %s
+
+// Test: 2D alloc with swizzle hint should be flattened to 1D
```

**Comment:**
also below

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/test/flatten_swizzle_hint_allocs.mlir:44`

```diff
@@ -0,0 +1,106 @@
+// RUN: iree-opt --allow-unregistered-dialect --pass-pipeline="builtin.module(func.func(iree-codegen-flatten-swizzle-hint-allocs))" \
+// RUN:   --split-input-file --mlir-print-local-scope %s | FileCheck %s
+
+// Test: 2D alloc with swizzle hint should be flattened to 1D
+func.func @flatten_2d_alloc() {
+  %alloc = memref.alloc() : memref<32x64xf32, #gpu.address_space<workgroup>>
+  %0 = iree_codegen.swizzle_hint %alloc[#iree_codegen.rotate_rows<64, 4>] : memref<32x64xf32, #gpu.address_space<workgroup>>
+  "test.use"(%0) : (memref<32x64xf32, #gpu.address_space<workgroup>>) -> ()
+  return
+}
+
+// CHECK-LABEL: func @flatten_2d_alloc
+//       CHECK:   %[[ALLOC1D:.+]] = memref.alloc() : memref<2048xf32, #gpu.address_space<workgroup>>
+//       CHECK:   %[[HINT:.+]] = iree_codegen.swizzle_hint %[[ALLOC1D]][#iree_codegen.rotate_rows<64, 4>] : memref<2048xf32, #gpu.address_space<workgroup>>
+//       CHECK:   %[[EXPAND:.+]] = memref.expand_shape %[[HINT]] {{\[\[}}0, 1{{\]\]}} output_shape [32, 64] : memref<2048xf32, #gpu.address_space<workgroup>> into memref<32x64xf32, #gpu.address_space<workgroup>>
+//       CHECK:   "test.use"(%[[EXPAND]])
+//   CHECK-NOT:   memref.alloc() : memref<32x64xf32
+//   CHECK-NOT:   iree_codegen.swizzle_hint {{.*}} : memref<32x64xf32
+//       CHECK:   return
+
+// -----
+
+// Test: 3D alloc with swizzle hint should be flattened to 1D
+func.func @flatten_3d_alloc() {
+  %alloc = memref.alloc() : memref<4x8x16xf32, #gpu.address_space<workgroup>>
+  %0 = iree_codegen.swizzle_hint %alloc[#iree_codegen.rotate_rows<64, 4>] : memref<4x8x16xf32, #gpu.address_space<workgroup>>
+  "test.use"(%0) : (memref<4x8x16xf32, #gpu.address_space<workgroup>>) -> ()
+  return
+}
+
+// CHECK-LABEL: func @flatten_3d_alloc
+//       CHECK:   %[[ALLOC1D:.+]] = memref.alloc() : memref<512xf32, #gpu.address_space<workgroup>>
+//       CHECK:   %[[HINT:.+]] = iree_codegen.swizzle_hint %[[ALLOC1D]][#iree_codegen.rotate_rows<64, 4>] : memref<512xf32, #gpu.address_space<workgroup>>
+//       CHECK:   %[[EXPAND:.+]] = memref.expand_shape %[[HINT]] {{\[\[}}0, 1, 2{{\]\]}} output_shape [4, 8, 16] : memref<512xf32, #gpu.address_space<workgroup>> into memref<4x8x16xf32, #gpu.address_space<workgroup>>
+//       CHECK:   "test.use"(%[[EXPAND]])
+//   CHECK-NOT:   memref.alloc() : memref<4x8x16xf32
+//   CHECK-NOT:   iree_codegen.swizzle_hint {{.*}} : memref<4x8x16xf32
+//       CHECK:   return
+
+// -----
+
+// Test: 1D alloc should NOT be flattened (already 1D)
+func.func @skip_1d_alloc() {
+  %alloc = memref.alloc() : memref<2048xf32, #gpu.address_space<workgroup>>
```

**Comment:**
Can we move it up to the very top? I think this is the base case.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/FlattenSwizzleHintAllocs.cpp:58`

```diff
@@ -0,0 +1,71 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/IR/PatternMatch.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_FLATTENSWIZZLEHINTALLOCSPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct FlattenSwizzleHintAllocsPass final
+    : public impl::FlattenSwizzleHintAllocsPassBase<
+          FlattenSwizzleHintAllocsPass> {
+  using Base::Base;
+  void runOnOperation() override;
+};
+} // namespace
+
+static void flattenSwizzleHintAllocs(RewriterBase &rewriter,
+                                     IREE::Codegen::SwizzleHintOp hintOp) {
+  auto allocOp = hintOp.getOperand().getDefiningOp<memref::AllocOp>();
+  if (!allocOp) {
+    return;
+  }
+  if (!allocOp->hasOneUse()) {
+    return;
+  }
+  auto resultType = allocOp.getType();
+  if (resultType.getRank() == 1) {
+    return;
+  }
+  auto newResultShape = SmallVector<int64_t>({resultType.getNumElements()});
+  MemRefType newResultType =
+      MemRefType::get(newResultShape, resultType.getElementType(), AffineMap(),
```

**Comment:**
Do we need to check that the initial allocation is contiguous (no strides, no layouts)? @krzysz00
We should have tests for this.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ResolveSwizzleHints.cpp:253`

```diff
@@ -242,6 +246,12 @@ void ResolveSwizzleHintsPass::runOnOperation() {
   // silently pass through for that hint.
   IRRewriter rewriter(funcOp->getContext());
   for (IREE::Codegen::SwizzleHintOp hintOp : hintOps) {
+    if (!verifyFlatSwizzleHintOp(hintOp)) {
+      hintOp.emitError() << "swizzle hint operand must be a flat memref, got "
+                         << hintOp.getOperand().getType();
+      signalPassFailure();
+      return;
```

**Comment:**
You can combine the two, it's quite common in IREE/MLIR
```suggestion
      return signalPassFailure();
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/resolve_swizzle_hints.mlir:331`

```diff
@@ -322,3 +322,14 @@ func.func @swizzle_raw_buffer_to_lds_ignore_dst_op(%global : memref<32768xi8, #a
 //   CHECK:   %[[LDSOFFSET:.+]] = arith.constant 0 : index
 //       CHECK:   %[[LDS:.+]] = memref.alloc() : memref<32768xi8, #gpu.address_space<workgroup>>
 //       CHECK:   amdgpu.gather_to_lds %[[SRC]][%[[SWOFF]]], %[[LDS]][%[[LDSOFFSET]]]
+
+// -----
+
+// Verify that swizzle_hint fails on non-flat (rank > 1) memrefs.
+func.func @swizzle_hint_non_flat_memref_error(%src: memref<32x64xf32>) -> vector<4xf32> {
+  // expected-error @+1 {{swizzle hint operand must be a flat memref, got 'memref<32x64xf32>'}}
+  %0 = iree_codegen.swizzle_hint %src[#iree_codegen.rotate_rows<64, 4>] : memref<32x64xf32>
```

**Comment:**
Can you also add a testcase when the memref is 1d but it's not contiguous?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/FlattenSwizzleHintAllocs.cpp:52`

```diff
@@ -0,0 +1,91 @@
+// Copyright 2026 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/MemRef/Utils/MemRefUtils.h"
+#include "mlir/IR/PatternMatch.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_FLATTENSWIZZLEHINTALLOCSPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct FlattenSwizzleHintAllocsPass final
+    : impl::FlattenSwizzleHintAllocsPassBase<FlattenSwizzleHintAllocsPass> {
+  using Base::Base;
+  void runOnOperation() override;
+};
+} // namespace
+
+/// This pass flattens swizzle hint ops that operate on allocations of rank > 1.
+/// This is required since swizzle hint op indices require flat memrefs.
+///
+/// Example:
+/// ```
+/// %0 = iree.alloc() : tensor<512x32xf4E2M1FN>
+/// %1 = iree.swizzle_hint %0 : tensor<512x32xf4E2M1FN> ->
+/// tensor<512x32xf4E2M1FN>
+/// ```
+///
+/// is flattened to:
+/// ```
+/// %0 = iree.alloc() : tensor<16384xf4E2M1FN>
+/// %1 = iree.swizzle_hint %0 : tensor<16384xf4E2M1FN> -> tensor<16384xf4E2M1FN>
+/// %2 = iree.expand_shape %1 : tensor<16384xf4E2M1FN> ->
+/// tensor<512x32xf4E2M1FN>
+/// ```
+static void flattenSwizzleHintAllocs(RewriterBase &rewriter,
+                                     IREE::Codegen::SwizzleHintOp hintOp) {
+  auto allocOp = hintOp.getOperand().getDefiningOp<memref::AllocOp>();
+  if (!allocOp) {
+    return;
+  }
+  if (!allocOp->hasOneUse()) {
+    return;
+  }
```

**Comment:**
nit/optional
```suggestion
  if (!allocOp || !allocOp->hasOneUse()) {
    return;
  }
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/FlattenSwizzleHintAllocs.cpp:52`

```diff
@@ -0,0 +1,91 @@
+// Copyright 2026 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/MemRef/Utils/MemRefUtils.h"
+#include "mlir/IR/PatternMatch.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_FLATTENSWIZZLEHINTALLOCSPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct FlattenSwizzleHintAllocsPass final
+    : impl::FlattenSwizzleHintAllocsPassBase<FlattenSwizzleHintAllocsPass> {
+  using Base::Base;
+  void runOnOperation() override;
+};
+} // namespace
+
+/// This pass flattens swizzle hint ops that operate on allocations of rank > 1.
+/// This is required since swizzle hint op indices require flat memrefs.
+///
+/// Example:
+/// ```
+/// %0 = iree.alloc() : tensor<512x32xf4E2M1FN>
+/// %1 = iree.swizzle_hint %0 : tensor<512x32xf4E2M1FN> ->
+/// tensor<512x32xf4E2M1FN>
+/// ```
+///
+/// is flattened to:
+/// ```
+/// %0 = iree.alloc() : tensor<16384xf4E2M1FN>
+/// %1 = iree.swizzle_hint %0 : tensor<16384xf4E2M1FN> -> tensor<16384xf4E2M1FN>
+/// %2 = iree.expand_shape %1 : tensor<16384xf4E2M1FN> ->
+/// tensor<512x32xf4E2M1FN>
+/// ```
+static void flattenSwizzleHintAllocs(RewriterBase &rewriter,
+                                     IREE::Codegen::SwizzleHintOp hintOp) {
+  auto allocOp = hintOp.getOperand().getDefiningOp<memref::AllocOp>();
+  if (!allocOp) {
+    return;
+  }
+  if (!allocOp->hasOneUse()) {
+    return;
+  }
+  MemRefType resultType = allocOp.getType();
+  if (resultType.getRank() == 1 || !resultType.getLayout().isIdentity() ||
+      !resultType.hasStaticShape() || !(resultType.getNumElements() > 0) ||
+      !memref::isStaticShapeAndContiguousRowMajor(resultType)) {
```

**Comment:**
Doesn't the last check subsume some of the previous ones?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/FlattenSwizzleHintAllocs.cpp:42`

```diff
@@ -0,0 +1,71 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/IR/PatternMatch.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_FLATTENSWIZZLEHINTALLOCSPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct FlattenSwizzleHintAllocsPass final
+    : public impl::FlattenSwizzleHintAllocsPassBase<
+          FlattenSwizzleHintAllocsPass> {
+  using Base::Base;
+  void runOnOperation() override;
+};
+} // namespace
+
+static void flattenSwizzleHintAllocs(RewriterBase &rewriter,
+                                     IREE::Codegen::SwizzleHintOp hintOp) {
+  auto allocOp = hintOp.getOperand().getDefiningOp<memref::AllocOp>();
+  if (!allocOp) {
+    return;
+  }
+  if (!allocOp->hasOneUse()) {
+    return;
+  }
+  auto resultType = allocOp.getType();
+  if (resultType.getRank() == 1) {
+    return;
+  }
+  auto newResultShape = SmallVector<int64_t>({resultType.getNumElements()});
+  MemRefType newResultType =
+      MemRefType::get(newResultShape, resultType.getElementType(), AffineMap(),
```

**Comment:**
not done

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ResolveSwizzleHints.cpp:255`

```diff
@@ -242,6 +252,9 @@ void ResolveSwizzleHintsPass::runOnOperation() {
   // silently pass through for that hint.
   IRRewriter rewriter(funcOp->getContext());
   for (IREE::Codegen::SwizzleHintOp hintOp : hintOps) {
+    if (verifyFlatSwizzleHintOp(hintOp).failed()) {
```

**Comment:**
```suggestion
    if (failed(verifyFlatSwizzleHintOp(hintOp))) {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/FlattenSwizzleHintAllocs.cpp:58`

```diff
@@ -0,0 +1,71 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/IR/PatternMatch.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_FLATTENSWIZZLEHINTALLOCSPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct FlattenSwizzleHintAllocsPass final
+    : public impl::FlattenSwizzleHintAllocsPassBase<
+          FlattenSwizzleHintAllocsPass> {
+  using Base::Base;
+  void runOnOperation() override;
+};
+} // namespace
+
+static void flattenSwizzleHintAllocs(RewriterBase &rewriter,
+                                     IREE::Codegen::SwizzleHintOp hintOp) {
+  auto allocOp = hintOp.getOperand().getDefiningOp<memref::AllocOp>();
+  if (!allocOp) {
+    return;
+  }
+  if (!allocOp->hasOneUse()) {
+    return;
+  }
+  auto resultType = allocOp.getType();
+  if (resultType.getRank() == 1) {
+    return;
+  }
+  auto newResultShape = SmallVector<int64_t>({resultType.getNumElements()});
+  MemRefType newResultType =
+      MemRefType::get(newResultShape, resultType.getElementType(), AffineMap(),
```

**Comment:**
IIUC, the underlying math operates on offsets, so dynamic 1d shapes should be fine

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ResolveSwizzleHints.cpp:168`

```diff
@@ -157,6 +158,22 @@ static void swizzleGatherToLDS(RewriterBase &rewriter,
   });
 }
 
+static LogicalResult
+verifyFlatContiguousSwizzleHintOp(IREE::Codegen::SwizzleHintOp hintOp) {
+  auto memrefType = cast<MemRefType>(hintOp.getOperand().getType());
+  // Swizzle hints require flat (rank 1) memrefs.
+  // For rank 1, allow dynamic memrefs or static contiguous row-major memrefs.
+  if ((memrefType.getRank() != 1 || !memrefType.getLayout().isIdentity()) ||
+      (memrefType.hasStaticShape() &&
+       !memref::isStaticShapeAndContiguousRowMajor(memrefType))) {
```

**Comment:**
Isn't rank + the last helper enough?

---


---


## [PR #23076](https://github.com/iree-org/iree/pull/23076): Reapply "[GPU][Codegen] Expand iteration space based on new `expand_dims` attribute"

### Review Summary

**APPROVED** (2026-01-12)


---


## [PR #23074](https://github.com/iree-org/iree/pull/23074): Revert "[LinalgExt] Generalize ArgCompareOp in GPUGeneralizeNamedOpsPass"

### Review Summary

**APPROVED** (2026-01-09)

Alternatively, we could keep the generalization logic but not perform it by default. But maybe better to remove this code if it's not going to be used.


---


## [PR #23068](https://github.com/iree-org/iree/pull/23068): Integrate llvm/llvm-project@eec258dcf38d

### Review Summary

**APPROVED** (2026-01-09)


---


## [PR #23055](https://github.com/iree-org/iree/pull/23055): [Codegen] Add pass to convert workgroup foralls to pcf

### Review Summary

**APPROVED** (2026-01-07)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertWorkgroupForallToPCF.cpp:23`

```diff
@@ -0,0 +1,79 @@
+// Copyright 2026 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenAttrs.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Transforms.h"
+#include "iree/compiler/Codegen/Transforms/Transforms.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_CONVERTWORKGROUPFORALLTOPCFPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct ConvertWorkgroupForall : public OpRewritePattern<scf::ForallOp> {
+  using OpRewritePattern<scf::ForallOp>::OpRewritePattern;
```

**Comment:**
```suggestion
struct ConvertWorkgroupForall : OpRewritePattern<scf::ForallOp> {
  using Base::Base;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/ConvertWorkgroupForallToPCF.cpp:62`

```diff
@@ -0,0 +1,79 @@
+// Copyright 2026 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenAttrs.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Transforms.h"
+#include "iree/compiler/Codegen/Transforms/Transforms.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_CONVERTWORKGROUPFORALLTOPCFPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+struct ConvertWorkgroupForall : public OpRewritePattern<scf::ForallOp> {
+  using OpRewritePattern<scf::ForallOp>::OpRewritePattern;
+  LogicalResult matchAndRewrite(scf::ForallOp op,
+                                PatternRewriter &rewriter) const override;
+};
+
+struct ConvertWorkgroupForallToPCFPass final
+    : public impl::ConvertWorkgroupForallToPCFPassBase<
+          ConvertWorkgroupForallToPCFPass> {
+  void runOnOperation() override;
+  using Base::Base;
+};
+
+} // namespace
+
+LogicalResult
+ConvertWorkgroupForall::matchAndRewrite(scf::ForallOp op,
+                                        PatternRewriter &rewriter) const {
+  ArrayAttr mappingAttr = op.getMappingAttr();
+  if (!mappingAttr || mappingAttr.empty() ||
+      !llvm::all_of(mappingAttr,
+                    llvm::IsaPred<IREE::Codegen::WorkgroupMappingAttr>)) {
+    return failure();
+  }
+  // Linearize all ids down to 1 so that in cases when there are multiple
+  // scf.foralls with incompatible delinearization bases. This technically
+  // may be a small pessimization in very specific static cases, so if someone
+  // ever finds they care they can try doing the analysis here to figure out
+  // when it's ok not to linearize.
+  //
+  // Interface is implemented via external models hence the cast.
+  auto scope = cast<IREE::PCF::ScopeAttrInterface>(
+      IREE::Codegen::WorkgroupScopeAttr::get(rewriter.getContext(),
+                                             /*linearize=*/true));
+  FailureOr<IREE::PCF::LoopOp> res = convertForallToPCF(rewriter, op, scope, 1);
+  if (failed(res)) {
+    return failure();
+  }
+
+  // Create a workgroup count hint to launch all workgroups along x.
+  SmallVector<OpFoldResult> counts =
```

**Comment:**
```suggestion
  auto counts =
```

---

**File:** `compiler/src/iree/compiler/Codegen/Transforms/Transforms.cpp:524`

```diff
@@ -516,8 +516,12 @@ LogicalResult createWorkgroupCountHint(RewriterBase &rewriter, Location loc,
                                        ArrayRef<OpFoldResult> workgroupCount,
                                        int maxWorkgroupParallelDims,
                                        bool reverse) {
-  SmallVector<OpFoldResult> results(reverse ? llvm::reverse(workgroupCount)
-                                            : workgroupCount);
+  SmallVector<OpFoldResult> results;
+  if (reverse) {
+    results.append(workgroupCount.rbegin(), workgroupCount.rend());
+  } else {
+    results.append(workgroupCount.begin(), workgroupCount.end());
+  }
```

**Comment:**
Use `llvm::to_vector(llvm::reverse_conditionally(workgroupCount, reverse))`

---


---


## [PR #23052](https://github.com/iree-org/iree/pull/23052): [Codegen] Implement workgroup execution scope

### Review Summary

**APPROVED** (2026-01-07)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenAttrs.cpp:885`

```diff
@@ -872,4 +872,30 @@ void setUKernelDescriptor(Operation *op,
   op->setAttr(kUKernelDescriptorName, descriptor);
 }
 
+//===----------------------------------------------------------------------===//
+// iree_codegen.workgroup_scope
+//===----------------------------------------------------------------------===//
+
+// Custom parser/printer to make the <> optional. It will either parse/print as
+//   #iree_codegen.workgroup_scope<linearize>
+// or without the <>.
+//   #iree_codegen.workgroup_scope
+Attribute IREE::Codegen::WorkgroupScopeAttr::parse(AsmParser &parser, Type) {
+  bool linearize = false;
+  if (parser.parseOptionalLess().succeeded()) {
```

**Comment:**
nit: use success(...)

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenAttrs.td:674`

```diff
@@ -664,4 +664,24 @@ def DenormalFpMathAttr :
   }];
 }
 
+//===---------------------------------------------------------------------===//
+// iree_codegen.workgroup_scope
+//===---------------------------------------------------------------------===//
+
+def WorkgroupScopeAttr : AttrDef<IREECodegen_Dialect, "WorkgroupScope", []> {
+  let mnemonic = "workgroup_scope";
+  let summary =
+      [{Attribute representing parallel execution across workgroups.}];
```

**Comment:**
```suggestion
  let summary =
      "Attribute representing parallel execution across workgroups.";
```

---

**File:** `compiler/src/iree/compiler/Codegen/ExternalInterfaces/CodegenExternalModels.cpp:48`

```diff
@@ -0,0 +1,142 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/ExternalInterfaces/CodegenExternalModels.h"
+
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenAttrs.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFInterfaces.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Dialect/HAL/IR/HALDialect.h"
+#include "iree/compiler/Dialect/HAL/IR/HALOps.h"
+#include "mlir/Dialect/Affine/IR/AffineOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+
+namespace mlir::iree_compiler::IREE::Codegen {
+
+//===----------------------------------------------------------------------===//
+// PCF Models
+//===----------------------------------------------------------------------===//
+
+struct WorkgroupScopeAttrModel final
+    : IREE::PCF::ScopeAttrInterface::ExternalModel<
+          WorkgroupScopeAttrModel, Codegen::WorkgroupScopeAttr> {
+  SmallVector<Value> getWorkerCounts(Attribute attr, OpBuilder &builder,
+                                     Location loc, int64_t numIds) const {
+    auto workgroupScopeAttr = cast<Codegen::WorkgroupScopeAttr>(attr);
+    bool linearize = workgroupScopeAttr.getLinearize();
+
+    int64_t numIdsToQuery = linearize ? 3 : std::min<int64_t>(3, numIds);
+
+    SmallVector<Value> counts;
+    for (int64_t i = 0, e = numIdsToQuery; i < e; ++i) {
+      counts.push_back(
+          IREE::HAL::InterfaceWorkgroupCountOp::create(builder, loc, i)
+              .getResult());
+    }
+
+    // If linearize is true, compute the product of all counts and replace the
+    // first ID with that.
+    if (linearize && !counts.empty()) {
+      Value one = counts.size() > 1
+                      ? arith::ConstantIndexOp::create(builder, loc, 1)
+                      : Value();
+      Value linearizedCount = counts[0];
+      for (int64_t i = 1, e = counts.size(); i < e; ++i) {
```

**Comment:**
Use `for (int64_t &count : llvm::drop_begin(counts))`?

---

**File:** `compiler/src/iree/compiler/Codegen/ExternalInterfaces/CodegenExternalModels.cpp:67`

```diff
@@ -0,0 +1,142 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/ExternalInterfaces/CodegenExternalModels.h"
+
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenAttrs.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFInterfaces.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Dialect/HAL/IR/HALDialect.h"
+#include "iree/compiler/Dialect/HAL/IR/HALOps.h"
+#include "mlir/Dialect/Affine/IR/AffineOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+
+namespace mlir::iree_compiler::IREE::Codegen {
+
+//===----------------------------------------------------------------------===//
+// PCF Models
+//===----------------------------------------------------------------------===//
+
+struct WorkgroupScopeAttrModel final
+    : IREE::PCF::ScopeAttrInterface::ExternalModel<
+          WorkgroupScopeAttrModel, Codegen::WorkgroupScopeAttr> {
+  SmallVector<Value> getWorkerCounts(Attribute attr, OpBuilder &builder,
+                                     Location loc, int64_t numIds) const {
+    auto workgroupScopeAttr = cast<Codegen::WorkgroupScopeAttr>(attr);
+    bool linearize = workgroupScopeAttr.getLinearize();
+
+    int64_t numIdsToQuery = linearize ? 3 : std::min<int64_t>(3, numIds);
+
+    SmallVector<Value> counts;
+    for (int64_t i = 0, e = numIdsToQuery; i < e; ++i) {
+      counts.push_back(
+          IREE::HAL::InterfaceWorkgroupCountOp::create(builder, loc, i)
+              .getResult());
+    }
+
+    // If linearize is true, compute the product of all counts and replace the
+    // first ID with that.
+    if (linearize && !counts.empty()) {
+      Value one = counts.size() > 1
+                      ? arith::ConstantIndexOp::create(builder, loc, 1)
+                      : Value();
+      Value linearizedCount = counts[0];
+      for (int64_t i = 1, e = counts.size(); i < e; ++i) {
+        linearizedCount =
+            arith::MulIOp::create(builder, loc, linearizedCount, counts[i])
+                .getResult();
+        counts[i] = one;
+      }
+      counts.front() = linearizedCount;
+      counts.resize(numIds > 3 ? 3 : numIds);
+    }
+
+    // Pad the outer most sizes with 1.
+    if (numIds > 3) {
+      Value one = arith::ConstantIndexOp::create(builder, loc, 1);
+      counts.append(numIds - 3, one);
+    }
+
+    return counts;
+  }
+  SmallVector<Value> getWorkerIDs(Attribute attr, OpBuilder &builder,
```

**Comment:**
```suggestion
  }
  
  SmallVector<Value> getWorkerIDs(Attribute attr, OpBuilder &builder,
```

---

**File:** `compiler/src/iree/compiler/Codegen/ExternalInterfaces/CodegenExternalModels.cpp:107`

```diff
@@ -0,0 +1,142 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/ExternalInterfaces/CodegenExternalModels.h"
+
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenAttrs.h"
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFInterfaces.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Dialect/HAL/IR/HALDialect.h"
+#include "iree/compiler/Dialect/HAL/IR/HALOps.h"
+#include "mlir/Dialect/Affine/IR/AffineOps.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+
+namespace mlir::iree_compiler::IREE::Codegen {
+
+//===----------------------------------------------------------------------===//
+// PCF Models
+//===----------------------------------------------------------------------===//
+
+struct WorkgroupScopeAttrModel final
+    : IREE::PCF::ScopeAttrInterface::ExternalModel<
+          WorkgroupScopeAttrModel, Codegen::WorkgroupScopeAttr> {
+  SmallVector<Value> getWorkerCounts(Attribute attr, OpBuilder &builder,
+                                     Location loc, int64_t numIds) const {
+    auto workgroupScopeAttr = cast<Codegen::WorkgroupScopeAttr>(attr);
+    bool linearize = workgroupScopeAttr.getLinearize();
+
+    int64_t numIdsToQuery = linearize ? 3 : std::min<int64_t>(3, numIds);
+
+    SmallVector<Value> counts;
+    for (int64_t i = 0, e = numIdsToQuery; i < e; ++i) {
+      counts.push_back(
+          IREE::HAL::InterfaceWorkgroupCountOp::create(builder, loc, i)
+              .getResult());
+    }
+
+    // If linearize is true, compute the product of all counts and replace the
+    // first ID with that.
+    if (linearize && !counts.empty()) {
+      Value one = counts.size() > 1
+                      ? arith::ConstantIndexOp::create(builder, loc, 1)
+                      : Value();
+      Value linearizedCount = counts[0];
+      for (int64_t i = 1, e = counts.size(); i < e; ++i) {
+        linearizedCount =
+            arith::MulIOp::create(builder, loc, linearizedCount, counts[i])
+                .getResult();
+        counts[i] = one;
+      }
+      counts.front() = linearizedCount;
+      counts.resize(numIds > 3 ? 3 : numIds);
+    }
+
+    // Pad the outer most sizes with 1.
+    if (numIds > 3) {
+      Value one = arith::ConstantIndexOp::create(builder, loc, 1);
+      counts.append(numIds - 3, one);
+    }
+
+    return counts;
+  }
+  SmallVector<Value> getWorkerIDs(Attribute attr, OpBuilder &builder,
+                                  Location loc, int64_t numIds) const {
+    auto workgroupScopeAttr = cast<Codegen::WorkgroupScopeAttr>(attr);
+    bool linearize = workgroupScopeAttr.getLinearize();
+
+    SmallVector<Value> ids;
+    for (int64_t i = 0, e = std::min<int64_t>(3, numIds); i < e; ++i) {
+      ids.push_back(IREE::HAL::InterfaceWorkgroupIDOp::create(builder, loc, i)
+                        .getResult());
+    }
+    if (numIds > 3) {
+      Value zero = arith::ConstantIndexOp::create(builder, loc, 0);
+      ids.append(numIds - 3, zero);
+    }
+
+    // If linearize is true, flatten to a single ID and pad with `0` until
+    // numIds.
+    if (linearize && numIds > 1) {
+      // First, get the counts to use as the delinearization shape. Construct
+      // them in reverse because thats the expected input order for
+      // linearize_index.
+      SmallVector<Value> dynamicCounts;
+      Value one = arith::ConstantIndexOp::create(builder, loc, 1);
+      if (numIds > 3) {
+        dynamicCounts.append(numIds - 3, one);
+      }
+      for (int64_t i = std::min<int64_t>(3, numIds) - 1, e = 0; i >= e; --i) {
+        dynamicCounts.push_back(
+            IREE::HAL::InterfaceWorkgroupCountOp::create(builder, loc, i)
+                .getResult());
+      }
+
+      // Linearize. IDs need to be reversed to match the counts and the builder
+      // can't take iterators.
+      SmallVector<Value> reverseIds(llvm::reverse(ids));
+      auto linearizeOp = affine::AffineLinearizeIndexOp::create(
+          builder, loc, reverseIds, dynamicCounts);
+
+      ids.front() = linearizeOp.getResult();
+      for (int64_t i = 1, e = ids.size(); i < e; ++i) {
+        ids[i] = one;
+      }
```

**Comment:**
```suggestion
      llvm::fill(llvm::drop_begin(ids), one);
```
I think this should work

---

**File:** `compiler/src/iree/compiler/Codegen/ExternalInterfaces/CodegenExternalModels.h:1`

```diff
@@ -0,0 +1,20 @@
+// Copyright 2025 The IREE Authors
```

**Comment:**
```suggestion
// Copyright 2026 The IREE Authors
```

---

**File:** `compiler/src/iree/compiler/Codegen/ExternalInterfaces/CodegenExternalModels.cpp:1`

```diff
@@ -0,0 +1,142 @@
+// Copyright 2025 The IREE Authors
```

**Comment:**
```suggestion
// Copyright 2026 The IREE Authors
```

---


---


## [PR #23048](https://github.com/iree-org/iree/pull/23048): Integrate llvm/llvm-project@55eaa6c27b

### Review Summary

**APPROVED** (2026-01-08)

I'm leaning towards landing what you have even with the dll workaround and starting the next integrate while we work out a proper fix


---


## [PR #23039](https://github.com/iree-org/iree/pull/23039): Flatbuffers: do not canonicalize empty strings to null string

### Review Summary

**APPROVED** (2026-01-06)

**COMMENTED** (2026-01-06)

Do we have unit tests for these utils?


---


## [PR #23034](https://github.com/iree-org/iree/pull/23034): [CI] Allow ci-extra to be set on integrates.

### Review Summary

**CHANGES_REQUESTED** (2026-01-06)

I'm not sure this implementation works as intended. For example, integrate PR used to trigger the windows job that is off for regular PRs. I'd expect we need to add a union of extra jobs and (all_jobs - shedule_only) instead.

**APPROVED** (2026-01-06)

We should check the unit tests mentioned by Scott


---


## [PR #23033](https://github.com/iree-org/iree/pull/23033): [CI] Add ci-extra flags for scheduled builds.

### Review Summary

**APPROVED** (2026-01-06)


---


## [PR #23022](https://github.com/iree-org/iree/pull/23022): Integrate llvm/llvm-project@10a245bd02

### Review Summary

**APPROVED** (2026-01-06)


---


## [PR #23015](https://github.com/iree-org/iree/pull/23015): [LinalgExt] Generalize ArgCompareOp in GPUGeneralizeNamedOpsPass

### Review Summary

**COMMENTED** (2026-01-05)

**APPROVED** (2026-01-05)

LGTM but PR description seems out of sync with the code after the most recent changes.

**COMMENTED** (2026-01-07)

**APPROVED** (2026-01-07)


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/AggregatedOpInterfaceImpl.cpp:1418`

```diff
@@ -1249,4 +1249,214 @@ FailureOr<SmallVector<Value>> ExpReductionOp::decomposeOperation(OpBuilder &b) {
       [](linalg::GenericOp op) -> Value { return op->getResult(0); });
 }
 
+//===----------------------------------------------------------------------===//
+// ArgCompareOp
+//===----------------------------------------------------------------------===//
+
+enum class ArgCompareKind {
+  Custom,
+  FloatArgmax,
+  FloatArgmin,
+  SignedArgmax,
+  SignedArgmin,
+  UnsignedArgmax,
+  UnsignedArgmin
+};
+
+/// Returns the ArgCompareKind for the ArgCompareOp. Returns Custom if the
+/// pattern is not a standard argmax or argmin. Only matches simple "cmp +
+/// yield" patterns with zero index_base.
+/// TODO(Bangtian): Support more complex patterns after plumbing through the
+/// VectorDistribute pipeline.
+static ArgCompareKind getArgCompareKind(ArgCompareOp argCompareOp) {
+  Type elemType = argCompareOp.getInputType().getElementType();
+  bool isFloat = isa<FloatType>(elemType);
+  bool isInt = isa<IntegerType>(elemType);
+  if (!isFloat && !isInt) {
+    return ArgCompareKind::Custom;
+  }
+
+  if (Value indexBase = argCompareOp.getIndexBase()) {
+    if (!isConstantIntValue(indexBase, 0)) {
+      return ArgCompareKind::Custom;
+    }
+  }
+
+  // The argCompareOp verifier ensures the block terminates with a yield, so 2
+  // operations means exactly one comparison op before yield.
+  Block &block = argCompareOp.getRegion().front();
+  if (block.getOperations().size() != 2) {
+    return ArgCompareKind::Custom;
+  }
+
+  // Verify the comparison operands match the block arguments in order.
+  Operation *cmpOp = &block.front();
+  if (cmpOp->getOperand(0) != block.getArgument(0) ||
+      cmpOp->getOperand(1) != block.getArgument(1)) {
+    return ArgCompareKind::Custom;
+  }
+
+  if (isFloat) {
+    auto cmpf = dyn_cast<arith::CmpFOp>(cmpOp);
+    if (!cmpf) {
+      return ArgCompareKind::Custom;
+    }
+    switch (cmpf.getPredicate()) {
+    case arith::CmpFPredicate::OGT:
+      return ArgCompareKind::FloatArgmax;
+    case arith::CmpFPredicate::OLT:
+      return ArgCompareKind::FloatArgmin;
+    default:
+      return ArgCompareKind::Custom;
+    }
+  }
+
+  auto cmpi = dyn_cast<arith::CmpIOp>(cmpOp);
+  if (!cmpi) {
+    return ArgCompareKind::Custom;
+  }
+  switch (cmpi.getPredicate()) {
+  case arith::CmpIPredicate::sgt:
+    return ArgCompareKind::SignedArgmax;
+  case arith::CmpIPredicate::slt:
+    return ArgCompareKind::SignedArgmin;
+  case arith::CmpIPredicate::ugt:
+    return ArgCompareKind::UnsignedArgmax;
+  case arith::CmpIPredicate::ult:
+    return ArgCompareKind::UnsignedArgmin;
+  default:
+    return ArgCompareKind::Custom;
+  }
+}
+
+FailureOr<SmallVector<Value>> ArgCompareOp::decomposeOperation(OpBuilder &b) {
+  ArgCompareKind kind = getArgCompareKind(*this);
+  if (kind == ArgCompareKind::Custom) {
+    return failure();
+  }
+
+  arith::AtomicRMWKind rmwKind;
+  switch (kind) {
+  case ArgCompareKind::Custom:
+    llvm_unreachable("Custom kind should not reach here");
+  case ArgCompareKind::FloatArgmax:
+    rmwKind = arith::AtomicRMWKind::maximumf;
+    break;
+  case ArgCompareKind::FloatArgmin:
+    rmwKind = arith::AtomicRMWKind::minimumf;
+    break;
+  case ArgCompareKind::SignedArgmax:
+    rmwKind = arith::AtomicRMWKind::maxs;
+    break;
+  case ArgCompareKind::SignedArgmin:
+    rmwKind = arith::AtomicRMWKind::mins;
+    break;
+  case ArgCompareKind::UnsignedArgmax:
+    rmwKind = arith::AtomicRMWKind::maxu;
+    break;
+  case ArgCompareKind::UnsignedArgmin:
+    rmwKind = arith::AtomicRMWKind::minu;
+    break;
+  }
+
+  Location loc = getLoc();
+  Value input = getInputValue();
+  Value outVal = outputValue();
+  Value outIdx = outputIndex();
+  int64_t reductionDim = getDimension();
+
+  ShapedType inputType = getInputType();
+  ShapedType outValType = getOutputValueType();
+  ShapedType outIdxType = getOutputIndexType();
+
+  Type elemType = inputType.getElementType();
+  Type idxElemType = outIdxType.getElementType();
+  int64_t rank = inputType.getRank();
+
+  Value identityVal = arith::getIdentityValue(rmwKind, elemType, b, loc);
+  Value zeroIdx =
+      arith::ConstantOp::create(b, loc, b.getIntegerAttr(idxElemType, 0));
+
+  Value filledVal =
+      linalg::FillOp::create(b, loc, identityVal, outVal).getResult(0);
+  Value filledIdx =
+      linalg::FillOp::create(b, loc, zeroIdx, outIdx).getResult(0);
+
+  SmallVector<AffineExpr> inputExprs, outputExprs;
+  for (int64_t i = 0; i < rank; ++i) {
+    inputExprs.push_back(b.getAffineDimExpr(i));
+    if (i != reductionDim) {
+      outputExprs.push_back(b.getAffineDimExpr(i));
+    }
+  }
+
+  MLIRContext *ctx = b.getContext();
+  AffineMap inputMap = AffineMap::get(rank, 0, inputExprs, ctx);
+  AffineMap outputMap = AffineMap::get(rank, 0, outputExprs, ctx);
+  SmallVector<AffineMap> indexingMaps = {inputMap, outputMap, outputMap};
+
+  SmallVector<utils::IteratorType> iteratorTypes(rank,
+                                                 utils::IteratorType::parallel);
+  iteratorTypes[reductionDim] = utils::IteratorType::reduction;
+
+  auto genericOp = linalg::GenericOp::create(
+      b, loc, TypeRange{outValType, outIdxType}, ValueRange{input},
+      ValueRange{filledVal, filledIdx}, indexingMaps, iteratorTypes,
+      [&](OpBuilder &nestedBuilder, Location nestedLoc, ValueRange args) {
+        Value inputElem = args[0];
+        Value accVal = args[1];
+        Value accIdx = args[2];
+
+        Value idx =
+            linalg::IndexOp::create(nestedBuilder, nestedLoc, reductionDim);
+        Value currentIdx = arith::IndexCastOp::create(nestedBuilder, nestedLoc,
+                                                      idxElemType, idx);
+
+        Value newVal = arith::getReductionOp(rmwKind, nestedBuilder, nestedLoc,
+                                             inputElem, accVal);
+
+        Value cmp;
```

**Comment:**
> https://mlir.llvm.org/docs/Dialects/ArithOps/#arithmaximumf-arithmaximumfop
> 
> > Returns the maximum of the two arguments, treating -0.0 as less than +0.0. If one of the arguments is NaN, then the result is also NaN.
> 
> Actually, `arith.cmpf` + `arith.select` isn't equal to `arith.maximumf` in the two cases mentioned above. Maybe this wouldn't be a legal canonicalization, then.

Nobody cares about positive/negative zero in ML and most implementations don't respect it anyway: https://discourse.llvm.org/t/rfc-a-consistent-set-of-semantics-for-the-floating-point-minimum-and-maximum-operations/89006

This minor difference is safe to ignore.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/test/gpu_generalize_named_ops.mlir:275`

```diff
@@ -168,3 +168,112 @@ func.func @reduce_op(%arg0: tensor<32x128xf32>, %arg1: tensor<32xf32>) -> tensor
 // CHECK:        ^bb0(%[[IN:.+]]: f32, %[[OUT:.+]]: f32):
 // CHECK:          %[[SUM:.+]] = arith.addf %[[IN]], %[[OUT]] : f32
 // CHECK:          linalg.yield %[[SUM]] : f32
+
+// -----
+
+func.func @arg_compare_argmax(%input: tensor<4x128xf32>,
+                              %out_val: tensor<4xf32>,
+                              %out_idx: tensor<4xi32>)
+    -> (tensor<4xf32>, tensor<4xi32>) {
+  %result:2 = iree_linalg_ext.arg_compare
+      dimension(1)
+      ins(%input : tensor<4x128xf32>)
+      outs(%out_val, %out_idx : tensor<4xf32>, tensor<4xi32>) {
+    ^bb0(%a: f32, %b: f32):
+      %cmp = arith.cmpf ogt, %a, %b : f32
+      iree_linalg_ext.yield %cmp : i1
+  } -> tensor<4xf32>, tensor<4xi32>
+  return %result#0, %result#1 : tensor<4xf32>, tensor<4xi32>
+}
+
+// CHECK-DAG:  #[[$MAP:.+]] = affine_map<(d0, d1) -> (d0, d1)>
+// CHECK-DAG:  #[[$MAP1:.+]] = affine_map<(d0, d1) -> (d0)>
+// CHECK-LABEL: func.func @arg_compare_argmax
+// CHECK-SAME:    %[[INPUT:[a-zA-Z0-9_]+]]: tensor<4x128xf32>
+// CHECK-SAME:    %[[OUT_VAL:[a-zA-Z0-9_]+]]: tensor<4xf32>
+// CHECK-SAME:    %[[OUT_IDX:[a-zA-Z0-9_]+]]: tensor<4xi32>
+// CHECK:        linalg.generic
+// CHECK-SAME:   indexing_maps = [#[[$MAP]], #[[$MAP1]], #[[$MAP1]]]
+// CHECK-SAME:   iterator_types = ["parallel", "reduction"]
+// CHECK-SAME:   ins(%[[INPUT]] : tensor<4x128xf32>)
+// CHECK-SAME:   outs(%[[OUT_VAL]], %[[OUT_IDX]] : tensor<4xf32>, tensor<4xi32>)
+// CHECK:        ^bb0(%[[IN:.+]]: f32, %[[ACC_VAL:.+]]: f32, %[[ACC_IDX:.+]]: i32):
+// CHECK:          %[[IDX:.+]] = linalg.index 1 : index
+// CHECK:          %[[IDX_CAST:.+]] = arith.index_cast %[[IDX]] : index to i32
+// CHECK:          %[[CMP:.+]] = arith.cmpf ogt, %[[IN]], %[[ACC_VAL]] : f32
+// CHECK:          %[[NEW_VAL:.+]] = arith.select %[[CMP]], %[[IN]], %[[ACC_VAL]] : f32
+// CHECK:          %[[NEW_IDX:.+]] = arith.select %[[CMP]], %[[IDX_CAST]], %[[ACC_IDX]] : i32
+// CHECK:          linalg.yield %[[NEW_VAL]], %[[NEW_IDX]] : f32, i32
+
+// -----
+
+func.func @arg_compare_with_index_base(%input: tensor<4x128xf32>,
+                                        %out_val: tensor<4xf32>,
+                                        %out_idx: tensor<4xi32>,
+                                        %index_base: index)
+    -> (tensor<4xf32>, tensor<4xi32>) {
+  %result:2 = iree_linalg_ext.arg_compare
+      dimension(1)
+      ins(%input : tensor<4x128xf32>)
+      outs(%out_val, %out_idx : tensor<4xf32>, tensor<4xi32>)
+      index_base(%index_base : index) {
+    ^bb0(%a: f32, %b: f32):
+      %cmp = arith.cmpf ogt, %a, %b : f32
+      iree_linalg_ext.yield %cmp : i1
+  } -> tensor<4xf32>, tensor<4xi32>
+  return %result#0, %result#1 : tensor<4xf32>, tensor<4xi32>
+}
+
+// CHECK-DAG:  #[[$MAP:.+]] = affine_map<(d0, d1) -> (d0, d1)>
+// CHECK-DAG:  #[[$MAP1:.+]] = affine_map<(d0, d1) -> (d0)>
+// CHECK-LABEL: func.func @arg_compare_with_index_base
+// CHECK-SAME:    %[[INPUT:[a-zA-Z0-9_]+]]: tensor<4x128xf32>
+// CHECK-SAME:    %[[OUT_VAL:[a-zA-Z0-9_]+]]: tensor<4xf32>
+// CHECK-SAME:    %[[OUT_IDX:[a-zA-Z0-9_]+]]: tensor<4xi32>
+// CHECK-SAME:    %[[INDEX_BASE:[a-zA-Z0-9_]+]]: index
+// CHECK:        linalg.generic
+// CHECK-SAME:   indexing_maps = [#[[$MAP]], #[[$MAP1]], #[[$MAP1]]]
+// CHECK-SAME:   iterator_types = ["parallel", "reduction"]
+// CHECK:        ^bb0(%[[IN:.+]]: f32, %[[ACC_VAL:.+]]: f32, %[[ACC_IDX:.+]]: i32):
+// CHECK:          %[[IDX:.+]] = linalg.index 1 : index
+// CHECK:          %[[IDX_OFFSET:.+]] = arith.addi %[[IDX]], %[[INDEX_BASE]] : index
+// CHECK:          %[[IDX_CAST:.+]] = arith.index_cast %[[IDX_OFFSET]] : index to i32
+// CHECK:          %[[CMP:.+]] = arith.cmpf ogt, %[[IN]], %[[ACC_VAL]] : f32
+// CHECK:          %[[NEW_VAL:.+]] = arith.select %[[CMP]], %[[IN]], %[[ACC_VAL]] : f32
+// CHECK:          %[[NEW_IDX:.+]] = arith.select %[[CMP]], %[[IDX_CAST]], %[[ACC_IDX]] : i32
+// CHECK:          linalg.yield %[[NEW_VAL]], %[[NEW_IDX]] : f32, i32
+
+// -----
+
+func.func @arg_compare_index_output(%input: tensor<4x128xf32>,
+                                    %out_val: tensor<4xf32>,
+                                    %out_idx: tensor<4xindex>)
+    -> (tensor<4xf32>, tensor<4xindex>) {
+  %result:2 = iree_linalg_ext.arg_compare
+      dimension(1)
+      ins(%input : tensor<4x128xf32>)
+      outs(%out_val, %out_idx : tensor<4xf32>, tensor<4xindex>) {
+    ^bb0(%a: f32, %b: f32):
+      %cmp = arith.cmpf ogt, %a, %b : f32
+      iree_linalg_ext.yield %cmp : i1
+  } -> tensor<4xf32>, tensor<4xindex>
+  return %result#0, %result#1 : tensor<4xf32>, tensor<4xindex>
+}
+
+// CHECK-DAG:  #[[$MAP:.+]] = affine_map<(d0, d1) -> (d0, d1)>
+// CHECK-DAG:  #[[$MAP1:.+]] = affine_map<(d0, d1) -> (d0)>
+// CHECK-LABEL: func.func @arg_compare_index_output
+// CHECK-SAME:    %[[INPUT:[a-zA-Z0-9_]+]]: tensor<4x128xf32>
+// CHECK-SAME:    %[[OUT_VAL:[a-zA-Z0-9_]+]]: tensor<4xf32>
+// CHECK-SAME:    %[[OUT_IDX:[a-zA-Z0-9_]+]]: tensor<4xindex>
+// CHECK:        linalg.generic
+// CHECK-SAME:   indexing_maps = [#[[$MAP]], #[[$MAP1]], #[[$MAP1]]]
+// CHECK-SAME:   iterator_types = ["parallel", "reduction"]
+// CHECK-SAME:   outs(%[[OUT_VAL]], %[[OUT_IDX]] : tensor<4xf32>, tensor<4xindex>)
+// CHECK:        ^bb0(%[[IN:.+]]: f32, %[[ACC_VAL:.+]]: f32, %[[ACC_IDX:.+]]: index):
+// CHECK:          %[[IDX:.+]] = linalg.index 1 : index
+// CHECK-NOT:      arith.index_cast
```

**Comment:**
I don't think we need this: index to index cast would fail to verify anyway

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUGeneralizeNamedOps.cpp:79`

```diff
@@ -68,6 +70,19 @@ struct GPUGeneralizeNamedOpsPass final
     if (failed(generalizeCandidates(&getContext(), namedOpCandidates))) {
       return signalPassFailure();
     }
+
+    SmallVector<IREE::LinalgExt::ArgCompareOp> argCompareOps;
+    funcOp.walk(
+        [&](IREE::LinalgExt::ArgCompareOp op) { argCompareOps.push_back(op); });
+
+    IRRewriter rewriter(&getContext());
+    for (auto argCompareOp : argCompareOps) {
```

**Comment:**
nit: spell out this type, per https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/Transforms/GeneralizeArgCompare.cpp:1`

```diff
@@ -0,0 +1,90 @@
+// Copyright 2025 The IREE Authors
```

**Comment:**
this should be 2026 now

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/Transforms/Transforms.h:196`

```diff
@@ -188,4 +188,39 @@ FailureOr<linalg::SplitReductionResult>
 splitArgmaxReduction(RewriterBase &rewriter, linalg::GenericOp genericOp,
                      linalg::ControlSplitReductionFn controlSplitReductionFn);
 
+/// Generalize an ArgCompareOp to a linalg.generic by inlining its comparator
+/// region. This transforms the high-level arg_compare operation into a
+/// linalg.generic with explicit index tracking and selection logic.
+///
+/// Example:
+/// ```
```

**Comment:**
```suggestion
/// ```mlir
```

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/Transforms/Transforms.h:206`

```diff
@@ -188,4 +188,39 @@ FailureOr<linalg::SplitReductionResult>
 splitArgmaxReduction(RewriterBase &rewriter, linalg::GenericOp genericOp,
                      linalg::ControlSplitReductionFn controlSplitReductionFn);
 
+/// Generalize an ArgCompareOp to a linalg.generic by inlining its comparator
+/// region. This transforms the high-level arg_compare operation into a
+/// linalg.generic with explicit index tracking and selection logic.
+///
+/// Example:
+/// ```
+/// %result:2 = iree_linalg_ext.arg_compare dimension(1)
+///     ins(%input : tensor<4x128xf32>)
+///     outs(%out_val, %out_idx : tensor<4xf32>, tensor<4xi32>) {
+///   ^bb0(%a: f32, %b: f32):
+///     %cmp = arith.cmpf ogt, %a, %b : f32
+///     iree_linalg_ext.yield %cmp : i1
+/// } -> tensor<4xf32>, tensor<4xi32>
+/// ```
+/// Becomes:
+/// ```
```

**Comment:**
```suggestion
/// ```mlir
```

---

**File:** `tests/external/iree-test-suites/torch_models/llama_8b_fp16/modules/scheduler_gfx942.json:6`

```diff
@@ -1,11 +1,11 @@
 {
   "mlir": "https://sharkpublic.blob.core.windows.net/sharkpublic/iree-test-suites/torch-models/llama_8b_fp16/greedy_scheduler.mlir",
   "compiler_flags": [
-      "--iree-hal-target-device=hip",
-      "--iree-hip-target=gfx942",
-      "--iree-opt-level=O1",
-      "--iree-stream-resource-memory-model=discrete",
-      "--iree-hal-indirect-command-buffers=true",
-      "--iree-hal-memoization=true"
+    "--iree-hal-target-device=hip",
+    "--iree-hip-target=gfx942",
+    "--iree-opt-level=O3",
```

**Comment:**
+1, this should be kept out

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/test/gpu_generalize_named_ops.mlir:257`

```diff
@@ -168,3 +168,112 @@ func.func @reduce_op(%arg0: tensor<32x128xf32>, %arg1: tensor<32xf32>) -> tensor
 // CHECK:        ^bb0(%[[IN:.+]]: f32, %[[OUT:.+]]: f32):
 // CHECK:          %[[SUM:.+]] = arith.addf %[[IN]], %[[OUT]] : f32
 // CHECK:          linalg.yield %[[SUM]] : f32
+
+// -----
+
+func.func @arg_compare_argmax(%input: tensor<4x128xf32>,
+                              %out_val: tensor<4xf32>,
+                              %out_idx: tensor<4xi32>)
+    -> (tensor<4xf32>, tensor<4xi32>) {
+  %result:2 = iree_linalg_ext.arg_compare
+      dimension(1)
+      ins(%input : tensor<4x128xf32>)
+      outs(%out_val, %out_idx : tensor<4xf32>, tensor<4xi32>) {
+    ^bb0(%a: f32, %b: f32):
+      %cmp = arith.cmpf ogt, %a, %b : f32
+      iree_linalg_ext.yield %cmp : i1
+  } -> tensor<4xf32>, tensor<4xi32>
+  return %result#0, %result#1 : tensor<4xf32>, tensor<4xi32>
+}
+
+// CHECK-DAG:  #[[$MAP:.+]] = affine_map<(d0, d1) -> (d0, d1)>
+// CHECK-DAG:  #[[$MAP1:.+]] = affine_map<(d0, d1) -> (d0)>
+// CHECK-LABEL: func.func @arg_compare_argmax
+// CHECK-SAME:    %[[INPUT:[a-zA-Z0-9_]+]]: tensor<4x128xf32>
+// CHECK-SAME:    %[[OUT_VAL:[a-zA-Z0-9_]+]]: tensor<4xf32>
+// CHECK-SAME:    %[[OUT_IDX:[a-zA-Z0-9_]+]]: tensor<4xi32>
+// CHECK:        linalg.generic
+// CHECK-SAME:   indexing_maps = [#[[$MAP]], #[[$MAP1]], #[[$MAP1]]]
+// CHECK-SAME:   iterator_types = ["parallel", "reduction"]
+// CHECK-SAME:   ins(%[[INPUT]] : tensor<4x128xf32>)
+// CHECK-SAME:   outs(%[[OUT_VAL]], %[[OUT_IDX]] : tensor<4xf32>, tensor<4xi32>)
+// CHECK:        ^bb0(%[[IN:.+]]: f32, %[[ACC_VAL:.+]]: f32, %[[ACC_IDX:.+]]: i32):
+// CHECK:          %[[IDX:.+]] = linalg.index 1 : index
+// CHECK:          %[[IDX_CAST:.+]] = arith.index_cast %[[IDX]] : index to i32
+// CHECK:          %[[CMP:.+]] = arith.cmpf ogt, %[[IN]], %[[ACC_VAL]] : f32
+// CHECK:          %[[NEW_VAL:.+]] = arith.select %[[CMP]], %[[IN]], %[[ACC_VAL]] : f32
+// CHECK:          %[[NEW_IDX:.+]] = arith.select %[[CMP]], %[[IDX_CAST]], %[[ACC_IDX]] : i32
+// CHECK:          linalg.yield %[[NEW_VAL]], %[[NEW_IDX]] : f32, i32
+
+// -----
+
+func.func @arg_compare_with_index_base(%input: tensor<4x128xf32>,
+                                        %out_val: tensor<4xf32>,
+                                        %out_idx: tensor<4xi32>,
+                                        %index_base: index)
+    -> (tensor<4xf32>, tensor<4xi32>) {
+  %result:2 = iree_linalg_ext.arg_compare
+      dimension(1)
+      ins(%input : tensor<4x128xf32>)
+      outs(%out_val, %out_idx : tensor<4xf32>, tensor<4xi32>)
+      index_base(%index_base : index) {
+    ^bb0(%a: f32, %b: f32):
+      %cmp = arith.cmpf ogt, %a, %b : f32
+      iree_linalg_ext.yield %cmp : i1
+  } -> tensor<4xf32>, tensor<4xi32>
+  return %result#0, %result#1 : tensor<4xf32>, tensor<4xi32>
+}
+
+// CHECK-DAG:  #[[$MAP:.+]] = affine_map<(d0, d1) -> (d0, d1)>
+// CHECK-DAG:  #[[$MAP1:.+]] = affine_map<(d0, d1) -> (d0)>
+// CHECK-LABEL: func.func @arg_compare_with_index_base
+// CHECK-SAME:    %[[INPUT:[a-zA-Z0-9_]+]]: tensor<4x128xf32>
+// CHECK-SAME:    %[[OUT_VAL:[a-zA-Z0-9_]+]]: tensor<4xf32>
+// CHECK-SAME:    %[[OUT_IDX:[a-zA-Z0-9_]+]]: tensor<4xi32>
+// CHECK-SAME:    %[[INDEX_BASE:[a-zA-Z0-9_]+]]: index
+// CHECK:        linalg.generic
+// CHECK-SAME:   indexing_maps = [#[[$MAP]], #[[$MAP1]], #[[$MAP1]]]
+// CHECK-SAME:   iterator_types = ["parallel", "reduction"]
+// CHECK:        ^bb0(%[[IN:.+]]: f32, %[[ACC_VAL:.+]]: f32, %[[ACC_IDX:.+]]: i32):
+// CHECK:          %[[IDX:.+]] = linalg.index 1 : index
+// CHECK:          %[[IDX_OFFSET:.+]] = arith.addi %[[IDX]], %[[INDEX_BASE]] : index
+// CHECK:          %[[IDX_CAST:.+]] = arith.index_cast %[[IDX_OFFSET]] : index to i32
+// CHECK:          %[[CMP:.+]] = arith.cmpf ogt, %[[IN]], %[[ACC_VAL]] : f32
+// CHECK:          %[[NEW_VAL:.+]] = arith.select %[[CMP]], %[[IN]], %[[ACC_VAL]] : f32
+// CHECK:          %[[NEW_IDX:.+]] = arith.select %[[CMP]], %[[IDX_CAST]], %[[ACC_IDX]] : i32
+// CHECK:          linalg.yield %[[NEW_VAL]], %[[NEW_IDX]] : f32, i32
+
+// -----
+
+func.func @arg_compare_index_output(%input: tensor<4x128xf32>,
+                                    %out_val: tensor<4xf32>,
+                                    %out_idx: tensor<4xindex>)
+    -> (tensor<4xf32>, tensor<4xindex>) {
+  %result:2 = iree_linalg_ext.arg_compare
+      dimension(1)
+      ins(%input : tensor<4x128xf32>)
+      outs(%out_val, %out_idx : tensor<4xf32>, tensor<4xindex>) {
+    ^bb0(%a: f32, %b: f32):
+      %cmp = arith.cmpf ogt, %a, %b : f32
```

**Comment:**
Can we also have a testcase for at least one integer case?

---


---


## [PR #23012](https://github.com/iree-org/iree/pull/23012): [DispatchCreation] Fold extract_slice of broadcast during split reduction tiling

### Review Summary

**COMMENTED** (2026-01-03)

Can we write a test for this?


---


## [PR #23011](https://github.com/iree-org/iree/pull/23011): [AMDGPU][LDS] Prefer contiguous subviews in `GPUConvertToCoalescedDMA`

### Review Summary

**COMMENTED** (2026-01-06)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUConvertToCoalescedDMA.cpp:580`

```diff
@@ -560,6 +560,54 @@ struct GPUConvertToCoalescedDMAPass final
   }
 
 private:
+  /// Compute tile sizes for subgroup-level distribution.
+  /// Returns {tileSizes, numTiledDims}.
+  ///
+  /// We keep the innermost dimension whole (not tiled) to ensure contiguous
+  /// memory access patterns, and greedily redistribute warps to outer
+  /// dimensions.
+  std::pair<SmallVector<OpFoldResult>, int64_t>
+  computeSubgroupTileSizes(IRRewriter &rewriter, ArrayRef<int64_t> shape,
+                           ArrayRef<int64_t> numWarps) {
+    SmallVector<OpFoldResult> tileSizes;
+    int64_t numTiledDims = 0;
+    int64_t rank = shape.size();
+
+    // Calculate total number of warps available.
+    // Note: numWarps may contain 0s for dimensions where wgSize < subgroupSize.
+    // We treat 0 as 1 for the purpose of counting total warps.
+    int64_t totalWarps = 1;
+    for (int64_t nw : numWarps) {
```

**Comment:**
also use `llvm::product_of`

---


---


## [PR #23008](https://github.com/iree-org/iree/pull/23008): [CI] Upgrade to GCC 11 to fix stablehlo template instantiation error

### Review Summary

**APPROVED** (2026-01-02)

Version bump sgtm, gcc11 came out almost 5 years ago.


---


## [PR #23002](https://github.com/iree-org/iree/pull/23002): [Codegen][ROCDL] Fix int64 overflow in `getSpannedBytes`

### Review Summary

**APPROVED** (2025-12-31)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/ROCDLConfigureBufferInstructions.cpp:101`

```diff
@@ -82,21 +83,31 @@ getSpannedBytes(IREE::HAL::InterfaceBindingSubspanOp binding,
           dyn_cast<IREE::TensorExt::DispatchTensorType>(binding.getType())) {
     resultTy = tensorType.asRankedTensorType();
   }
-  if (!resultTy || !resultTy.hasRank())
+  if (!resultTy || !resultTy.hasRank()) {
     return std::nullopt;
+  }
   for (Value dynArg : binding.getResultDynamicDims(0)) {
     FailureOr<int64_t> dimMax = getDynamicUpperBound(dynArg, solver);
-    if (failed(dimMax))
+    if (failed(dimMax)) {
+      return std::nullopt;
+    }
+    if (llvm::MulOverflow(maxNumElems, *dimMax, maxNumElems)) {
       return std::nullopt;
-    maxNumElems *= (*dimMax);
+    }
   }
   for (int64_t dim : resultTy.getShape()) {
-    if (ShapedType::isDynamic(dim))
+    if (ShapedType::isDynamic(dim)) {
       continue;
-    maxNumElems *= dim;
+    }
```

**Comment:**
btw, you can also write this as:
`for (int64_t dim : llvm::make_filter_range(resultTy.getShape(), ShapedType::IsStatic)) {`

---


---


## [PR #22997](https://github.com/iree-org/iree/pull/22997): [VM] Fix integer to f64 cast folding

### Review Summary

**APPROVED** (2025-12-30)


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/VM/IR/VMOpFolders.cpp:1717`

```diff
@@ -1714,7 +1714,7 @@ OpFoldResult CastSI64F64Op::fold(FoldAdaptor operands) {
   return constFoldCastOp<IntegerAttr, FloatAttr>(
       Float64Type::get(getContext()), operands.getOperand(),
       [&](const APInt &a) {
-        APFloat b = APFloat(0.0f);
+        APFloat b = APFloat(0.0);
```

**Comment:**
```suggestion
        APFloat b(0.0);
```

---

**File:** `compiler/src/iree/compiler/Dialect/VM/IR/VMOpFolders.cpp:1727`

```diff
@@ -1724,7 +1724,7 @@ OpFoldResult CastUI64F64Op::fold(FoldAdaptor operands) {
   return constFoldCastOp<IntegerAttr, FloatAttr>(
       Float64Type::get(getContext()), operands.getOperand(),
       [&](const APInt &a) {
-        APFloat b = APFloat(0.0f);
+        APFloat b = APFloat(0.0);
```

**Comment:**
```suggestion
        APFloat b(0.0);
```

---


---


## [PR #22996](https://github.com/iree-org/iree/pull/22996): Revert "[Codegen][ROCDL] Improve dynamic dimension bounds handling in ROCDLConfigureBufferInstructions (#22892)"

### Review Summary

**COMMENTED** (2025-12-30)

Do we know any more details? Are you confident it's this PR that caused the regression?


---


## [PR #22995](https://github.com/iree-org/iree/pull/22995): [AMDGPU][LDS] Unify DMA transfer linearization based on destination contiguity

### Review Summary

**COMMENTED** (2026-01-05)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/test/amdgpu_lower_coalesced_dma_to_gather_lds.mlir:227`

```diff
@@ -173,6 +185,55 @@ func.func @lower_coalesced_copy_dma_1d(
 
 #translation_32 = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [32, 1, 1] subgroup_size = 32>
 
+// Test: Single-row 2D memref (1x128) with 32 lanes.
+// This verifies that a 2D memref with only 1 row produces a single transfer,
+// equivalent to the 1D case. The delinearization should still work correctly.
+//   * Elements per lane = 128 / 32 = 4 (each lane reads 4 contiguous f32s)
+//   * Only one gather_to_lds op (single row = single transfer)
+//
+// CHECK-LABEL: func.func @lower_coalesced_copy_dma_single_row_2d
+// CHECK-SAME:    %[[SRC:[a-zA-Z0-9]+]]: memref<1x128xf32, #amdgpu.address_space<fat_raw_buffer>>
+// CHECK-SAME:    %[[DST:[a-zA-Z0-9]+]]: memref<1x128xf32, #gpu.address_space<workgroup>>
+func.func @lower_coalesced_copy_dma_single_row_2d(
+    %source: memref<1x128xf32, #amdgpu.address_space<fat_raw_buffer>>,
+    %dest: memref<1x128xf32, #gpu.address_space<workgroup>>)
+  attributes {
+    hal.executable.target = #executable_target_rocm_hsaco_fb,
+    translation_info = #translation_32} {
+  // CHECK: scf.forall (%[[LANE_ID:[a-zA-Z0-9]+]]) in (32)
+  scf.forall (%arg6) in (32) {
+    // CHECK: %[[C4:[a-zA-Z0-9_]+]] = arith.constant 4
+    // CHECK: %[[LANE_OFFSET:[a-zA-Z0-9_]+]] = arith.muli %[[LANE_ID]], %[[C4]]
+    //
+    // Single transfer: linear offset 0 â†’ [0, 0]
+    // CHECK: %[[C0:.+]] = arith.constant 0 : index
+    // CHECK: %[[DELINEAR:.+]]:2 = affine.delinearize_index %[[C0]] into (1, 128)
+    // CHECK: %[[SRC_COL:.+]] = arith.addi %[[DELINEAR]]#1, %[[LANE_OFFSET]]
+    // CHECK: amdgpu.gather_to_lds %[[SRC]][%[[DELINEAR]]#0, %[[SRC_COL]]], %[[DST]][%[[DELINEAR]]#0, %[[DELINEAR]]#1] : vector<4xf32>
+    // CHECK-NOT: amdgpu.gather_to_lds
+    // CHECK-NOT: iree_gpu.coalesced_gather_dma
+    iree_gpu.coalesced_gather_dma %source into %dest lane(%arg6) : memref<1x128xf32, #amdgpu.address_space<fat_raw_buffer>>, memref<1x128xf32, #gpu.address_space<workgroup>>, index
+  } {mapping = [#gpu.thread<linear_dim_0>]}
+  return
+}
+
+// -----
+
+#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm",
+  "rocm-hsaco-fb", {iree_codegen.target_info = #iree_gpu.target<
+  arch = "gfx950", features = "", wgp = <
+    compute = fp64|fp32|fp16|int64|int32|int16|int8,
+    storage = b64|b32|b16|b8, subgroup = shuffle|arithmetic,
+    dot = dp4xi8toi32, mma = [], subgroup_size_choices = [32, 32],
```

**Comment:**
Can we trim these down? For example, do these tests care about all the compute types/subgroup operations/dot product ops, or would `dot = none` work?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/test/amdgpu_lower_coalesced_dma_to_gather_lds.mlir:227`

```diff
@@ -173,6 +185,55 @@ func.func @lower_coalesced_copy_dma_1d(
 
 #translation_32 = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [32, 1, 1] subgroup_size = 32>
 
+// Test: Single-row 2D memref (1x128) with 32 lanes.
+// This verifies that a 2D memref with only 1 row produces a single transfer,
+// equivalent to the 1D case. The delinearization should still work correctly.
+//   * Elements per lane = 128 / 32 = 4 (each lane reads 4 contiguous f32s)
+//   * Only one gather_to_lds op (single row = single transfer)
+//
+// CHECK-LABEL: func.func @lower_coalesced_copy_dma_single_row_2d
+// CHECK-SAME:    %[[SRC:[a-zA-Z0-9]+]]: memref<1x128xf32, #amdgpu.address_space<fat_raw_buffer>>
+// CHECK-SAME:    %[[DST:[a-zA-Z0-9]+]]: memref<1x128xf32, #gpu.address_space<workgroup>>
+func.func @lower_coalesced_copy_dma_single_row_2d(
+    %source: memref<1x128xf32, #amdgpu.address_space<fat_raw_buffer>>,
+    %dest: memref<1x128xf32, #gpu.address_space<workgroup>>)
+  attributes {
+    hal.executable.target = #executable_target_rocm_hsaco_fb,
+    translation_info = #translation_32} {
+  // CHECK: scf.forall (%[[LANE_ID:[a-zA-Z0-9]+]]) in (32)
+  scf.forall (%arg6) in (32) {
+    // CHECK: %[[C4:[a-zA-Z0-9_]+]] = arith.constant 4
+    // CHECK: %[[LANE_OFFSET:[a-zA-Z0-9_]+]] = arith.muli %[[LANE_ID]], %[[C4]]
+    //
+    // Single transfer: linear offset 0 â†’ [0, 0]
+    // CHECK: %[[C0:.+]] = arith.constant 0 : index
+    // CHECK: %[[DELINEAR:.+]]:2 = affine.delinearize_index %[[C0]] into (1, 128)
+    // CHECK: %[[SRC_COL:.+]] = arith.addi %[[DELINEAR]]#1, %[[LANE_OFFSET]]
+    // CHECK: amdgpu.gather_to_lds %[[SRC]][%[[DELINEAR]]#0, %[[SRC_COL]]], %[[DST]][%[[DELINEAR]]#0, %[[DELINEAR]]#1] : vector<4xf32>
+    // CHECK-NOT: amdgpu.gather_to_lds
+    // CHECK-NOT: iree_gpu.coalesced_gather_dma
+    iree_gpu.coalesced_gather_dma %source into %dest lane(%arg6) : memref<1x128xf32, #amdgpu.address_space<fat_raw_buffer>>, memref<1x128xf32, #gpu.address_space<workgroup>>, index
+  } {mapping = [#gpu.thread<linear_dim_0>]}
+  return
+}
+
+// -----
+
+#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm",
+  "rocm-hsaco-fb", {iree_codegen.target_info = #iree_gpu.target<
+  arch = "gfx950", features = "", wgp = <
+    compute = fp64|fp32|fp16|int64|int32|int16|int8,
+    storage = b64|b32|b16|b8, subgroup = shuffle|arithmetic,
+    dot = dp4xi8toi32, mma = [], subgroup_size_choices = [32, 32],
```

**Comment:**
Same elsewhere

---


---


## [PR #22994](https://github.com/iree-org/iree/pull/22994): [LinalgExt] Fix map_scatter verifier

### Review Summary

**COMMENTED** (2025-12-30)

**COMMENTED** (2026-01-07)


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:642`

```diff
@@ -636,6 +636,10 @@ LogicalResult MapScatterOp::verify() {
                     llvm::IsaPred<IndexType>)) {
     return emitOpError("expected block arguments to be index types");
   }
+  if (!transformBody.mightHaveTerminator() ||
+      !isa<IREE::LinalgExt::YieldOp>(transformBody.getTerminator())) {
+    return emitOpError("expected transformation_region to have a terminator");
+  }
```

**Comment:**
FTR, I don't have strong opinions on this either way

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/test/invalid.mlir:567`

```diff
@@ -555,6 +555,19 @@ func.func @map_scatter_wrong_mask_type(
 
 // -----
 
+func.func @map_scatter_no_terminator(
+    %input: memref<4xf32>, %output: memref<4xf32>
+){
+  // expected-error@+1 {{expected transformation_region to have a terminator}}
+  "iree_linalg_ext.map_scatter"(%input, %output) ({
+  ^bb0(%idx0: index):
+    %0 = "arith.constant"() <{value = true}> : () -> i1
+  }) : (memref<4xf32>, memref<4xf32>) -> ()
```

**Comment:**
Do we have to use the generic IR format here?

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/test/invalid.mlir:567`

```diff
@@ -555,6 +555,19 @@ func.func @map_scatter_wrong_mask_type(
 
 // -----
 
+func.func @map_scatter_no_terminator(
+    %input: memref<4xf32>, %output: memref<4xf32>
+){
+  // expected-error@+1 {{expected transformation_region to have a terminator}}
+  "iree_linalg_ext.map_scatter"(%input, %output) ({
+  ^bb0(%idx0: index):
+    %0 = "arith.constant"() <{value = true}> : () -> i1
+  }) : (memref<4xf32>, memref<4xf32>) -> ()
```

**Comment:**
Maybe add a code comment that explains this?

---


---


## [PR #22989](https://github.com/iree-org/iree/pull/22989): Bump dawidd6/action-download-artifact from 11 to 12 in the github-actions group

### Review Summary

**APPROVED** (2025-12-30)


---


## [PR #22985](https://github.com/iree-org/iree/pull/22985): Integrate LLVM at 75a0347

### Review Summary

**APPROVED** (2025-12-29)


---


## [PR #22953](https://github.com/iree-org/iree/pull/22953): [Codegen] add FoldExtractSliceOfBroadcast pattern to TileAndDistributeToWorkgroups

### Review Summary

**COMMENTED** (2025-12-29)

**COMMENTED** (2025-12-29)

**APPROVED** (2025-12-29)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/test/tile_and_distribute_workgroups_using_forall.mlir:1380`

```diff
@@ -1364,3 +1364,60 @@ attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPU
 //       CHECK:   scf.forall.in_parallel
 //       CHECK:     tensor.parallel_insert_slice %[[RES]] into %[[OUT0]][%[[OFFSET0]], 0, %[[OFFSET1]]]
 //   CHECK: {mapping = [#iree_codegen.workgroup_mapping<y>, #iree_codegen.workgroup_mapping<x>]}
+
+// -----
+
+func.func @arg_compare_fold_broadcast() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUDistribute workgroup_size = [128, 1, 1] subgroup_size = 64>} {
+  %c0_i32 = arith.constant 0 : i32
+  %c0 = arith.constant 0 : index
+  %cst = arith.constant 0xFC00 : f16
+  %c-1 = arith.constant -1 : index
+  %c1336 = arith.constant 1336 : index
+  %c768 = arith.constant 768 : index
+  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x128256xf16>>
+  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x1x96xf16, #hal.descriptor_type<storage_buffer>>
+  %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x1x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x1x96xf16, #amdgpu.address_space<fat_raw_buffer>>
+  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c768) flags(Indirect) : memref<4x1x96xi32, strided<[96, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
```

**Comment:**
Would it be possible to turn these into function arguments to simplify the test?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/Transforms.cpp:377`

```diff
@@ -354,6 +355,108 @@ void populateSwapExtractWithExpandPattern(RewritePatternSet &patterns) {
   patterns.add<SwapExpandShapeWithSlicePattern>(patterns.getContext());
 }
 
+namespace {
+
+/// Pattern to fold extract_slice(broadcast) into the broadcast input when the
+/// extract_slice is rank-reducing and only extracts along the broadcasted
+/// dimensions (with size 1), leaving all non-broadcasted dimensions intact.
+/// This is valid because the broadcast only duplicates data along the
+/// broadcasted dimension, and extracting a single slice from that dimension
+/// gives back the original input.
+///
+/// Example:
+///   %broadcast = linalg.broadcast ins(%in : tensor<4x1xf16>)
+///                                 outs(%out : tensor<4x1x1xf16>) dimensions =
+///                                 [2]
+///   %extract = tensor.extract_slice %broadcast[0, 0, 0] [4, 1, 1] [1, 1, 1]
+///              : tensor<4x1x1xf16> to tensor<4x1xf16>
+/// ->
+///   %extract is replaced by %in (tensor<4x1xf16>)
+struct FoldExtractSliceOfBroadcast final
+    : OpRewritePattern<tensor::ExtractSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
```

**Comment:**
```suggestion
  using Base::Base;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/Transforms.cpp:416`

```diff
@@ -354,6 +355,108 @@ void populateSwapExtractWithExpandPattern(RewritePatternSet &patterns) {
   patterns.add<SwapExpandShapeWithSlicePattern>(patterns.getContext());
 }
 
+namespace {
+
+/// Pattern to fold extract_slice(broadcast) into the broadcast input when the
+/// extract_slice is rank-reducing and only extracts along the broadcasted
+/// dimensions (with size 1), leaving all non-broadcasted dimensions intact.
+/// This is valid because the broadcast only duplicates data along the
+/// broadcasted dimension, and extracting a single slice from that dimension
+/// gives back the original input.
+///
+/// Example:
+///   %broadcast = linalg.broadcast ins(%in : tensor<4x1xf16>)
+///                                 outs(%out : tensor<4x1x1xf16>) dimensions =
+///                                 [2]
+///   %extract = tensor.extract_slice %broadcast[0, 0, 0] [4, 1, 1] [1, 1, 1]
+///              : tensor<4x1x1xf16> to tensor<4x1xf16>
+/// ->
+///   %extract is replaced by %in (tensor<4x1xf16>)
+struct FoldExtractSliceOfBroadcast final
+    : OpRewritePattern<tensor::ExtractSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp extractOp,
+                                PatternRewriter &rewriter) const override {
+    auto broadcastOp =
+        extractOp.getSource().getDefiningOp<linalg::BroadcastOp>();
+    if (!broadcastOp) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "source is not a linalg.broadcast operation");
+    }
+
+    if (!extractOp.hasUnitStride()) {
+      return rewriter.notifyMatchFailure(extractOp,
+                                         "extract_slice has non-unit stride");
+    }
+
+    auto inputType =
+        dyn_cast<RankedTensorType>(broadcastOp.getInput().getType());
+    auto broadcastOutputType =
+        dyn_cast<RankedTensorType>(broadcastOp.getInit().getType());
+    auto extractResultType =
+        dyn_cast<RankedTensorType>(extractOp.getResult().getType());
+    if (!inputType || !broadcastOutputType || !extractResultType) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "operand or result types are not RankedTensorType");
+    }
+
+    // Extract result type must match broadcast input type.
+    if (inputType != extractResultType) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "extract result type does not match broadcast input type");
+    }
+
+    // Verify that we're extracting from offset [0, 0, ..., 0] with the same
+    // shape as the input (essentially undoing the broadcast).
+    SmallVector<OpFoldResult> offsets = extractOp.getMixedOffsets();
+    SmallVector<OpFoldResult> sizes = extractOp.getMixedSizes();
+    if (llvm::any_of(offsets, [](OpFoldResult offset) {
+          return !isConstantIntValue(offset, 0);
+        })) {
```

**Comment:**
```suggestion
    if (!llvm::all_of(offsets, isZeroInteger)) {
```
I think this aligns better with the match failure message.

(I realize this code is moved over, so fine to keep as-is in this PR)

---

**File:** `compiler/src/iree/compiler/Codegen/Common/Transforms.cpp:429`

```diff
@@ -354,6 +355,108 @@ void populateSwapExtractWithExpandPattern(RewritePatternSet &patterns) {
   patterns.add<SwapExpandShapeWithSlicePattern>(patterns.getContext());
 }
 
+namespace {
+
+/// Pattern to fold extract_slice(broadcast) into the broadcast input when the
+/// extract_slice is rank-reducing and only extracts along the broadcasted
+/// dimensions (with size 1), leaving all non-broadcasted dimensions intact.
+/// This is valid because the broadcast only duplicates data along the
+/// broadcasted dimension, and extracting a single slice from that dimension
+/// gives back the original input.
+///
+/// Example:
+///   %broadcast = linalg.broadcast ins(%in : tensor<4x1xf16>)
+///                                 outs(%out : tensor<4x1x1xf16>) dimensions =
+///                                 [2]
+///   %extract = tensor.extract_slice %broadcast[0, 0, 0] [4, 1, 1] [1, 1, 1]
+///              : tensor<4x1x1xf16> to tensor<4x1xf16>
+/// ->
+///   %extract is replaced by %in (tensor<4x1xf16>)
+struct FoldExtractSliceOfBroadcast final
+    : OpRewritePattern<tensor::ExtractSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp extractOp,
+                                PatternRewriter &rewriter) const override {
+    auto broadcastOp =
+        extractOp.getSource().getDefiningOp<linalg::BroadcastOp>();
+    if (!broadcastOp) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "source is not a linalg.broadcast operation");
+    }
+
+    if (!extractOp.hasUnitStride()) {
+      return rewriter.notifyMatchFailure(extractOp,
+                                         "extract_slice has non-unit stride");
+    }
+
+    auto inputType =
+        dyn_cast<RankedTensorType>(broadcastOp.getInput().getType());
+    auto broadcastOutputType =
+        dyn_cast<RankedTensorType>(broadcastOp.getInit().getType());
+    auto extractResultType =
+        dyn_cast<RankedTensorType>(extractOp.getResult().getType());
+    if (!inputType || !broadcastOutputType || !extractResultType) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "operand or result types are not RankedTensorType");
+    }
+
+    // Extract result type must match broadcast input type.
+    if (inputType != extractResultType) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "extract result type does not match broadcast input type");
+    }
+
+    // Verify that we're extracting from offset [0, 0, ..., 0] with the same
+    // shape as the input (essentially undoing the broadcast).
+    SmallVector<OpFoldResult> offsets = extractOp.getMixedOffsets();
+    SmallVector<OpFoldResult> sizes = extractOp.getMixedSizes();
+    if (llvm::any_of(offsets, [](OpFoldResult offset) {
+          return !isConstantIntValue(offset, 0);
+        })) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "extract_slice offsets are not all zeros");
+    }
+
+    // Sizes should match input dimensions (accounting for broadcast dims).
+    ArrayRef<int64_t> broadcastDims = broadcastOp.getDimensions();
+    int64_t broadcastRank = broadcastOutputType.getRank();
+
+    // Verify that for broadcast dimensions, the size is 1.
+    if (llvm::any_of(broadcastDims, [&](int64_t broadcastDim) {
+          return !isConstantIntValue(sizes[broadcastDim], 1);
+        })) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "broadcast dimensions do not all have size 1");
```

**Comment:**
Similar here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/Transforms.cpp:435`

```diff
@@ -354,6 +355,108 @@ void populateSwapExtractWithExpandPattern(RewritePatternSet &patterns) {
   patterns.add<SwapExpandShapeWithSlicePattern>(patterns.getContext());
 }
 
+namespace {
+
+/// Pattern to fold extract_slice(broadcast) into the broadcast input when the
+/// extract_slice is rank-reducing and only extracts along the broadcasted
+/// dimensions (with size 1), leaving all non-broadcasted dimensions intact.
+/// This is valid because the broadcast only duplicates data along the
+/// broadcasted dimension, and extracting a single slice from that dimension
+/// gives back the original input.
+///
+/// Example:
+///   %broadcast = linalg.broadcast ins(%in : tensor<4x1xf16>)
+///                                 outs(%out : tensor<4x1x1xf16>) dimensions =
+///                                 [2]
+///   %extract = tensor.extract_slice %broadcast[0, 0, 0] [4, 1, 1] [1, 1, 1]
+///              : tensor<4x1x1xf16> to tensor<4x1xf16>
+/// ->
+///   %extract is replaced by %in (tensor<4x1xf16>)
+struct FoldExtractSliceOfBroadcast final
+    : OpRewritePattern<tensor::ExtractSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(tensor::ExtractSliceOp extractOp,
+                                PatternRewriter &rewriter) const override {
+    auto broadcastOp =
+        extractOp.getSource().getDefiningOp<linalg::BroadcastOp>();
+    if (!broadcastOp) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "source is not a linalg.broadcast operation");
+    }
+
+    if (!extractOp.hasUnitStride()) {
+      return rewriter.notifyMatchFailure(extractOp,
+                                         "extract_slice has non-unit stride");
+    }
+
+    auto inputType =
+        dyn_cast<RankedTensorType>(broadcastOp.getInput().getType());
+    auto broadcastOutputType =
+        dyn_cast<RankedTensorType>(broadcastOp.getInit().getType());
+    auto extractResultType =
+        dyn_cast<RankedTensorType>(extractOp.getResult().getType());
+    if (!inputType || !broadcastOutputType || !extractResultType) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "operand or result types are not RankedTensorType");
+    }
+
+    // Extract result type must match broadcast input type.
+    if (inputType != extractResultType) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "extract result type does not match broadcast input type");
+    }
+
+    // Verify that we're extracting from offset [0, 0, ..., 0] with the same
+    // shape as the input (essentially undoing the broadcast).
+    SmallVector<OpFoldResult> offsets = extractOp.getMixedOffsets();
+    SmallVector<OpFoldResult> sizes = extractOp.getMixedSizes();
+    if (llvm::any_of(offsets, [](OpFoldResult offset) {
+          return !isConstantIntValue(offset, 0);
+        })) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "extract_slice offsets are not all zeros");
+    }
+
+    // Sizes should match input dimensions (accounting for broadcast dims).
+    ArrayRef<int64_t> broadcastDims = broadcastOp.getDimensions();
+    int64_t broadcastRank = broadcastOutputType.getRank();
+
+    // Verify that for broadcast dimensions, the size is 1.
+    if (llvm::any_of(broadcastDims, [&](int64_t broadcastDim) {
+          return !isConstantIntValue(sizes[broadcastDim], 1);
+        })) {
+      return rewriter.notifyMatchFailure(
+          extractOp, "broadcast dimensions do not all have size 1");
+    }
+
+    // Collect the indices of dimensions in the broadcast output that were not
+    // broadcasted (i.e., dimensions that existed in the original input).
+    auto nonBroadcastDims = llvm::to_vector(llvm::make_filter_range(
```

**Comment:**
`llvm::filter_vector`

---

**File:** `compiler/src/iree/compiler/Codegen/Common/Transforms.cpp:377`

```diff
@@ -354,6 +355,108 @@ void populateSwapExtractWithExpandPattern(RewritePatternSet &patterns) {
   patterns.add<SwapExpandShapeWithSlicePattern>(patterns.getContext());
 }
 
+namespace {
+
+/// Pattern to fold extract_slice(broadcast) into the broadcast input when the
+/// extract_slice is rank-reducing and only extracts along the broadcasted
+/// dimensions (with size 1), leaving all non-broadcasted dimensions intact.
+/// This is valid because the broadcast only duplicates data along the
+/// broadcasted dimension, and extracting a single slice from that dimension
+/// gives back the original input.
+///
+/// Example:
+///   %broadcast = linalg.broadcast ins(%in : tensor<4x1xf16>)
+///                                 outs(%out : tensor<4x1x1xf16>) dimensions =
+///                                 [2]
+///   %extract = tensor.extract_slice %broadcast[0, 0, 0] [4, 1, 1] [1, 1, 1]
+///              : tensor<4x1x1xf16> to tensor<4x1xf16>
+/// ->
+///   %extract is replaced by %in (tensor<4x1xf16>)
+struct FoldExtractSliceOfBroadcast final
+    : OpRewritePattern<tensor::ExtractSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
```

**Comment:**
this used to use `Base::Base` but got dropped after the move for some reason

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/tile_and_distribute_workgroups_using_forall.mlir:1384`

```diff
@@ -1367,52 +1367,64 @@ attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPU
 
 // -----
 
-func.func @arg_compare_fold_broadcast() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUDistribute workgroup_size = [128, 1, 1] subgroup_size = 64>} {
+func.func @arg_compare_fold_broadcast(%input : tensor<4x1x128256xf16>)
+    -> (tensor<4x1x96xf16>, tensor<4x1x96xi32>) {
   %c0_i32 = arith.constant 0 : i32
   %c0 = arith.constant 0 : index
   %cst = arith.constant 0xFC00 : f16
   %c-1 = arith.constant -1 : index
   %c1336 = arith.constant 1336 : index
-  %c768 = arith.constant 768 : index
-  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x128256xf16>>
-  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x1x96xf16, #hal.descriptor_type<storage_buffer>>
-  %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x1x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x1x96xf16, #amdgpu.address_space<fat_raw_buffer>>
-  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c768) flags(Indirect) : memref<4x1x96xi32, strided<[96, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
-  %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x1x96xi32, strided<[96, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x1x96xi32, #amdgpu.address_space<fat_raw_buffer>>
-  %5 = tensor.empty() : tensor<4x1x96xi32>
-  %6 = tensor.empty() : tensor<4x1x96xf16>
-  %7 = tensor.empty() : tensor<4x1xi32>
-  %8 = tensor.empty() : tensor<4x1xf16>
-  %9 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%cst : f16) outs(%8 : tensor<4x1xf16>) -> tensor<4x1xf16>
-  %10 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%c0_i32 : i32) outs(%7 : tensor<4x1xi32>) -> tensor<4x1xi32>
-  %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x1x96xf16>, tensor<4x1x96xi32>) {
-    %12 = arith.cmpi slt, %arg0, %c0 : index
-    %13 = arith.subi %c-1, %arg0 : index
-    %14 = arith.select %12, %13, %arg0 : index
-    %15 = arith.divsi %14, %c1336 : index
-    %16 = arith.subi %c-1, %15 : index
-    %17 = arith.select %12, %16, %15 : index
-    %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 1, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x128256xf16>> -> tensor<4x1x1336xf16>
-    %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 1, 1] [1, 1, 1] : tensor<4x1x96xf16> to tensor<4x1x1xf16>
-    %broadcasted = linalg.broadcast ins(%9 : tensor<4x1xf16>) outs(%extracted_slice : tensor<4x1x1xf16>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
-    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 1, 1] [1, 1, 1] : tensor<4x1x1xf16> to tensor<4x1xf16>
-    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 1, 1] [1, 1, 1] : tensor<4x1x96xi32> to tensor<4x1x1xi32>
-    %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x1xi32>) outs(%extracted_slice_1 : tensor<4x1x1xi32>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
-    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 1, 1] [1, 1, 1] : tensor<4x1x1xi32> to tensor<4x1xi32>
-    %19 = arith.muli %17, %c1336 : index
-    %20:2 = iree_linalg_ext.arg_compare {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} dimension(2) ins(%18 : tensor<4x1x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x1xf16>, tensor<4x1xi32>) index_base(%19 : index) {
-    ^bb0(%arg3: f16, %arg4: f16):
-      %21 = arith.cmpf ogt, %arg3, %arg4 : f16
-      iree_linalg_ext.yield %21 : i1
+  %empty_f16 = tensor.empty() : tensor<4x1x96xf16>
+  %empty_i32 = tensor.empty() : tensor<4x1x96xi32>
+  %init_f16 = tensor.empty() : tensor<4x1xf16>
+  %init_i32 = tensor.empty() : tensor<4x1xi32>
+  %fill_f16 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
+      ins(%cst : f16) outs(%init_f16 : tensor<4x1xf16>) -> tensor<4x1xf16>
+  %fill_i32 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
+      ins(%c0_i32 : i32) outs(%init_i32 : tensor<4x1xi32>) -> tensor<4x1xi32>
```

**Comment:**
Could these also be function arguments? Do we care about the lowering config (I don't know)

---


---


## [PR #22935](https://github.com/iree-org/iree/pull/22935): [CI][torch models] Increase acceptable tolerance in punet

### Review Summary

**APPROVED** (2025-12-17)


---


## [PR #22917](https://github.com/iree-org/iree/pull/22917): [Codegen] Add ResolveTokensPass

### Review Summary

**COMMENTED** (2025-12-29)

**COMMENTED** (2026-01-03)

**APPROVED** (2026-01-03)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.td:142`

```diff
@@ -135,4 +135,23 @@ def ConvertSRefToMemRefPass : Pass<"iree-pcf-convert-sref-to-memref", ""> {
                            "::mlir::vector::VectorDialect"];
 }
 
+def ResolveTokensPass : Pass<"iree-pcf-resolve-tokens", ""> {
+  let summary = "Resolves synchronization scopes on `pcf.sref` types.";
+  let description = [{
+    Resolves synchronization scopes attached to `pcf.sref` types by expanding
+    them to their concrete representations. This pass runs before
```

**Comment:**
nit/pedantic: The pass doesn't really know about other passes, so I'd say that it **should run** before sref-to-memref.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.td:150`

```diff
@@ -135,4 +135,23 @@ def ConvertSRefToMemRefPass : Pass<"iree-pcf-convert-sref-to-memref", ""> {
                            "::mlir::vector::VectorDialect"];
 }
 
+def ResolveTokensPass : Pass<"iree-pcf-resolve-tokens", ""> {
+  let summary = "Resolves synchronization scopes on `pcf.sref` types.";
+  let description = [{
+    Resolves synchronization scopes attached to `pcf.sref` types by expanding
+    them to their concrete representations. This pass runs before
+    `iree-pcf-convert-sref-to-memref`.
+
+    The input is IR containing `pcf.sref` types with sync scope attributes.
+    The pass expands `pcf.sref<..., sync_scope>` types into a `pcf.sref`
+    without sync scope plus any concrete types required by the sync scope
+    attribute. For shaped refs with `sync_on_return` scope, the parent
+    `pcf.generic` or `pcf.loop` op has its `sync_on_return` flag set to true,
+    ensuring a barrier is inserted when the op is lowered.
```

**Comment:**
Do we have any tests for this?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ResolveTokens.cpp:68`

```diff
@@ -0,0 +1,335 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFInterfaces.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/LogicalResult.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Interfaces/FunctionInterfaces.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-resolve-tokens"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_RESOLVETOKENSPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadTokenLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ResolveTokensPass final
+    : impl::ResolveTokensPassBase<ResolveTokensPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+/// Flatten the given value ranges into a single vector of values.
+static SmallVector<Value> flattenValues(ArrayRef<ValueRange> values) {
+  SmallVector<Value> result;
+  for (const ValueRange &vals : values)
```

**Comment:**
```suggestion
  for (ValueRange vals : values)
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ResolveTokens.cpp:96`

```diff
@@ -0,0 +1,335 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFInterfaces.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/LogicalResult.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Interfaces/FunctionInterfaces.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-resolve-tokens"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_RESOLVETOKENSPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadTokenLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ResolveTokensPass final
+    : impl::ResolveTokensPassBase<ResolveTokensPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+/// Flatten the given value ranges into a single vector of values.
+static SmallVector<Value> flattenValues(ArrayRef<ValueRange> values) {
+  SmallVector<Value> result;
+  for (const ValueRange &vals : values)
+    llvm::append_range(result, vals);
+  return result;
+}
+
+/// Helper function for converting branch ops. This function converts the
+/// signature of the given block. If the new block signature is different from
+/// `expectedTypes`, returns "failure".
+static FailureOr<Block *> getConvertedBlock(ConversionPatternRewriter &rewriter,
+                                            const TypeConverter *converter,
+                                            Operation *branchOp, Block *block,
+                                            TypeRange expectedTypes) {
+  assert(converter && "expected non-null type converter");
+  assert(!block->isEntryBlock() && "entry blocks have no predecessors");
+
+  // There is nothing to do if the types already match.
+  if (block->getArgumentTypes() == expectedTypes)
+    return block;
+
+  // Compute the new block argument types and convert the block.
+  std::optional<TypeConverter::SignatureConversion> conversion =
+      converter->convertBlockSignature(block);
+  if (!conversion)
+    return rewriter.notifyMatchFailure(branchOp,
+                                       "could not compute block signature");
+  if (expectedTypes != conversion->getConvertedTypes())
+    return rewriter.notifyMatchFailure(
+        branchOp,
+        "mismatch between adaptor operand types and computed block signature");
```

**Comment:**
missing braces everywhere

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ResolveTokens.cpp:100`

```diff
@@ -0,0 +1,335 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFInterfaces.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/LogicalResult.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Interfaces/FunctionInterfaces.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-resolve-tokens"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_RESOLVETOKENSPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadTokenLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ResolveTokensPass final
+    : impl::ResolveTokensPassBase<ResolveTokensPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+/// Flatten the given value ranges into a single vector of values.
+static SmallVector<Value> flattenValues(ArrayRef<ValueRange> values) {
+  SmallVector<Value> result;
+  for (const ValueRange &vals : values)
+    llvm::append_range(result, vals);
+  return result;
+}
+
+/// Helper function for converting branch ops. This function converts the
+/// signature of the given block. If the new block signature is different from
+/// `expectedTypes`, returns "failure".
+static FailureOr<Block *> getConvertedBlock(ConversionPatternRewriter &rewriter,
+                                            const TypeConverter *converter,
+                                            Operation *branchOp, Block *block,
+                                            TypeRange expectedTypes) {
+  assert(converter && "expected non-null type converter");
+  assert(!block->isEntryBlock() && "entry blocks have no predecessors");
+
+  // There is nothing to do if the types already match.
+  if (block->getArgumentTypes() == expectedTypes)
+    return block;
+
+  // Compute the new block argument types and convert the block.
+  std::optional<TypeConverter::SignatureConversion> conversion =
+      converter->convertBlockSignature(block);
+  if (!conversion)
+    return rewriter.notifyMatchFailure(branchOp,
+                                       "could not compute block signature");
+  if (expectedTypes != conversion->getConvertedTypes())
+    return rewriter.notifyMatchFailure(
+        branchOp,
+        "mismatch between adaptor operand types and computed block signature");
+  return rewriter.applySignatureConversion(block, *conversion, converter);
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
```

**Comment:**
```suggestion
struct ConvertGenericOp final : OpConversionPattern<PCF::GenericOp> {
```
also below

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ResolveTokens.cpp:248`

```diff
@@ -0,0 +1,335 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFInterfaces.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/LogicalResult.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Interfaces/FunctionInterfaces.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-resolve-tokens"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_RESOLVETOKENSPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadTokenLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ResolveTokensPass final
+    : impl::ResolveTokensPassBase<ResolveTokensPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+/// Flatten the given value ranges into a single vector of values.
+static SmallVector<Value> flattenValues(ArrayRef<ValueRange> values) {
+  SmallVector<Value> result;
+  for (const ValueRange &vals : values)
+    llvm::append_range(result, vals);
+  return result;
+}
+
+/// Helper function for converting branch ops. This function converts the
+/// signature of the given block. If the new block signature is different from
+/// `expectedTypes`, returns "failure".
+static FailureOr<Block *> getConvertedBlock(ConversionPatternRewriter &rewriter,
+                                            const TypeConverter *converter,
+                                            Operation *branchOp, Block *block,
+                                            TypeRange expectedTypes) {
+  assert(converter && "expected non-null type converter");
+  assert(!block->isEntryBlock() && "entry blocks have no predecessors");
+
+  // There is nothing to do if the types already match.
+  if (block->getArgumentTypes() == expectedTypes)
+    return block;
+
+  // Compute the new block argument types and convert the block.
+  std::optional<TypeConverter::SignatureConversion> conversion =
+      converter->convertBlockSignature(block);
+  if (!conversion)
+    return rewriter.notifyMatchFailure(branchOp,
+                                       "could not compute block signature");
+  if (expectedTypes != conversion->getConvertedTypes())
+    return rewriter.notifyMatchFailure(
+        branchOp,
+        "mismatch between adaptor operand types and computed block signature");
+  return rewriter.applySignatureConversion(block, *conversion, converter);
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    bool madeChange = false;
+    if (llvm::any_of(genericOp.getRegionRefArgs(), [](BlockArgument b) {
+          return cast<PCF::ShapedRefType>(b.getType()).isReturnOnlySync();
+        })) {
+      genericOp.setSyncOnReturn(true);
+      madeChange = true;
+    }
+
+    SmallVector<Block *> blocksToConvert = llvm::map_to_vector(
+        genericOp.getRegion().getBlocks(), [](Block &b) { return &b; });
+    for (Block *block : blocksToConvert) {
+      std::optional<TypeConverter::SignatureConversion> signatureConverter =
+          getTypeConverter()->convertBlockSignature(block);
+      if (signatureConverter) {
+        madeChange = true;
+        rewriter.applySignatureConversion(block, signatureConverter.value(),
+                                          getTypeConverter());
+      }
+    }
+    return success(madeChange);
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    bool madeChange = false;
+    if (llvm::any_of(loopOp.getRegionRefArgs(), [](BlockArgument b) {
+          return cast<PCF::ShapedRefType>(b.getType()).isReturnOnlySync();
+        })) {
+      loopOp.setSyncOnReturn(true);
+      madeChange = true;
+    }
+
+    std::optional<TypeConverter::SignatureConversion> signatureConverter =
+        getTypeConverter()->convertBlockSignature(loopOp.getBody());
+    if (signatureConverter) {
+      madeChange = true;
+      rewriter.applySignatureConversion(
+          loopOp.getBody(), signatureConverter.value(), getTypeConverter());
+    }
+    return success(madeChange);
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    if (failed(getTypeConverter()->convertType(allocOp.getResultType(),
+                                               resultTypes))) {
+      return failure();
+    }
+
+    auto syncScope = cast_if_present<PCF::SyncScopeAttrInterface>(
+        allocOp.getResultType().getSyncScope());
+    SmallVector<Value> replacements;
+    if (syncScope) {
+      replacements = syncScope.allocate(rewriter);
+    }
+    auto newAlloc =
+        PCF::AllocOp::create(rewriter, allocOp.getLoc(),
+                             cast<PCF::ShapedRefType>(resultTypes.front()),
+                             allocOp.getDynamicSizes());
+    replacements.insert(replacements.begin(), newAlloc.getResult());
+    rewriter.replaceOp(allocOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // Replace the destination with the unscoped sref.
+    ValueRange splitDest = adaptor.getDest();
+
+    rewriter.startOpModification(writeOp);
+    writeOp.getDestMutable().assign(splitDest.front());
+
+    // Enqueue the write via the attribute interface immediately after it.
+    auto syncScope = cast_if_present<PCF::SyncScopeAttrInterface>(
+        writeOp.getDestType().getSyncScope());
+    if (syncScope) {
+      rewriter.setInsertionPointAfter(writeOp);
+      syncScope.enqueueWrite(rewriter, splitDest.drop_front(), writeOp);
+    }
+    rewriter.finalizeOpModification(writeOp);
+    return success();
+  }
+};
+
+/// Convert the destination block signature if necessary.
+struct ConvertBranchOp : public OpConversionPattern<cf::BranchOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(cf::BranchOp op, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    const TypeConverter *typeConverter = getTypeConverter();
+    if (llvm::all_of(op->getOperandTypes(),
+                     [&](Type t) { return typeConverter->isLegal(t); })) {
+      // Nothing to do.
+      return failure();
+    }
+    SmallVector<Value> flattenedAdaptor = flattenValues(adaptor.getOperands());
+    FailureOr<Block *> convertedBlock =
+        getConvertedBlock(rewriter, typeConverter, op, op.getSuccessor(),
+                          TypeRange(ValueRange(flattenedAdaptor)));
+    if (failed(convertedBlock))
+      return failure();
+    op.getDestOperandsMutable().assign(flattenedAdaptor);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, flattenValues(adaptor.getOperands()));
+    return success();
+  }
+};
+
+void ResolveTokensPass::runOnOperation() {
+  auto *context = &getContext();
+
+  TypeConverter typeConverter;
+  ConversionTarget target(*context);
+  RewritePatternSet patterns(&getContext());
```

**Comment:**
Err, decide how you want your context?

also type

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ResolveTokens.cpp:283`

```diff
@@ -0,0 +1,335 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFInterfaces.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/LogicalResult.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Interfaces/FunctionInterfaces.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-resolve-tokens"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_RESOLVETOKENSPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadTokenLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ResolveTokensPass final
+    : impl::ResolveTokensPassBase<ResolveTokensPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+/// Flatten the given value ranges into a single vector of values.
+static SmallVector<Value> flattenValues(ArrayRef<ValueRange> values) {
+  SmallVector<Value> result;
+  for (const ValueRange &vals : values)
+    llvm::append_range(result, vals);
+  return result;
+}
+
+/// Helper function for converting branch ops. This function converts the
+/// signature of the given block. If the new block signature is different from
+/// `expectedTypes`, returns "failure".
+static FailureOr<Block *> getConvertedBlock(ConversionPatternRewriter &rewriter,
+                                            const TypeConverter *converter,
+                                            Operation *branchOp, Block *block,
+                                            TypeRange expectedTypes) {
+  assert(converter && "expected non-null type converter");
+  assert(!block->isEntryBlock() && "entry blocks have no predecessors");
+
+  // There is nothing to do if the types already match.
+  if (block->getArgumentTypes() == expectedTypes)
+    return block;
+
+  // Compute the new block argument types and convert the block.
+  std::optional<TypeConverter::SignatureConversion> conversion =
+      converter->convertBlockSignature(block);
+  if (!conversion)
+    return rewriter.notifyMatchFailure(branchOp,
+                                       "could not compute block signature");
+  if (expectedTypes != conversion->getConvertedTypes())
+    return rewriter.notifyMatchFailure(
+        branchOp,
+        "mismatch between adaptor operand types and computed block signature");
+  return rewriter.applySignatureConversion(block, *conversion, converter);
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    bool madeChange = false;
+    if (llvm::any_of(genericOp.getRegionRefArgs(), [](BlockArgument b) {
+          return cast<PCF::ShapedRefType>(b.getType()).isReturnOnlySync();
+        })) {
+      genericOp.setSyncOnReturn(true);
+      madeChange = true;
+    }
+
+    SmallVector<Block *> blocksToConvert = llvm::map_to_vector(
+        genericOp.getRegion().getBlocks(), [](Block &b) { return &b; });
+    for (Block *block : blocksToConvert) {
+      std::optional<TypeConverter::SignatureConversion> signatureConverter =
+          getTypeConverter()->convertBlockSignature(block);
+      if (signatureConverter) {
+        madeChange = true;
+        rewriter.applySignatureConversion(block, signatureConverter.value(),
+                                          getTypeConverter());
+      }
+    }
+    return success(madeChange);
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    bool madeChange = false;
+    if (llvm::any_of(loopOp.getRegionRefArgs(), [](BlockArgument b) {
+          return cast<PCF::ShapedRefType>(b.getType()).isReturnOnlySync();
+        })) {
+      loopOp.setSyncOnReturn(true);
+      madeChange = true;
+    }
+
+    std::optional<TypeConverter::SignatureConversion> signatureConverter =
+        getTypeConverter()->convertBlockSignature(loopOp.getBody());
+    if (signatureConverter) {
+      madeChange = true;
+      rewriter.applySignatureConversion(
+          loopOp.getBody(), signatureConverter.value(), getTypeConverter());
+    }
+    return success(madeChange);
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    if (failed(getTypeConverter()->convertType(allocOp.getResultType(),
+                                               resultTypes))) {
+      return failure();
+    }
+
+    auto syncScope = cast_if_present<PCF::SyncScopeAttrInterface>(
+        allocOp.getResultType().getSyncScope());
+    SmallVector<Value> replacements;
+    if (syncScope) {
+      replacements = syncScope.allocate(rewriter);
+    }
+    auto newAlloc =
+        PCF::AllocOp::create(rewriter, allocOp.getLoc(),
+                             cast<PCF::ShapedRefType>(resultTypes.front()),
+                             allocOp.getDynamicSizes());
+    replacements.insert(replacements.begin(), newAlloc.getResult());
+    rewriter.replaceOp(allocOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // Replace the destination with the unscoped sref.
+    ValueRange splitDest = adaptor.getDest();
+
+    rewriter.startOpModification(writeOp);
+    writeOp.getDestMutable().assign(splitDest.front());
+
+    // Enqueue the write via the attribute interface immediately after it.
+    auto syncScope = cast_if_present<PCF::SyncScopeAttrInterface>(
+        writeOp.getDestType().getSyncScope());
+    if (syncScope) {
+      rewriter.setInsertionPointAfter(writeOp);
+      syncScope.enqueueWrite(rewriter, splitDest.drop_front(), writeOp);
+    }
+    rewriter.finalizeOpModification(writeOp);
+    return success();
+  }
+};
+
+/// Convert the destination block signature if necessary.
+struct ConvertBranchOp : public OpConversionPattern<cf::BranchOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(cf::BranchOp op, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    const TypeConverter *typeConverter = getTypeConverter();
+    if (llvm::all_of(op->getOperandTypes(),
+                     [&](Type t) { return typeConverter->isLegal(t); })) {
+      // Nothing to do.
+      return failure();
+    }
+    SmallVector<Value> flattenedAdaptor = flattenValues(adaptor.getOperands());
+    FailureOr<Block *> convertedBlock =
+        getConvertedBlock(rewriter, typeConverter, op, op.getSuccessor(),
+                          TypeRange(ValueRange(flattenedAdaptor)));
+    if (failed(convertedBlock))
+      return failure();
+    op.getDestOperandsMutable().assign(flattenedAdaptor);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, flattenValues(adaptor.getOperands()));
+    return success();
+  }
+};
+
+void ResolveTokensPass::runOnOperation() {
+  auto *context = &getContext();
+
+  TypeConverter typeConverter;
+  ConversionTarget target(*context);
+  RewritePatternSet patterns(&getContext());
+
+  // Passthrough converter for everything else. Type conversions are iterated
+  // in reverse, meaning this will be checked after subsequently added
+  // specialized variants.
+  typeConverter.addConversion(
+      [](Type type, SmallVectorImpl<Type> &resultTypes) -> LogicalResult {
+        resultTypes.push_back(type);
+        return success();
+      });
+
+  // Expand shaped refs into one without sync scope and the scope's concrete
+  // types.
+  typeConverter.addConversion(
+      [=](PCF::ShapedRefType type,
+          SmallVectorImpl<Type> &resultTypes) -> std::optional<LogicalResult> {
+        auto syncScope =
+            cast_if_present<PCF::SyncScopeAttrInterface>(type.getSyncScope());
+        if (!syncScope) {
+          resultTypes.push_back(type);
+          return success();
+        }
+
+        auto newRefType =
+            PCF::ShapedRefType::get(type.getContext(), type.getShape(),
+                                    type.getElementType(), type.getScope());
+        resultTypes.push_back(newRefType);
+        for (Type expandedType :
+             syncScope.getConcreteTypes(type.getContext())) {
+          resultTypes.push_back(expandedType);
+        }
+        return success();
+      });
+
+  patterns
+      .insert<ConvertGenericOp, ConvertLoopOp, ConvertAllocOp,
```

**Comment:**
```suggestion
      .add<ConvertGenericOp, ConvertLoopOp, ConvertAllocOp,
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ResolveTokens.cpp:298`

```diff
@@ -0,0 +1,335 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFInterfaces.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/LogicalResult.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Interfaces/FunctionInterfaces.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-resolve-tokens"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_RESOLVETOKENSPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadTokenLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ResolveTokensPass final
+    : impl::ResolveTokensPassBase<ResolveTokensPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+/// Flatten the given value ranges into a single vector of values.
+static SmallVector<Value> flattenValues(ArrayRef<ValueRange> values) {
+  SmallVector<Value> result;
+  for (const ValueRange &vals : values)
+    llvm::append_range(result, vals);
+  return result;
+}
+
+/// Helper function for converting branch ops. This function converts the
+/// signature of the given block. If the new block signature is different from
+/// `expectedTypes`, returns "failure".
+static FailureOr<Block *> getConvertedBlock(ConversionPatternRewriter &rewriter,
+                                            const TypeConverter *converter,
+                                            Operation *branchOp, Block *block,
+                                            TypeRange expectedTypes) {
+  assert(converter && "expected non-null type converter");
+  assert(!block->isEntryBlock() && "entry blocks have no predecessors");
+
+  // There is nothing to do if the types already match.
+  if (block->getArgumentTypes() == expectedTypes)
+    return block;
+
+  // Compute the new block argument types and convert the block.
+  std::optional<TypeConverter::SignatureConversion> conversion =
+      converter->convertBlockSignature(block);
+  if (!conversion)
+    return rewriter.notifyMatchFailure(branchOp,
+                                       "could not compute block signature");
+  if (expectedTypes != conversion->getConvertedTypes())
+    return rewriter.notifyMatchFailure(
+        branchOp,
+        "mismatch between adaptor operand types and computed block signature");
+  return rewriter.applySignatureConversion(block, *conversion, converter);
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    bool madeChange = false;
+    if (llvm::any_of(genericOp.getRegionRefArgs(), [](BlockArgument b) {
+          return cast<PCF::ShapedRefType>(b.getType()).isReturnOnlySync();
+        })) {
+      genericOp.setSyncOnReturn(true);
+      madeChange = true;
+    }
+
+    SmallVector<Block *> blocksToConvert = llvm::map_to_vector(
+        genericOp.getRegion().getBlocks(), [](Block &b) { return &b; });
+    for (Block *block : blocksToConvert) {
+      std::optional<TypeConverter::SignatureConversion> signatureConverter =
+          getTypeConverter()->convertBlockSignature(block);
+      if (signatureConverter) {
+        madeChange = true;
+        rewriter.applySignatureConversion(block, signatureConverter.value(),
+                                          getTypeConverter());
+      }
+    }
+    return success(madeChange);
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    bool madeChange = false;
+    if (llvm::any_of(loopOp.getRegionRefArgs(), [](BlockArgument b) {
+          return cast<PCF::ShapedRefType>(b.getType()).isReturnOnlySync();
+        })) {
+      loopOp.setSyncOnReturn(true);
+      madeChange = true;
+    }
+
+    std::optional<TypeConverter::SignatureConversion> signatureConverter =
+        getTypeConverter()->convertBlockSignature(loopOp.getBody());
+    if (signatureConverter) {
+      madeChange = true;
+      rewriter.applySignatureConversion(
+          loopOp.getBody(), signatureConverter.value(), getTypeConverter());
+    }
+    return success(madeChange);
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    if (failed(getTypeConverter()->convertType(allocOp.getResultType(),
+                                               resultTypes))) {
+      return failure();
+    }
+
+    auto syncScope = cast_if_present<PCF::SyncScopeAttrInterface>(
+        allocOp.getResultType().getSyncScope());
+    SmallVector<Value> replacements;
+    if (syncScope) {
+      replacements = syncScope.allocate(rewriter);
+    }
+    auto newAlloc =
+        PCF::AllocOp::create(rewriter, allocOp.getLoc(),
+                             cast<PCF::ShapedRefType>(resultTypes.front()),
+                             allocOp.getDynamicSizes());
+    replacements.insert(replacements.begin(), newAlloc.getResult());
+    rewriter.replaceOp(allocOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // Replace the destination with the unscoped sref.
+    ValueRange splitDest = adaptor.getDest();
+
+    rewriter.startOpModification(writeOp);
+    writeOp.getDestMutable().assign(splitDest.front());
+
+    // Enqueue the write via the attribute interface immediately after it.
+    auto syncScope = cast_if_present<PCF::SyncScopeAttrInterface>(
+        writeOp.getDestType().getSyncScope());
+    if (syncScope) {
+      rewriter.setInsertionPointAfter(writeOp);
+      syncScope.enqueueWrite(rewriter, splitDest.drop_front(), writeOp);
+    }
+    rewriter.finalizeOpModification(writeOp);
+    return success();
+  }
+};
+
+/// Convert the destination block signature if necessary.
+struct ConvertBranchOp : public OpConversionPattern<cf::BranchOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(cf::BranchOp op, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    const TypeConverter *typeConverter = getTypeConverter();
+    if (llvm::all_of(op->getOperandTypes(),
+                     [&](Type t) { return typeConverter->isLegal(t); })) {
+      // Nothing to do.
+      return failure();
+    }
+    SmallVector<Value> flattenedAdaptor = flattenValues(adaptor.getOperands());
+    FailureOr<Block *> convertedBlock =
+        getConvertedBlock(rewriter, typeConverter, op, op.getSuccessor(),
+                          TypeRange(ValueRange(flattenedAdaptor)));
+    if (failed(convertedBlock))
+      return failure();
+    op.getDestOperandsMutable().assign(flattenedAdaptor);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, flattenValues(adaptor.getOperands()));
+    return success();
+  }
+};
+
+void ResolveTokensPass::runOnOperation() {
+  auto *context = &getContext();
+
+  TypeConverter typeConverter;
+  ConversionTarget target(*context);
+  RewritePatternSet patterns(&getContext());
+
+  // Passthrough converter for everything else. Type conversions are iterated
+  // in reverse, meaning this will be checked after subsequently added
+  // specialized variants.
+  typeConverter.addConversion(
+      [](Type type, SmallVectorImpl<Type> &resultTypes) -> LogicalResult {
+        resultTypes.push_back(type);
+        return success();
+      });
+
+  // Expand shaped refs into one without sync scope and the scope's concrete
+  // types.
+  typeConverter.addConversion(
+      [=](PCF::ShapedRefType type,
+          SmallVectorImpl<Type> &resultTypes) -> std::optional<LogicalResult> {
+        auto syncScope =
+            cast_if_present<PCF::SyncScopeAttrInterface>(type.getSyncScope());
+        if (!syncScope) {
+          resultTypes.push_back(type);
+          return success();
+        }
+
+        auto newRefType =
+            PCF::ShapedRefType::get(type.getContext(), type.getShape(),
+                                    type.getElementType(), type.getScope());
+        resultTypes.push_back(newRefType);
+        for (Type expandedType :
+             syncScope.getConcreteTypes(type.getContext())) {
+          resultTypes.push_back(expandedType);
+        }
+        return success();
+      });
+
+  patterns
+      .insert<ConvertGenericOp, ConvertLoopOp, ConvertAllocOp,
+              ConvertWriteSliceOp, ConvertOptimizationBarrier, ConvertBranchOp>(
+          typeConverter, context);
+
+  // Verify that all operand, result, and region argument types have been
+  // converted.
+  auto isLegallyTypedOp = [&](Operation *op) -> bool {
+    for (Type type : op->getResultTypes()) {
+      if (!typeConverter.isLegal(type))
+        return false;
+    }
+    for (Type type : op->getOperandTypes()) {
+      if (!typeConverter.isLegal(type))
+        return false;
+    }
+    for (auto &region : op->getRegions()) {
```

**Comment:**
type

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ResolveTokens.cpp:299`

```diff
@@ -0,0 +1,335 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFInterfaces.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/LogicalResult.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Interfaces/FunctionInterfaces.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-resolve-tokens"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_RESOLVETOKENSPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadTokenLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ResolveTokensPass final
+    : impl::ResolveTokensPassBase<ResolveTokensPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+/// Flatten the given value ranges into a single vector of values.
+static SmallVector<Value> flattenValues(ArrayRef<ValueRange> values) {
+  SmallVector<Value> result;
+  for (const ValueRange &vals : values)
+    llvm::append_range(result, vals);
+  return result;
+}
+
+/// Helper function for converting branch ops. This function converts the
+/// signature of the given block. If the new block signature is different from
+/// `expectedTypes`, returns "failure".
+static FailureOr<Block *> getConvertedBlock(ConversionPatternRewriter &rewriter,
+                                            const TypeConverter *converter,
+                                            Operation *branchOp, Block *block,
+                                            TypeRange expectedTypes) {
+  assert(converter && "expected non-null type converter");
+  assert(!block->isEntryBlock() && "entry blocks have no predecessors");
+
+  // There is nothing to do if the types already match.
+  if (block->getArgumentTypes() == expectedTypes)
+    return block;
+
+  // Compute the new block argument types and convert the block.
+  std::optional<TypeConverter::SignatureConversion> conversion =
+      converter->convertBlockSignature(block);
+  if (!conversion)
+    return rewriter.notifyMatchFailure(branchOp,
+                                       "could not compute block signature");
+  if (expectedTypes != conversion->getConvertedTypes())
+    return rewriter.notifyMatchFailure(
+        branchOp,
+        "mismatch between adaptor operand types and computed block signature");
+  return rewriter.applySignatureConversion(block, *conversion, converter);
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    bool madeChange = false;
+    if (llvm::any_of(genericOp.getRegionRefArgs(), [](BlockArgument b) {
+          return cast<PCF::ShapedRefType>(b.getType()).isReturnOnlySync();
+        })) {
+      genericOp.setSyncOnReturn(true);
+      madeChange = true;
+    }
+
+    SmallVector<Block *> blocksToConvert = llvm::map_to_vector(
+        genericOp.getRegion().getBlocks(), [](Block &b) { return &b; });
+    for (Block *block : blocksToConvert) {
+      std::optional<TypeConverter::SignatureConversion> signatureConverter =
+          getTypeConverter()->convertBlockSignature(block);
+      if (signatureConverter) {
+        madeChange = true;
+        rewriter.applySignatureConversion(block, signatureConverter.value(),
+                                          getTypeConverter());
+      }
+    }
+    return success(madeChange);
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    bool madeChange = false;
+    if (llvm::any_of(loopOp.getRegionRefArgs(), [](BlockArgument b) {
+          return cast<PCF::ShapedRefType>(b.getType()).isReturnOnlySync();
+        })) {
+      loopOp.setSyncOnReturn(true);
+      madeChange = true;
+    }
+
+    std::optional<TypeConverter::SignatureConversion> signatureConverter =
+        getTypeConverter()->convertBlockSignature(loopOp.getBody());
+    if (signatureConverter) {
+      madeChange = true;
+      rewriter.applySignatureConversion(
+          loopOp.getBody(), signatureConverter.value(), getTypeConverter());
+    }
+    return success(madeChange);
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    if (failed(getTypeConverter()->convertType(allocOp.getResultType(),
+                                               resultTypes))) {
+      return failure();
+    }
+
+    auto syncScope = cast_if_present<PCF::SyncScopeAttrInterface>(
+        allocOp.getResultType().getSyncScope());
+    SmallVector<Value> replacements;
+    if (syncScope) {
+      replacements = syncScope.allocate(rewriter);
+    }
+    auto newAlloc =
+        PCF::AllocOp::create(rewriter, allocOp.getLoc(),
+                             cast<PCF::ShapedRefType>(resultTypes.front()),
+                             allocOp.getDynamicSizes());
+    replacements.insert(replacements.begin(), newAlloc.getResult());
+    rewriter.replaceOp(allocOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // Replace the destination with the unscoped sref.
+    ValueRange splitDest = adaptor.getDest();
+
+    rewriter.startOpModification(writeOp);
+    writeOp.getDestMutable().assign(splitDest.front());
+
+    // Enqueue the write via the attribute interface immediately after it.
+    auto syncScope = cast_if_present<PCF::SyncScopeAttrInterface>(
+        writeOp.getDestType().getSyncScope());
+    if (syncScope) {
+      rewriter.setInsertionPointAfter(writeOp);
+      syncScope.enqueueWrite(rewriter, splitDest.drop_front(), writeOp);
+    }
+    rewriter.finalizeOpModification(writeOp);
+    return success();
+  }
+};
+
+/// Convert the destination block signature if necessary.
+struct ConvertBranchOp : public OpConversionPattern<cf::BranchOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(cf::BranchOp op, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    const TypeConverter *typeConverter = getTypeConverter();
+    if (llvm::all_of(op->getOperandTypes(),
+                     [&](Type t) { return typeConverter->isLegal(t); })) {
+      // Nothing to do.
+      return failure();
+    }
+    SmallVector<Value> flattenedAdaptor = flattenValues(adaptor.getOperands());
+    FailureOr<Block *> convertedBlock =
+        getConvertedBlock(rewriter, typeConverter, op, op.getSuccessor(),
+                          TypeRange(ValueRange(flattenedAdaptor)));
+    if (failed(convertedBlock))
+      return failure();
+    op.getDestOperandsMutable().assign(flattenedAdaptor);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OneToNOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, flattenValues(adaptor.getOperands()));
+    return success();
+  }
+};
+
+void ResolveTokensPass::runOnOperation() {
+  auto *context = &getContext();
+
+  TypeConverter typeConverter;
+  ConversionTarget target(*context);
+  RewritePatternSet patterns(&getContext());
+
+  // Passthrough converter for everything else. Type conversions are iterated
+  // in reverse, meaning this will be checked after subsequently added
+  // specialized variants.
+  typeConverter.addConversion(
+      [](Type type, SmallVectorImpl<Type> &resultTypes) -> LogicalResult {
+        resultTypes.push_back(type);
+        return success();
+      });
+
+  // Expand shaped refs into one without sync scope and the scope's concrete
+  // types.
+  typeConverter.addConversion(
+      [=](PCF::ShapedRefType type,
+          SmallVectorImpl<Type> &resultTypes) -> std::optional<LogicalResult> {
+        auto syncScope =
+            cast_if_present<PCF::SyncScopeAttrInterface>(type.getSyncScope());
+        if (!syncScope) {
+          resultTypes.push_back(type);
+          return success();
+        }
+
+        auto newRefType =
+            PCF::ShapedRefType::get(type.getContext(), type.getShape(),
+                                    type.getElementType(), type.getScope());
+        resultTypes.push_back(newRefType);
+        for (Type expandedType :
+             syncScope.getConcreteTypes(type.getContext())) {
+          resultTypes.push_back(expandedType);
+        }
+        return success();
+      });
+
+  patterns
+      .insert<ConvertGenericOp, ConvertLoopOp, ConvertAllocOp,
+              ConvertWriteSliceOp, ConvertOptimizationBarrier, ConvertBranchOp>(
+          typeConverter, context);
+
+  // Verify that all operand, result, and region argument types have been
+  // converted.
+  auto isLegallyTypedOp = [&](Operation *op) -> bool {
+    for (Type type : op->getResultTypes()) {
+      if (!typeConverter.isLegal(type))
+        return false;
+    }
+    for (Type type : op->getOperandTypes()) {
+      if (!typeConverter.isLegal(type))
+        return false;
+    }
+    for (auto &region : op->getRegions()) {
+      for (auto type : region.getArgumentTypes()) {
```

**Comment:**
type

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.td:150`

```diff
@@ -135,4 +135,23 @@ def ConvertSRefToMemRefPass : Pass<"iree-pcf-convert-sref-to-memref", ""> {
                            "::mlir::vector::VectorDialect"];
 }
 
+def ResolveTokensPass : Pass<"iree-pcf-resolve-tokens", ""> {
+  let summary = "Resolves synchronization scopes on `pcf.sref` types.";
+  let description = [{
+    Resolves synchronization scopes attached to `pcf.sref` types by expanding
+    them to their concrete representations. This pass runs before
+    `iree-pcf-convert-sref-to-memref`.
+
+    The input is IR containing `pcf.sref` types with sync scope attributes.
+    The pass expands `pcf.sref<..., sync_scope>` types into a `pcf.sref`
+    without sync scope plus any concrete types required by the sync scope
+    attribute. For shaped refs with `sync_on_return` scope, the parent
+    `pcf.generic` or `pcf.loop` op has its `sync_on_return` flag set to true,
+    ensuring a barrier is inserted when the op is lowered.
```

**Comment:**
Ah, I didn't see that because I was looking for `sync_on_return` in the IR

---


---


## [PR #22916](https://github.com/iree-org/iree/pull/22916): [Codegen] Add ConvertSRefToMemRefPass

### Review Summary

**COMMENTED** (2025-12-29)

Looks good overall, just some minor issues

**COMMENTED** (2026-01-05)

**APPROVED** (2026-01-05)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/test/convert_sref_to_memref.mlir:1`

```diff
@@ -0,0 +1,464 @@
+// RUN: iree-opt %s --pass-pipeline="builtin.module(iree-pcf-convert-sref-to-memref)" --split-input-file --verify-diagnostics | FileCheck %s
```

**Comment:**
I don't see any diagnostics in this file? Do we need the flag?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/test/convert_sref_to_memref.mlir:154`

```diff
@@ -0,0 +1,464 @@
+// RUN: iree-opt %s --pass-pipeline="builtin.module(iree-pcf-convert-sref-to-memref)" --split-input-file --verify-diagnostics | FileCheck %s
+
+util.func private @convert_generic_with_init(%arg0: memref<?x?xi32, strided<[?, 1]>, 3>) {
+  %0 = pcf.generic scope(#pcf.test_scope)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>) {
+    util.optimization_barrier %ref : !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0 : memref<?x?xi32, strided<[?, 1]>, 3>
+  util.return
+}
+
+// CHECK-LABEL: @convert_generic_with_init
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ARG0]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ARG0]]
+
+// -----
+
+util.func private @convert_generic_with_alloc(%d0: index, %d1: index, %d2: index) {
+  %0:2 = pcf.generic scope(#pcf.test_scope)
+    execute(%ref, %ref_1)[%id: index, %count: index]
+         : (!pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?xi32>{%d0}, memref<?x?xi32>{%d1, %d2}) {
+    util.optimization_barrier %ref, %ref_1 : !pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0#0, %0#1 : memref<?xi32>, memref<?x?xi32>
+  util.return
+}
+
+// CHECK-LABEL: @convert_generic_with_alloc
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG1:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG2:[A-Za-z0-9_]+]]: index
+//  CHECK-DAG:    %[[ALLOC:.+]] = memref.alloc(%[[ARG0]]) {alignment = 16 : i64} : memref<?xi32>
+//  CHECK-DAG:    %[[ALLOC1:.+]] = memref.alloc(%[[ARG1]], %[[ARG2]]) {alignment = 16 : i64} : memref<?x?xi32>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+
+// -----
+
+util.func private @inline_generic_initializer(%arg0: memref<?x?xi32, strided<[?, 1]>, 3>) {
+  %0 = pcf.generic scope(#pcf.test_scope)
+    initialize {
+      %c42 = arith.constant 42 : index
+      pcf.yield %c42 : index
+    } -> (%i: index)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>) {
+    util.optimization_barrier %i, %ref : index, !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @inline_generic_initializer
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     %[[I:.+]] = arith.constant 42 : index
+//  CHECK-NEXT:     util.optimization_barrier %[[I]], %[[ARG0]]
+//  CHECK-NEXT:     pcf.return
+
+// -----
+
+util.func private @inline_generic_initializer_with_alloc() {
+  pcf.generic scope(#pcf.sequential)
+    initialize {
+      %c42 = arith.constant 42 : index
+      %alloc = pcf.alloc(%c42) : !pcf.sref<?x5xi32, #pcf.sequential>
+      pcf.yield %c42, %alloc : index, !pcf.sref<?x5xi32, #pcf.sequential>
+    } -> (%i: index, %aref: !pcf.sref<?x5xi32, #pcf.sequential>)
+    execute[%id: index, %n: index] {
+    util.optimization_barrier %i, %aref : index, !pcf.sref<?x5xi32, #pcf.sequential>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @inline_generic_initializer_with_alloc
+//       CHECK:   pcf.generic scope(#pcf.sequential)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     %[[I:.+]] = arith.constant 42 : index
+//  CHECK-NEXT:     %[[ALLOC:.+]] = memref.alloc(%[[I]]) {alignment = 16 : i64} : memref<?x5xi32>
+//  CHECK-NEXT:     util.optimization_barrier %[[I]], %[[ALLOC]]
+//  CHECK-NEXT:     pcf.return
+
+// -----
+
+util.func private @convert_loop_with_init(%arg0: memref<?x?xi32, strided<[?, 1]>, 3>, %n: index) {
+  %0 = pcf.loop scope(#pcf.test_scope) count(%n)
+    execute(%ref = %arg0)[%count: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>) {
+    util.optimization_barrier %ref : !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0 : memref<?x?xi32, strided<[?, 1]>, 3>
+  util.return
+}
+
+// CHECK-LABEL: @convert_loop_with_init
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//       CHECK:   pcf.loop scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ARG0]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ARG0]]
+
+// -----
+
+util.func private @convert_loop_with_alloc(%d0: index, %d1: index, %d2: index, %n: index) {
+  %0:2 = pcf.loop scope(#pcf.test_scope) count(%n)
+    execute(%ref, %ref_1)[%count: index]
+         : (!pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?xi32>{%d0}, memref<?x?xi32>{%d1, %d2}) {
+    util.optimization_barrier %ref, %ref_1 : !pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0#0, %0#1 : memref<?xi32>, memref<?x?xi32>
+  util.return
+}
+
+// CHECK-LABEL: @convert_loop_with_alloc
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG1:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG2:[A-Za-z0-9_]+]]: index
+//  CHECK-DAG:    %[[ALLOC:.+]] = memref.alloc(%[[ARG0]]) {alignment = 16 : i64} : memref<?xi32>
+//  CHECK-DAG:    %[[ALLOC1:.+]] = memref.alloc(%[[ARG1]], %[[ARG2]]) {alignment = 16 : i64} : memref<?x?xi32>
+//       CHECK:   pcf.loop scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+
+// -----
+
+util.func private @convert_memref_write_slice(%arg0: memref<?x?xi32>) {
+  pcf.generic scope(#pcf.test_scope)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32>) {
+    %src = memref.alloc() : memref<3x4xi32>
+    pcf.write_slice %src into %ref[1, 2] [3, 4] [1, 1] : memref<3x4xi32> into !pcf.sref<?x?xi32, #pcf.test_scope>
```

**Comment:**
Do we care that this is copying uninitialized memory? Is it possible it will get folded by something?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/test/convert_sref_to_memref.mlir:3`

```diff
@@ -0,0 +1,464 @@
+// RUN: iree-opt %s --pass-pipeline="builtin.module(iree-pcf-convert-sref-to-memref)" --split-input-file --verify-diagnostics | FileCheck %s
+
+util.func private @convert_generic_with_init(%arg0: memref<?x?xi32, strided<[?, 1]>, 3>) {
```

**Comment:**
nit: do we care if these function are private or public? I think the default is `public` like with `func.func`?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/test/convert_sref_to_memref.mlir:282`

```diff
@@ -0,0 +1,464 @@
+// RUN: iree-opt %s --pass-pipeline="builtin.module(iree-pcf-convert-sref-to-memref)" --split-input-file --verify-diagnostics | FileCheck %s
+
+util.func private @convert_generic_with_init(%arg0: memref<?x?xi32, strided<[?, 1]>, 3>) {
+  %0 = pcf.generic scope(#pcf.test_scope)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>) {
+    util.optimization_barrier %ref : !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0 : memref<?x?xi32, strided<[?, 1]>, 3>
+  util.return
+}
+
+// CHECK-LABEL: @convert_generic_with_init
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ARG0]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ARG0]]
+
+// -----
+
+util.func private @convert_generic_with_alloc(%d0: index, %d1: index, %d2: index) {
+  %0:2 = pcf.generic scope(#pcf.test_scope)
+    execute(%ref, %ref_1)[%id: index, %count: index]
+         : (!pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?xi32>{%d0}, memref<?x?xi32>{%d1, %d2}) {
+    util.optimization_barrier %ref, %ref_1 : !pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0#0, %0#1 : memref<?xi32>, memref<?x?xi32>
+  util.return
+}
+
+// CHECK-LABEL: @convert_generic_with_alloc
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG1:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG2:[A-Za-z0-9_]+]]: index
+//  CHECK-DAG:    %[[ALLOC:.+]] = memref.alloc(%[[ARG0]]) {alignment = 16 : i64} : memref<?xi32>
+//  CHECK-DAG:    %[[ALLOC1:.+]] = memref.alloc(%[[ARG1]], %[[ARG2]]) {alignment = 16 : i64} : memref<?x?xi32>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+
+// -----
+
+util.func private @inline_generic_initializer(%arg0: memref<?x?xi32, strided<[?, 1]>, 3>) {
+  %0 = pcf.generic scope(#pcf.test_scope)
+    initialize {
+      %c42 = arith.constant 42 : index
+      pcf.yield %c42 : index
+    } -> (%i: index)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>) {
+    util.optimization_barrier %i, %ref : index, !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @inline_generic_initializer
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     %[[I:.+]] = arith.constant 42 : index
+//  CHECK-NEXT:     util.optimization_barrier %[[I]], %[[ARG0]]
+//  CHECK-NEXT:     pcf.return
+
+// -----
+
+util.func private @inline_generic_initializer_with_alloc() {
+  pcf.generic scope(#pcf.sequential)
+    initialize {
+      %c42 = arith.constant 42 : index
+      %alloc = pcf.alloc(%c42) : !pcf.sref<?x5xi32, #pcf.sequential>
+      pcf.yield %c42, %alloc : index, !pcf.sref<?x5xi32, #pcf.sequential>
+    } -> (%i: index, %aref: !pcf.sref<?x5xi32, #pcf.sequential>)
+    execute[%id: index, %n: index] {
+    util.optimization_barrier %i, %aref : index, !pcf.sref<?x5xi32, #pcf.sequential>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @inline_generic_initializer_with_alloc
+//       CHECK:   pcf.generic scope(#pcf.sequential)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     %[[I:.+]] = arith.constant 42 : index
+//  CHECK-NEXT:     %[[ALLOC:.+]] = memref.alloc(%[[I]]) {alignment = 16 : i64} : memref<?x5xi32>
+//  CHECK-NEXT:     util.optimization_barrier %[[I]], %[[ALLOC]]
+//  CHECK-NEXT:     pcf.return
+
+// -----
+
+util.func private @convert_loop_with_init(%arg0: memref<?x?xi32, strided<[?, 1]>, 3>, %n: index) {
+  %0 = pcf.loop scope(#pcf.test_scope) count(%n)
+    execute(%ref = %arg0)[%count: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>) {
+    util.optimization_barrier %ref : !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0 : memref<?x?xi32, strided<[?, 1]>, 3>
+  util.return
+}
+
+// CHECK-LABEL: @convert_loop_with_init
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//       CHECK:   pcf.loop scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ARG0]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ARG0]]
+
+// -----
+
+util.func private @convert_loop_with_alloc(%d0: index, %d1: index, %d2: index, %n: index) {
+  %0:2 = pcf.loop scope(#pcf.test_scope) count(%n)
+    execute(%ref, %ref_1)[%count: index]
+         : (!pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?xi32>{%d0}, memref<?x?xi32>{%d1, %d2}) {
+    util.optimization_barrier %ref, %ref_1 : !pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0#0, %0#1 : memref<?xi32>, memref<?x?xi32>
+  util.return
+}
+
+// CHECK-LABEL: @convert_loop_with_alloc
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG1:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG2:[A-Za-z0-9_]+]]: index
+//  CHECK-DAG:    %[[ALLOC:.+]] = memref.alloc(%[[ARG0]]) {alignment = 16 : i64} : memref<?xi32>
+//  CHECK-DAG:    %[[ALLOC1:.+]] = memref.alloc(%[[ARG1]], %[[ARG2]]) {alignment = 16 : i64} : memref<?x?xi32>
+//       CHECK:   pcf.loop scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+
+// -----
+
+util.func private @convert_memref_write_slice(%arg0: memref<?x?xi32>) {
+  pcf.generic scope(#pcf.test_scope)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32>) {
+    %src = memref.alloc() : memref<3x4xi32>
+    pcf.write_slice %src into %ref[1, 2] [3, 4] [1, 1] : memref<3x4xi32> into !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @convert_memref_write_slice
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32>
+//       CHECK:   pcf.generic
+//  CHECK-NEXT:     execute[{{.*}}] {
+//   CHECK-DAG:     %[[SRC:.+]] = memref.alloc()
+//   CHECK-DAG:     %[[SV:.+]] = memref.subview %[[ARG0]][1, 2] [3, 4] [1, 1] : memref<?x?xi32> to memref<3x4xi32, strided<[?, 1], offset: ?>>
+//       CHECK:     memref.copy %[[SRC]], %[[SV]]
+//       CHECK:     pcf.return
+
+// -----
+
+util.func private @convert_vector_write_slice(%arg0: memref<?x?xi32>) {
+  pcf.generic scope(#pcf.test_scope)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32>) {
+    %src = arith.constant dense<0> : vector<3x4xi32>
+    pcf.write_slice %src into %ref[1, 2] [3, 3] [1, 1] : vector<3x4xi32> into !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @convert_vector_write_slice
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32>
+//       CHECK:   pcf.generic
+//  CHECK-NEXT:     execute[{{.*}}] {
+//   CHECK-DAG:     %[[SRC:.+]] = arith.constant dense<0> : vector<3x4xi32>
+//   CHECK-DAG:     %[[SV:.+]] = memref.subview %[[ARG0]][1, 2] [3, 3] [1, 1] : memref<?x?xi32> to memref<3x3xi32, strided<[?, 1], offset: ?>>
+//       CHECK:     vector.transfer_write %[[SRC]], %[[SV]][%c0, %c0] {in_bounds = [true, false]}
+//       CHECK:     pcf.return
+
+// -----
+
+util.func private @convert_tensor_write_slice(%arg0: memref<?x?xi32>) {
+  pcf.generic scope(#pcf.test_scope)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32>) {
+    %src = arith.constant dense<0> : tensor<3x4xi32>
+    pcf.write_slice %src into %ref[1, 2] [3, 4] [1, 1] : tensor<3x4xi32> into !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @convert_tensor_write_slice
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32>
+//       CHECK:   pcf.generic
+//  CHECK-NEXT:     execute[{{.*}}] {
+//   CHECK-DAG:     %[[SRC:.+]] = arith.constant dense<0> : tensor<3x4xi32>
+//   CHECK-DAG:     %[[SV:.+]] = memref.subview %[[ARG0]][1, 2] [3, 4] [1, 1] : memref<?x?xi32> to memref<3x4xi32, strided<[?, 1], offset: ?>>
+//       CHECK:     iree_codegen.store_to_buffer %[[SRC]], %[[SV]]
+//       CHECK:     pcf.return
+
+// -----
+
+util.func private @convert_for_loop_swap(
+    %arg0: memref<?x?xi32, strided<[?, 1]>, 3>,
+    %arg1: memref<?x?xi32, strided<[?, 1]>, 3>) {
+  %c0 = arith.constant 0 : index
+  %c1 = arith.constant 1 : index
+  %c10 = arith.constant 10 : index
+  %0:2 = pcf.generic scope(#pcf.test_scope)
+    execute(%ref0 = %arg0, %ref1 = %arg1)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>, memref<?x?xi32, strided<[?, 1]>, 3>) {
+    scf.for %i = %c0 to %c10 step %c1 iter_args(%iter0 = %ref0, %iter1 = %ref1)
+        -> (!pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>) {
+      %b:2 = util.optimization_barrier %iter1, %iter0 : !pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+      scf.yield %b#0, %b#1 : !pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+    }
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @convert_for_loop_swap
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//  CHECK-SAME:     %[[ARG1:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     scf.for {{.*}} iter_args(%[[ITER0:.+]] = %[[ARG0]], %[[ITER1:.+]] = %[[ARG1]])
+//  CHECK-NEXT:       %[[B:.+]]:2 = util.optimization_barrier %[[ITER1]], %[[ITER0]]
+//  CHECK-NEXT:       scf.yield %[[B]]#0, %[[B]]#1
+//       CHECK:     pcf.return
+
+// -----
+
+util.func private @convert_for_loop_swap_conflict(
+    %arg0: memref<?x?xi32, strided<[3, 1]>, 3>,
+    %arg1: memref<?x?xi32, strided<[4, 1]>, 3>) {
+  %c0 = arith.constant 0 : index
+  %c1 = arith.constant 1 : index
+  %c10 = arith.constant 10 : index
+  %0:2 = pcf.generic scope(#pcf.test_scope)
+    execute(%ref0 = %arg0, %ref1 = %arg1)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[3, 1]>, 3>, memref<?x?xi32, strided<[4, 1]>, 3>) {
+    scf.for %i = %c0 to %c10 step %c1 iter_args(%iter0 = %ref0, %iter1 = %ref1)
+        -> (!pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>) {
+      %b:2 = util.optimization_barrier %iter1, %iter0 : !pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+      scf.yield %b#0, %b#1 : !pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+    }
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @convert_for_loop_swap_conflict
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[3, 1]>, 3>
+//  CHECK-SAME:     %[[ARG1:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[4, 1]>, 3>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//   CHECK-DAG:     %[[CAST0:.+]] = memref.cast %[[ARG0]] : memref<?x?xi32, strided<[3, 1]>, 3> to memref<?x?xi32, strided<[?, 1]>, 3>
+//   CHECK-DAG:     %[[CAST1:.+]] = memref.cast %[[ARG1]] : memref<?x?xi32, strided<[4, 1]>, 3> to memref<?x?xi32, strided<[?, 1]>, 3>
+//  CHECK-NEXT:     scf.for {{.*}} iter_args(%[[ITER0:.+]] = %[[CAST0]], %[[ITER1:.+]] = %[[CAST1]])
+//  CHECK-NEXT:       %[[B:.+]]:2 = util.optimization_barrier %[[ITER1]], %[[ITER0]]
+//  CHECK-NEXT:       scf.yield %[[B]]#0, %[[B]]#1
+//       CHECK:     pcf.return
+
+// -----
+
+func.func @convert_alloc(%d0: index) -> !pcf.sref<?x5xi32, #pcf.sequential> {
```

**Comment:**
Why not `util.func` like elsewhere?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/test/convert_sref_to_memref.mlir:321`

```diff
@@ -0,0 +1,464 @@
+// RUN: iree-opt %s --pass-pipeline="builtin.module(iree-pcf-convert-sref-to-memref)" --split-input-file --verify-diagnostics | FileCheck %s
+
+util.func private @convert_generic_with_init(%arg0: memref<?x?xi32, strided<[?, 1]>, 3>) {
+  %0 = pcf.generic scope(#pcf.test_scope)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>) {
+    util.optimization_barrier %ref : !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0 : memref<?x?xi32, strided<[?, 1]>, 3>
+  util.return
+}
+
+// CHECK-LABEL: @convert_generic_with_init
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ARG0]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ARG0]]
+
+// -----
+
+util.func private @convert_generic_with_alloc(%d0: index, %d1: index, %d2: index) {
+  %0:2 = pcf.generic scope(#pcf.test_scope)
+    execute(%ref, %ref_1)[%id: index, %count: index]
+         : (!pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?xi32>{%d0}, memref<?x?xi32>{%d1, %d2}) {
+    util.optimization_barrier %ref, %ref_1 : !pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0#0, %0#1 : memref<?xi32>, memref<?x?xi32>
+  util.return
+}
+
+// CHECK-LABEL: @convert_generic_with_alloc
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG1:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG2:[A-Za-z0-9_]+]]: index
+//  CHECK-DAG:    %[[ALLOC:.+]] = memref.alloc(%[[ARG0]]) {alignment = 16 : i64} : memref<?xi32>
+//  CHECK-DAG:    %[[ALLOC1:.+]] = memref.alloc(%[[ARG1]], %[[ARG2]]) {alignment = 16 : i64} : memref<?x?xi32>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+
+// -----
+
+util.func private @inline_generic_initializer(%arg0: memref<?x?xi32, strided<[?, 1]>, 3>) {
+  %0 = pcf.generic scope(#pcf.test_scope)
+    initialize {
+      %c42 = arith.constant 42 : index
+      pcf.yield %c42 : index
+    } -> (%i: index)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>) {
+    util.optimization_barrier %i, %ref : index, !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @inline_generic_initializer
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     %[[I:.+]] = arith.constant 42 : index
+//  CHECK-NEXT:     util.optimization_barrier %[[I]], %[[ARG0]]
+//  CHECK-NEXT:     pcf.return
+
+// -----
+
+util.func private @inline_generic_initializer_with_alloc() {
+  pcf.generic scope(#pcf.sequential)
+    initialize {
+      %c42 = arith.constant 42 : index
+      %alloc = pcf.alloc(%c42) : !pcf.sref<?x5xi32, #pcf.sequential>
+      pcf.yield %c42, %alloc : index, !pcf.sref<?x5xi32, #pcf.sequential>
+    } -> (%i: index, %aref: !pcf.sref<?x5xi32, #pcf.sequential>)
+    execute[%id: index, %n: index] {
+    util.optimization_barrier %i, %aref : index, !pcf.sref<?x5xi32, #pcf.sequential>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @inline_generic_initializer_with_alloc
+//       CHECK:   pcf.generic scope(#pcf.sequential)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     %[[I:.+]] = arith.constant 42 : index
+//  CHECK-NEXT:     %[[ALLOC:.+]] = memref.alloc(%[[I]]) {alignment = 16 : i64} : memref<?x5xi32>
+//  CHECK-NEXT:     util.optimization_barrier %[[I]], %[[ALLOC]]
+//  CHECK-NEXT:     pcf.return
+
+// -----
+
+util.func private @convert_loop_with_init(%arg0: memref<?x?xi32, strided<[?, 1]>, 3>, %n: index) {
+  %0 = pcf.loop scope(#pcf.test_scope) count(%n)
+    execute(%ref = %arg0)[%count: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>) {
+    util.optimization_barrier %ref : !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0 : memref<?x?xi32, strided<[?, 1]>, 3>
+  util.return
+}
+
+// CHECK-LABEL: @convert_loop_with_init
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//       CHECK:   pcf.loop scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ARG0]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ARG0]]
+
+// -----
+
+util.func private @convert_loop_with_alloc(%d0: index, %d1: index, %d2: index, %n: index) {
+  %0:2 = pcf.loop scope(#pcf.test_scope) count(%n)
+    execute(%ref, %ref_1)[%count: index]
+         : (!pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?xi32>{%d0}, memref<?x?xi32>{%d1, %d2}) {
+    util.optimization_barrier %ref, %ref_1 : !pcf.sref<?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.optimization_barrier %0#0, %0#1 : memref<?xi32>, memref<?x?xi32>
+  util.return
+}
+
+// CHECK-LABEL: @convert_loop_with_alloc
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG1:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[ARG2:[A-Za-z0-9_]+]]: index
+//  CHECK-DAG:    %[[ALLOC:.+]] = memref.alloc(%[[ARG0]]) {alignment = 16 : i64} : memref<?xi32>
+//  CHECK-DAG:    %[[ALLOC1:.+]] = memref.alloc(%[[ARG1]], %[[ARG2]]) {alignment = 16 : i64} : memref<?x?xi32>
+//       CHECK:   pcf.loop scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+//  CHECK-NEXT:     pcf.return
+//       CHECK:   util.optimization_barrier %[[ALLOC]], %[[ALLOC1]]
+
+// -----
+
+util.func private @convert_memref_write_slice(%arg0: memref<?x?xi32>) {
+  pcf.generic scope(#pcf.test_scope)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32>) {
+    %src = memref.alloc() : memref<3x4xi32>
+    pcf.write_slice %src into %ref[1, 2] [3, 4] [1, 1] : memref<3x4xi32> into !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @convert_memref_write_slice
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32>
+//       CHECK:   pcf.generic
+//  CHECK-NEXT:     execute[{{.*}}] {
+//   CHECK-DAG:     %[[SRC:.+]] = memref.alloc()
+//   CHECK-DAG:     %[[SV:.+]] = memref.subview %[[ARG0]][1, 2] [3, 4] [1, 1] : memref<?x?xi32> to memref<3x4xi32, strided<[?, 1], offset: ?>>
+//       CHECK:     memref.copy %[[SRC]], %[[SV]]
+//       CHECK:     pcf.return
+
+// -----
+
+util.func private @convert_vector_write_slice(%arg0: memref<?x?xi32>) {
+  pcf.generic scope(#pcf.test_scope)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32>) {
+    %src = arith.constant dense<0> : vector<3x4xi32>
+    pcf.write_slice %src into %ref[1, 2] [3, 3] [1, 1] : vector<3x4xi32> into !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @convert_vector_write_slice
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32>
+//       CHECK:   pcf.generic
+//  CHECK-NEXT:     execute[{{.*}}] {
+//   CHECK-DAG:     %[[SRC:.+]] = arith.constant dense<0> : vector<3x4xi32>
+//   CHECK-DAG:     %[[SV:.+]] = memref.subview %[[ARG0]][1, 2] [3, 3] [1, 1] : memref<?x?xi32> to memref<3x3xi32, strided<[?, 1], offset: ?>>
+//       CHECK:     vector.transfer_write %[[SRC]], %[[SV]][%c0, %c0] {in_bounds = [true, false]}
+//       CHECK:     pcf.return
+
+// -----
+
+util.func private @convert_tensor_write_slice(%arg0: memref<?x?xi32>) {
+  pcf.generic scope(#pcf.test_scope)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32>) {
+    %src = arith.constant dense<0> : tensor<3x4xi32>
+    pcf.write_slice %src into %ref[1, 2] [3, 4] [1, 1] : tensor<3x4xi32> into !pcf.sref<?x?xi32, #pcf.test_scope>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @convert_tensor_write_slice
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32>
+//       CHECK:   pcf.generic
+//  CHECK-NEXT:     execute[{{.*}}] {
+//   CHECK-DAG:     %[[SRC:.+]] = arith.constant dense<0> : tensor<3x4xi32>
+//   CHECK-DAG:     %[[SV:.+]] = memref.subview %[[ARG0]][1, 2] [3, 4] [1, 1] : memref<?x?xi32> to memref<3x4xi32, strided<[?, 1], offset: ?>>
+//       CHECK:     iree_codegen.store_to_buffer %[[SRC]], %[[SV]]
+//       CHECK:     pcf.return
+
+// -----
+
+util.func private @convert_for_loop_swap(
+    %arg0: memref<?x?xi32, strided<[?, 1]>, 3>,
+    %arg1: memref<?x?xi32, strided<[?, 1]>, 3>) {
+  %c0 = arith.constant 0 : index
+  %c1 = arith.constant 1 : index
+  %c10 = arith.constant 10 : index
+  %0:2 = pcf.generic scope(#pcf.test_scope)
+    execute(%ref0 = %arg0, %ref1 = %arg1)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>, memref<?x?xi32, strided<[?, 1]>, 3>) {
+    scf.for %i = %c0 to %c10 step %c1 iter_args(%iter0 = %ref0, %iter1 = %ref1)
+        -> (!pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>) {
+      %b:2 = util.optimization_barrier %iter1, %iter0 : !pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+      scf.yield %b#0, %b#1 : !pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+    }
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @convert_for_loop_swap
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//  CHECK-SAME:     %[[ARG1:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//  CHECK-NEXT:     scf.for {{.*}} iter_args(%[[ITER0:.+]] = %[[ARG0]], %[[ITER1:.+]] = %[[ARG1]])
+//  CHECK-NEXT:       %[[B:.+]]:2 = util.optimization_barrier %[[ITER1]], %[[ITER0]]
+//  CHECK-NEXT:       scf.yield %[[B]]#0, %[[B]]#1
+//       CHECK:     pcf.return
+
+// -----
+
+util.func private @convert_for_loop_swap_conflict(
+    %arg0: memref<?x?xi32, strided<[3, 1]>, 3>,
+    %arg1: memref<?x?xi32, strided<[4, 1]>, 3>) {
+  %c0 = arith.constant 0 : index
+  %c1 = arith.constant 1 : index
+  %c10 = arith.constant 10 : index
+  %0:2 = pcf.generic scope(#pcf.test_scope)
+    execute(%ref0 = %arg0, %ref1 = %arg1)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[3, 1]>, 3>, memref<?x?xi32, strided<[4, 1]>, 3>) {
+    scf.for %i = %c0 to %c10 step %c1 iter_args(%iter0 = %ref0, %iter1 = %ref1)
+        -> (!pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>) {
+      %b:2 = util.optimization_barrier %iter1, %iter0 : !pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+      scf.yield %b#0, %b#1 : !pcf.sref<?x?xi32, #pcf.test_scope>, !pcf.sref<?x?xi32, #pcf.test_scope>
+    }
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @convert_for_loop_swap_conflict
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[3, 1]>, 3>
+//  CHECK-SAME:     %[[ARG1:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[4, 1]>, 3>
+//       CHECK:   pcf.generic scope(#pcf.test_scope)
+//  CHECK-NEXT:     execute[{{.*}}] {
+//   CHECK-DAG:     %[[CAST0:.+]] = memref.cast %[[ARG0]] : memref<?x?xi32, strided<[3, 1]>, 3> to memref<?x?xi32, strided<[?, 1]>, 3>
+//   CHECK-DAG:     %[[CAST1:.+]] = memref.cast %[[ARG1]] : memref<?x?xi32, strided<[4, 1]>, 3> to memref<?x?xi32, strided<[?, 1]>, 3>
+//  CHECK-NEXT:     scf.for {{.*}} iter_args(%[[ITER0:.+]] = %[[CAST0]], %[[ITER1:.+]] = %[[CAST1]])
+//  CHECK-NEXT:       %[[B:.+]]:2 = util.optimization_barrier %[[ITER1]], %[[ITER0]]
+//  CHECK-NEXT:       scf.yield %[[B]]#0, %[[B]]#1
+//       CHECK:     pcf.return
+
+// -----
+
+func.func @convert_alloc(%d0: index) -> !pcf.sref<?x5xi32, #pcf.sequential> {
+  %0 = pcf.alloc(%d0) : !pcf.sref<?x5xi32, #pcf.sequential>
+  return %0 : !pcf.sref<?x5xi32, #pcf.sequential>
+}
+
+// CHECK-LABEL: @convert_alloc
+//  CHECK-SAME:   %[[D0:[A-Za-z0-9]+]]: index
+//       CHECK:   %[[ALLOC:.+]] = memref.alloc(%[[D0]]) {alignment = 16 : i64} : memref<?x5xi32>
+//       CHECK:   return %[[ALLOC]] : memref<?x5xi32>
+
+// -----
+
+util.func private @convert_get_memref(%arg0: memref<?x?xi32, strided<[?, 1]>, 3>, %s0: index, %s1: index) {
+  pcf.generic scope(#pcf.test_scope)
+    execute(%ref = %arg0)[%id: index, %n: index]
+         : (!pcf.sref<?x?xi32, #pcf.test_scope>)
+        -> (memref<?x?xi32, strided<[?, 1]>, 3>) {
+    %view = pcf.get_memref %ref[0, 1] [%s0, %s1] [1, 1] : !pcf.sref<?x?xi32, #pcf.test_scope> to memref<?x?xi32, strided<[?, ?], offset: ?>>
+    util.optimization_barrier %view : memref<?x?xi32, strided<[?, ?], offset: ?>>
+    pcf.return
+  }
+  util.return
+}
+
+// CHECK-LABEL: @convert_get_memref
+//  CHECK-SAME:     %[[ARG0:[A-Za-z0-9_]+]]: memref<?x?xi32, strided<[?, 1]>, 3>
+//  CHECK-SAME:     %[[S0:[A-Za-z0-9_]+]]: index
+//  CHECK-SAME:     %[[S1:[A-Za-z0-9_]+]]: index
+//       CHECK:   pcf.generic
+//  CHECK-NEXT:     execute[{{.*}}] {
+//   CHECK-DAG:     %[[CAST:.+]] = memref.memory_space_cast %[[ARG0]] : memref<?x?xi32, strided<[?, 1]>, 3> to memref<?x?xi32, strided<[?, 1]>>
+//   CHECK-DAG:     %[[CAST2:.+]] = memref.cast %[[CAST]] : memref<?x?xi32, strided<[?, 1]>> to memref<?x?xi32, strided<[?, ?], offset: ?>>
+//   CHECK-DAG:     %[[SV:.+]] = memref.subview %[[CAST2]][0, 1] [%[[S0]], %[[S1]]] [1, 1] : memref<?x?xi32, strided<[?, ?], offset: ?>> to memref<?x?xi32, strided<[?, ?], offset: ?>>
+//       CHECK:     util.optimization_barrier %[[SV]]
+//       CHECK:     pcf.return
+
+// -----
+
+util.func private @convert_get_memref_no_space(%arg0: memref<?x?xi32, strided<[?, 1]>>, %s0: index, %s1: index) {
```

**Comment:**
nit: Maybe put it above the previous test case since this is the less specialized case?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:8`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
```

**Comment:**
Why do we need these? Can you trim down the includes?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:89`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
```

**Comment:**
```suggestion
struct StridedLayoutState final : DFX::AbstractState {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:254`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
```

**Comment:**
```suggestion
        .Case([&](PCF::AllocOp allocOp) {
```
also below

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:325`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
```

**Comment:**
Can we return early here?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:398`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
```

**Comment:**
nit: spell this out

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:419`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
```

**Comment:**
`walkValuesOfType<PCF::ShapedRefType>`?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:427`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
```

**Comment:**
spell this out

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:472`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
```

**Comment:**
spell this out

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:573`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
```

**Comment:**
```suggestion
struct ConvertGenericOp final : OpConversionPattern<PCF::GenericOp> {
```
also elsewhere below

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:633`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
```

**Comment:**
type

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:627`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
```

**Comment:**
llvm::append_range

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:651`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
```

**Comment:**
```suggestion
    if (!llvm::all_of(loopOp.getResultTypes(), llvm::IsaPred<MemRefType>)) {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:729`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
```

**Comment:**
nit: `continue` early

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:709`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newLoopOp =
+        PCF::LoopOp::create(rewriter, loc, loopOp.getScope(), loopOp.getCount(),
+                            loopOp.getSyncOnReturn());
+    SmallVector<Value> newArgs(replacements);
+    newArgs.append(newLoopOp.getRegion().getArguments().begin(),
+                   newLoopOp.getRegion().getArguments().end());
+    Block *entryBlock = &loopOp.getRegion().front();
+    Block *newEntry = &newLoopOp.getRegion().front();
+    // Inline the body into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               newArgs);
+
+    // replace the old op.
+    rewriter.replaceOp(loopOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value destSlice = memref::SubViewOp::create(
+        rewriter, writeOp.getLoc(), adaptor.getDest(),
+        writeOp.getMixedOffsets(), writeOp.getMixedSizes(),
+        writeOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(writeOp.getSourceType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
```

**Comment:**
```suggestion
        .Case([&](RankedTensorType tensor) {
```
Also below

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:749`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newLoopOp =
+        PCF::LoopOp::create(rewriter, loc, loopOp.getScope(), loopOp.getCount(),
+                            loopOp.getSyncOnReturn());
+    SmallVector<Value> newArgs(replacements);
+    newArgs.append(newLoopOp.getRegion().getArguments().begin(),
+                   newLoopOp.getRegion().getArguments().end());
+    Block *entryBlock = &loopOp.getRegion().front();
+    Block *newEntry = &newLoopOp.getRegion().front();
+    // Inline the body into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               newArgs);
+
+    // replace the old op.
+    rewriter.replaceOp(loopOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value destSlice = memref::SubViewOp::create(
+        rewriter, writeOp.getLoc(), adaptor.getDest(),
+        writeOp.getMixedOffsets(), writeOp.getMixedSizes(),
+        writeOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(writeOp.getSourceType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::StoreToBufferOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<MemRefType>([&](MemRefType memref) {
+          rewriter.replaceOpWithNewOp<memref::CopyOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, storeSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), writeOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == storeSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, writeOp.getLoc(), 0));
+          rewriter.replaceOpWithNewOp<vector::TransferWriteOp>(
+              writeOp, writeOp.getSource(), destSlice, offsets, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertReadSliceOp : public OpConversionPattern<PCF::ReadSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::ReadSliceOp readOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value sourceSlice = memref::SubViewOp::create(
+        rewriter, readOp.getLoc(), adaptor.getSource(),
+        readOp.getMixedOffsets(), readOp.getMixedSizes(),
+        readOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(readOp.getResultType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:821`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newLoopOp =
+        PCF::LoopOp::create(rewriter, loc, loopOp.getScope(), loopOp.getCount(),
+                            loopOp.getSyncOnReturn());
+    SmallVector<Value> newArgs(replacements);
+    newArgs.append(newLoopOp.getRegion().getArguments().begin(),
+                   newLoopOp.getRegion().getArguments().end());
+    Block *entryBlock = &loopOp.getRegion().front();
+    Block *newEntry = &newLoopOp.getRegion().front();
+    // Inline the body into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               newArgs);
+
+    // replace the old op.
+    rewriter.replaceOp(loopOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value destSlice = memref::SubViewOp::create(
+        rewriter, writeOp.getLoc(), adaptor.getDest(),
+        writeOp.getMixedOffsets(), writeOp.getMixedSizes(),
+        writeOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(writeOp.getSourceType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::StoreToBufferOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<MemRefType>([&](MemRefType memref) {
+          rewriter.replaceOpWithNewOp<memref::CopyOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, storeSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), writeOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == storeSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, writeOp.getLoc(), 0));
+          rewriter.replaceOpWithNewOp<vector::TransferWriteOp>(
+              writeOp, writeOp.getSource(), destSlice, offsets, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertReadSliceOp : public OpConversionPattern<PCF::ReadSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::ReadSliceOp readOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value sourceSlice = memref::SubViewOp::create(
+        rewriter, readOp.getLoc(), adaptor.getSource(),
+        readOp.getMixedOffsets(), readOp.getMixedSizes(),
+        readOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(readOp.getResultType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::LoadFromBufferOp>(
+              readOp, tensor, sourceSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, loadSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), readOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == loadSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, readOp.getLoc(), 0));
+          // Use zero padding value.
+          Value zeroPadding = arith::ConstantOp::create(
+              rewriter, readOp.getLoc(), vector.getElementType(),
+              rewriter.getZeroAttr(vector.getElementType()));
+          rewriter.replaceOpWithNewOp<vector::TransferReadOp>(
+              readOp, vector, sourceSlice, offsets, zeroPadding, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertGetMemrefOp : public OpConversionPattern<PCF::GetMemrefOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GetMemrefOp getMemrefOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // The source has been converted to memref with refined layout and memory
+    // space. We need to cast it to match the maximally dynamic layout and no
+    // memory space of the GetMemrefOp result type.
+    Value source = adaptor.getSource();
+    auto sourceType = cast<MemRefType>(source.getType());
+    auto resultType = getMemrefOp.getResultType();
+
+    // Cast away memory space if present.
+    if (sourceType.getMemorySpace()) {
+      auto noMemSpaceType =
+          MemRefType::get(sourceType.getShape(), sourceType.getElementType(),
+                          sourceType.getLayout(), nullptr);
+      source = memref::MemorySpaceCastOp::create(rewriter, getMemrefOp.getLoc(),
+                                                 noMemSpaceType, source);
+      sourceType = noMemSpaceType;
+    }
+
+    // Cast to maximally dynamic layout to match the return type.
+    if (sourceType.getLayout() != resultType.getLayout()) {
+      source = memref::CastOp::create(rewriter, getMemrefOp.getLoc(),
+                                      resultType, source);
+    }
+
+    // Create subview using the slice params.
+    rewriter.replaceOpWithNewOp<memref::SubViewOp>(
+        getMemrefOp, source, getMemrefOp.getMixedOffsets(),
+        getMemrefOp.getMixedSizes(), getMemrefOp.getMixedStrides());
+    return success();
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto allocType = cast_if_present<MemRefType>(
+        getTypeConverter()->convertType(allocOp.getResult()));
```

**Comment:**
```suggestion
    auto allocType =
        getTypeConverter()->convertType<MemRefType>(allocOp.getResult());
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:882`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newLoopOp =
+        PCF::LoopOp::create(rewriter, loc, loopOp.getScope(), loopOp.getCount(),
+                            loopOp.getSyncOnReturn());
+    SmallVector<Value> newArgs(replacements);
+    newArgs.append(newLoopOp.getRegion().getArguments().begin(),
+                   newLoopOp.getRegion().getArguments().end());
+    Block *entryBlock = &loopOp.getRegion().front();
+    Block *newEntry = &newLoopOp.getRegion().front();
+    // Inline the body into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               newArgs);
+
+    // replace the old op.
+    rewriter.replaceOp(loopOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value destSlice = memref::SubViewOp::create(
+        rewriter, writeOp.getLoc(), adaptor.getDest(),
+        writeOp.getMixedOffsets(), writeOp.getMixedSizes(),
+        writeOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(writeOp.getSourceType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::StoreToBufferOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<MemRefType>([&](MemRefType memref) {
+          rewriter.replaceOpWithNewOp<memref::CopyOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, storeSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), writeOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == storeSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, writeOp.getLoc(), 0));
+          rewriter.replaceOpWithNewOp<vector::TransferWriteOp>(
+              writeOp, writeOp.getSource(), destSlice, offsets, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertReadSliceOp : public OpConversionPattern<PCF::ReadSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::ReadSliceOp readOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value sourceSlice = memref::SubViewOp::create(
+        rewriter, readOp.getLoc(), adaptor.getSource(),
+        readOp.getMixedOffsets(), readOp.getMixedSizes(),
+        readOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(readOp.getResultType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::LoadFromBufferOp>(
+              readOp, tensor, sourceSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, loadSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), readOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == loadSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, readOp.getLoc(), 0));
+          // Use zero padding value.
+          Value zeroPadding = arith::ConstantOp::create(
+              rewriter, readOp.getLoc(), vector.getElementType(),
+              rewriter.getZeroAttr(vector.getElementType()));
+          rewriter.replaceOpWithNewOp<vector::TransferReadOp>(
+              readOp, vector, sourceSlice, offsets, zeroPadding, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertGetMemrefOp : public OpConversionPattern<PCF::GetMemrefOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GetMemrefOp getMemrefOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // The source has been converted to memref with refined layout and memory
+    // space. We need to cast it to match the maximally dynamic layout and no
+    // memory space of the GetMemrefOp result type.
+    Value source = adaptor.getSource();
+    auto sourceType = cast<MemRefType>(source.getType());
+    auto resultType = getMemrefOp.getResultType();
+
+    // Cast away memory space if present.
+    if (sourceType.getMemorySpace()) {
+      auto noMemSpaceType =
+          MemRefType::get(sourceType.getShape(), sourceType.getElementType(),
+                          sourceType.getLayout(), nullptr);
+      source = memref::MemorySpaceCastOp::create(rewriter, getMemrefOp.getLoc(),
+                                                 noMemSpaceType, source);
+      sourceType = noMemSpaceType;
+    }
+
+    // Cast to maximally dynamic layout to match the return type.
+    if (sourceType.getLayout() != resultType.getLayout()) {
+      source = memref::CastOp::create(rewriter, getMemrefOp.getLoc(),
+                                      resultType, source);
+    }
+
+    // Create subview using the slice params.
+    rewriter.replaceOpWithNewOp<memref::SubViewOp>(
+        getMemrefOp, source, getMemrefOp.getMixedOffsets(),
+        getMemrefOp.getMixedSizes(), getMemrefOp.getMixedStrides());
+    return success();
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto allocType = cast_if_present<MemRefType>(
+        getTypeConverter()->convertType(allocOp.getResult()));
+    if (!allocType) {
+      return rewriter.notifyMatchFailure(allocOp,
+                                         "failed to convert alloc type");
+    }
+
+    // TODO: This pattern is a hack. We should be directly allocating memory as
+    // a global here. Instead we rely on dubious hoisting patterns to make this
+    // work as intended.
+    IntegerAttr alignment =
+        allocOp.getResultType().getScope().getPreferredAllocAlignment(
+            rewriter.getContext());
+    rewriter.replaceOpWithNewOp<memref::AllocOp>(
+        allocOp, allocType, adaptor.getDynamicSizes(),
+        /*symbolOperands=*/ValueRange(), alignment);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, adaptor.getOperands());
+    return success();
+  }
+};
+
+//===----------------------------------------------------------------------===//
+// Control Flow Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertFuncOp final : OpConversionPattern<func::FuncOp> {
+  ConvertFuncOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    FailureOr<FunctionType> maybeNewFunctionType =
+        mapping.lookupConvertedFunctionArgs(funcOp);
+    if (failed(maybeNewFunctionType)) {
+      return failure();
+    }
+    FunctionType newFunctionType = maybeNewFunctionType.value();
+
+    // Convert the original function types.
+    TypeConverter::SignatureConversion result(newFunctionType.getNumInputs());
+    for (auto [i, t] : llvm::enumerate(newFunctionType.getInputs())) {
+      result.addInputs(i, t);
+    }
+    if (failed(rewriter.convertRegionTypes(&funcOp.getFunctionBody(),
+                                           *getTypeConverter(), &result)))
+      return failure();
```

**Comment:**
nit: missing braces

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:1081`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newLoopOp =
+        PCF::LoopOp::create(rewriter, loc, loopOp.getScope(), loopOp.getCount(),
+                            loopOp.getSyncOnReturn());
+    SmallVector<Value> newArgs(replacements);
+    newArgs.append(newLoopOp.getRegion().getArguments().begin(),
+                   newLoopOp.getRegion().getArguments().end());
+    Block *entryBlock = &loopOp.getRegion().front();
+    Block *newEntry = &newLoopOp.getRegion().front();
+    // Inline the body into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               newArgs);
+
+    // replace the old op.
+    rewriter.replaceOp(loopOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value destSlice = memref::SubViewOp::create(
+        rewriter, writeOp.getLoc(), adaptor.getDest(),
+        writeOp.getMixedOffsets(), writeOp.getMixedSizes(),
+        writeOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(writeOp.getSourceType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::StoreToBufferOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<MemRefType>([&](MemRefType memref) {
+          rewriter.replaceOpWithNewOp<memref::CopyOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, storeSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), writeOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == storeSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, writeOp.getLoc(), 0));
+          rewriter.replaceOpWithNewOp<vector::TransferWriteOp>(
+              writeOp, writeOp.getSource(), destSlice, offsets, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertReadSliceOp : public OpConversionPattern<PCF::ReadSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::ReadSliceOp readOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value sourceSlice = memref::SubViewOp::create(
+        rewriter, readOp.getLoc(), adaptor.getSource(),
+        readOp.getMixedOffsets(), readOp.getMixedSizes(),
+        readOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(readOp.getResultType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::LoadFromBufferOp>(
+              readOp, tensor, sourceSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, loadSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), readOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == loadSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, readOp.getLoc(), 0));
+          // Use zero padding value.
+          Value zeroPadding = arith::ConstantOp::create(
+              rewriter, readOp.getLoc(), vector.getElementType(),
+              rewriter.getZeroAttr(vector.getElementType()));
+          rewriter.replaceOpWithNewOp<vector::TransferReadOp>(
+              readOp, vector, sourceSlice, offsets, zeroPadding, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertGetMemrefOp : public OpConversionPattern<PCF::GetMemrefOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GetMemrefOp getMemrefOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // The source has been converted to memref with refined layout and memory
+    // space. We need to cast it to match the maximally dynamic layout and no
+    // memory space of the GetMemrefOp result type.
+    Value source = adaptor.getSource();
+    auto sourceType = cast<MemRefType>(source.getType());
+    auto resultType = getMemrefOp.getResultType();
+
+    // Cast away memory space if present.
+    if (sourceType.getMemorySpace()) {
+      auto noMemSpaceType =
+          MemRefType::get(sourceType.getShape(), sourceType.getElementType(),
+                          sourceType.getLayout(), nullptr);
+      source = memref::MemorySpaceCastOp::create(rewriter, getMemrefOp.getLoc(),
+                                                 noMemSpaceType, source);
+      sourceType = noMemSpaceType;
+    }
+
+    // Cast to maximally dynamic layout to match the return type.
+    if (sourceType.getLayout() != resultType.getLayout()) {
+      source = memref::CastOp::create(rewriter, getMemrefOp.getLoc(),
+                                      resultType, source);
+    }
+
+    // Create subview using the slice params.
+    rewriter.replaceOpWithNewOp<memref::SubViewOp>(
+        getMemrefOp, source, getMemrefOp.getMixedOffsets(),
+        getMemrefOp.getMixedSizes(), getMemrefOp.getMixedStrides());
+    return success();
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto allocType = cast_if_present<MemRefType>(
+        getTypeConverter()->convertType(allocOp.getResult()));
+    if (!allocType) {
+      return rewriter.notifyMatchFailure(allocOp,
+                                         "failed to convert alloc type");
+    }
+
+    // TODO: This pattern is a hack. We should be directly allocating memory as
+    // a global here. Instead we rely on dubious hoisting patterns to make this
+    // work as intended.
+    IntegerAttr alignment =
+        allocOp.getResultType().getScope().getPreferredAllocAlignment(
+            rewriter.getContext());
+    rewriter.replaceOpWithNewOp<memref::AllocOp>(
+        allocOp, allocType, adaptor.getDynamicSizes(),
+        /*symbolOperands=*/ValueRange(), alignment);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, adaptor.getOperands());
+    return success();
+  }
+};
+
+//===----------------------------------------------------------------------===//
+// Control Flow Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertFuncOp final : OpConversionPattern<func::FuncOp> {
+  ConvertFuncOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    FailureOr<FunctionType> maybeNewFunctionType =
+        mapping.lookupConvertedFunctionArgs(funcOp);
+    if (failed(maybeNewFunctionType)) {
+      return failure();
+    }
+    FunctionType newFunctionType = maybeNewFunctionType.value();
+
+    // Convert the original function types.
+    TypeConverter::SignatureConversion result(newFunctionType.getNumInputs());
+    for (auto [i, t] : llvm::enumerate(newFunctionType.getInputs())) {
+      result.addInputs(i, t);
+    }
+    if (failed(rewriter.convertRegionTypes(&funcOp.getFunctionBody(),
+                                           *getTypeConverter(), &result)))
+      return failure();
+
+    rewriter.modifyOpInPlace(funcOp, [&] { funcOp.setType(newFunctionType); });
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+class ConvertReturnOp : public OpConversionPattern<func::ReturnOp> {
+public:
+  ConvertReturnOp(TypeConverter &typeConverter, MLIRContext *context,
+                  SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  LogicalResult
+  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const final {
+    auto parent = cast<func::FuncOp>(op->getParentOp());
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(parent);
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(op,
+                                         "failed to get converted parent type");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), operands,
+                     targetFuncType.value().getResults());
+    rewriter.replaceOpWithNewOp<func::ReturnOp>(op, operands);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertCallOp final : OpConversionPattern<func::CallOp> {
+  ConvertCallOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::CallOp callOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the context aware result types.
+    for (Value v : callOp.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(callOp,
+                                           "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(
+            callOp, cast<SymbolRefAttr>(callOp.getCallableForCallee()));
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(callOp,
+                                         "could not convert argument types");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, callOp.getLoc(), operands,
+                     targetFuncType.value().getInputs());
+
+    // Substitute with the new result types from the corresponding FuncType
+    // conversion.
+    auto newCallOp = func::CallOp::create(
+        rewriter, callOp.getLoc(), callOp.getCallee(), resultTypes, operands);
+    rewriter.replaceOp(callOp, newCallOp);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+//===----------------------------------------------------------------------===//
+// SCF Conversion Pattern overrides
+//===----------------------------------------------------------------------===//
+
+struct ConvertForOp : public OpConversionPattern<scf::ForOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::ForOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getInitArgs());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::ForOp::create(rewriter, op.getLoc(), adaptor.getLowerBound(),
+                           adaptor.getUpperBound(), adaptor.getStep(), inits,
+                           /*bodyBuilder=*/nullptr, adaptor.getUnsignedCmp());
+    if (failed(rewriter.convertRegionTypes(&op.getRegion(), *typeConverter)))
+      return failure();
+
+    // Drop the rewriter created block.
+    rewriter.eraseBlock(newOp.getBody(0));
+
+    // Inline the original (now converted) body.
+    auto &dstRegion = newOp.getRegion();
+    rewriter.inlineRegionBefore(op.getRegion(), dstRegion, dstRegion.end());
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+struct ConvertWhileOp : public OpConversionPattern<scf::WhileOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::WhileOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::WhileOp::create(rewriter, op.getLoc(), resultTypes, inits);
+    for (auto i : {0u, 1u}) {
+      if (failed(rewriter.convertRegionTypes(&op.getRegion(i), *typeConverter)))
+        return failure();
+      auto &dstRegion = newOp.getRegion(i);
+      rewriter.inlineRegionBefore(op.getRegion(i), dstRegion, dstRegion.end());
+    }
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+void ConvertSRefToMemRefPass::runOnOperation() {
+  auto *context = &getContext();
+
+  SRefLayoutAnalysis analysis(getOperation());
+  if (failed(analysis.run())) {
+    return signalPassFailure();
+  }
+
+  TypeConverter typeConverter;
+  ConversionTarget conversionTarget(getContext());
+  RewritePatternSet patterns(&getContext());
+
+  // Add a context aware type converter that uses the layout analysis.
+  typeConverter.addConversion([&](Value v) -> std::optional<Type> {
+    if (isa<PCF::ShapedRefType>(v.getType())) {
+      FailureOr<MemRefType> maybeConvertedType = analysis.getConvertedType(v);
+      if (failed(maybeConvertedType)) {
+        return Type();
+      }
+      return maybeConvertedType.value();
+    }
+    // Passthrough for everything else.
+    return v.getType();
+  });
+
+  ConversionTarget target(*context);
+  auto isIllegalType = [&](Type t) { return isa<PCF::ShapedRefType>(t); };
+
+  // Verify that all operand, result, and region argument types have been
+  // converted. This does not use the type converter because the type converter
+  // only implements context specific conversions.
+  auto isLegallyTypedOp = [&](Operation *op) -> bool {
+    for (Type type : op->getResultTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (Type type : op->getOperandTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (auto &region : op->getRegions()) {
```

**Comment:**
type

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:1082`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newLoopOp =
+        PCF::LoopOp::create(rewriter, loc, loopOp.getScope(), loopOp.getCount(),
+                            loopOp.getSyncOnReturn());
+    SmallVector<Value> newArgs(replacements);
+    newArgs.append(newLoopOp.getRegion().getArguments().begin(),
+                   newLoopOp.getRegion().getArguments().end());
+    Block *entryBlock = &loopOp.getRegion().front();
+    Block *newEntry = &newLoopOp.getRegion().front();
+    // Inline the body into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               newArgs);
+
+    // replace the old op.
+    rewriter.replaceOp(loopOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value destSlice = memref::SubViewOp::create(
+        rewriter, writeOp.getLoc(), adaptor.getDest(),
+        writeOp.getMixedOffsets(), writeOp.getMixedSizes(),
+        writeOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(writeOp.getSourceType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::StoreToBufferOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<MemRefType>([&](MemRefType memref) {
+          rewriter.replaceOpWithNewOp<memref::CopyOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, storeSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), writeOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == storeSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, writeOp.getLoc(), 0));
+          rewriter.replaceOpWithNewOp<vector::TransferWriteOp>(
+              writeOp, writeOp.getSource(), destSlice, offsets, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertReadSliceOp : public OpConversionPattern<PCF::ReadSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::ReadSliceOp readOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value sourceSlice = memref::SubViewOp::create(
+        rewriter, readOp.getLoc(), adaptor.getSource(),
+        readOp.getMixedOffsets(), readOp.getMixedSizes(),
+        readOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(readOp.getResultType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::LoadFromBufferOp>(
+              readOp, tensor, sourceSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, loadSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), readOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == loadSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, readOp.getLoc(), 0));
+          // Use zero padding value.
+          Value zeroPadding = arith::ConstantOp::create(
+              rewriter, readOp.getLoc(), vector.getElementType(),
+              rewriter.getZeroAttr(vector.getElementType()));
+          rewriter.replaceOpWithNewOp<vector::TransferReadOp>(
+              readOp, vector, sourceSlice, offsets, zeroPadding, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertGetMemrefOp : public OpConversionPattern<PCF::GetMemrefOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GetMemrefOp getMemrefOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // The source has been converted to memref with refined layout and memory
+    // space. We need to cast it to match the maximally dynamic layout and no
+    // memory space of the GetMemrefOp result type.
+    Value source = adaptor.getSource();
+    auto sourceType = cast<MemRefType>(source.getType());
+    auto resultType = getMemrefOp.getResultType();
+
+    // Cast away memory space if present.
+    if (sourceType.getMemorySpace()) {
+      auto noMemSpaceType =
+          MemRefType::get(sourceType.getShape(), sourceType.getElementType(),
+                          sourceType.getLayout(), nullptr);
+      source = memref::MemorySpaceCastOp::create(rewriter, getMemrefOp.getLoc(),
+                                                 noMemSpaceType, source);
+      sourceType = noMemSpaceType;
+    }
+
+    // Cast to maximally dynamic layout to match the return type.
+    if (sourceType.getLayout() != resultType.getLayout()) {
+      source = memref::CastOp::create(rewriter, getMemrefOp.getLoc(),
+                                      resultType, source);
+    }
+
+    // Create subview using the slice params.
+    rewriter.replaceOpWithNewOp<memref::SubViewOp>(
+        getMemrefOp, source, getMemrefOp.getMixedOffsets(),
+        getMemrefOp.getMixedSizes(), getMemrefOp.getMixedStrides());
+    return success();
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto allocType = cast_if_present<MemRefType>(
+        getTypeConverter()->convertType(allocOp.getResult()));
+    if (!allocType) {
+      return rewriter.notifyMatchFailure(allocOp,
+                                         "failed to convert alloc type");
+    }
+
+    // TODO: This pattern is a hack. We should be directly allocating memory as
+    // a global here. Instead we rely on dubious hoisting patterns to make this
+    // work as intended.
+    IntegerAttr alignment =
+        allocOp.getResultType().getScope().getPreferredAllocAlignment(
+            rewriter.getContext());
+    rewriter.replaceOpWithNewOp<memref::AllocOp>(
+        allocOp, allocType, adaptor.getDynamicSizes(),
+        /*symbolOperands=*/ValueRange(), alignment);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, adaptor.getOperands());
+    return success();
+  }
+};
+
+//===----------------------------------------------------------------------===//
+// Control Flow Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertFuncOp final : OpConversionPattern<func::FuncOp> {
+  ConvertFuncOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    FailureOr<FunctionType> maybeNewFunctionType =
+        mapping.lookupConvertedFunctionArgs(funcOp);
+    if (failed(maybeNewFunctionType)) {
+      return failure();
+    }
+    FunctionType newFunctionType = maybeNewFunctionType.value();
+
+    // Convert the original function types.
+    TypeConverter::SignatureConversion result(newFunctionType.getNumInputs());
+    for (auto [i, t] : llvm::enumerate(newFunctionType.getInputs())) {
+      result.addInputs(i, t);
+    }
+    if (failed(rewriter.convertRegionTypes(&funcOp.getFunctionBody(),
+                                           *getTypeConverter(), &result)))
+      return failure();
+
+    rewriter.modifyOpInPlace(funcOp, [&] { funcOp.setType(newFunctionType); });
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+class ConvertReturnOp : public OpConversionPattern<func::ReturnOp> {
+public:
+  ConvertReturnOp(TypeConverter &typeConverter, MLIRContext *context,
+                  SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  LogicalResult
+  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const final {
+    auto parent = cast<func::FuncOp>(op->getParentOp());
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(parent);
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(op,
+                                         "failed to get converted parent type");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), operands,
+                     targetFuncType.value().getResults());
+    rewriter.replaceOpWithNewOp<func::ReturnOp>(op, operands);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertCallOp final : OpConversionPattern<func::CallOp> {
+  ConvertCallOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::CallOp callOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the context aware result types.
+    for (Value v : callOp.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(callOp,
+                                           "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(
+            callOp, cast<SymbolRefAttr>(callOp.getCallableForCallee()));
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(callOp,
+                                         "could not convert argument types");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, callOp.getLoc(), operands,
+                     targetFuncType.value().getInputs());
+
+    // Substitute with the new result types from the corresponding FuncType
+    // conversion.
+    auto newCallOp = func::CallOp::create(
+        rewriter, callOp.getLoc(), callOp.getCallee(), resultTypes, operands);
+    rewriter.replaceOp(callOp, newCallOp);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+//===----------------------------------------------------------------------===//
+// SCF Conversion Pattern overrides
+//===----------------------------------------------------------------------===//
+
+struct ConvertForOp : public OpConversionPattern<scf::ForOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::ForOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getInitArgs());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::ForOp::create(rewriter, op.getLoc(), adaptor.getLowerBound(),
+                           adaptor.getUpperBound(), adaptor.getStep(), inits,
+                           /*bodyBuilder=*/nullptr, adaptor.getUnsignedCmp());
+    if (failed(rewriter.convertRegionTypes(&op.getRegion(), *typeConverter)))
+      return failure();
+
+    // Drop the rewriter created block.
+    rewriter.eraseBlock(newOp.getBody(0));
+
+    // Inline the original (now converted) body.
+    auto &dstRegion = newOp.getRegion();
+    rewriter.inlineRegionBefore(op.getRegion(), dstRegion, dstRegion.end());
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+struct ConvertWhileOp : public OpConversionPattern<scf::WhileOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::WhileOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::WhileOp::create(rewriter, op.getLoc(), resultTypes, inits);
+    for (auto i : {0u, 1u}) {
+      if (failed(rewriter.convertRegionTypes(&op.getRegion(i), *typeConverter)))
+        return failure();
+      auto &dstRegion = newOp.getRegion(i);
+      rewriter.inlineRegionBefore(op.getRegion(i), dstRegion, dstRegion.end());
+    }
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+void ConvertSRefToMemRefPass::runOnOperation() {
+  auto *context = &getContext();
+
+  SRefLayoutAnalysis analysis(getOperation());
+  if (failed(analysis.run())) {
+    return signalPassFailure();
+  }
+
+  TypeConverter typeConverter;
+  ConversionTarget conversionTarget(getContext());
+  RewritePatternSet patterns(&getContext());
+
+  // Add a context aware type converter that uses the layout analysis.
+  typeConverter.addConversion([&](Value v) -> std::optional<Type> {
+    if (isa<PCF::ShapedRefType>(v.getType())) {
+      FailureOr<MemRefType> maybeConvertedType = analysis.getConvertedType(v);
+      if (failed(maybeConvertedType)) {
+        return Type();
+      }
+      return maybeConvertedType.value();
+    }
+    // Passthrough for everything else.
+    return v.getType();
+  });
+
+  ConversionTarget target(*context);
+  auto isIllegalType = [&](Type t) { return isa<PCF::ShapedRefType>(t); };
+
+  // Verify that all operand, result, and region argument types have been
+  // converted. This does not use the type converter because the type converter
+  // only implements context specific conversions.
+  auto isLegallyTypedOp = [&](Operation *op) -> bool {
+    for (Type type : op->getResultTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (Type type : op->getOperandTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (auto &region : op->getRegions()) {
+      for (auto type : region.getArgumentTypes()) {
```

**Comment:**
type

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:1089`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newLoopOp =
+        PCF::LoopOp::create(rewriter, loc, loopOp.getScope(), loopOp.getCount(),
+                            loopOp.getSyncOnReturn());
+    SmallVector<Value> newArgs(replacements);
+    newArgs.append(newLoopOp.getRegion().getArguments().begin(),
+                   newLoopOp.getRegion().getArguments().end());
+    Block *entryBlock = &loopOp.getRegion().front();
+    Block *newEntry = &newLoopOp.getRegion().front();
+    // Inline the body into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               newArgs);
+
+    // replace the old op.
+    rewriter.replaceOp(loopOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value destSlice = memref::SubViewOp::create(
+        rewriter, writeOp.getLoc(), adaptor.getDest(),
+        writeOp.getMixedOffsets(), writeOp.getMixedSizes(),
+        writeOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(writeOp.getSourceType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::StoreToBufferOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<MemRefType>([&](MemRefType memref) {
+          rewriter.replaceOpWithNewOp<memref::CopyOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, storeSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), writeOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == storeSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, writeOp.getLoc(), 0));
+          rewriter.replaceOpWithNewOp<vector::TransferWriteOp>(
+              writeOp, writeOp.getSource(), destSlice, offsets, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertReadSliceOp : public OpConversionPattern<PCF::ReadSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::ReadSliceOp readOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value sourceSlice = memref::SubViewOp::create(
+        rewriter, readOp.getLoc(), adaptor.getSource(),
+        readOp.getMixedOffsets(), readOp.getMixedSizes(),
+        readOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(readOp.getResultType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::LoadFromBufferOp>(
+              readOp, tensor, sourceSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, loadSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), readOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == loadSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, readOp.getLoc(), 0));
+          // Use zero padding value.
+          Value zeroPadding = arith::ConstantOp::create(
+              rewriter, readOp.getLoc(), vector.getElementType(),
+              rewriter.getZeroAttr(vector.getElementType()));
+          rewriter.replaceOpWithNewOp<vector::TransferReadOp>(
+              readOp, vector, sourceSlice, offsets, zeroPadding, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertGetMemrefOp : public OpConversionPattern<PCF::GetMemrefOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GetMemrefOp getMemrefOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // The source has been converted to memref with refined layout and memory
+    // space. We need to cast it to match the maximally dynamic layout and no
+    // memory space of the GetMemrefOp result type.
+    Value source = adaptor.getSource();
+    auto sourceType = cast<MemRefType>(source.getType());
+    auto resultType = getMemrefOp.getResultType();
+
+    // Cast away memory space if present.
+    if (sourceType.getMemorySpace()) {
+      auto noMemSpaceType =
+          MemRefType::get(sourceType.getShape(), sourceType.getElementType(),
+                          sourceType.getLayout(), nullptr);
+      source = memref::MemorySpaceCastOp::create(rewriter, getMemrefOp.getLoc(),
+                                                 noMemSpaceType, source);
+      sourceType = noMemSpaceType;
+    }
+
+    // Cast to maximally dynamic layout to match the return type.
+    if (sourceType.getLayout() != resultType.getLayout()) {
+      source = memref::CastOp::create(rewriter, getMemrefOp.getLoc(),
+                                      resultType, source);
+    }
+
+    // Create subview using the slice params.
+    rewriter.replaceOpWithNewOp<memref::SubViewOp>(
+        getMemrefOp, source, getMemrefOp.getMixedOffsets(),
+        getMemrefOp.getMixedSizes(), getMemrefOp.getMixedStrides());
+    return success();
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto allocType = cast_if_present<MemRefType>(
+        getTypeConverter()->convertType(allocOp.getResult()));
+    if (!allocType) {
+      return rewriter.notifyMatchFailure(allocOp,
+                                         "failed to convert alloc type");
+    }
+
+    // TODO: This pattern is a hack. We should be directly allocating memory as
+    // a global here. Instead we rely on dubious hoisting patterns to make this
+    // work as intended.
+    IntegerAttr alignment =
+        allocOp.getResultType().getScope().getPreferredAllocAlignment(
+            rewriter.getContext());
+    rewriter.replaceOpWithNewOp<memref::AllocOp>(
+        allocOp, allocType, adaptor.getDynamicSizes(),
+        /*symbolOperands=*/ValueRange(), alignment);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, adaptor.getOperands());
+    return success();
+  }
+};
+
+//===----------------------------------------------------------------------===//
+// Control Flow Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertFuncOp final : OpConversionPattern<func::FuncOp> {
+  ConvertFuncOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    FailureOr<FunctionType> maybeNewFunctionType =
+        mapping.lookupConvertedFunctionArgs(funcOp);
+    if (failed(maybeNewFunctionType)) {
+      return failure();
+    }
+    FunctionType newFunctionType = maybeNewFunctionType.value();
+
+    // Convert the original function types.
+    TypeConverter::SignatureConversion result(newFunctionType.getNumInputs());
+    for (auto [i, t] : llvm::enumerate(newFunctionType.getInputs())) {
+      result.addInputs(i, t);
+    }
+    if (failed(rewriter.convertRegionTypes(&funcOp.getFunctionBody(),
+                                           *getTypeConverter(), &result)))
+      return failure();
+
+    rewriter.modifyOpInPlace(funcOp, [&] { funcOp.setType(newFunctionType); });
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+class ConvertReturnOp : public OpConversionPattern<func::ReturnOp> {
+public:
+  ConvertReturnOp(TypeConverter &typeConverter, MLIRContext *context,
+                  SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  LogicalResult
+  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const final {
+    auto parent = cast<func::FuncOp>(op->getParentOp());
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(parent);
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(op,
+                                         "failed to get converted parent type");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), operands,
+                     targetFuncType.value().getResults());
+    rewriter.replaceOpWithNewOp<func::ReturnOp>(op, operands);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertCallOp final : OpConversionPattern<func::CallOp> {
+  ConvertCallOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::CallOp callOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the context aware result types.
+    for (Value v : callOp.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(callOp,
+                                           "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(
+            callOp, cast<SymbolRefAttr>(callOp.getCallableForCallee()));
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(callOp,
+                                         "could not convert argument types");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, callOp.getLoc(), operands,
+                     targetFuncType.value().getInputs());
+
+    // Substitute with the new result types from the corresponding FuncType
+    // conversion.
+    auto newCallOp = func::CallOp::create(
+        rewriter, callOp.getLoc(), callOp.getCallee(), resultTypes, operands);
+    rewriter.replaceOp(callOp, newCallOp);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+//===----------------------------------------------------------------------===//
+// SCF Conversion Pattern overrides
+//===----------------------------------------------------------------------===//
+
+struct ConvertForOp : public OpConversionPattern<scf::ForOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::ForOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getInitArgs());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::ForOp::create(rewriter, op.getLoc(), adaptor.getLowerBound(),
+                           adaptor.getUpperBound(), adaptor.getStep(), inits,
+                           /*bodyBuilder=*/nullptr, adaptor.getUnsignedCmp());
+    if (failed(rewriter.convertRegionTypes(&op.getRegion(), *typeConverter)))
+      return failure();
+
+    // Drop the rewriter created block.
+    rewriter.eraseBlock(newOp.getBody(0));
+
+    // Inline the original (now converted) body.
+    auto &dstRegion = newOp.getRegion();
+    rewriter.inlineRegionBefore(op.getRegion(), dstRegion, dstRegion.end());
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+struct ConvertWhileOp : public OpConversionPattern<scf::WhileOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::WhileOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::WhileOp::create(rewriter, op.getLoc(), resultTypes, inits);
+    for (auto i : {0u, 1u}) {
+      if (failed(rewriter.convertRegionTypes(&op.getRegion(i), *typeConverter)))
+        return failure();
+      auto &dstRegion = newOp.getRegion(i);
+      rewriter.inlineRegionBefore(op.getRegion(i), dstRegion, dstRegion.end());
+    }
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+void ConvertSRefToMemRefPass::runOnOperation() {
+  auto *context = &getContext();
+
+  SRefLayoutAnalysis analysis(getOperation());
+  if (failed(analysis.run())) {
+    return signalPassFailure();
+  }
+
+  TypeConverter typeConverter;
+  ConversionTarget conversionTarget(getContext());
+  RewritePatternSet patterns(&getContext());
+
+  // Add a context aware type converter that uses the layout analysis.
+  typeConverter.addConversion([&](Value v) -> std::optional<Type> {
+    if (isa<PCF::ShapedRefType>(v.getType())) {
+      FailureOr<MemRefType> maybeConvertedType = analysis.getConvertedType(v);
+      if (failed(maybeConvertedType)) {
+        return Type();
+      }
+      return maybeConvertedType.value();
+    }
+    // Passthrough for everything else.
+    return v.getType();
+  });
+
+  ConversionTarget target(*context);
+  auto isIllegalType = [&](Type t) { return isa<PCF::ShapedRefType>(t); };
+
+  // Verify that all operand, result, and region argument types have been
+  // converted. This does not use the type converter because the type converter
+  // only implements context specific conversions.
+  auto isLegallyTypedOp = [&](Operation *op) -> bool {
+    for (Type type : op->getResultTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (Type type : op->getOperandTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (auto &region : op->getRegions()) {
+      for (auto type : region.getArgumentTypes()) {
+        if (isIllegalType(type))
+          return false;
+      }
+    }
+    if (auto funcInterface = dyn_cast<FunctionOpInterface>(op)) {
+      if (llvm::any_of(funcInterface.getArgumentTypes(),
+                       [&](Type t) { return isIllegalType(t); })) {
```

**Comment:**
```suggestion
      if (llvm::any_of(funcInterface.getArgumentTypes(), isIllegalType)) {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:1067`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newLoopOp =
+        PCF::LoopOp::create(rewriter, loc, loopOp.getScope(), loopOp.getCount(),
+                            loopOp.getSyncOnReturn());
+    SmallVector<Value> newArgs(replacements);
+    newArgs.append(newLoopOp.getRegion().getArguments().begin(),
+                   newLoopOp.getRegion().getArguments().end());
+    Block *entryBlock = &loopOp.getRegion().front();
+    Block *newEntry = &newLoopOp.getRegion().front();
+    // Inline the body into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               newArgs);
+
+    // replace the old op.
+    rewriter.replaceOp(loopOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value destSlice = memref::SubViewOp::create(
+        rewriter, writeOp.getLoc(), adaptor.getDest(),
+        writeOp.getMixedOffsets(), writeOp.getMixedSizes(),
+        writeOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(writeOp.getSourceType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::StoreToBufferOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<MemRefType>([&](MemRefType memref) {
+          rewriter.replaceOpWithNewOp<memref::CopyOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, storeSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), writeOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == storeSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, writeOp.getLoc(), 0));
+          rewriter.replaceOpWithNewOp<vector::TransferWriteOp>(
+              writeOp, writeOp.getSource(), destSlice, offsets, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertReadSliceOp : public OpConversionPattern<PCF::ReadSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::ReadSliceOp readOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value sourceSlice = memref::SubViewOp::create(
+        rewriter, readOp.getLoc(), adaptor.getSource(),
+        readOp.getMixedOffsets(), readOp.getMixedSizes(),
+        readOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(readOp.getResultType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::LoadFromBufferOp>(
+              readOp, tensor, sourceSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, loadSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), readOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == loadSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, readOp.getLoc(), 0));
+          // Use zero padding value.
+          Value zeroPadding = arith::ConstantOp::create(
+              rewriter, readOp.getLoc(), vector.getElementType(),
+              rewriter.getZeroAttr(vector.getElementType()));
+          rewriter.replaceOpWithNewOp<vector::TransferReadOp>(
+              readOp, vector, sourceSlice, offsets, zeroPadding, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertGetMemrefOp : public OpConversionPattern<PCF::GetMemrefOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GetMemrefOp getMemrefOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // The source has been converted to memref with refined layout and memory
+    // space. We need to cast it to match the maximally dynamic layout and no
+    // memory space of the GetMemrefOp result type.
+    Value source = adaptor.getSource();
+    auto sourceType = cast<MemRefType>(source.getType());
+    auto resultType = getMemrefOp.getResultType();
+
+    // Cast away memory space if present.
+    if (sourceType.getMemorySpace()) {
+      auto noMemSpaceType =
+          MemRefType::get(sourceType.getShape(), sourceType.getElementType(),
+                          sourceType.getLayout(), nullptr);
+      source = memref::MemorySpaceCastOp::create(rewriter, getMemrefOp.getLoc(),
+                                                 noMemSpaceType, source);
+      sourceType = noMemSpaceType;
+    }
+
+    // Cast to maximally dynamic layout to match the return type.
+    if (sourceType.getLayout() != resultType.getLayout()) {
+      source = memref::CastOp::create(rewriter, getMemrefOp.getLoc(),
+                                      resultType, source);
+    }
+
+    // Create subview using the slice params.
+    rewriter.replaceOpWithNewOp<memref::SubViewOp>(
+        getMemrefOp, source, getMemrefOp.getMixedOffsets(),
+        getMemrefOp.getMixedSizes(), getMemrefOp.getMixedStrides());
+    return success();
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto allocType = cast_if_present<MemRefType>(
+        getTypeConverter()->convertType(allocOp.getResult()));
+    if (!allocType) {
+      return rewriter.notifyMatchFailure(allocOp,
+                                         "failed to convert alloc type");
+    }
+
+    // TODO: This pattern is a hack. We should be directly allocating memory as
+    // a global here. Instead we rely on dubious hoisting patterns to make this
+    // work as intended.
+    IntegerAttr alignment =
+        allocOp.getResultType().getScope().getPreferredAllocAlignment(
+            rewriter.getContext());
+    rewriter.replaceOpWithNewOp<memref::AllocOp>(
+        allocOp, allocType, adaptor.getDynamicSizes(),
+        /*symbolOperands=*/ValueRange(), alignment);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, adaptor.getOperands());
+    return success();
+  }
+};
+
+//===----------------------------------------------------------------------===//
+// Control Flow Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertFuncOp final : OpConversionPattern<func::FuncOp> {
+  ConvertFuncOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    FailureOr<FunctionType> maybeNewFunctionType =
+        mapping.lookupConvertedFunctionArgs(funcOp);
+    if (failed(maybeNewFunctionType)) {
+      return failure();
+    }
+    FunctionType newFunctionType = maybeNewFunctionType.value();
+
+    // Convert the original function types.
+    TypeConverter::SignatureConversion result(newFunctionType.getNumInputs());
+    for (auto [i, t] : llvm::enumerate(newFunctionType.getInputs())) {
+      result.addInputs(i, t);
+    }
+    if (failed(rewriter.convertRegionTypes(&funcOp.getFunctionBody(),
+                                           *getTypeConverter(), &result)))
+      return failure();
+
+    rewriter.modifyOpInPlace(funcOp, [&] { funcOp.setType(newFunctionType); });
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+class ConvertReturnOp : public OpConversionPattern<func::ReturnOp> {
+public:
+  ConvertReturnOp(TypeConverter &typeConverter, MLIRContext *context,
+                  SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  LogicalResult
+  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const final {
+    auto parent = cast<func::FuncOp>(op->getParentOp());
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(parent);
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(op,
+                                         "failed to get converted parent type");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), operands,
+                     targetFuncType.value().getResults());
+    rewriter.replaceOpWithNewOp<func::ReturnOp>(op, operands);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertCallOp final : OpConversionPattern<func::CallOp> {
+  ConvertCallOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::CallOp callOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the context aware result types.
+    for (Value v : callOp.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(callOp,
+                                           "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(
+            callOp, cast<SymbolRefAttr>(callOp.getCallableForCallee()));
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(callOp,
+                                         "could not convert argument types");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, callOp.getLoc(), operands,
+                     targetFuncType.value().getInputs());
+
+    // Substitute with the new result types from the corresponding FuncType
+    // conversion.
+    auto newCallOp = func::CallOp::create(
+        rewriter, callOp.getLoc(), callOp.getCallee(), resultTypes, operands);
+    rewriter.replaceOp(callOp, newCallOp);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+//===----------------------------------------------------------------------===//
+// SCF Conversion Pattern overrides
+//===----------------------------------------------------------------------===//
+
+struct ConvertForOp : public OpConversionPattern<scf::ForOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::ForOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getInitArgs());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::ForOp::create(rewriter, op.getLoc(), adaptor.getLowerBound(),
+                           adaptor.getUpperBound(), adaptor.getStep(), inits,
+                           /*bodyBuilder=*/nullptr, adaptor.getUnsignedCmp());
+    if (failed(rewriter.convertRegionTypes(&op.getRegion(), *typeConverter)))
+      return failure();
+
+    // Drop the rewriter created block.
+    rewriter.eraseBlock(newOp.getBody(0));
+
+    // Inline the original (now converted) body.
+    auto &dstRegion = newOp.getRegion();
+    rewriter.inlineRegionBefore(op.getRegion(), dstRegion, dstRegion.end());
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+struct ConvertWhileOp : public OpConversionPattern<scf::WhileOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::WhileOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::WhileOp::create(rewriter, op.getLoc(), resultTypes, inits);
+    for (auto i : {0u, 1u}) {
+      if (failed(rewriter.convertRegionTypes(&op.getRegion(i), *typeConverter)))
+        return failure();
+      auto &dstRegion = newOp.getRegion(i);
+      rewriter.inlineRegionBefore(op.getRegion(i), dstRegion, dstRegion.end());
+    }
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+void ConvertSRefToMemRefPass::runOnOperation() {
+  auto *context = &getContext();
+
+  SRefLayoutAnalysis analysis(getOperation());
+  if (failed(analysis.run())) {
+    return signalPassFailure();
+  }
+
+  TypeConverter typeConverter;
+  ConversionTarget conversionTarget(getContext());
+  RewritePatternSet patterns(&getContext());
+
+  // Add a context aware type converter that uses the layout analysis.
+  typeConverter.addConversion([&](Value v) -> std::optional<Type> {
+    if (isa<PCF::ShapedRefType>(v.getType())) {
+      FailureOr<MemRefType> maybeConvertedType = analysis.getConvertedType(v);
+      if (failed(maybeConvertedType)) {
+        return Type();
+      }
+      return maybeConvertedType.value();
+    }
+    // Passthrough for everything else.
+    return v.getType();
+  });
+
+  ConversionTarget target(*context);
+  auto isIllegalType = [&](Type t) { return isa<PCF::ShapedRefType>(t); };
```

**Comment:**
```suggestion
  static constexpr auto isIllegalType = llvm::IsaPred<PCF::ShapedRefType>;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:1093`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newLoopOp =
+        PCF::LoopOp::create(rewriter, loc, loopOp.getScope(), loopOp.getCount(),
+                            loopOp.getSyncOnReturn());
+    SmallVector<Value> newArgs(replacements);
+    newArgs.append(newLoopOp.getRegion().getArguments().begin(),
+                   newLoopOp.getRegion().getArguments().end());
+    Block *entryBlock = &loopOp.getRegion().front();
+    Block *newEntry = &newLoopOp.getRegion().front();
+    // Inline the body into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               newArgs);
+
+    // replace the old op.
+    rewriter.replaceOp(loopOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value destSlice = memref::SubViewOp::create(
+        rewriter, writeOp.getLoc(), adaptor.getDest(),
+        writeOp.getMixedOffsets(), writeOp.getMixedSizes(),
+        writeOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(writeOp.getSourceType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::StoreToBufferOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<MemRefType>([&](MemRefType memref) {
+          rewriter.replaceOpWithNewOp<memref::CopyOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, storeSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), writeOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == storeSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, writeOp.getLoc(), 0));
+          rewriter.replaceOpWithNewOp<vector::TransferWriteOp>(
+              writeOp, writeOp.getSource(), destSlice, offsets, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertReadSliceOp : public OpConversionPattern<PCF::ReadSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::ReadSliceOp readOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value sourceSlice = memref::SubViewOp::create(
+        rewriter, readOp.getLoc(), adaptor.getSource(),
+        readOp.getMixedOffsets(), readOp.getMixedSizes(),
+        readOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(readOp.getResultType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::LoadFromBufferOp>(
+              readOp, tensor, sourceSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, loadSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), readOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == loadSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, readOp.getLoc(), 0));
+          // Use zero padding value.
+          Value zeroPadding = arith::ConstantOp::create(
+              rewriter, readOp.getLoc(), vector.getElementType(),
+              rewriter.getZeroAttr(vector.getElementType()));
+          rewriter.replaceOpWithNewOp<vector::TransferReadOp>(
+              readOp, vector, sourceSlice, offsets, zeroPadding, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertGetMemrefOp : public OpConversionPattern<PCF::GetMemrefOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GetMemrefOp getMemrefOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // The source has been converted to memref with refined layout and memory
+    // space. We need to cast it to match the maximally dynamic layout and no
+    // memory space of the GetMemrefOp result type.
+    Value source = adaptor.getSource();
+    auto sourceType = cast<MemRefType>(source.getType());
+    auto resultType = getMemrefOp.getResultType();
+
+    // Cast away memory space if present.
+    if (sourceType.getMemorySpace()) {
+      auto noMemSpaceType =
+          MemRefType::get(sourceType.getShape(), sourceType.getElementType(),
+                          sourceType.getLayout(), nullptr);
+      source = memref::MemorySpaceCastOp::create(rewriter, getMemrefOp.getLoc(),
+                                                 noMemSpaceType, source);
+      sourceType = noMemSpaceType;
+    }
+
+    // Cast to maximally dynamic layout to match the return type.
+    if (sourceType.getLayout() != resultType.getLayout()) {
+      source = memref::CastOp::create(rewriter, getMemrefOp.getLoc(),
+                                      resultType, source);
+    }
+
+    // Create subview using the slice params.
+    rewriter.replaceOpWithNewOp<memref::SubViewOp>(
+        getMemrefOp, source, getMemrefOp.getMixedOffsets(),
+        getMemrefOp.getMixedSizes(), getMemrefOp.getMixedStrides());
+    return success();
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto allocType = cast_if_present<MemRefType>(
+        getTypeConverter()->convertType(allocOp.getResult()));
+    if (!allocType) {
+      return rewriter.notifyMatchFailure(allocOp,
+                                         "failed to convert alloc type");
+    }
+
+    // TODO: This pattern is a hack. We should be directly allocating memory as
+    // a global here. Instead we rely on dubious hoisting patterns to make this
+    // work as intended.
+    IntegerAttr alignment =
+        allocOp.getResultType().getScope().getPreferredAllocAlignment(
+            rewriter.getContext());
+    rewriter.replaceOpWithNewOp<memref::AllocOp>(
+        allocOp, allocType, adaptor.getDynamicSizes(),
+        /*symbolOperands=*/ValueRange(), alignment);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, adaptor.getOperands());
+    return success();
+  }
+};
+
+//===----------------------------------------------------------------------===//
+// Control Flow Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertFuncOp final : OpConversionPattern<func::FuncOp> {
+  ConvertFuncOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    FailureOr<FunctionType> maybeNewFunctionType =
+        mapping.lookupConvertedFunctionArgs(funcOp);
+    if (failed(maybeNewFunctionType)) {
+      return failure();
+    }
+    FunctionType newFunctionType = maybeNewFunctionType.value();
+
+    // Convert the original function types.
+    TypeConverter::SignatureConversion result(newFunctionType.getNumInputs());
+    for (auto [i, t] : llvm::enumerate(newFunctionType.getInputs())) {
+      result.addInputs(i, t);
+    }
+    if (failed(rewriter.convertRegionTypes(&funcOp.getFunctionBody(),
+                                           *getTypeConverter(), &result)))
+      return failure();
+
+    rewriter.modifyOpInPlace(funcOp, [&] { funcOp.setType(newFunctionType); });
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+class ConvertReturnOp : public OpConversionPattern<func::ReturnOp> {
+public:
+  ConvertReturnOp(TypeConverter &typeConverter, MLIRContext *context,
+                  SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  LogicalResult
+  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const final {
+    auto parent = cast<func::FuncOp>(op->getParentOp());
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(parent);
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(op,
+                                         "failed to get converted parent type");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), operands,
+                     targetFuncType.value().getResults());
+    rewriter.replaceOpWithNewOp<func::ReturnOp>(op, operands);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertCallOp final : OpConversionPattern<func::CallOp> {
+  ConvertCallOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::CallOp callOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the context aware result types.
+    for (Value v : callOp.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(callOp,
+                                           "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(
+            callOp, cast<SymbolRefAttr>(callOp.getCallableForCallee()));
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(callOp,
+                                         "could not convert argument types");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, callOp.getLoc(), operands,
+                     targetFuncType.value().getInputs());
+
+    // Substitute with the new result types from the corresponding FuncType
+    // conversion.
+    auto newCallOp = func::CallOp::create(
+        rewriter, callOp.getLoc(), callOp.getCallee(), resultTypes, operands);
+    rewriter.replaceOp(callOp, newCallOp);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+//===----------------------------------------------------------------------===//
+// SCF Conversion Pattern overrides
+//===----------------------------------------------------------------------===//
+
+struct ConvertForOp : public OpConversionPattern<scf::ForOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::ForOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getInitArgs());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::ForOp::create(rewriter, op.getLoc(), adaptor.getLowerBound(),
+                           adaptor.getUpperBound(), adaptor.getStep(), inits,
+                           /*bodyBuilder=*/nullptr, adaptor.getUnsignedCmp());
+    if (failed(rewriter.convertRegionTypes(&op.getRegion(), *typeConverter)))
+      return failure();
+
+    // Drop the rewriter created block.
+    rewriter.eraseBlock(newOp.getBody(0));
+
+    // Inline the original (now converted) body.
+    auto &dstRegion = newOp.getRegion();
+    rewriter.inlineRegionBefore(op.getRegion(), dstRegion, dstRegion.end());
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+struct ConvertWhileOp : public OpConversionPattern<scf::WhileOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::WhileOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::WhileOp::create(rewriter, op.getLoc(), resultTypes, inits);
+    for (auto i : {0u, 1u}) {
+      if (failed(rewriter.convertRegionTypes(&op.getRegion(i), *typeConverter)))
+        return failure();
+      auto &dstRegion = newOp.getRegion(i);
+      rewriter.inlineRegionBefore(op.getRegion(i), dstRegion, dstRegion.end());
+    }
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+void ConvertSRefToMemRefPass::runOnOperation() {
+  auto *context = &getContext();
+
+  SRefLayoutAnalysis analysis(getOperation());
+  if (failed(analysis.run())) {
+    return signalPassFailure();
+  }
+
+  TypeConverter typeConverter;
+  ConversionTarget conversionTarget(getContext());
+  RewritePatternSet patterns(&getContext());
+
+  // Add a context aware type converter that uses the layout analysis.
+  typeConverter.addConversion([&](Value v) -> std::optional<Type> {
+    if (isa<PCF::ShapedRefType>(v.getType())) {
+      FailureOr<MemRefType> maybeConvertedType = analysis.getConvertedType(v);
+      if (failed(maybeConvertedType)) {
+        return Type();
+      }
+      return maybeConvertedType.value();
+    }
+    // Passthrough for everything else.
+    return v.getType();
+  });
+
+  ConversionTarget target(*context);
+  auto isIllegalType = [&](Type t) { return isa<PCF::ShapedRefType>(t); };
+
+  // Verify that all operand, result, and region argument types have been
+  // converted. This does not use the type converter because the type converter
+  // only implements context specific conversions.
+  auto isLegallyTypedOp = [&](Operation *op) -> bool {
+    for (Type type : op->getResultTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (Type type : op->getOperandTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (auto &region : op->getRegions()) {
+      for (auto type : region.getArgumentTypes()) {
+        if (isIllegalType(type))
+          return false;
+      }
+    }
+    if (auto funcInterface = dyn_cast<FunctionOpInterface>(op)) {
+      if (llvm::any_of(funcInterface.getArgumentTypes(),
+                       [&](Type t) { return isIllegalType(t); })) {
+        return false;
+      }
+      if (llvm::any_of(funcInterface.getResultTypes(),
+                       [&](Type t) { return isIllegalType(t); })) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:1103`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newLoopOp =
+        PCF::LoopOp::create(rewriter, loc, loopOp.getScope(), loopOp.getCount(),
+                            loopOp.getSyncOnReturn());
+    SmallVector<Value> newArgs(replacements);
+    newArgs.append(newLoopOp.getRegion().getArguments().begin(),
+                   newLoopOp.getRegion().getArguments().end());
+    Block *entryBlock = &loopOp.getRegion().front();
+    Block *newEntry = &newLoopOp.getRegion().front();
+    // Inline the body into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               newArgs);
+
+    // replace the old op.
+    rewriter.replaceOp(loopOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value destSlice = memref::SubViewOp::create(
+        rewriter, writeOp.getLoc(), adaptor.getDest(),
+        writeOp.getMixedOffsets(), writeOp.getMixedSizes(),
+        writeOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(writeOp.getSourceType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::StoreToBufferOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<MemRefType>([&](MemRefType memref) {
+          rewriter.replaceOpWithNewOp<memref::CopyOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, storeSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), writeOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == storeSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, writeOp.getLoc(), 0));
+          rewriter.replaceOpWithNewOp<vector::TransferWriteOp>(
+              writeOp, writeOp.getSource(), destSlice, offsets, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertReadSliceOp : public OpConversionPattern<PCF::ReadSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::ReadSliceOp readOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value sourceSlice = memref::SubViewOp::create(
+        rewriter, readOp.getLoc(), adaptor.getSource(),
+        readOp.getMixedOffsets(), readOp.getMixedSizes(),
+        readOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(readOp.getResultType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::LoadFromBufferOp>(
+              readOp, tensor, sourceSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, loadSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), readOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == loadSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, readOp.getLoc(), 0));
+          // Use zero padding value.
+          Value zeroPadding = arith::ConstantOp::create(
+              rewriter, readOp.getLoc(), vector.getElementType(),
+              rewriter.getZeroAttr(vector.getElementType()));
+          rewriter.replaceOpWithNewOp<vector::TransferReadOp>(
+              readOp, vector, sourceSlice, offsets, zeroPadding, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertGetMemrefOp : public OpConversionPattern<PCF::GetMemrefOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GetMemrefOp getMemrefOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // The source has been converted to memref with refined layout and memory
+    // space. We need to cast it to match the maximally dynamic layout and no
+    // memory space of the GetMemrefOp result type.
+    Value source = adaptor.getSource();
+    auto sourceType = cast<MemRefType>(source.getType());
+    auto resultType = getMemrefOp.getResultType();
+
+    // Cast away memory space if present.
+    if (sourceType.getMemorySpace()) {
+      auto noMemSpaceType =
+          MemRefType::get(sourceType.getShape(), sourceType.getElementType(),
+                          sourceType.getLayout(), nullptr);
+      source = memref::MemorySpaceCastOp::create(rewriter, getMemrefOp.getLoc(),
+                                                 noMemSpaceType, source);
+      sourceType = noMemSpaceType;
+    }
+
+    // Cast to maximally dynamic layout to match the return type.
+    if (sourceType.getLayout() != resultType.getLayout()) {
+      source = memref::CastOp::create(rewriter, getMemrefOp.getLoc(),
+                                      resultType, source);
+    }
+
+    // Create subview using the slice params.
+    rewriter.replaceOpWithNewOp<memref::SubViewOp>(
+        getMemrefOp, source, getMemrefOp.getMixedOffsets(),
+        getMemrefOp.getMixedSizes(), getMemrefOp.getMixedStrides());
+    return success();
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto allocType = cast_if_present<MemRefType>(
+        getTypeConverter()->convertType(allocOp.getResult()));
+    if (!allocType) {
+      return rewriter.notifyMatchFailure(allocOp,
+                                         "failed to convert alloc type");
+    }
+
+    // TODO: This pattern is a hack. We should be directly allocating memory as
+    // a global here. Instead we rely on dubious hoisting patterns to make this
+    // work as intended.
+    IntegerAttr alignment =
+        allocOp.getResultType().getScope().getPreferredAllocAlignment(
+            rewriter.getContext());
+    rewriter.replaceOpWithNewOp<memref::AllocOp>(
+        allocOp, allocType, adaptor.getDynamicSizes(),
+        /*symbolOperands=*/ValueRange(), alignment);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, adaptor.getOperands());
+    return success();
+  }
+};
+
+//===----------------------------------------------------------------------===//
+// Control Flow Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertFuncOp final : OpConversionPattern<func::FuncOp> {
+  ConvertFuncOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    FailureOr<FunctionType> maybeNewFunctionType =
+        mapping.lookupConvertedFunctionArgs(funcOp);
+    if (failed(maybeNewFunctionType)) {
+      return failure();
+    }
+    FunctionType newFunctionType = maybeNewFunctionType.value();
+
+    // Convert the original function types.
+    TypeConverter::SignatureConversion result(newFunctionType.getNumInputs());
+    for (auto [i, t] : llvm::enumerate(newFunctionType.getInputs())) {
+      result.addInputs(i, t);
+    }
+    if (failed(rewriter.convertRegionTypes(&funcOp.getFunctionBody(),
+                                           *getTypeConverter(), &result)))
+      return failure();
+
+    rewriter.modifyOpInPlace(funcOp, [&] { funcOp.setType(newFunctionType); });
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+class ConvertReturnOp : public OpConversionPattern<func::ReturnOp> {
+public:
+  ConvertReturnOp(TypeConverter &typeConverter, MLIRContext *context,
+                  SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  LogicalResult
+  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const final {
+    auto parent = cast<func::FuncOp>(op->getParentOp());
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(parent);
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(op,
+                                         "failed to get converted parent type");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), operands,
+                     targetFuncType.value().getResults());
+    rewriter.replaceOpWithNewOp<func::ReturnOp>(op, operands);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertCallOp final : OpConversionPattern<func::CallOp> {
+  ConvertCallOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::CallOp callOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the context aware result types.
+    for (Value v : callOp.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(callOp,
+                                           "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(
+            callOp, cast<SymbolRefAttr>(callOp.getCallableForCallee()));
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(callOp,
+                                         "could not convert argument types");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, callOp.getLoc(), operands,
+                     targetFuncType.value().getInputs());
+
+    // Substitute with the new result types from the corresponding FuncType
+    // conversion.
+    auto newCallOp = func::CallOp::create(
+        rewriter, callOp.getLoc(), callOp.getCallee(), resultTypes, operands);
+    rewriter.replaceOp(callOp, newCallOp);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+//===----------------------------------------------------------------------===//
+// SCF Conversion Pattern overrides
+//===----------------------------------------------------------------------===//
+
+struct ConvertForOp : public OpConversionPattern<scf::ForOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::ForOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getInitArgs());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::ForOp::create(rewriter, op.getLoc(), adaptor.getLowerBound(),
+                           adaptor.getUpperBound(), adaptor.getStep(), inits,
+                           /*bodyBuilder=*/nullptr, adaptor.getUnsignedCmp());
+    if (failed(rewriter.convertRegionTypes(&op.getRegion(), *typeConverter)))
+      return failure();
+
+    // Drop the rewriter created block.
+    rewriter.eraseBlock(newOp.getBody(0));
+
+    // Inline the original (now converted) body.
+    auto &dstRegion = newOp.getRegion();
+    rewriter.inlineRegionBefore(op.getRegion(), dstRegion, dstRegion.end());
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+struct ConvertWhileOp : public OpConversionPattern<scf::WhileOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::WhileOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::WhileOp::create(rewriter, op.getLoc(), resultTypes, inits);
+    for (auto i : {0u, 1u}) {
+      if (failed(rewriter.convertRegionTypes(&op.getRegion(i), *typeConverter)))
+        return failure();
+      auto &dstRegion = newOp.getRegion(i);
+      rewriter.inlineRegionBefore(op.getRegion(i), dstRegion, dstRegion.end());
+    }
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+void ConvertSRefToMemRefPass::runOnOperation() {
+  auto *context = &getContext();
+
+  SRefLayoutAnalysis analysis(getOperation());
+  if (failed(analysis.run())) {
+    return signalPassFailure();
+  }
+
+  TypeConverter typeConverter;
+  ConversionTarget conversionTarget(getContext());
+  RewritePatternSet patterns(&getContext());
+
+  // Add a context aware type converter that uses the layout analysis.
+  typeConverter.addConversion([&](Value v) -> std::optional<Type> {
+    if (isa<PCF::ShapedRefType>(v.getType())) {
+      FailureOr<MemRefType> maybeConvertedType = analysis.getConvertedType(v);
+      if (failed(maybeConvertedType)) {
+        return Type();
+      }
+      return maybeConvertedType.value();
+    }
+    // Passthrough for everything else.
+    return v.getType();
+  });
+
+  ConversionTarget target(*context);
+  auto isIllegalType = [&](Type t) { return isa<PCF::ShapedRefType>(t); };
+
+  // Verify that all operand, result, and region argument types have been
+  // converted. This does not use the type converter because the type converter
+  // only implements context specific conversions.
+  auto isLegallyTypedOp = [&](Operation *op) -> bool {
+    for (Type type : op->getResultTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (Type type : op->getOperandTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (auto &region : op->getRegions()) {
+      for (auto type : region.getArgumentTypes()) {
+        if (isIllegalType(type))
+          return false;
+      }
+    }
+    if (auto funcInterface = dyn_cast<FunctionOpInterface>(op)) {
+      if (llvm::any_of(funcInterface.getArgumentTypes(),
+                       [&](Type t) { return isIllegalType(t); })) {
+        return false;
+      }
+      if (llvm::any_of(funcInterface.getResultTypes(),
+                       [&](Type t) { return isIllegalType(t); })) {
+        return false;
+      }
+    }
+    return true;
+  };
+  target.markUnknownOpDynamicallyLegal(isLegallyTypedOp);
+  ConversionConfig config;
+  config.allowPatternRollback = false;
+
+  patterns.insert<ConvertGenericOp, ConvertLoopOp, ConvertWriteSliceOp,
```

**Comment:**
```suggestion
  patterns.add<ConvertGenericOp, ConvertLoopOp, ConvertWriteSliceOp,
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/ConvertSRefToMemRef.cpp:1109`

```diff
@@ -0,0 +1,1128 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include <codecvt>
+#include <sstream>
+#include "iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Element.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/Solver.h"
+#include "iree/compiler/Dialect/Util/Analysis/DFX/State.h"
+#include "iree/compiler/Dialect/Util/Analysis/Explorer.h"
+#include "iree/compiler/Dialect/Util/IR/UtilOps.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Func/Transforms/FuncConversions.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/Transforms/Patterns.h"
+#include "mlir/Dialect/Vector/IR/VectorOps.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/Interfaces/ControlFlowInterfaces.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#define DEBUG_TYPE "iree-pcf-convert-sref-to-memref"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_CONVERTSREFTOMEMREFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadSRefLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct ConvertSRefToMemRefPass final
+    : impl::ConvertSRefToMemRefPassBase<ConvertSRefToMemRefPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::Codegen::IREECodegenDialect,
+                    iree_compiler::IREE::PCF::PCFDialect, arith::ArithDialect,
+                    memref::MemRefDialect, vector::VectorDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+//===----------------------------------------------------------------------===//
+// Layout Propagation Analysis Impl
+//===----------------------------------------------------------------------===//
+
+static bool isDefaultOrStrided(Attribute layout) {
+  auto mapAttr = dyn_cast<AffineMapAttr>(layout);
+  return !layout || (mapAttr && mapAttr.isIdentity()) ||
+         isa<StridedLayoutAttr>(layout);
+}
+
+// State that tracks floating point ranges and flags.
+struct StridedLayoutState : public DFX::AbstractState {
+  bool isValidState() const override { return isValid; }
+  bool isAtFixpoint() const override { return !isValid || isFinalized; }
+
+  void invalidate() { isValid = false; }
+
+  ChangeStatus indicateOptimisticFixpoint() override {
+    isFinalized = true;
+    return ChangeStatus::UNCHANGED;
+  }
+
+  ChangeStatus indicatePessimisticFixpoint() override {
+    isFinalized = true;
+    assumed = MemRefType();
+    return ChangeStatus::CHANGED;
+  }
+
+  MemRefType getAssumed() const { return assumed; }
+
+  // Resets the assumed value to the given value. This does no unioning and
+  // assumes it is a proper fixpoint minimum.
+  void setAssumed(MemRefType newAssumed) { assumed = newAssumed; }
+
+  // "Clamps" this state with |rhs|. The assumed value will contain the matching
+  // static strides of both assumed layouts, or dynamic if sizes don't match.
+  void operator^=(const StridedLayoutState &rhs) {
+    // Ignore if this state is already at a fixed point. In this case it should
+    // be a pessimistic fixed point as all optimistic fixed points are
+    // determined on initialization/first update. Also nothing to do on an
+    // invalid state.
+    if (isFinalized || !isValid) {
+      return;
+    }
+
+    if (!rhs.isValidState()) {
+      return invalidate();
+    }
+
+    // If no value is assumed yet, take RHS.
+    if (!assumed) {
+      assumed = rhs.getAssumed();
+      return;
+    }
+
+    if (!rhs.getAssumed()) {
+      // Two possibilities if the rhs is undefined. First, rhs is at a
+      // pessimistic fixed point, in which case we take it.
+      if (rhs.isAtFixpoint()) {
+        assumed = rhs.getAssumed();
+      }
+      // Otherwise nothing to do.
+      return;
+    }
+
+    // Invalidate if memory space mismatches. We could allow for falling back to
+    // a more generic memory space but this is in all cases today going to arise
+    // from an earlier uncaught failure.
+    if (rhs.getAssumed().getMemorySpace() != assumed.getMemorySpace()) {
+      return invalidate();
+    }
+
+    // Shape and element type should be guaranteed because the sref type
+    // carries them so we assert instead.
+    assert(rhs.getAssumed().getShape() == assumed.getShape() &&
+           rhs.getAssumed().getElementType() == assumed.getElementType() &&
+           "Unexpected shape or element type mismatch");
+
+    MemRefLayoutAttrInterface newLayout = MemRefLayoutAttrInterface();
+    if (!rhs.getAssumed().getLayout()) {
+      if (!isDefaultOrStrided(assumed.getLayout())) {
+        // Fail if there is a non-strided and non-default layout.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else if (!assumed.getLayout()) {
+      if (!isDefaultOrStrided(rhs.getAssumed().getLayout())) {
+        // Same here.
+        return invalidate();
+      }
+      newLayout = assumed.getLayout();
+    } else {
+      // Union the strided layouts.
+      auto rhsStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(rhs.getAssumed().getLayout());
+      auto thisStridedLayout =
+          dyn_cast_if_present<StridedLayoutAttr>(assumed.getLayout());
+      if (!rhsStridedLayout || !thisStridedLayout ||
+          rhsStridedLayout.getStrides().size() !=
+              thisStridedLayout.getStrides().size()) {
+        return invalidate();
+      }
+
+      auto dynamicizeIfUnequal = [](int64_t l, int64_t r) {
+        return l != r ? ShapedType::kDynamic : l;
+      };
+
+      SmallVector<int64_t> newStrides(thisStridedLayout.getStrides());
+      int64_t newOffset = dynamicizeIfUnequal(thisStridedLayout.getOffset(),
+                                              rhsStridedLayout.getOffset());
+
+      for (auto [lStride, rStride] :
+           llvm::zip_equal(newStrides, rhsStridedLayout.getStrides())) {
+        lStride = dynamicizeIfUnequal(lStride, rStride);
+      }
+
+      newLayout = StridedLayoutAttr::get(thisStridedLayout.getContext(),
+                                         newOffset, newStrides);
+    }
+    assumed = MemRefType::get(assumed.getShape(), assumed.getElementType(),
+                              newLayout, assumed.getMemorySpace());
+  }
+
+private:
+  MemRefType assumed = MemRefType();
+  bool isFinalized = false;
+  bool isValid = true;
+};
+
+// Attribute known floating point range and flags to an IR Value.
+class StridedLayoutValueElement
+    : public DFX::StateWrapper<StridedLayoutState, DFX::ValueElement> {
+public:
+  using BaseType = DFX::StateWrapper<StridedLayoutState, DFX::ValueElement>;
+  using BaseType::BaseType;
+
+  static StridedLayoutValueElement &createForPosition(const Position &pos,
+                                                      DFX::Solver &solver) {
+    return *(new (solver.getAllocator()) StridedLayoutValueElement(pos));
+  }
+
+  // Identity definitions.
+  static const char ID;
+  const std::string getName() const override {
+    return "StridedLayoutValueElement";
+  }
+  const void *getID() const override { return &ID; }
+  static bool classof(const DFX::AbstractElement *element) {
+    return (element->getID() == &ID);
+  }
+  const std::string getAsStr(AsmState &asmState) const override;
+
+private:
+  void initializeValue(Value value, DFX::Solver &solver) override;
+  ChangeStatus updateValue(Value value, DFX::Solver &solver) override;
+};
+const char StridedLayoutValueElement::ID = 0;
+
+void StridedLayoutValueElement::initializeValue(Value value,
+                                                DFX::Solver &solver) {
+  if (!isa<PCF::ShapedRefType>(value.getType())) {
+    indicatePessimisticFixpoint();
+    return;
+  }
+}
+
+ChangeStatus StridedLayoutValueElement::updateValue(Value value,
+                                                    DFX::Solver &solver) {
+  StridedLayoutState newState = getState();
+
+  if (auto result = llvm::dyn_cast<OpResult>(value)) {
+    llvm::TypeSwitch<Operation *, void>(result.getOwner())
+        .Case<PCF::AllocOp>([&](PCF::AllocOp allocOp) {
+          PCF::ShapedRefType resultType = allocOp.getResultType();
+          FailureOr<Attribute> memSpace =
+              resultType.getScope().getAllocMemSpace(allocOp.getContext());
+          if (failed(memSpace)) {
+            allocOp->emitOpError("failed to get memory space for allocation");
+            newState.invalidate();
+            return;
+          }
+          newState.setAssumed(MemRefType::get(
+              resultType.getShape(), resultType.getElementType(),
+              MemRefLayoutAttrInterface{}, memSpace.value()));
+          newState.indicateOptimisticFixpoint();
+        })
+        .Case<RegionBranchOpInterface>([&](RegionBranchOpInterface regionOp) {
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  regionOp.getOperation(), [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<CallOpInterface>([&](CallOpInterface callOp) {
+          // Give up pessimistically on indirect calls.
+          if (isa<Value>(callOp.getCallableForCallee())) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+          auto targetSymbol =
+              cast<SymbolRefAttr>(callOp.getCallableForCallee());
+          auto callableOp = solver.getExplorer()
+                                .getSymbolTables()
+                                .lookupNearestSymbolFrom<CallableOpInterface>(
+                                    callOp, targetSymbol);
+          assert(callableOp && "call target not found");
+          // For region branch ops get the result layout from the union of
+          // return sites.
+          if (solver.getExplorer().walkReturnOperands(
+                  callableOp, [&](OperandRange returnOperands) {
+                    auto returnOperand =
+                        returnOperands[result.getResultNumber()];
+                    auto returnState =
+                        solver.getElementFor<StridedLayoutValueElement>(
+                            *this, Position::forValue(returnOperand),
+                            DFX::Resolution::REQUIRED);
+                    newState ^= returnState;
+                    return WalkResult::advance();
+                  }) == TraversalResult::INCOMPLETE) {
+            newState.indicatePessimisticFixpoint();
+            return;
+          }
+        })
+        .Case<Util::OptimizationBarrierOp>(
+            [&](Util::OptimizationBarrierOp barrierOp) {
+              auto returnState =
+                  solver.getElementFor<StridedLayoutValueElement>(
+                      *this,
+                      Position::forValue(
+                          barrierOp.getOperand(result.getResultNumber())),
+                      DFX::Resolution::REQUIRED);
+              newState ^= returnState;
+            });
+  } else if (auto bbArg = llvm::dyn_cast<BlockArgument>(value)) {
+    bool didUpdate = false;
+    if (bbArg.getParentBlock()->isEntryBlock()) {
+      didUpdate =
+          llvm::TypeSwitch<Operation *, bool>(bbArg.getOwner()->getParentOp())
+              .Case<PCF::GenericOp>([&](PCF::GenericOp genericOp) {
+                if (genericOp.isRegionRefArg(bbArg)) {
+                  auto resultType = dyn_cast<MemRefType>(
+                      genericOp.getTiedResult(bbArg).getType());
+                  if (!resultType ||
+                      !isDefaultOrStrided(resultType.getLayout())) {
+                    genericOp->emitOpError(
+                        "unexpected non-strided or default memref result type ")
+                        << resultType;
+                    newState.invalidate();
+                  } else {
+                    newState.setAssumed(resultType);
+                    newState.indicateOptimisticFixpoint();
+                  }
+                } else {
+                  // pcf.sref arguments must either be result tied or
+                  // initialized per the verifier.
+                  assert(genericOp.isInitializedArg(bbArg) &&
+                         "unexpected non-initialized arg");
+                  auto yield = cast<PCF::YieldOp>(
+                      genericOp.getInitializer().front().getTerminator());
+                  auto initializerState =
+                      solver.getElementFor<StridedLayoutValueElement>(
+                          *this,
+                          Position::forValue(
+                              yield->getOperand(bbArg.getArgNumber())),
+                          DFX::Resolution::REQUIRED);
+                  newState ^= initializerState;
+                }
+                return true;
+              })
+              .Case<PCF::LoopOp>([&](PCF::LoopOp loopOp) {
+                auto resultType =
+                    dyn_cast<MemRefType>(loopOp.getTiedResult(bbArg).getType());
+                if (!resultType ||
+                    !isDefaultOrStrided(resultType.getLayout())) {
+                  loopOp->emitOpError(
+                      "unexpected non-strided or default memref result type ")
+                      << resultType;
+                  newState.invalidate();
+                } else {
+                  newState.setAssumed(resultType);
+                  newState.indicateOptimisticFixpoint();
+                }
+                return true;
+              })
+              .Default([&](Operation *) { return false; });
+    }
+    if (!didUpdate) {
+      solver.getExplorer().walkIncomingBranchOperands(
+          bbArg.getOwner(),
+          [&](Block *sourceBlock, OperandRange operands, size_t offset) {
+            auto bbArgState = solver.getElementFor<StridedLayoutValueElement>(
+                *this,
+                Position::forValue(operands[bbArg.getArgNumber() + offset]),
+                DFX::Resolution::REQUIRED);
+            newState ^= bbArgState;
+            return WalkResult::advance();
+          });
+    }
+  }
+
+  return DFX::clampStateAndIndicateChange(getState(), newState);
+}
+
+const std::string
+StridedLayoutValueElement::getAsStr(AsmState &asmState) const {
+  auto range = getAssumed();
+  std::string s("layout: ");
+  llvm::raw_string_ostream os(s);
+  range.print(os, asmState);
+  return s;
+}
+
+class SRefLayoutAnalysis {
+public:
+  explicit SRefLayoutAnalysis(Operation *rootOp)
+      : explorer(rootOp, TraversalAction::RECURSE),
+        solver(explorer, allocator) {
+    explorer.initialize();
+  }
+
+  AsmState &getAsmState() { return solver.getAsmState(); }
+  Explorer &getExplorer() { return explorer; }
+
+  LogicalResult run() {
+    // Initialize all shaped ref values to maximally dynamic layouts.
+    explorer.walkValues([&](Value v) {
+      if (isa<PCF::ShapedRefType>(v.getType())) {
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(v));
+      }
+      return WalkResult::advance();
+    });
+
+    // Run solver to completion.
+    auto result = solver.run();
+    LLVM_DEBUG(solver.print(llvm::dbgs()));
+    return result;
+  }
+
+  // Returns the memref type this value should be converted to.
+  FailureOr<MemRefType> getConvertedType(Value value) {
+    assert(isa<PCF::ShapedRefType>(value.getType()) &&
+           "unexpected non sref type");
+    // memref -> sref conversions come in as unrealized casts.
+    if (auto unrealizedCast =
+            value.getDefiningOp<UnrealizedConversionCastOp>()) {
+      return cast<MemRefType>(unrealizedCast->getOperandTypes()[0]);
+    }
+
+    // sref -> memref conversions for op results and produced block args are
+    // queried from the analysis.
+    auto &stridedLayout =
+        solver.getOrCreateElementFor<StridedLayoutValueElement>(
+            Position::forValue(value));
+    if (!stridedLayout.isValidState() || !stridedLayout.getAssumed()) {
+      return failure();
+    }
+    StridedLayoutState state = stridedLayout.getState();
+    return state.getAssumed();
+  }
+
+  // Returns the function type this callable should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(CallableOpInterface callableOp) {
+    // Check if we have a cached conversion.
+    FunctionType cachedType =
+        cachedFunctionTypeMap.lookup_or(callableOp, FunctionType());
+    if (cachedType) {
+      return cachedType;
+    }
+
+    // Since the function op conversion pattern calls this function we're
+    // guaranteed the argument/result types will be unconverted when this is
+    // first called irrespective of pattern application order.
+    SmallVector<Type> argumentTypes;
+    Region *region = callableOp.getCallableRegion();
+    if (!region) {
+      return failure();
+    }
+    for (auto bbArg : callableOp.getCallableRegion()->getArguments()) {
+      if (!isa<PCF::ShapedRefType>(bbArg.getType())) {
+        argumentTypes.push_back(bbArg.getType());
+        continue;
+      }
+      FailureOr<MemRefType> maybeConvertedType = getConvertedType(bbArg);
+      if (failed(maybeConvertedType)) {
+        return failure();
+      }
+      argumentTypes.push_back(maybeConvertedType.value());
+    }
+
+    int64_t numResultsToConvert = llvm::count_if(
+        callableOp.getResultTypes(), llvm::IsaPred<PCF::ShapedRefType>);
+    if (numResultsToConvert == 0) {
+      return FunctionType::get(callableOp->getContext(), argumentTypes,
+                               callableOp.getResultTypes());
+    }
+
+    SmallVector<StridedLayoutState> states(numResultsToConvert,
+                                           StridedLayoutState());
+    if (solver.getExplorer().walkReturnOperands(
+            callableOp, [&](OperandRange range) {
+              int64_t currState = 0;
+              for (Value v : range) {
+                if (isa<PCF::ShapedRefType>(v.getType())) {
+                  FailureOr<MemRefType> maybeConvertedType =
+                      getConvertedType(v);
+                  if (failed(maybeConvertedType)) {
+                    return WalkResult::interrupt();
+                  }
+                  StridedLayoutState newState;
+                  newState.setAssumed(maybeConvertedType.value());
+                  newState.indicateOptimisticFixpoint();
+                  states[currState] ^= newState;
+                  ++currState;
+                }
+              }
+              return WalkResult::advance();
+            }) == TraversalResult::INCOMPLETE) {
+      return failure();
+    }
+
+    SmallVector<Type> newResultTypes(callableOp.getResultTypes());
+    int64_t currState = 0;
+    for (auto &type : newResultTypes) {
+      if (isa<PCF::ShapedRefType>(type)) {
+        auto &state = states[currState];
+        if (!state.isValidState() || !state.getAssumed()) {
+          return failure();
+        }
+        type = state.getAssumed();
+      }
+    }
+    auto newFuncType = FunctionType::get(callableOp->getContext(),
+                                         argumentTypes, newResultTypes);
+    cachedFunctionTypeMap[callableOp] = newFuncType;
+    return newFuncType;
+  }
+
+  // Returns the function type the callee of this caller should be converted to.
+  FailureOr<FunctionType>
+  lookupConvertedFunctionArgs(Operation *caller, SymbolRefAttr targetSymbol) {
+    auto callableOp =
+        solver.getExplorer()
+            .getSymbolTables()
+            .lookupNearestSymbolFrom<CallableOpInterface>(caller, targetSymbol);
+    if (!callableOp) {
+      return failure();
+    }
+    return lookupConvertedFunctionArgs(callableOp);
+  }
+
+private:
+  Explorer explorer;
+  llvm::BumpPtrAllocator allocator;
+  DFX::Solver solver;
+  llvm::SmallDenseMap<Operation *, FunctionType> cachedFunctionTypeMap;
+};
+
+//===----------------------------------------------------------------------===//
+// Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+static Value castIfMismatched(OpBuilder &b, Location loc, Value v, Type t) {
+  if (v.getType() != t) {
+    assert(isa<MemRefType>(v.getType()) &&
+           "unexpected non-memref type mismatch");
+    return memref::CastOp::create(b, loc, t, v);
+  }
+  return v;
+}
+
+static void castIfMismatched(OpBuilder &b, Location loc,
+                             MutableArrayRef<Value> vals,
+                             TypeRange targetTypes) {
+  for (auto [v, t] : llvm::zip_equal(vals, targetTypes)) {
+    v = castIfMismatched(b, loc, v, t);
+  }
+}
+
+struct ConvertGenericOp : public OpConversionPattern<PCF::GenericOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GenericOp genericOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(genericOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = genericOp.getLoc();
+    IntegerAttr alignment =
+        genericOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = genericOp.getInits().begin();
+    ValueRange dynamicSizes = genericOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(genericOp.getResultTypes(), genericOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, genericOp.getScope(), genericOp.getNumIterators(),
+        genericOp.getSyncOnReturn());
+    SmallVector<Value> newArgs;
+    Block *newEntry = &newGenericOp.getRegion().front();
+    // By this point all globally initialized values should have been resolved.
+    // Inline the initializer into the main body.
+    if (!genericOp.getInitializer().empty()) {
+      Block &initializerBlock = genericOp.getInitializer().front();
+      auto initProducedVals =
+          cast<PCF::YieldOp>(initializerBlock.getTerminator()).getOperands();
+      newArgs.append(initProducedVals.begin(), initProducedVals.end());
+      rewriter.eraseOp(initializerBlock.getTerminator());
+      rewriter.inlineBlockBefore(&initializerBlock, newEntry,
+                                 newEntry->begin());
+    }
+    newArgs.append(replacements);
+    newArgs.append(newGenericOp.getRegion().getArguments().begin(),
+                   newGenericOp.getRegion().getArguments().end());
+    // Inline the entry block into the new region.
+    Block *entryBlock = &genericOp.getRegion().front();
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->end(), newArgs);
+
+    // Move the remaining blocks into the new region.
+    for (auto &block : genericOp.getRegion()) {
+      rewriter.moveBlockBefore(&block, &newGenericOp.getRegion(),
+                               newGenericOp.getRegion().end());
+    }
+
+    // replace the old op.
+    rewriter.replaceOp(genericOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertLoopOp : public OpConversionPattern<PCF::LoopOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::LoopOp loopOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (llvm::any_of(loopOp.getResultTypes(),
+                     [](Type t) { return !isa<MemRefType>(t); })) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "expected all parallel op results to be of memref type");
+    }
+
+    Location loc = loopOp.getLoc();
+    IntegerAttr alignment =
+        loopOp.getScope().getPreferredAllocAlignment(rewriter.getContext());
+    SmallVector<Value> replacements;
+
+    // Init iterator.
+    auto currInit = loopOp.getInits().begin();
+    ValueRange dynamicSizes = loopOp.getDynamicSizes();
+    for (auto [resultType, isTied] :
+         llvm::zip_equal(loopOp.getResultTypes(), loopOp.getIsTied())) {
+      if (isTied) {
+        replacements.push_back(*currInit);
+        ++currInit;
+      } else {
+        int64_t numDynamicDims =
+            cast<ShapedType>(resultType).getNumDynamicDims();
+        replacements.push_back(memref::AllocOp::create(
+            rewriter, loc, resultType, dynamicSizes.take_front(numDynamicDims),
+            /*symbolOperands=*/ValueRange(), alignment));
+        dynamicSizes = dynamicSizes.drop_front(numDynamicDims);
+      }
+    }
+
+    // Create a new op and take the body of the current one.
+    auto newLoopOp =
+        PCF::LoopOp::create(rewriter, loc, loopOp.getScope(), loopOp.getCount(),
+                            loopOp.getSyncOnReturn());
+    SmallVector<Value> newArgs(replacements);
+    newArgs.append(newLoopOp.getRegion().getArguments().begin(),
+                   newLoopOp.getRegion().getArguments().end());
+    Block *entryBlock = &loopOp.getRegion().front();
+    Block *newEntry = &newLoopOp.getRegion().front();
+    // Inline the body into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               newArgs);
+
+    // replace the old op.
+    rewriter.replaceOp(loopOp, replacements);
+    return success();
+  }
+};
+
+struct ConvertWriteSliceOp : public OpConversionPattern<PCF::WriteSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::WriteSliceOp writeOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value destSlice = memref::SubViewOp::create(
+        rewriter, writeOp.getLoc(), adaptor.getDest(),
+        writeOp.getMixedOffsets(), writeOp.getMixedSizes(),
+        writeOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(writeOp.getSourceType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::StoreToBufferOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<MemRefType>([&](MemRefType memref) {
+          rewriter.replaceOpWithNewOp<memref::CopyOp>(
+              writeOp, writeOp.getSource(), destSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, storeSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), writeOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == storeSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, writeOp.getLoc(), 0));
+          rewriter.replaceOpWithNewOp<vector::TransferWriteOp>(
+              writeOp, writeOp.getSource(), destSlice, offsets, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertReadSliceOp : public OpConversionPattern<PCF::ReadSliceOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::ReadSliceOp readOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Value sourceSlice = memref::SubViewOp::create(
+        rewriter, readOp.getLoc(), adaptor.getSource(),
+        readOp.getMixedOffsets(), readOp.getMixedSizes(),
+        readOp.getMixedStrides());
+    return llvm::TypeSwitch<Type, LogicalResult>(readOp.getResultType())
+        .Case<RankedTensorType>([&](RankedTensorType tensor) {
+          rewriter.replaceOpWithNewOp<IREE::Codegen::LoadFromBufferOp>(
+              readOp, tensor, sourceSlice);
+          return success();
+        })
+        .Case<VectorType>([&](VectorType vector) {
+          SmallVector<bool> inBounds(vector.getRank(), true);
+          for (auto [inBound, vecSize, loadSize] : llvm::zip_equal(
+                   inBounds, vector.getShape(), readOp.getStaticSizes())) {
+            // Since vectors must be statically sized we can just check for
+            // equality here.
+            inBound = vecSize == loadSize;
+          }
+          SmallVector<Value> offsets(
+              vector.getRank(),
+              arith::ConstantIndexOp::create(rewriter, readOp.getLoc(), 0));
+          // Use zero padding value.
+          Value zeroPadding = arith::ConstantOp::create(
+              rewriter, readOp.getLoc(), vector.getElementType(),
+              rewriter.getZeroAttr(vector.getElementType()));
+          rewriter.replaceOpWithNewOp<vector::TransferReadOp>(
+              readOp, vector, sourceSlice, offsets, zeroPadding, inBounds);
+          return success();
+        })
+        .Default([](Type) { return failure(); });
+  }
+};
+
+struct ConvertGetMemrefOp : public OpConversionPattern<PCF::GetMemrefOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::GetMemrefOp getMemrefOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    // The source has been converted to memref with refined layout and memory
+    // space. We need to cast it to match the maximally dynamic layout and no
+    // memory space of the GetMemrefOp result type.
+    Value source = adaptor.getSource();
+    auto sourceType = cast<MemRefType>(source.getType());
+    auto resultType = getMemrefOp.getResultType();
+
+    // Cast away memory space if present.
+    if (sourceType.getMemorySpace()) {
+      auto noMemSpaceType =
+          MemRefType::get(sourceType.getShape(), sourceType.getElementType(),
+                          sourceType.getLayout(), nullptr);
+      source = memref::MemorySpaceCastOp::create(rewriter, getMemrefOp.getLoc(),
+                                                 noMemSpaceType, source);
+      sourceType = noMemSpaceType;
+    }
+
+    // Cast to maximally dynamic layout to match the return type.
+    if (sourceType.getLayout() != resultType.getLayout()) {
+      source = memref::CastOp::create(rewriter, getMemrefOp.getLoc(),
+                                      resultType, source);
+    }
+
+    // Create subview using the slice params.
+    rewriter.replaceOpWithNewOp<memref::SubViewOp>(
+        getMemrefOp, source, getMemrefOp.getMixedOffsets(),
+        getMemrefOp.getMixedSizes(), getMemrefOp.getMixedStrides());
+    return success();
+  }
+};
+
+struct ConvertAllocOp : public OpConversionPattern<PCF::AllocOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(PCF::AllocOp allocOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto allocType = cast_if_present<MemRefType>(
+        getTypeConverter()->convertType(allocOp.getResult()));
+    if (!allocType) {
+      return rewriter.notifyMatchFailure(allocOp,
+                                         "failed to convert alloc type");
+    }
+
+    // TODO: This pattern is a hack. We should be directly allocating memory as
+    // a global here. Instead we rely on dubious hoisting patterns to make this
+    // work as intended.
+    IntegerAttr alignment =
+        allocOp.getResultType().getScope().getPreferredAllocAlignment(
+            rewriter.getContext());
+    rewriter.replaceOpWithNewOp<memref::AllocOp>(
+        allocOp, allocType, adaptor.getDynamicSizes(),
+        /*symbolOperands=*/ValueRange(), alignment);
+    return success();
+  }
+};
+
+struct ConvertOptimizationBarrier
+    : public OpConversionPattern<Util::OptimizationBarrierOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(Util::OptimizationBarrierOp barrier, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<Util::OptimizationBarrierOp>(
+        barrier, adaptor.getOperands());
+    return success();
+  }
+};
+
+//===----------------------------------------------------------------------===//
+// Control Flow Conversion Patterns
+//===----------------------------------------------------------------------===//
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertFuncOp final : OpConversionPattern<func::FuncOp> {
+  ConvertFuncOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    FailureOr<FunctionType> maybeNewFunctionType =
+        mapping.lookupConvertedFunctionArgs(funcOp);
+    if (failed(maybeNewFunctionType)) {
+      return failure();
+    }
+    FunctionType newFunctionType = maybeNewFunctionType.value();
+
+    // Convert the original function types.
+    TypeConverter::SignatureConversion result(newFunctionType.getNumInputs());
+    for (auto [i, t] : llvm::enumerate(newFunctionType.getInputs())) {
+      result.addInputs(i, t);
+    }
+    if (failed(rewriter.convertRegionTypes(&funcOp.getFunctionBody(),
+                                           *getTypeConverter(), &result)))
+      return failure();
+
+    rewriter.modifyOpInPlace(funcOp, [&] { funcOp.setType(newFunctionType); });
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+class ConvertReturnOp : public OpConversionPattern<func::ReturnOp> {
+public:
+  ConvertReturnOp(TypeConverter &typeConverter, MLIRContext *context,
+                  SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  LogicalResult
+  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const final {
+    auto parent = cast<func::FuncOp>(op->getParentOp());
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(parent);
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(op,
+                                         "failed to get converted parent type");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), operands,
+                     targetFuncType.value().getResults());
+    rewriter.replaceOpWithNewOp<func::ReturnOp>(op, operands);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+/// Converts the operand and result types of the CallOp, used together with the
+/// FuncOpSignatureConversion.
+struct ConvertCallOp final : OpConversionPattern<func::CallOp> {
+  ConvertCallOp(TypeConverter &typeConverter, MLIRContext *context,
+                SRefLayoutAnalysis &mapping)
+      : OpConversionPattern(typeConverter, context), mapping(mapping) {}
+
+  /// Hook for derived classes to implement combined matching and rewriting.
+  LogicalResult
+  matchAndRewrite(func::CallOp callOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the context aware result types.
+    for (Value v : callOp.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(callOp,
+                                           "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    FailureOr<FunctionType> targetFuncType =
+        mapping.lookupConvertedFunctionArgs(
+            callOp, cast<SymbolRefAttr>(callOp.getCallableForCallee()));
+    if (failed(targetFuncType)) {
+      return rewriter.notifyMatchFailure(callOp,
+                                         "could not convert argument types");
+    }
+
+    SmallVector<Value> operands(adaptor.getOperands());
+    castIfMismatched(rewriter, callOp.getLoc(), operands,
+                     targetFuncType.value().getInputs());
+
+    // Substitute with the new result types from the corresponding FuncType
+    // conversion.
+    auto newCallOp = func::CallOp::create(
+        rewriter, callOp.getLoc(), callOp.getCallee(), resultTypes, operands);
+    rewriter.replaceOp(callOp, newCallOp);
+    return success();
+  }
+
+private:
+  SRefLayoutAnalysis &mapping;
+};
+
+//===----------------------------------------------------------------------===//
+// SCF Conversion Pattern overrides
+//===----------------------------------------------------------------------===//
+
+struct ConvertForOp : public OpConversionPattern<scf::ForOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::ForOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getInitArgs());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::ForOp::create(rewriter, op.getLoc(), adaptor.getLowerBound(),
+                           adaptor.getUpperBound(), adaptor.getStep(), inits,
+                           /*bodyBuilder=*/nullptr, adaptor.getUnsignedCmp());
+    if (failed(rewriter.convertRegionTypes(&op.getRegion(), *typeConverter)))
+      return failure();
+
+    // Drop the rewriter created block.
+    rewriter.eraseBlock(newOp.getBody(0));
+
+    // Inline the original (now converted) body.
+    auto &dstRegion = newOp.getRegion();
+    rewriter.inlineRegionBefore(op.getRegion(), dstRegion, dstRegion.end());
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+struct ConvertWhileOp : public OpConversionPattern<scf::WhileOp> {
+  using Base::Base;
+
+  LogicalResult
+  matchAndRewrite(scf::WhileOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    SmallVector<Type> resultTypes;
+    // Convert the result types.
+    for (Value v : op.getResults()) {
+      Type newType = typeConverter->convertType(v);
+      if (!newType) {
+        return rewriter.notifyMatchFailure(op, "could not convert result type");
+      }
+      resultTypes.push_back(newType);
+    }
+
+    SmallVector<Value> inits(adaptor.getOperands());
+    castIfMismatched(rewriter, op.getLoc(), inits, resultTypes);
+
+    auto newOp =
+        scf::WhileOp::create(rewriter, op.getLoc(), resultTypes, inits);
+    for (auto i : {0u, 1u}) {
+      if (failed(rewriter.convertRegionTypes(&op.getRegion(i), *typeConverter)))
+        return failure();
+      auto &dstRegion = newOp.getRegion(i);
+      rewriter.inlineRegionBefore(op.getRegion(i), dstRegion, dstRegion.end());
+    }
+
+    rewriter.replaceOp(op, newOp);
+    return success();
+  }
+};
+
+void ConvertSRefToMemRefPass::runOnOperation() {
+  auto *context = &getContext();
+
+  SRefLayoutAnalysis analysis(getOperation());
+  if (failed(analysis.run())) {
+    return signalPassFailure();
+  }
+
+  TypeConverter typeConverter;
+  ConversionTarget conversionTarget(getContext());
+  RewritePatternSet patterns(&getContext());
+
+  // Add a context aware type converter that uses the layout analysis.
+  typeConverter.addConversion([&](Value v) -> std::optional<Type> {
+    if (isa<PCF::ShapedRefType>(v.getType())) {
+      FailureOr<MemRefType> maybeConvertedType = analysis.getConvertedType(v);
+      if (failed(maybeConvertedType)) {
+        return Type();
+      }
+      return maybeConvertedType.value();
+    }
+    // Passthrough for everything else.
+    return v.getType();
+  });
+
+  ConversionTarget target(*context);
+  auto isIllegalType = [&](Type t) { return isa<PCF::ShapedRefType>(t); };
+
+  // Verify that all operand, result, and region argument types have been
+  // converted. This does not use the type converter because the type converter
+  // only implements context specific conversions.
+  auto isLegallyTypedOp = [&](Operation *op) -> bool {
+    for (Type type : op->getResultTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (Type type : op->getOperandTypes()) {
+      if (isIllegalType(type))
+        return false;
+    }
+    for (auto &region : op->getRegions()) {
+      for (auto type : region.getArgumentTypes()) {
+        if (isIllegalType(type))
+          return false;
+      }
+    }
+    if (auto funcInterface = dyn_cast<FunctionOpInterface>(op)) {
+      if (llvm::any_of(funcInterface.getArgumentTypes(),
+                       [&](Type t) { return isIllegalType(t); })) {
+        return false;
+      }
+      if (llvm::any_of(funcInterface.getResultTypes(),
+                       [&](Type t) { return isIllegalType(t); })) {
+        return false;
+      }
+    }
+    return true;
+  };
+  target.markUnknownOpDynamicallyLegal(isLegallyTypedOp);
+  ConversionConfig config;
+  config.allowPatternRollback = false;
+
+  patterns.insert<ConvertGenericOp, ConvertLoopOp, ConvertWriteSliceOp,
+                  ConvertReadSliceOp, ConvertGetMemrefOp, ConvertAllocOp,
+                  ConvertOptimizationBarrier>(typeConverter, context);
+
+  // Function related conversion patterns need the analysis to lookup function
+  // type conversions.
+  patterns.insert<ConvertFuncOp, ConvertReturnOp, ConvertCallOp>(
```

**Comment:**
```suggestion
  patterns.add<ConvertFuncOp, ConvertReturnOp, ConvertCallOp>(
```

---


---


## [PR #22915](https://github.com/iree-org/iree/pull/22915): [Codegen] Add LowerStructuralPCFPass

### Review Summary

**COMMENTED** (2025-12-17)

The logic looks good to me, I'll leave the exact lowering to scf for others to decide

**APPROVED** (2025-12-17)

**COMMENTED** (2026-01-02)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/LowerStructuralPCF.cpp:112`

```diff
@@ -0,0 +1,253 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Utils/RewriteUtils.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlow.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-pcf-lower-structural-pcf"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_LOWERSTRUCTURALPCFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadStructuralLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct LowerStructuralPCFPass final
+    : impl::LowerStructuralPCFPassBase<LowerStructuralPCFPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::PCF::PCFDialect, scf::SCFDialect,
+                    cf::ControlFlowDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+struct LowerGenericOp : public OpRewritePattern<IREE::PCF::GenericOp> {
+  using Base::Base;
+  LogicalResult matchAndRewrite(IREE::PCF::GenericOp genericOp,
+                                PatternRewriter &rewriter) const override {
+    if (genericOp->getNumResults() != 0) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "unexpected attempt to resolve generic op with results");
+    }
+
+    if (genericOp.getSyncOnReturn()) {
+      OpBuilder::InsertionGuard g(rewriter);
+      rewriter.setInsertionPointAfter(genericOp);
+      if (failed(genericOp.getScope().addBarrier(rewriter))) {
+        genericOp.emitOpError("failed to construct requested barrier");
+        return failure();
+      }
+    }
+
+    Location loc = genericOp.getLoc();
+    SmallVector<Value> indexArgs = genericOp.getScope().getWorkerIDs(
+        rewriter, loc, genericOp.getNumIterators());
+    assert(indexArgs.size() == genericOp.getNumIterators() &&
+           "expected num worker ids to match number of iterators");
+
+    indexArgs.append(genericOp.getScope().getWorkerCounts(
+        rewriter, loc, genericOp.getNumIterators()));
+    assert(indexArgs.size() == genericOp.getNumIndexArgs() &&
+           "expected num worker counts to match number of iterators");
+
+    auto executeRegion =
+        scf::ExecuteRegionOp::create(rewriter, loc, TypeRange());
+    Block *entryBlock = &genericOp.getRegion().front();
+    Block *newEntry = rewriter.createBlock(&executeRegion.getRegion(),
+                                           executeRegion.getRegion().begin());
+    // Inline the entry block into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               indexArgs);
+
+    // Move the remaining blocks into the new region.
+    SmallVector<Block *> blocksToMove = llvm::map_to_vector(
+        genericOp.getRegion().getBlocks(), [](Block &b) { return &b; });
+    for (Block *block : blocksToMove) {
+      rewriter.moveBlockBefore(block, &executeRegion.getRegion(),
+                               executeRegion.getRegion().end());
+    }
+
+    // Replace all pcf.return ops with scf.yield ops. We make this pattern
+    // responsible for terminator conversion to try to keep IR valid between
+    // pattern rewrites.
+    for (auto &block : executeRegion.getRegion()) {
```

**Comment:**
nit: type

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/LowerStructuralPCF.cpp:126`

```diff
@@ -0,0 +1,253 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Utils/RewriteUtils.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlow.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-pcf-lower-structural-pcf"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_LOWERSTRUCTURALPCFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadStructuralLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct LowerStructuralPCFPass final
+    : impl::LowerStructuralPCFPassBase<LowerStructuralPCFPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::PCF::PCFDialect, scf::SCFDialect,
+                    cf::ControlFlowDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+struct LowerGenericOp : public OpRewritePattern<IREE::PCF::GenericOp> {
+  using Base::Base;
+  LogicalResult matchAndRewrite(IREE::PCF::GenericOp genericOp,
+                                PatternRewriter &rewriter) const override {
+    if (genericOp->getNumResults() != 0) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "unexpected attempt to resolve generic op with results");
+    }
+
+    if (genericOp.getSyncOnReturn()) {
+      OpBuilder::InsertionGuard g(rewriter);
+      rewriter.setInsertionPointAfter(genericOp);
+      if (failed(genericOp.getScope().addBarrier(rewriter))) {
+        genericOp.emitOpError("failed to construct requested barrier");
+        return failure();
+      }
+    }
+
+    Location loc = genericOp.getLoc();
+    SmallVector<Value> indexArgs = genericOp.getScope().getWorkerIDs(
+        rewriter, loc, genericOp.getNumIterators());
+    assert(indexArgs.size() == genericOp.getNumIterators() &&
+           "expected num worker ids to match number of iterators");
+
+    indexArgs.append(genericOp.getScope().getWorkerCounts(
+        rewriter, loc, genericOp.getNumIterators()));
+    assert(indexArgs.size() == genericOp.getNumIndexArgs() &&
+           "expected num worker counts to match number of iterators");
+
+    auto executeRegion =
+        scf::ExecuteRegionOp::create(rewriter, loc, TypeRange());
+    Block *entryBlock = &genericOp.getRegion().front();
+    Block *newEntry = rewriter.createBlock(&executeRegion.getRegion(),
+                                           executeRegion.getRegion().begin());
+    // Inline the entry block into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               indexArgs);
+
+    // Move the remaining blocks into the new region.
+    SmallVector<Block *> blocksToMove = llvm::map_to_vector(
+        genericOp.getRegion().getBlocks(), [](Block &b) { return &b; });
+    for (Block *block : blocksToMove) {
+      rewriter.moveBlockBefore(block, &executeRegion.getRegion(),
+                               executeRegion.getRegion().end());
+    }
+
+    // Replace all pcf.return ops with scf.yield ops. We make this pattern
+    // responsible for terminator conversion to try to keep IR valid between
+    // pattern rewrites.
+    for (auto &block : executeRegion.getRegion()) {
+      auto pcfTerminator = dyn_cast<PCF::ReturnOp>(block.getTerminator());
+      if (pcfTerminator) {
+        rewriter.setInsertionPoint(pcfTerminator);
+        rewriter.replaceOpWithNewOp<scf::YieldOp>(pcfTerminator);
+      }
+    }
+
+    rewriter.eraseOp(genericOp);
+
+    return success();
+  }
+};
+
+struct LowerLoopOp : public OpRewritePattern<IREE::PCF::LoopOp> {
```

**Comment:**
```suggestion
struct LowerLoopOp final : OpRewritePattern<IREE::PCF::LoopOp> {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/LowerStructuralPCF.cpp:163`

```diff
@@ -0,0 +1,253 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Utils/RewriteUtils.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlow.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-pcf-lower-structural-pcf"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_LOWERSTRUCTURALPCFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadStructuralLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct LowerStructuralPCFPass final
+    : impl::LowerStructuralPCFPassBase<LowerStructuralPCFPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::PCF::PCFDialect, scf::SCFDialect,
+                    cf::ControlFlowDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+struct LowerGenericOp : public OpRewritePattern<IREE::PCF::GenericOp> {
+  using Base::Base;
+  LogicalResult matchAndRewrite(IREE::PCF::GenericOp genericOp,
+                                PatternRewriter &rewriter) const override {
+    if (genericOp->getNumResults() != 0) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "unexpected attempt to resolve generic op with results");
+    }
+
+    if (genericOp.getSyncOnReturn()) {
+      OpBuilder::InsertionGuard g(rewriter);
+      rewriter.setInsertionPointAfter(genericOp);
+      if (failed(genericOp.getScope().addBarrier(rewriter))) {
+        genericOp.emitOpError("failed to construct requested barrier");
+        return failure();
+      }
+    }
+
+    Location loc = genericOp.getLoc();
+    SmallVector<Value> indexArgs = genericOp.getScope().getWorkerIDs(
+        rewriter, loc, genericOp.getNumIterators());
+    assert(indexArgs.size() == genericOp.getNumIterators() &&
+           "expected num worker ids to match number of iterators");
+
+    indexArgs.append(genericOp.getScope().getWorkerCounts(
+        rewriter, loc, genericOp.getNumIterators()));
+    assert(indexArgs.size() == genericOp.getNumIndexArgs() &&
+           "expected num worker counts to match number of iterators");
+
+    auto executeRegion =
+        scf::ExecuteRegionOp::create(rewriter, loc, TypeRange());
+    Block *entryBlock = &genericOp.getRegion().front();
+    Block *newEntry = rewriter.createBlock(&executeRegion.getRegion(),
+                                           executeRegion.getRegion().begin());
+    // Inline the entry block into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               indexArgs);
+
+    // Move the remaining blocks into the new region.
+    SmallVector<Block *> blocksToMove = llvm::map_to_vector(
+        genericOp.getRegion().getBlocks(), [](Block &b) { return &b; });
+    for (Block *block : blocksToMove) {
+      rewriter.moveBlockBefore(block, &executeRegion.getRegion(),
+                               executeRegion.getRegion().end());
+    }
+
+    // Replace all pcf.return ops with scf.yield ops. We make this pattern
+    // responsible for terminator conversion to try to keep IR valid between
+    // pattern rewrites.
+    for (auto &block : executeRegion.getRegion()) {
+      auto pcfTerminator = dyn_cast<PCF::ReturnOp>(block.getTerminator());
+      if (pcfTerminator) {
+        rewriter.setInsertionPoint(pcfTerminator);
+        rewriter.replaceOpWithNewOp<scf::YieldOp>(pcfTerminator);
+      }
+    }
+
+    rewriter.eraseOp(genericOp);
+
+    return success();
+  }
+};
+
+struct LowerLoopOp : public OpRewritePattern<IREE::PCF::LoopOp> {
+  using Base::Base;
+  LogicalResult matchAndRewrite(IREE::PCF::LoopOp loopOp,
+                                PatternRewriter &rewriter) const override {
+    if (loopOp->getNumResults() != 0) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "unexpected attempt to resolve loop op with results");
+    }
+
+    if (loopOp.getSyncOnReturn()) {
+      OpBuilder::InsertionGuard g(rewriter);
+      rewriter.setInsertionPointAfter(loopOp);
+      if (failed(loopOp.getScope().addBarrier(rewriter))) {
+        loopOp.emitOpError("failed to construct requested barrier");
+        return failure();
+      }
+    }
+
+    Location loc = loopOp.getLoc();
+    SmallVector<Value> workerCounts =
+        loopOp.getScope().getWorkerCounts(rewriter, loc, loopOp.getNumIdArgs());
+    SmallVector<Value> ids =
+        loopOp.getScope().getWorkerIDs(rewriter, loc, loopOp.getNumIdArgs());
+    auto pcfTerminator = cast<PCF::ReturnOp>(loopOp.getBody()->getTerminator());
+
+    assert(workerCounts.size() == loopOp.getNumIdArgs() &&
+           "expected worker count to match number of id args");
+    assert(ids.size() == loopOp.getNumIdArgs() &&
+           "expected worker id count to match number of id args");
+
+    // Create an scf.forall op with no mapping. This is moderately more
+    // expressive than scf.for because it indicates that loop iterations
+    // are independent of one another.
+    //
+    // `scf.forall` orders its loop parameters from slowest to fasted varying,
+    // so reverse lbs/ubs/steps.
+    SmallVector<OpFoldResult> lbs(ids.rbegin(), ids.rend());
+    SmallVector<OpFoldResult> steps(workerCounts.rbegin(), workerCounts.rend());
```

**Comment:**
nit/optional: `auto bar = to_vector(reverse(foo));`

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/LowerStructuralPCF.cpp:213`

```diff
@@ -0,0 +1,253 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Utils/RewriteUtils.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlow.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-pcf-lower-structural-pcf"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_LOWERSTRUCTURALPCFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadStructuralLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct LowerStructuralPCFPass final
+    : impl::LowerStructuralPCFPassBase<LowerStructuralPCFPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::PCF::PCFDialect, scf::SCFDialect,
+                    cf::ControlFlowDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+struct LowerGenericOp : public OpRewritePattern<IREE::PCF::GenericOp> {
+  using Base::Base;
+  LogicalResult matchAndRewrite(IREE::PCF::GenericOp genericOp,
+                                PatternRewriter &rewriter) const override {
+    if (genericOp->getNumResults() != 0) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "unexpected attempt to resolve generic op with results");
+    }
+
+    if (genericOp.getSyncOnReturn()) {
+      OpBuilder::InsertionGuard g(rewriter);
+      rewriter.setInsertionPointAfter(genericOp);
+      if (failed(genericOp.getScope().addBarrier(rewriter))) {
+        genericOp.emitOpError("failed to construct requested barrier");
+        return failure();
+      }
+    }
+
+    Location loc = genericOp.getLoc();
+    SmallVector<Value> indexArgs = genericOp.getScope().getWorkerIDs(
+        rewriter, loc, genericOp.getNumIterators());
+    assert(indexArgs.size() == genericOp.getNumIterators() &&
+           "expected num worker ids to match number of iterators");
+
+    indexArgs.append(genericOp.getScope().getWorkerCounts(
+        rewriter, loc, genericOp.getNumIterators()));
+    assert(indexArgs.size() == genericOp.getNumIndexArgs() &&
+           "expected num worker counts to match number of iterators");
+
+    auto executeRegion =
+        scf::ExecuteRegionOp::create(rewriter, loc, TypeRange());
+    Block *entryBlock = &genericOp.getRegion().front();
+    Block *newEntry = rewriter.createBlock(&executeRegion.getRegion(),
+                                           executeRegion.getRegion().begin());
+    // Inline the entry block into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               indexArgs);
+
+    // Move the remaining blocks into the new region.
+    SmallVector<Block *> blocksToMove = llvm::map_to_vector(
+        genericOp.getRegion().getBlocks(), [](Block &b) { return &b; });
+    for (Block *block : blocksToMove) {
+      rewriter.moveBlockBefore(block, &executeRegion.getRegion(),
+                               executeRegion.getRegion().end());
+    }
+
+    // Replace all pcf.return ops with scf.yield ops. We make this pattern
+    // responsible for terminator conversion to try to keep IR valid between
+    // pattern rewrites.
+    for (auto &block : executeRegion.getRegion()) {
+      auto pcfTerminator = dyn_cast<PCF::ReturnOp>(block.getTerminator());
+      if (pcfTerminator) {
+        rewriter.setInsertionPoint(pcfTerminator);
+        rewriter.replaceOpWithNewOp<scf::YieldOp>(pcfTerminator);
+      }
+    }
+
+    rewriter.eraseOp(genericOp);
+
+    return success();
+  }
+};
+
+struct LowerLoopOp : public OpRewritePattern<IREE::PCF::LoopOp> {
+  using Base::Base;
+  LogicalResult matchAndRewrite(IREE::PCF::LoopOp loopOp,
+                                PatternRewriter &rewriter) const override {
+    if (loopOp->getNumResults() != 0) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "unexpected attempt to resolve loop op with results");
+    }
+
+    if (loopOp.getSyncOnReturn()) {
+      OpBuilder::InsertionGuard g(rewriter);
+      rewriter.setInsertionPointAfter(loopOp);
+      if (failed(loopOp.getScope().addBarrier(rewriter))) {
+        loopOp.emitOpError("failed to construct requested barrier");
+        return failure();
+      }
+    }
+
+    Location loc = loopOp.getLoc();
+    SmallVector<Value> workerCounts =
+        loopOp.getScope().getWorkerCounts(rewriter, loc, loopOp.getNumIdArgs());
+    SmallVector<Value> ids =
+        loopOp.getScope().getWorkerIDs(rewriter, loc, loopOp.getNumIdArgs());
+    auto pcfTerminator = cast<PCF::ReturnOp>(loopOp.getBody()->getTerminator());
+
+    assert(workerCounts.size() == loopOp.getNumIdArgs() &&
+           "expected worker count to match number of id args");
+    assert(ids.size() == loopOp.getNumIdArgs() &&
+           "expected worker id count to match number of id args");
+
+    // Create an scf.forall op with no mapping. This is moderately more
+    // expressive than scf.for because it indicates that loop iterations
+    // are independent of one another.
+    //
+    // `scf.forall` orders its loop parameters from slowest to fasted varying,
+    // so reverse lbs/ubs/steps.
+    SmallVector<OpFoldResult> lbs(ids.rbegin(), ids.rend());
+    SmallVector<OpFoldResult> steps(workerCounts.rbegin(), workerCounts.rend());
+    SmallVector<OpFoldResult> ubs(llvm::reverse(loopOp.getCount()));
+    auto forallOp = scf::ForallOp::create(
+        rewriter, loc, lbs, ubs, steps, ValueRange(), /*mapping=*/std::nullopt);
+    // We can take the body of the loop op directly since the index body args
+    // already represent the tile ids.
+    forallOp.getRegion().takeBody(loopOp.getRegion());
+    Block *body = forallOp.getBody();
+
+    // Since forall loop ids are ordered from slowest to fastest varying we need
+    // to reverse the ordering of their uses.
+    SmallVector<int64_t> reversePerm = llvm::to_vector(
+        llvm::reverse(llvm::seq<int64_t>(body->getNumArguments())));
+    permuteValues(rewriter, loc, body->getArguments(), reversePerm);
+
+    // Replace pcf.return terminator with scf.forall.in_parallel.
+    rewriter.setInsertionPoint(pcfTerminator);
+    rewriter.replaceOpWithNewOp<scf::InParallelOp>(pcfTerminator);
+
+    // Erase the old loop op.
+    rewriter.eraseOp(loopOp);
+
+    return success();
+  }
+};
+
+static bool hasReturnOnlyBody(Block &b) {
+  return llvm::hasSingleElement(b.getOperations()) &&
+         isa<PCF::ReturnOp>(&b.getOperations().back());
+}
+
+static Block *getOrCreateReturnBlock(RewriterBase &rewriter, Location loc,
+                                     Region *region, TypeRange argTypes) {
+  assert(!region->getBlocks().empty() && "unexpected empty region");
+  Block &endBlock = region->getBlocks().back();
+  if (endBlock.getArgumentTypes() == argTypes && hasReturnOnlyBody(endBlock)) {
+    return &endBlock;
+  }
+
+  // Insertion guard back to the original point. Creating a block sets the
+  // insertion point to the end of the current block.
+  OpBuilder::InsertionGuard g(rewriter);
+  Block *newBlock =
+      rewriter.createBlock(region, region->end(), argTypes,
+                           SmallVector<Location>(argTypes.size(), loc));
+  PCF::ReturnOp::create(rewriter, loc);
+  return newBlock;
+}
+
+struct LowerBranchCondReturnOp
+    : public OpRewritePattern<IREE::PCF::BranchCondReturnOp> {
```

**Comment:**
```suggestion
struct LowerBranchCondReturnOp final
    : OpRewritePattern<IREE::PCF::BranchCondReturnOp> {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/LowerStructuralPCF.cpp:163`

```diff
@@ -0,0 +1,253 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCF.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/ConversionDialectInterface.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Utils/RewriteUtils.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlow.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Support/WalkResult.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-pcf-lower-structural-pcf"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_LOWERSTRUCTURALPCFPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+class LoadDependentDialectExtension : public DialectExtensionBase {
+public:
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LoadDependentDialectExtension)
+
+  LoadDependentDialectExtension() : DialectExtensionBase(/*dialectNames=*/{}) {}
+
+  void apply(MLIRContext *context,
+             MutableArrayRef<Dialect *> dialects) const final {
+    for (Dialect *dialect : dialects) {
+      auto *iface = dyn_cast<PCFConversionDialectInterface>(dialect);
+      if (!iface) {
+        continue;
+      }
+      iface->loadStructuralLoweringDependentDialects(context);
+    }
+  }
+
+  /// Return a copy of this extension.
+  std::unique_ptr<DialectExtensionBase> clone() const final {
+    return std::make_unique<LoadDependentDialectExtension>(*this);
+  }
+};
+
+struct LowerStructuralPCFPass final
+    : impl::LowerStructuralPCFPassBase<LowerStructuralPCFPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    // Direct dialect deps.
+    registry.insert<iree_compiler::IREE::PCF::PCFDialect, scf::SCFDialect,
+                    cf::ControlFlowDialect>();
+    registry.addExtensions<LoadDependentDialectExtension>();
+  }
+  void runOnOperation() override;
+};
+
+struct LowerGenericOp : public OpRewritePattern<IREE::PCF::GenericOp> {
+  using Base::Base;
+  LogicalResult matchAndRewrite(IREE::PCF::GenericOp genericOp,
+                                PatternRewriter &rewriter) const override {
+    if (genericOp->getNumResults() != 0) {
+      return rewriter.notifyMatchFailure(
+          genericOp, "unexpected attempt to resolve generic op with results");
+    }
+
+    if (genericOp.getSyncOnReturn()) {
+      OpBuilder::InsertionGuard g(rewriter);
+      rewriter.setInsertionPointAfter(genericOp);
+      if (failed(genericOp.getScope().addBarrier(rewriter))) {
+        genericOp.emitOpError("failed to construct requested barrier");
+        return failure();
+      }
+    }
+
+    Location loc = genericOp.getLoc();
+    SmallVector<Value> indexArgs = genericOp.getScope().getWorkerIDs(
+        rewriter, loc, genericOp.getNumIterators());
+    assert(indexArgs.size() == genericOp.getNumIterators() &&
+           "expected num worker ids to match number of iterators");
+
+    indexArgs.append(genericOp.getScope().getWorkerCounts(
+        rewriter, loc, genericOp.getNumIterators()));
+    assert(indexArgs.size() == genericOp.getNumIndexArgs() &&
+           "expected num worker counts to match number of iterators");
+
+    auto executeRegion =
+        scf::ExecuteRegionOp::create(rewriter, loc, TypeRange());
+    Block *entryBlock = &genericOp.getRegion().front();
+    Block *newEntry = rewriter.createBlock(&executeRegion.getRegion(),
+                                           executeRegion.getRegion().begin());
+    // Inline the entry block into the new region.
+    rewriter.inlineBlockBefore(entryBlock, newEntry, newEntry->begin(),
+                               indexArgs);
+
+    // Move the remaining blocks into the new region.
+    SmallVector<Block *> blocksToMove = llvm::map_to_vector(
+        genericOp.getRegion().getBlocks(), [](Block &b) { return &b; });
+    for (Block *block : blocksToMove) {
+      rewriter.moveBlockBefore(block, &executeRegion.getRegion(),
+                               executeRegion.getRegion().end());
+    }
+
+    // Replace all pcf.return ops with scf.yield ops. We make this pattern
+    // responsible for terminator conversion to try to keep IR valid between
+    // pattern rewrites.
+    for (auto &block : executeRegion.getRegion()) {
+      auto pcfTerminator = dyn_cast<PCF::ReturnOp>(block.getTerminator());
+      if (pcfTerminator) {
+        rewriter.setInsertionPoint(pcfTerminator);
+        rewriter.replaceOpWithNewOp<scf::YieldOp>(pcfTerminator);
+      }
+    }
+
+    rewriter.eraseOp(genericOp);
+
+    return success();
+  }
+};
+
+struct LowerLoopOp : public OpRewritePattern<IREE::PCF::LoopOp> {
+  using Base::Base;
+  LogicalResult matchAndRewrite(IREE::PCF::LoopOp loopOp,
+                                PatternRewriter &rewriter) const override {
+    if (loopOp->getNumResults() != 0) {
+      return rewriter.notifyMatchFailure(
+          loopOp, "unexpected attempt to resolve loop op with results");
+    }
+
+    if (loopOp.getSyncOnReturn()) {
+      OpBuilder::InsertionGuard g(rewriter);
+      rewriter.setInsertionPointAfter(loopOp);
+      if (failed(loopOp.getScope().addBarrier(rewriter))) {
+        loopOp.emitOpError("failed to construct requested barrier");
+        return failure();
+      }
+    }
+
+    Location loc = loopOp.getLoc();
+    SmallVector<Value> workerCounts =
+        loopOp.getScope().getWorkerCounts(rewriter, loc, loopOp.getNumIdArgs());
+    SmallVector<Value> ids =
+        loopOp.getScope().getWorkerIDs(rewriter, loc, loopOp.getNumIdArgs());
+    auto pcfTerminator = cast<PCF::ReturnOp>(loopOp.getBody()->getTerminator());
+
+    assert(workerCounts.size() == loopOp.getNumIdArgs() &&
+           "expected worker count to match number of id args");
+    assert(ids.size() == loopOp.getNumIdArgs() &&
+           "expected worker id count to match number of id args");
+
+    // Create an scf.forall op with no mapping. This is moderately more
+    // expressive than scf.for because it indicates that loop iterations
+    // are independent of one another.
+    //
+    // `scf.forall` orders its loop parameters from slowest to fasted varying,
+    // so reverse lbs/ubs/steps.
+    SmallVector<OpFoldResult> lbs(ids.rbegin(), ids.rend());
+    SmallVector<OpFoldResult> steps(workerCounts.rbegin(), workerCounts.rend());
```

**Comment:**
you can do `to_vector_of<OpFoldResult>`?

---


---


## [PR #22914](https://github.com/iree-org/iree/pull/22914): [Codegen] Add FusePCFWritesPass

### Review Summary

**APPROVED** (2025-12-29)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/FusePCFWrites.cpp:37`

```diff
@@ -0,0 +1,262 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Transforms.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/Arith/Utils/Utils.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "mlir/Transforms/RegionUtils.h"
+
+#define DEBUG_TYPE "iree-pcf-fuse-pcf-writes"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_FUSEPCFWRITESPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+struct FusePCFWritesPass final
+    : impl::FusePCFWritesPassBase<FusePCFWritesPass> {
+  void runOnOperation() override;
+};
+
+/// Pattern to fuse pcf.write_slice with tensor.parallel_insert_slice from
+/// scf.forall terminators.
+struct FuseWriteSliceWithParallelInsert
+    : public OpRewritePattern<PCF::WriteSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
```

**Comment:**
```suggestion
struct FuseWriteSliceWithParallelInsert final
    : OpRewritePattern<PCF::WriteSliceOp> {
  using Base::Base;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/FusePCFWrites.cpp:54`

```diff
@@ -0,0 +1,262 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Transforms.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/Arith/Utils/Utils.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "mlir/Transforms/RegionUtils.h"
+
+#define DEBUG_TYPE "iree-pcf-fuse-pcf-writes"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_FUSEPCFWRITESPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+struct FusePCFWritesPass final
+    : impl::FusePCFWritesPassBase<FusePCFWritesPass> {
+  void runOnOperation() override;
+};
+
+/// Pattern to fuse pcf.write_slice with tensor.parallel_insert_slice from
+/// scf.forall terminators.
+struct FuseWriteSliceWithParallelInsert
+    : public OpRewritePattern<PCF::WriteSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(PCF::WriteSliceOp writeSliceOp,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<PCF::WriteSliceOp> newWriteSlice =
+        composeWriteSliceWithParallelInsert(rewriter, writeSliceOp);
+    if (failed(newWriteSlice)) {
+      return rewriter.notifyMatchFailure(
+          writeSliceOp,
+          "source is not an scf.forall with tensor.parallel_insert_slice");
+    }
+    return success();
+  }
+};
+
+void FusePCFWritesPass::runOnOperation() {
+  RewritePatternSet patterns(&getContext());
+  patterns.add<FuseWriteSliceWithParallelInsert>(&getContext());
```

**Comment:**
nit/optional: put context in a local variable?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/FusePCFWrites.cpp:117`

```diff
@@ -0,0 +1,262 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Transforms.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/Arith/Utils/Utils.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "mlir/Transforms/RegionUtils.h"
+
+#define DEBUG_TYPE "iree-pcf-fuse-pcf-writes"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_FUSEPCFWRITESPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+struct FusePCFWritesPass final
+    : impl::FusePCFWritesPassBase<FusePCFWritesPass> {
+  void runOnOperation() override;
+};
+
+/// Pattern to fuse pcf.write_slice with tensor.parallel_insert_slice from
+/// scf.forall terminators.
+struct FuseWriteSliceWithParallelInsert
+    : public OpRewritePattern<PCF::WriteSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(PCF::WriteSliceOp writeSliceOp,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<PCF::WriteSliceOp> newWriteSlice =
+        composeWriteSliceWithParallelInsert(rewriter, writeSliceOp);
+    if (failed(newWriteSlice)) {
+      return rewriter.notifyMatchFailure(
+          writeSliceOp,
+          "source is not an scf.forall with tensor.parallel_insert_slice");
+    }
+    return success();
+  }
+};
+
+void FusePCFWritesPass::runOnOperation() {
+  RewritePatternSet patterns(&getContext());
+  patterns.add<FuseWriteSliceWithParallelInsert>(&getContext());
+
+  // Forall canonicalizations to drop unused results.
+  scf::ForallOp::getCanonicalizationPatterns(patterns, &getContext());
+
+  if (failed(applyPatternsGreedily(getOperation(), std::move(patterns)))) {
+    return signalPassFailure();
+  }
+}
+
+} // namespace
+
+FailureOr<PCF::WriteSliceOp>
+composeWriteSliceWithParallelInsert(RewriterBase &rewriter,
+                                    PCF::WriteSliceOp writeSliceOp) {
+  // Check if the source is produced by an scf.forall.
+  auto forallOp = writeSliceOp.getSource().getDefiningOp<scf::ForallOp>();
+  if (!forallOp) {
+    return failure();
+  }
+
+  // Get the result index being written.
+  auto forallResult = dyn_cast<OpResult>(writeSliceOp.getSource());
+  if (!forallResult) {
+    return failure();
+  }
+  unsigned resultIdx = forallResult.getResultNumber();
+
+  // Get the in_parallel terminator
+  auto inParallelOp =
+      cast<scf::InParallelOp>(forallOp.getRegion().front().getTerminator());
+
+  // Find the tensor.parallel_insert_slice for this result.
+  tensor::ParallelInsertSliceOp insertSliceOp = nullptr;
+  for (Operation &op : inParallelOp.getYieldingOps()) {
+    if (auto insertOp = dyn_cast<tensor::ParallelInsertSliceOp>(&op)) {
+      // Check if this insert targets the correct shared_out
+      auto destArg = dyn_cast<BlockArgument>(insertOp.getDest());
+      if (destArg && destArg.getOwner() == &forallOp.getRegion().front()) {
+        // Map block argument to result index
+        unsigned argIdx = destArg.getArgNumber() - forallOp.getRank();
+        if (argIdx == resultIdx) {
+          if (insertSliceOp) {
+            return rewriter.notifyMatchFailure(
+                forallOp, "unimplemented: multiple insert_slice producers");
+          }
+          insertSliceOp = insertOp;
+        }
+      }
+    }
+  }
+
+  if (!insertSliceOp) {
+    return failure();
+  }
+
+  // Collect all values used by the write_slice that are not the forall result.
+  // These need to be available inside the forall body.
+  SmallVector<Value> writeSliceOperands;
+  writeSliceOperands.push_back(writeSliceOp.getDest());
+  writeSliceOperands.append(writeSliceOp.getOffsets().begin(),
+                            writeSliceOp.getOffsets().end());
+  writeSliceOperands.append(writeSliceOp.getStrides().begin(),
+                            writeSliceOp.getStrides().end());
```

**Comment:**
`llvm::append_range`

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/FusePCFWrites.cpp:152`

```diff
@@ -0,0 +1,262 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Transforms.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/Arith/Utils/Utils.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "mlir/Transforms/RegionUtils.h"
+
+#define DEBUG_TYPE "iree-pcf-fuse-pcf-writes"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_FUSEPCFWRITESPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+struct FusePCFWritesPass final
+    : impl::FusePCFWritesPassBase<FusePCFWritesPass> {
+  void runOnOperation() override;
+};
+
+/// Pattern to fuse pcf.write_slice with tensor.parallel_insert_slice from
+/// scf.forall terminators.
+struct FuseWriteSliceWithParallelInsert
+    : public OpRewritePattern<PCF::WriteSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(PCF::WriteSliceOp writeSliceOp,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<PCF::WriteSliceOp> newWriteSlice =
+        composeWriteSliceWithParallelInsert(rewriter, writeSliceOp);
+    if (failed(newWriteSlice)) {
+      return rewriter.notifyMatchFailure(
+          writeSliceOp,
+          "source is not an scf.forall with tensor.parallel_insert_slice");
+    }
+    return success();
+  }
+};
+
+void FusePCFWritesPass::runOnOperation() {
+  RewritePatternSet patterns(&getContext());
+  patterns.add<FuseWriteSliceWithParallelInsert>(&getContext());
+
+  // Forall canonicalizations to drop unused results.
+  scf::ForallOp::getCanonicalizationPatterns(patterns, &getContext());
+
+  if (failed(applyPatternsGreedily(getOperation(), std::move(patterns)))) {
+    return signalPassFailure();
+  }
+}
+
+} // namespace
+
+FailureOr<PCF::WriteSliceOp>
+composeWriteSliceWithParallelInsert(RewriterBase &rewriter,
+                                    PCF::WriteSliceOp writeSliceOp) {
+  // Check if the source is produced by an scf.forall.
+  auto forallOp = writeSliceOp.getSource().getDefiningOp<scf::ForallOp>();
+  if (!forallOp) {
+    return failure();
+  }
+
+  // Get the result index being written.
+  auto forallResult = dyn_cast<OpResult>(writeSliceOp.getSource());
+  if (!forallResult) {
+    return failure();
+  }
+  unsigned resultIdx = forallResult.getResultNumber();
+
+  // Get the in_parallel terminator
+  auto inParallelOp =
+      cast<scf::InParallelOp>(forallOp.getRegion().front().getTerminator());
+
+  // Find the tensor.parallel_insert_slice for this result.
+  tensor::ParallelInsertSliceOp insertSliceOp = nullptr;
+  for (Operation &op : inParallelOp.getYieldingOps()) {
+    if (auto insertOp = dyn_cast<tensor::ParallelInsertSliceOp>(&op)) {
+      // Check if this insert targets the correct shared_out
+      auto destArg = dyn_cast<BlockArgument>(insertOp.getDest());
+      if (destArg && destArg.getOwner() == &forallOp.getRegion().front()) {
+        // Map block argument to result index
+        unsigned argIdx = destArg.getArgNumber() - forallOp.getRank();
+        if (argIdx == resultIdx) {
+          if (insertSliceOp) {
+            return rewriter.notifyMatchFailure(
+                forallOp, "unimplemented: multiple insert_slice producers");
+          }
+          insertSliceOp = insertOp;
+        }
+      }
+    }
+  }
+
+  if (!insertSliceOp) {
+    return failure();
+  }
+
+  // Collect all values used by the write_slice that are not the forall result.
+  // These need to be available inside the forall body.
+  SmallVector<Value> writeSliceOperands;
+  writeSliceOperands.push_back(writeSliceOp.getDest());
+  writeSliceOperands.append(writeSliceOp.getOffsets().begin(),
+                            writeSliceOp.getOffsets().end());
+  writeSliceOperands.append(writeSliceOp.getStrides().begin(),
+                            writeSliceOp.getStrides().end());
+
+  // Move the definitions of these operands before the forall if they are
+  // defined after it. This can happen if the producer of an operand dominates
+  // the forall but is placed after it in the IR.
+  if (failed(moveValueDefinitions(rewriter, writeSliceOperands, forallOp))) {
+    return rewriter.notifyMatchFailure(
+        writeSliceOp,
+        "failed to move write_slice operand definitions before forall");
+  }
+
+  // Compose the offsets, sizes, and strides and insert the new write_slice
+  // before the parallel_insert_slice in the forall body.
+  // The new write_slice should use:
+  // - source: insertSlice.getSource()
+  // - dest: writeSlice.getDest()
+  // - offsets: writeSlice.offsets + insertSlice.offsets * writeSlice.strides
+  // - sizes: insertSlice.sizes
+  // - strides: writeSlice.strides * insertSlice.strides
+
+  SmallVector<OpFoldResult> composedOffsets;
+  SmallVector<OpFoldResult> composedSizes = insertSliceOp.getMixedSizes();
+  SmallVector<OpFoldResult> composedStrides;
+
+  OpBuilder::InsertionGuard guard(rewriter);
+  // Insert before the in_parallel terminator, not inside it.
+  rewriter.setInsertionPoint(inParallelOp);
+
+  SmallVector<OpFoldResult> writeOffsets = writeSliceOp.getMixedOffsets();
+  SmallVector<OpFoldResult> insertOffsets = insertSliceOp.getMixedOffsets();
+  SmallVector<OpFoldResult> writeStrides = writeSliceOp.getMixedStrides();
+  SmallVector<OpFoldResult> insertStrides = insertSliceOp.getMixedStrides();
+
+  // Compose offsets: writeOffset + insertOffset * writeStride.
+  for (auto [writeOffset, insertOffset, writeStride] :
+       llvm::zip(writeOffsets, insertOffsets, writeStrides)) {
```

**Comment:**
can we zip_equal?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/FusePCFWrites.cpp:169`

```diff
@@ -0,0 +1,262 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Transforms.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/Arith/Utils/Utils.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "mlir/Transforms/RegionUtils.h"
+
+#define DEBUG_TYPE "iree-pcf-fuse-pcf-writes"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_FUSEPCFWRITESPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+struct FusePCFWritesPass final
+    : impl::FusePCFWritesPassBase<FusePCFWritesPass> {
+  void runOnOperation() override;
+};
+
+/// Pattern to fuse pcf.write_slice with tensor.parallel_insert_slice from
+/// scf.forall terminators.
+struct FuseWriteSliceWithParallelInsert
+    : public OpRewritePattern<PCF::WriteSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(PCF::WriteSliceOp writeSliceOp,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<PCF::WriteSliceOp> newWriteSlice =
+        composeWriteSliceWithParallelInsert(rewriter, writeSliceOp);
+    if (failed(newWriteSlice)) {
+      return rewriter.notifyMatchFailure(
+          writeSliceOp,
+          "source is not an scf.forall with tensor.parallel_insert_slice");
+    }
+    return success();
+  }
+};
+
+void FusePCFWritesPass::runOnOperation() {
+  RewritePatternSet patterns(&getContext());
+  patterns.add<FuseWriteSliceWithParallelInsert>(&getContext());
+
+  // Forall canonicalizations to drop unused results.
+  scf::ForallOp::getCanonicalizationPatterns(patterns, &getContext());
+
+  if (failed(applyPatternsGreedily(getOperation(), std::move(patterns)))) {
+    return signalPassFailure();
+  }
+}
+
+} // namespace
+
+FailureOr<PCF::WriteSliceOp>
+composeWriteSliceWithParallelInsert(RewriterBase &rewriter,
+                                    PCF::WriteSliceOp writeSliceOp) {
+  // Check if the source is produced by an scf.forall.
+  auto forallOp = writeSliceOp.getSource().getDefiningOp<scf::ForallOp>();
+  if (!forallOp) {
+    return failure();
+  }
+
+  // Get the result index being written.
+  auto forallResult = dyn_cast<OpResult>(writeSliceOp.getSource());
+  if (!forallResult) {
+    return failure();
+  }
+  unsigned resultIdx = forallResult.getResultNumber();
+
+  // Get the in_parallel terminator
+  auto inParallelOp =
+      cast<scf::InParallelOp>(forallOp.getRegion().front().getTerminator());
+
+  // Find the tensor.parallel_insert_slice for this result.
+  tensor::ParallelInsertSliceOp insertSliceOp = nullptr;
+  for (Operation &op : inParallelOp.getYieldingOps()) {
+    if (auto insertOp = dyn_cast<tensor::ParallelInsertSliceOp>(&op)) {
+      // Check if this insert targets the correct shared_out
+      auto destArg = dyn_cast<BlockArgument>(insertOp.getDest());
+      if (destArg && destArg.getOwner() == &forallOp.getRegion().front()) {
+        // Map block argument to result index
+        unsigned argIdx = destArg.getArgNumber() - forallOp.getRank();
+        if (argIdx == resultIdx) {
+          if (insertSliceOp) {
+            return rewriter.notifyMatchFailure(
+                forallOp, "unimplemented: multiple insert_slice producers");
+          }
+          insertSliceOp = insertOp;
+        }
+      }
+    }
+  }
+
+  if (!insertSliceOp) {
+    return failure();
+  }
+
+  // Collect all values used by the write_slice that are not the forall result.
+  // These need to be available inside the forall body.
+  SmallVector<Value> writeSliceOperands;
+  writeSliceOperands.push_back(writeSliceOp.getDest());
+  writeSliceOperands.append(writeSliceOp.getOffsets().begin(),
+                            writeSliceOp.getOffsets().end());
+  writeSliceOperands.append(writeSliceOp.getStrides().begin(),
+                            writeSliceOp.getStrides().end());
+
+  // Move the definitions of these operands before the forall if they are
+  // defined after it. This can happen if the producer of an operand dominates
+  // the forall but is placed after it in the IR.
+  if (failed(moveValueDefinitions(rewriter, writeSliceOperands, forallOp))) {
+    return rewriter.notifyMatchFailure(
+        writeSliceOp,
+        "failed to move write_slice operand definitions before forall");
+  }
+
+  // Compose the offsets, sizes, and strides and insert the new write_slice
+  // before the parallel_insert_slice in the forall body.
+  // The new write_slice should use:
+  // - source: insertSlice.getSource()
+  // - dest: writeSlice.getDest()
+  // - offsets: writeSlice.offsets + insertSlice.offsets * writeSlice.strides
+  // - sizes: insertSlice.sizes
+  // - strides: writeSlice.strides * insertSlice.strides
+
+  SmallVector<OpFoldResult> composedOffsets;
+  SmallVector<OpFoldResult> composedSizes = insertSliceOp.getMixedSizes();
+  SmallVector<OpFoldResult> composedStrides;
+
+  OpBuilder::InsertionGuard guard(rewriter);
+  // Insert before the in_parallel terminator, not inside it.
+  rewriter.setInsertionPoint(inParallelOp);
+
+  SmallVector<OpFoldResult> writeOffsets = writeSliceOp.getMixedOffsets();
+  SmallVector<OpFoldResult> insertOffsets = insertSliceOp.getMixedOffsets();
+  SmallVector<OpFoldResult> writeStrides = writeSliceOp.getMixedStrides();
+  SmallVector<OpFoldResult> insertStrides = insertSliceOp.getMixedStrides();
+
+  // Compose offsets: writeOffset + insertOffset * writeStride.
+  for (auto [writeOffset, insertOffset, writeStride] :
+       llvm::zip(writeOffsets, insertOffsets, writeStrides)) {
+    Value writeOffsetVal = getValueOrCreateConstantIndexOp(
+        rewriter, insertSliceOp.getLoc(), writeOffset);
+    Value insertOffsetVal = getValueOrCreateConstantIndexOp(
+        rewriter, insertSliceOp.getLoc(), insertOffset);
+    Value writeStrideVal = getValueOrCreateConstantIndexOp(
+        rewriter, insertSliceOp.getLoc(), writeStride);
+
+    Value scaled = rewriter.createOrFold<arith::MulIOp>(
+        insertSliceOp.getLoc(), insertOffsetVal, writeStrideVal);
+    Value composed = rewriter.createOrFold<arith::AddIOp>(
+        insertSliceOp.getLoc(), writeOffsetVal, scaled);
+    composedOffsets.push_back(composed);
+  }
+
+  // Compose strides: writeStride * insertStride.
+  for (auto [writeStride, insertStride] :
+       llvm::zip(writeStrides, insertStrides)) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/Transforms/FusePCFWrites.cpp:239`

```diff
@@ -0,0 +1,262 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Transforms.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/Arith/Utils/Utils.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "mlir/Transforms/RegionUtils.h"
+
+#define DEBUG_TYPE "iree-pcf-fuse-pcf-writes"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+#define GEN_PASS_DEF_FUSEPCFWRITESPASS
+#include "iree/compiler/Codegen/Dialect/PCF/Transforms/Passes.h.inc"
+
+namespace {
+
+struct FusePCFWritesPass final
+    : impl::FusePCFWritesPassBase<FusePCFWritesPass> {
+  void runOnOperation() override;
+};
+
+/// Pattern to fuse pcf.write_slice with tensor.parallel_insert_slice from
+/// scf.forall terminators.
+struct FuseWriteSliceWithParallelInsert
+    : public OpRewritePattern<PCF::WriteSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(PCF::WriteSliceOp writeSliceOp,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<PCF::WriteSliceOp> newWriteSlice =
+        composeWriteSliceWithParallelInsert(rewriter, writeSliceOp);
+    if (failed(newWriteSlice)) {
+      return rewriter.notifyMatchFailure(
+          writeSliceOp,
+          "source is not an scf.forall with tensor.parallel_insert_slice");
+    }
+    return success();
+  }
+};
+
+void FusePCFWritesPass::runOnOperation() {
+  RewritePatternSet patterns(&getContext());
+  patterns.add<FuseWriteSliceWithParallelInsert>(&getContext());
+
+  // Forall canonicalizations to drop unused results.
+  scf::ForallOp::getCanonicalizationPatterns(patterns, &getContext());
+
+  if (failed(applyPatternsGreedily(getOperation(), std::move(patterns)))) {
+    return signalPassFailure();
+  }
+}
+
+} // namespace
+
+FailureOr<PCF::WriteSliceOp>
+composeWriteSliceWithParallelInsert(RewriterBase &rewriter,
+                                    PCF::WriteSliceOp writeSliceOp) {
+  // Check if the source is produced by an scf.forall.
+  auto forallOp = writeSliceOp.getSource().getDefiningOp<scf::ForallOp>();
+  if (!forallOp) {
+    return failure();
+  }
+
+  // Get the result index being written.
+  auto forallResult = dyn_cast<OpResult>(writeSliceOp.getSource());
+  if (!forallResult) {
+    return failure();
+  }
+  unsigned resultIdx = forallResult.getResultNumber();
+
+  // Get the in_parallel terminator
+  auto inParallelOp =
+      cast<scf::InParallelOp>(forallOp.getRegion().front().getTerminator());
+
+  // Find the tensor.parallel_insert_slice for this result.
+  tensor::ParallelInsertSliceOp insertSliceOp = nullptr;
+  for (Operation &op : inParallelOp.getYieldingOps()) {
+    if (auto insertOp = dyn_cast<tensor::ParallelInsertSliceOp>(&op)) {
+      // Check if this insert targets the correct shared_out
+      auto destArg = dyn_cast<BlockArgument>(insertOp.getDest());
+      if (destArg && destArg.getOwner() == &forallOp.getRegion().front()) {
+        // Map block argument to result index
+        unsigned argIdx = destArg.getArgNumber() - forallOp.getRank();
+        if (argIdx == resultIdx) {
+          if (insertSliceOp) {
+            return rewriter.notifyMatchFailure(
+                forallOp, "unimplemented: multiple insert_slice producers");
+          }
+          insertSliceOp = insertOp;
+        }
+      }
+    }
+  }
+
+  if (!insertSliceOp) {
+    return failure();
+  }
+
+  // Collect all values used by the write_slice that are not the forall result.
+  // These need to be available inside the forall body.
+  SmallVector<Value> writeSliceOperands;
+  writeSliceOperands.push_back(writeSliceOp.getDest());
+  writeSliceOperands.append(writeSliceOp.getOffsets().begin(),
+                            writeSliceOp.getOffsets().end());
+  writeSliceOperands.append(writeSliceOp.getStrides().begin(),
+                            writeSliceOp.getStrides().end());
+
+  // Move the definitions of these operands before the forall if they are
+  // defined after it. This can happen if the producer of an operand dominates
+  // the forall but is placed after it in the IR.
+  if (failed(moveValueDefinitions(rewriter, writeSliceOperands, forallOp))) {
+    return rewriter.notifyMatchFailure(
+        writeSliceOp,
+        "failed to move write_slice operand definitions before forall");
+  }
+
+  // Compose the offsets, sizes, and strides and insert the new write_slice
+  // before the parallel_insert_slice in the forall body.
+  // The new write_slice should use:
+  // - source: insertSlice.getSource()
+  // - dest: writeSlice.getDest()
+  // - offsets: writeSlice.offsets + insertSlice.offsets * writeSlice.strides
+  // - sizes: insertSlice.sizes
+  // - strides: writeSlice.strides * insertSlice.strides
+
+  SmallVector<OpFoldResult> composedOffsets;
+  SmallVector<OpFoldResult> composedSizes = insertSliceOp.getMixedSizes();
+  SmallVector<OpFoldResult> composedStrides;
+
+  OpBuilder::InsertionGuard guard(rewriter);
+  // Insert before the in_parallel terminator, not inside it.
+  rewriter.setInsertionPoint(inParallelOp);
+
+  SmallVector<OpFoldResult> writeOffsets = writeSliceOp.getMixedOffsets();
+  SmallVector<OpFoldResult> insertOffsets = insertSliceOp.getMixedOffsets();
+  SmallVector<OpFoldResult> writeStrides = writeSliceOp.getMixedStrides();
+  SmallVector<OpFoldResult> insertStrides = insertSliceOp.getMixedStrides();
+
+  // Compose offsets: writeOffset + insertOffset * writeStride.
+  for (auto [writeOffset, insertOffset, writeStride] :
+       llvm::zip(writeOffsets, insertOffsets, writeStrides)) {
+    Value writeOffsetVal = getValueOrCreateConstantIndexOp(
+        rewriter, insertSliceOp.getLoc(), writeOffset);
+    Value insertOffsetVal = getValueOrCreateConstantIndexOp(
+        rewriter, insertSliceOp.getLoc(), insertOffset);
+    Value writeStrideVal = getValueOrCreateConstantIndexOp(
+        rewriter, insertSliceOp.getLoc(), writeStride);
+
+    Value scaled = rewriter.createOrFold<arith::MulIOp>(
+        insertSliceOp.getLoc(), insertOffsetVal, writeStrideVal);
+    Value composed = rewriter.createOrFold<arith::AddIOp>(
+        insertSliceOp.getLoc(), writeOffsetVal, scaled);
+    composedOffsets.push_back(composed);
+  }
+
+  // Compose strides: writeStride * insertStride.
+  for (auto [writeStride, insertStride] :
+       llvm::zip(writeStrides, insertStrides)) {
+    Value writeStrideVal = getValueOrCreateConstantIndexOp(
+        rewriter, insertSliceOp.getLoc(), writeStride);
+    Value insertStrideVal = getValueOrCreateConstantIndexOp(
+        rewriter, insertSliceOp.getLoc(), insertStride);
+    Value composed = rewriter.createOrFold<arith::MulIOp>(
+        insertSliceOp.getLoc(), writeStrideVal, insertStrideVal);
+    composedStrides.push_back(composed);
+  }
+
+  // Handle rank-reduced parallel_insert_slice sources.
+  // The source may have fewer dimensions than the destination sref (e.g.,
+  // tensor<1024xf32> being inserted into tensor<512x10240xf32> with sizes
+  // [1, 1024]). We need to expand the source to match the sref rank.
+  Value source = insertSliceOp.getSource();
+  auto sourceType = cast<RankedTensorType>(source.getType());
+  auto srefType = cast<ShapedRefType>(writeSliceOp.getDest().getType());
+  int64_t sourceRank = sourceType.getRank();
+  int64_t destRank = srefType.getRank();
+
+  if (sourceRank < destRank) {
+    // Build reassociation map for expand_shape.
+    // Unit dimensions (size 1) from composedSizes indicate dropped dimensions.
+    SmallVector<int64_t> expandedShape;
+    SmallVector<ReassociationIndices> reassociation;
+
+    int64_t sourceIdx = 0;
+    ReassociationIndices currentGroup;
+
+    for (int64_t i = 0; i < destRank; ++i) {
+      // Check if this dimension is a unit dimension (was dropped in rank
+      // reduction).
+      std::optional<int64_t> staticSize = getConstantIntValue(composedSizes[i]);
+      bool isUnitDim = staticSize && *staticSize == 1;
+
+      if (isUnitDim && sourceIdx < sourceRank) {
+        // This is a unit dimension - check if it matches the source
+        int64_t sourceDimSize = sourceType.getDimSize(sourceIdx);
+        if (sourceDimSize == 1) {
+          // Source also has this as size 1, include it normally
+          expandedShape.push_back(1);
+          currentGroup.push_back(i);
+          sourceIdx++;
+          if (sourceIdx <= sourceRank) {
+            reassociation.push_back(currentGroup);
+            currentGroup.clear();
+          }
+        } else {
+          // This is a truly dropped dimension - add to current group
+          expandedShape.push_back(1);
+          currentGroup.push_back(i);
+        }
+      } else if (sourceIdx < sourceRank) {
+        // Non-unit dimension - maps to a source dimension
+        expandedShape.push_back(sourceType.getDimSize(sourceIdx));
+        currentGroup.push_back(i);
+        sourceIdx++;
+        reassociation.push_back(currentGroup);
+        currentGroup.clear();
+      } else {
+        // Extra trailing unit dimensions
+        expandedShape.push_back(1);
+        if (!reassociation.empty()) {
+          reassociation.back().push_back(i);
+        } else {
+          currentGroup.push_back(i);
+        }
+      }
+    }
+
+    // Handle any remaining group
```

**Comment:**
```suggestion
    // Handle any remaining group.
```

---


---


## [PR #22910](https://github.com/iree-org/iree/pull/22910): [Codegen] Add PCF Transforms boilerplate

### Review Summary

**APPROVED** (2026-01-05)


---


## [PR #22906](https://github.com/iree-org/iree/pull/22906): [LLVMGPU] Remove fold unit extent dims from vector distribute

### Review Summary

**APPROVED** (2025-12-15)


---


## [PR #22901](https://github.com/iree-org/iree/pull/22901): [GlobalOpt] Fuse transpose into matmul-looking linalg.generic

### Review Summary

**COMMENTED** (2025-12-16)

Just some nits

**APPROVED** (2025-12-16)


### Code Comments

**File:** `compiler/src/iree/compiler/GlobalOptimization/test/propagate_linalg_transpose.mlir:849`

```diff
@@ -859,3 +843,140 @@ util.func public @dont_sink_through_edge_expand_shape(%arg0 : tensor<2x3x4xf32>)
 //       ENABLE-EDGE-PROP:   %[[EXP:.+]] = tensor.expand_shape
 //       ENABLE-EDGE-PROP:   %[[RES:.+]] = linalg.transpose
 //       ENABLE-EDGE-PROP:   util.return %[[RES]]
+
+// -----
+
+// Matmul generic transpose fusion
```

**Comment:**
```suggestion
// Matmul generic transpose fusion.
```

---

**File:** `compiler/src/iree/compiler/GlobalOptimization/test/propagate_linalg_transpose.mlir:924`

```diff
@@ -859,3 +843,140 @@ util.func public @dont_sink_through_edge_expand_shape(%arg0 : tensor<2x3x4xf32>)
 //       ENABLE-EDGE-PROP:   %[[EXP:.+]] = tensor.expand_shape
 //       ENABLE-EDGE-PROP:   %[[RES:.+]] = linalg.transpose
 //       ENABLE-EDGE-PROP:   util.return %[[RES]]
+
+// -----
+
+// Matmul generic transpose fusion
+#map_lhs = affine_map<(d0, d1, d2) -> (d0, d2)>
+#map_rhs = affine_map<(d0, d1, d2) -> (d2, d1)>
+#map_out = affine_map<(d0, d1, d2) -> (d0, d1)>
+util.func public @fuse_transpose_through_generic_matmul(
+  %lhs: tensor<16x32xf32>, %transposed_rhs: tensor<16x32xf32>) -> tensor<16x16xf32> {
+  %empty = tensor.empty(): tensor<32x16xf32>
+  %rhs = linalg.transpose ins(%transposed_rhs : tensor<16x32xf32>)
+      outs(%empty : tensor<32x16xf32>) permutation = [1, 0]
+  %init = tensor.empty(): tensor<16x16xf32>
+  %cst = arith.constant 0.0 : f32
+  %fill = linalg.fill ins(%cst : f32) outs(%init : tensor<16x16xf32>) -> tensor<16x16xf32>
+  %matmul = linalg.generic {
+      indexing_maps = [#map_lhs, #map_rhs, #map_out],
+      iterator_types = ["parallel", "parallel", "reduction"]}
+      ins(%lhs, %rhs : tensor<16x32xf32>, tensor<32x16xf32>)
+      outs(%fill : tensor<16x16xf32>) {
+    ^bb0(%a: f32, %b: f32, %c: f32):
+      %mul = arith.mulf %a, %b : f32
+      %add = arith.addf %c, %mul : f32
+      linalg.yield %add : f32
+  } -> tensor<16x16xf32>
+  util.return %matmul : tensor<16x16xf32>
+}
+//   CHECK-DAG: #[[$MAP_LHS:.+]] = affine_map<(d0, d1, d2) -> (d0, d2)>
+//   CHECK-DAG: #[[$MAP_RHS_TRANSPOSED:.+]] = affine_map<(d0, d1, d2) -> (d1, d2)>
+//   CHECK-DAG: #[[$MAP_OUT:.+]] = affine_map<(d0, d1, d2) -> (d0, d1)>
+// CHECK-LABEL: util.func public @fuse_transpose_through_generic_matmul
+//  CHECK-SAME:     %[[LHS:[a-zA-Z0-9]+]]: tensor<16x32xf32>
+//  CHECK-SAME:     %[[TRANSPOSED_RHS:[a-zA-Z0-9]+]]: tensor<16x32xf32>
+//   CHECK-NOT:   linalg.transpose
+//       CHECK:   %[[MATMUL:.+]] = linalg.generic
+//  CHECK-SAME:     indexing_maps = [#[[$MAP_LHS]], #[[$MAP_RHS_TRANSPOSED]], #[[$MAP_OUT]]]
+//  CHECK-SAME:     ins(%[[LHS]], %[[TRANSPOSED_RHS]] : tensor<16x32xf32>, tensor<16x32xf32>)
+//       CHECK:   util.return %[[MATMUL]]
+
+// -----
+
+// Batch matmul generic transpose fusion
+#map_bmm_lhs = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
+#map_bmm_rhs = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
+#map_bmm_out = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
+util.func public @fuse_transpose_through_generic_batch_matmul(
+  %lhs: tensor<2x16x32xf32>, %transposed_rhs: tensor<2x16x32xf32>) -> tensor<2x16x16xf32> {
+  %empty = tensor.empty(): tensor<2x32x16xf32>
+  %rhs = linalg.transpose ins(%transposed_rhs : tensor<2x16x32xf32>)
+      outs(%empty : tensor<2x32x16xf32>) permutation = [0, 2, 1]
+  %init = tensor.empty(): tensor<2x16x16xf32>
+  %cst = arith.constant 0.0 : f32
+  %fill = linalg.fill ins(%cst : f32) outs(%init : tensor<2x16x16xf32>) -> tensor<2x16x16xf32>
+  %bmm = linalg.generic {
+      indexing_maps = [#map_bmm_lhs, #map_bmm_rhs, #map_bmm_out],
+      iterator_types = ["parallel", "parallel", "parallel", "reduction"]}
+      ins(%lhs, %rhs : tensor<2x16x32xf32>, tensor<2x32x16xf32>)
+      outs(%fill : tensor<2x16x16xf32>) {
+    ^bb0(%a: f32, %b: f32, %c: f32):
+      %mul = arith.mulf %a, %b : f32
+      %add = arith.addf %c, %mul : f32
+      linalg.yield %add : f32
+  } -> tensor<2x16x16xf32>
+  util.return %bmm : tensor<2x16x16xf32>
+}
+//   CHECK-DAG: #[[$MAP_BMM_LHS:.+]] = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
+//   CHECK-DAG: #[[$MAP_BMM_RHS_TRANSPOSED:.+]] = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
+//   CHECK-DAG: #[[$MAP_BMM_OUT:.+]] = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
+// CHECK-LABEL: util.func public @fuse_transpose_through_generic_batch_matmul
+//  CHECK-SAME:     %[[LHS:[a-zA-Z0-9]+]]: tensor<2x16x32xf32>
+//  CHECK-SAME:     %[[TRANSPOSED_RHS:[a-zA-Z0-9]+]]: tensor<2x16x32xf32>
+//   CHECK-NOT:   linalg.transpose
+//       CHECK:   %[[BMM:.+]] = linalg.generic
+//  CHECK-SAME:     indexing_maps = [#[[$MAP_BMM_LHS]], #[[$MAP_BMM_RHS_TRANSPOSED]], #[[$MAP_BMM_OUT]]]
+//  CHECK-SAME:     ins(%[[LHS]], %[[TRANSPOSED_RHS]] : tensor<2x16x32xf32>, tensor<2x16x32xf32>)
+//       CHECK:   util.return %[[BMM]]
+// -----
+
+// Generic reduction transpose fusion
```

**Comment:**
```suggestion
// Generic reduction transpose fusion.
```

---

**File:** `compiler/src/iree/compiler/GlobalOptimization/PropagateLinalgTranspose.cpp:1015`

```diff
@@ -1007,6 +1007,83 @@ class NamedOpConversion : public OpRewritePattern<OpTy> {
   SmallVector<int64_t> permutation;
 };
 
+// Fuses a transpose into a reduction linalg.generic by absorbing it into the
+// indexing map.
+class FuseTransposeThroughGenericReduction
+    : public OpRewritePattern<linalg::GenericOp> {
+public:
+  using OpRewritePattern<linalg::GenericOp>::OpRewritePattern;
```

**Comment:**
```suggestion
class FuseTransposeThroughGenericReduction
    : public OpRewritePattern<linalg::GenericOp> {
public:
  using Base::Base;
```

---

**File:** `compiler/src/iree/compiler/GlobalOptimization/PropagateLinalgTranspose.cpp:1037`

```diff
@@ -1007,6 +1007,83 @@ class NamedOpConversion : public OpRewritePattern<OpTy> {
   SmallVector<int64_t> permutation;
 };
 
+// Fuses a transpose into a reduction linalg.generic by absorbing it into the
+// indexing map.
+class FuseTransposeThroughGenericReduction
+    : public OpRewritePattern<linalg::GenericOp> {
+public:
+  using OpRewritePattern<linalg::GenericOp>::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(linalg::GenericOp genericOp,
+                                PatternRewriter &rewriter) const override {
+    if (!IREE::Flow::isNonNullAndOutsideDispatch(genericOp)) {
+      return failure();
+    }
+
+    // Only try to fuse when the generic has at least one reduction dimension.
+    if (genericOp.getNumParallelLoops() == genericOp.getNumLoops()) {
+      return rewriter.notifyMatchFailure(genericOp, "not a reduction");
+    }
+
+    // All maps must be projected permutations.
+    if (!llvm::all_of(genericOp.getIndexingMapsArray(), [](AffineMap map) {
+          return map.isProjectedPermutation();
+        })) {
+      return rewriter.notifyMatchFailure(genericOp,
+                                         "not a projected permutation");
+    }
+
+    // Look for a transpose on any input.
+    for (int64_t inputIdx = 0; inputIdx < genericOp.getNumDpsInputs();
```

**Comment:**
nit: don't re-evaluate the end value

```suggestion
    for (int64_t inputIdx = 0, endIdx = genericOp.getNumDpsInputs(); inputIdx < endIdx
```

---

**File:** `compiler/src/iree/compiler/GlobalOptimization/PropagateLinalgTranspose.cpp:1060`

```diff
@@ -1007,6 +1007,83 @@ class NamedOpConversion : public OpRewritePattern<OpTy> {
   SmallVector<int64_t> permutation;
 };
 
+// Fuses a transpose into a reduction linalg.generic by absorbing it into the
+// indexing map.
+class FuseTransposeThroughGenericReduction
+    : public OpRewritePattern<linalg::GenericOp> {
+public:
+  using OpRewritePattern<linalg::GenericOp>::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(linalg::GenericOp genericOp,
+                                PatternRewriter &rewriter) const override {
+    if (!IREE::Flow::isNonNullAndOutsideDispatch(genericOp)) {
+      return failure();
+    }
+
+    // Only try to fuse when the generic has at least one reduction dimension.
+    if (genericOp.getNumParallelLoops() == genericOp.getNumLoops()) {
+      return rewriter.notifyMatchFailure(genericOp, "not a reduction");
+    }
+
+    // All maps must be projected permutations.
+    if (!llvm::all_of(genericOp.getIndexingMapsArray(), [](AffineMap map) {
+          return map.isProjectedPermutation();
+        })) {
+      return rewriter.notifyMatchFailure(genericOp,
+                                         "not a projected permutation");
+    }
+
+    // Look for a transpose on any input.
+    for (int64_t inputIdx = 0; inputIdx < genericOp.getNumDpsInputs();
+         ++inputIdx) {
+      auto transpose = genericOp.getDpsInputs()[inputIdx]
+                           .getDefiningOp<linalg::TransposeOp>();
+      if (!transpose) {
+        continue;
+      }
+
+      // Update the indexing map according to the transpose.
+      AffineMap inputMap = genericOp.getMatchingIndexingMap(
+          genericOp.getDpsInputOperand(inputIdx));
+
+      // Fuse by updating the indexing map to absorb the transpose.
+      auto invPerm = invertPermutationVector(transpose.getPermutation());
+      SmallVector<AffineExpr> newExprs =
+          applyPermutation(inputMap.getResults(), invPerm);
+
+      // Only fuse if dimension indices are in increasing order to maintain
+      // efficient memory access patterns. This should offset the fact that the
+      // transpose may have multiple uses.
+      int64_t prevDim = -1;
+      for (int64_t i = 0; i < newExprs.size(); ++i) {
+        int64_t dim = cast<AffineDimExpr>(newExprs[i]).getPosition();
+        if (dim < prevDim) {
```

**Comment:**
Use a range for loop

---

**File:** `compiler/src/iree/compiler/GlobalOptimization/PropagateLinalgTranspose.cpp:1067`

```diff
@@ -1007,6 +1007,83 @@ class NamedOpConversion : public OpRewritePattern<OpTy> {
   SmallVector<int64_t> permutation;
 };
 
+// Fuses a transpose into a reduction linalg.generic by absorbing it into the
+// indexing map.
+class FuseTransposeThroughGenericReduction
+    : public OpRewritePattern<linalg::GenericOp> {
+public:
+  using OpRewritePattern<linalg::GenericOp>::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(linalg::GenericOp genericOp,
+                                PatternRewriter &rewriter) const override {
+    if (!IREE::Flow::isNonNullAndOutsideDispatch(genericOp)) {
+      return failure();
+    }
+
+    // Only try to fuse when the generic has at least one reduction dimension.
+    if (genericOp.getNumParallelLoops() == genericOp.getNumLoops()) {
+      return rewriter.notifyMatchFailure(genericOp, "not a reduction");
+    }
+
+    // All maps must be projected permutations.
+    if (!llvm::all_of(genericOp.getIndexingMapsArray(), [](AffineMap map) {
+          return map.isProjectedPermutation();
+        })) {
+      return rewriter.notifyMatchFailure(genericOp,
+                                         "not a projected permutation");
+    }
+
+    // Look for a transpose on any input.
+    for (int64_t inputIdx = 0; inputIdx < genericOp.getNumDpsInputs();
+         ++inputIdx) {
+      auto transpose = genericOp.getDpsInputs()[inputIdx]
+                           .getDefiningOp<linalg::TransposeOp>();
+      if (!transpose) {
+        continue;
+      }
+
+      // Update the indexing map according to the transpose.
+      AffineMap inputMap = genericOp.getMatchingIndexingMap(
+          genericOp.getDpsInputOperand(inputIdx));
+
+      // Fuse by updating the indexing map to absorb the transpose.
+      auto invPerm = invertPermutationVector(transpose.getPermutation());
+      SmallVector<AffineExpr> newExprs =
+          applyPermutation(inputMap.getResults(), invPerm);
+
+      // Only fuse if dimension indices are in increasing order to maintain
+      // efficient memory access patterns. This should offset the fact that the
+      // transpose may have multiple uses.
+      int64_t prevDim = -1;
+      for (int64_t i = 0; i < newExprs.size(); ++i) {
+        int64_t dim = cast<AffineDimExpr>(newExprs[i]).getPosition();
+        if (dim < prevDim) {
+          return rewriter.notifyMatchFailure(genericOp,
+                                             "newExprs are not contiguous");
+        }
+        prevDim = dim;
+      }
+
+      AffineMap transposedMap =
```

**Comment:**
```suggestion
      auto transposedMap =
```

---


---


## [PR #22899](https://github.com/iree-org/iree/pull/22899): [ROCM][DT] Add architecture matching to ukernel_info attribute

### Review Summary

**APPROVED** (2025-12-12)

**COMMENTED** (2025-12-12)

**COMMENTED** (2025-12-12)

**COMMENTED** (2025-12-12)


### Code Comments

**File:** `compiler/plugins/target/ROCM/Dialect/ROCM/IR/ROCMAttrs.cpp:175`

```diff
@@ -171,6 +171,19 @@ Attribute TensorUKernelProviderAttr::getDataLayoutForUKernel(
         continue;
       }
     }
+    // Match architecture if specified. Expects an ArrayAttr of StringAttr,
+    // e.g., archs = ["gfx942", "gfx950"]
```

**Comment:**
```suggestion
    // e.g., archs = ["gfx942", "gfx950"].
```

---

**File:** `compiler/plugins/target/ROCM/builtins/mlir_ukernel/iree_uk_amdgpu_dt_matmul_f16.mlir:20`

```diff
@@ -17,6 +17,7 @@
 util.func @pingpong_dt_large_f16(%lhs_base: !lhs_base_ty, %rhs_base: !rhs_base_ty, %unused_acc: !acc_base_ty) -> !acc_base_ty attributes {
   ukernel_info = #rocm.ukernel_info<
     match = {
+      archs = ["gfx942"],
```

**Comment:**
How did you decide on this name instead of following the naming in target env?

---

**File:** `compiler/plugins/target/ROCM/builtins/mlir_ukernel/iree_uk_amdgpu_dt_matmul_f16.mlir:20`

```diff
@@ -17,6 +17,7 @@
 util.func @pingpong_dt_large_f16(%lhs_base: !lhs_base_ty, %rhs_base: !rhs_base_ty, %unused_acc: !acc_base_ty) -> !acc_base_ty attributes {
   ukernel_info = #rocm.ukernel_info<
     match = {
+      archs = ["gfx942"],
```

**Comment:**
yes

---

**File:** `compiler/plugins/target/ROCM/builtins/mlir_ukernel/iree_uk_amdgpu_dt_matmul_f16.mlir:20`

```diff
@@ -17,6 +17,7 @@
 util.func @pingpong_dt_large_f16(%lhs_base: !lhs_base_ty, %rhs_base: !rhs_base_ty, %unused_acc: !acc_base_ty) -> !acc_base_ty attributes {
   ukernel_info = #rocm.ukernel_info<
     match = {
+      archs = ["gfx942"],
```

**Comment:**
oh, it's actually arch in the target env, nevermind then, I misremembered (`arch = "gfx950"`)

---


---


## [PR #22892](https://github.com/iree-org/iree/pull/22892): [Codegen][ROCDL] Improve dynamic dimension bounds handling in ROCDLConfigureBufferInstructions

### Review Summary

**APPROVED** (2025-12-29)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Utils/Utils.cpp:1136`

```diff
@@ -1109,6 +1112,31 @@ int64_t getMinElementBitwidth(linalg::LinalgOp linalgOp) {
   return bitwidth;
 };
 
+//===---------------------------------------------------------------------===//
+// Integer range analysis utility functions
+//===---------------------------------------------------------------------===//
+
+FailureOr<int64_t> getDynamicUpperBound(Value value,
+                                        const DataFlowSolver &solver) {
+  // First try IntegerRangeAnalysis (cached, efficient).
+  if (auto *maybeRange =
+          solver.lookupState<dataflow::IntegerValueRangeLattice>(value)) {
+    IntegerValueRange range = maybeRange->getValue();
+    if (!range.isUninitialized() &&
+        range.getValue().smax() !=
+            IntegerValueRange::getMaxRange(value).getValue().smax()) {
+      return range.getValue().smax().getSExtValue();
+    }
+  }
+  // Fallback to ValueBoundsConstraintSet for complex cases.
+  auto ub = ValueBoundsConstraintSet::computeConstantBound(
+      presburger::BoundType::UB, {value, std::nullopt},
+      /*stopCondition=*/nullptr, /*closedUB=*/true);
+  if (succeeded(ub))
+    return ub.value();
```

**Comment:**
nit: missing braces https://iree.dev/developers/general/contributing/#compiler

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/ROCDLConfigureBufferInstructions.cpp:86`

```diff
@@ -67,28 +71,11 @@ static bool isDefinitelyWorkgroupUniform(Value arg) {
   });
 }
 
-/// Return the maximum value that has been `util.assume.int`'d about this value
-/// if there is one.
-/// TODO: it'd be nice to be able to run the IntRangeAnalysis just up to the
-/// value in question, but we don't have that, so we approximate it.
-static std::optional<int64_t> getDynamicSizeMax(Value size) {
-  size = stripIntegerCasts(size);
-  // Special case for constants that're still dynamic.
-  APInt constVal;
-  if (matchPattern(size, m_ConstantInt(&constVal))) {
-    return constVal.getZExtValue();
-  }
-  auto assumeOp = size.getDefiningOp<IREE::Util::AssumeIntOp>();
-  if (!assumeOp)
-    return std::nullopt;
-  std::optional<int64_t> maybeMax =
-      assumeOp.getUnionedUnsignedRange(cast<OpResult>(size).getResultNumber())
-          .second;
```

**Comment:**
Does this still work with assumptions or did we lose it? I assume it does because I can see that we have relevant tests.

---


---


## [PR #22890](https://github.com/iree-org/iree/pull/22890): [LDS] CoalescedGatherDMAOp support even more transfer sizes.

### Review Summary

**APPROVED** (2025-12-17)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUConvertToCoalescedDMA.cpp:104`

```diff
@@ -79,6 +81,47 @@ computeThreadNumThreadsImpl(OpBuilder &builder, Operation *op,
     return {};
   }
 
+  // Skip coalesced DMA if the innermost dimension is smaller than the minimum
+  // transfer size. The minimum transfer size is subgroupSize *
+  // minElementsPerLane.
+  int64_t rank = outputType.getRank();
+  int64_t innermostDim = outputType.getShape()[rank - 1];
+  if (ShapedType::isDynamic(innermostDim)) {
+    return {};
+  }
+
+  // Get element type bit width.
+  Type elementType = outputType.getElementType();
+  int64_t elementBits = elementType.getIntOrFloatBitWidth();
+
+  // Get DMA sizes from target to compute minimum transfer size.
+  IREE::GPU::TargetAttr target = getGPUTargetAttr(funcOp);
+  if (!target) {
+    return {};
+  }
+
+  ArrayRef<int64_t> dmaSizes;
+  if (auto dmaSizesAttr = target.getWgp().getDmaSizes()) {
```

**Comment:**
nit: spell out the type

---


---


## [PR #22885](https://github.com/iree-org/iree/pull/22885): Integrates/llvm 20251211

### Review Summary

**APPROVED** (2025-12-11)


---


## [PR #22883](https://github.com/iree-org/iree/pull/22883): Update shark/SHARK to amd-shark/AMD-SHARK in documentation and URLs in IREE

### Review Summary

**COMMENTED** (2025-12-11)

**COMMENTED** (2025-12-11)

**APPROVED** (2025-12-11)


### Code Comments

**File:** `samples/colab/pytorch_huggingface_whisper.ipynb:2`

```diff
@@ -1,704 +1,506 @@
 {
-  "nbformat": 4,
-  "nbformat_minor": 0,
-  "metadata": {
+ "nbformat": 4,
```

**Comment:**
Can we update this doc without changing the structure so much?

---

**File:** `samples/colab/pytorch_huggingface_whisper.ipynb:2`

```diff
@@ -1,704 +1,506 @@
 {
-  "nbformat": 4,
-  "nbformat_minor": 0,
-  "metadata": {
+ "nbformat": 4,
```

**Comment:**
You can edit this in a text editor to avoid surprises

---


---


## [PR #22879](https://github.com/iree-org/iree/pull/22879): [LDS] Improve multiple transfers per lane

### Review Summary

**COMMENTED** (2025-12-10)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/AMDGPULowerCoalescedDMAToGatherLDS.cpp:124`

```diff
@@ -118,16 +118,39 @@ struct LowerCoalescedGatherDMAPattern final
     }
     LDBG() << "Subgroup size: " << *subgroupSize;
 
-    // Check that transfer size matches one of the target DMA sizes.
-    int64_t transferSizePerLane = transferSizeBits / *subgroupSize;
-    LDBG() << "Transfer size per lane: " << transferSizePerLane << " bits";
+    // Find a suitable DMA size that allows the innermost dimension to be
+    // evenly divided into N transfers. We prefer larger DMA sizes for
+    // efficiency. Sort DMA sizes in descending order to prefer larger sizes.
+    auto sortedDmaSizes = llvm::to_vector(targetDmaSizes);
```

**Comment:**
nit: can you spell out the type? You can do `to_vector_of<int64_t>(...)`

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/AMDGPULowerCoalescedDMAToGatherLDS.cpp:125`

```diff
@@ -118,16 +118,39 @@ struct LowerCoalescedGatherDMAPattern final
     }
     LDBG() << "Subgroup size: " << *subgroupSize;
 
-    // Check that transfer size matches one of the target DMA sizes.
-    int64_t transferSizePerLane = transferSizeBits / *subgroupSize;
-    LDBG() << "Transfer size per lane: " << transferSizePerLane << " bits";
+    // Find a suitable DMA size that allows the innermost dimension to be
+    // evenly divided into N transfers. We prefer larger DMA sizes for
+    // efficiency. Sort DMA sizes in descending order to prefer larger sizes.
+    auto sortedDmaSizes = llvm::to_vector(targetDmaSizes);
+    llvm::sort(sortedDmaSizes, std::greater<int64_t>());
```

**Comment:**
```suggestion
    llvm::sort(sortedDmaSizes, std::greater<>());
```

---


---


## [PR #22862](https://github.com/iree-org/iree/pull/22862): [LDS] Remove `GPULowerToGlobalLoadsPass` and `GlobalLoadDMAOp`

### Review Summary

**APPROVED** (2025-12-15)


---


## [PR #22847](https://github.com/iree-org/iree/pull/22847): Add layout for mma.sync.m16n8k16

### Review Summary

**COMMENTED** (2025-12-06)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.cpp:679`

```diff
@@ -652,6 +671,20 @@ static VectorType getThreadVectorType(MLIRContext *context,
   Type elemType = isIntrinsicLhs<MMAIntrinsicType>(operandIndex)   ? o.aType
                   : isIntrinsicRhs<MMAIntrinsicType>(operandIndex) ? o.bType
                                                                    : o.cType;
+  if constexpr (std::is_same_v<MMAIntrinsicType, MMAIntrinsic>) {
+    if (intrinsic == MMAIntrinsic::NV_MMA_SYNC_F32_16x8x16_F16 ||
+        intrinsic == MMAIntrinsic::NV_MMA_SYNC_F16_16x8x16_F16) {
+      if (operandIndex == kMMAOperandLhs) {
+        return VectorType::get({4, 2}, elemType);
+      }
+      if (operandIndex == kMMAOperandRhs) {
+        return VectorType::get({2, 2}, elemType);
+      }
+      if (operandIndex == kMMAOperandAcc) {
+        return VectorType::get({2, 2}, elemType);
+      }
+    }
```

**Comment:**
Why can't this be calculated based of single subgroup layout?

---


---


## [PR #22805](https://github.com/iree-org/iree/pull/22805): [Codegen] Add PCF bufferization interfaces

### Review Summary

**APPROVED** (2025-12-12)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.cpp:29`

```diff
@@ -0,0 +1,327 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//===- BufferizationExternalModels.cpp -----------------------------------===//
+//
+// This file implements bufferization interfaces for PCF ops.
+//
+//===---------------------------------------------------------------------===//
+
+#include "iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.h"
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+using namespace mlir::bufferization;
+
+namespace {
+
+struct GenericOpInterface
+    : public BufferizableOpInterface::ExternalModel<GenericOpInterface,
```

**Comment:**
```suggestion
    : BufferizableOpInterface::ExternalModel<GenericOpInterface,
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.cpp:63`

```diff
@@ -0,0 +1,327 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//===- BufferizationExternalModels.cpp -----------------------------------===//
+//
+// This file implements bufferization interfaces for PCF ops.
+//
+//===---------------------------------------------------------------------===//
+
+#include "iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.h"
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+using namespace mlir::bufferization;
+
+namespace {
+
+struct GenericOpInterface
+    : public BufferizableOpInterface::ExternalModel<GenericOpInterface,
+                                                    PCF::GenericOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // Parallel ops can be treated as though they never read.
+    return false;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // Generic ops must always be assumed to write to a tensor (init) operand.
+    return true;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    OpResult tiedResult = genericOp.getTiedResult(opOperand);
+    if (!tiedResult) {
+      return {};
+    }
+
+    return {{tiedResult, BufferRelation::Equivalent,
+             /*isDefinite=*/true}};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    Location loc = genericOp.getLoc();
+
+    SmallVector<Value> newInits;
+    newInits.reserve(genericOp.getInits().size());
+    for (auto init : genericOp.getInits()) {
```

**Comment:**
`Value init`?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.cpp:76`

```diff
@@ -0,0 +1,327 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//===- BufferizationExternalModels.cpp -----------------------------------===//
+//
+// This file implements bufferization interfaces for PCF ops.
+//
+//===---------------------------------------------------------------------===//
+
+#include "iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.h"
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+using namespace mlir::bufferization;
+
+namespace {
+
+struct GenericOpInterface
+    : public BufferizableOpInterface::ExternalModel<GenericOpInterface,
+                                                    PCF::GenericOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // Parallel ops can be treated as though they never read.
+    return false;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // Generic ops must always be assumed to write to a tensor (init) operand.
+    return true;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    OpResult tiedResult = genericOp.getTiedResult(opOperand);
+    if (!tiedResult) {
+      return {};
+    }
+
+    return {{tiedResult, BufferRelation::Equivalent,
+             /*isDefinite=*/true}};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    Location loc = genericOp.getLoc();
+
+    SmallVector<Value> newInits;
+    newInits.reserve(genericOp.getInits().size());
+    for (auto init : genericOp.getInits()) {
+      if (isa<RankedTensorType>(init.getType())) {
+        FailureOr<Value> newInit = getBuffer(rewriter, init, options, state);
+        if (failed(newInit)) {
+          return op->emitError() << "failed to get init buffer";
+        }
+        newInits.push_back(*newInit);
+      } else {
+        newInits.push_back(init);
+      }
+    }
+
+    SmallVector<Type> newResultTypes;
+    for (auto result : genericOp.getResults()) {
```

**Comment:**
`Value result`?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.cpp:130`

```diff
@@ -0,0 +1,327 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//===- BufferizationExternalModels.cpp -----------------------------------===//
+//
+// This file implements bufferization interfaces for PCF ops.
+//
+//===---------------------------------------------------------------------===//
+
+#include "iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.h"
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+using namespace mlir::bufferization;
+
+namespace {
+
+struct GenericOpInterface
+    : public BufferizableOpInterface::ExternalModel<GenericOpInterface,
+                                                    PCF::GenericOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // Parallel ops can be treated as though they never read.
+    return false;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // Generic ops must always be assumed to write to a tensor (init) operand.
+    return true;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    OpResult tiedResult = genericOp.getTiedResult(opOperand);
+    if (!tiedResult) {
+      return {};
+    }
+
+    return {{tiedResult, BufferRelation::Equivalent,
+             /*isDefinite=*/true}};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    Location loc = genericOp.getLoc();
+
+    SmallVector<Value> newInits;
+    newInits.reserve(genericOp.getInits().size());
+    for (auto init : genericOp.getInits()) {
+      if (isa<RankedTensorType>(init.getType())) {
+        FailureOr<Value> newInit = getBuffer(rewriter, init, options, state);
+        if (failed(newInit)) {
+          return op->emitError() << "failed to get init buffer";
+        }
+        newInits.push_back(*newInit);
+      } else {
+        newInits.push_back(init);
+      }
+    }
+
+    SmallVector<Type> newResultTypes;
+    for (auto result : genericOp.getResults()) {
+      if (isa<TensorType>(result.getType())) {
+        FailureOr<BufferLikeType> resultType =
+            bufferization::getBufferType(result, options, state);
+        if (failed(resultType)) {
+          return failure();
+        }
+        newResultTypes.push_back(*resultType);
+      } else {
+        newResultTypes.push_back(result.getType());
+      }
+    }
+
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, newResultTypes, genericOp.getScope(), newInits,
+        genericOp.getDynamicSizes(), genericOp.getIsTied(),
+        genericOp.getNumIterators(), genericOp.getSyncOnReturn());
+    newGenericOp.getRegion().takeBody(genericOp.getRegion());
+    newGenericOp.getInitializer().takeBody(genericOp.getInitializer());
+    replaceOpWithBufferizedValues(rewriter, op, newGenericOp.getResults());
+    return success();
+  }
+
+  FailureOr<BufferLikeType>
+  getBufferType(Operation *op, Value value, const BufferizationOptions &options,
+                const BufferizationState &state,
+                SmallVector<Value> &invocationStack) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+
+    // Block arguments are `pcf.sref`, so this must always be an opresult.
+    auto result = cast<OpResult>(value);
+    assert(result.getOwner() == op && "invalid value");
+
+    // If the result has a tied init, use that as the buffer type.
+    OpOperand *tiedInit = genericOp.getTiedInit(result.getResultNumber());
+    if (tiedInit) {
+      return bufferization::detail::asMemRefType(bufferization::getBufferType(
+          tiedInit->get(), options, state, invocationStack));
+    }
+
+    auto resultType = cast<RankedTensorType>(result.getType());
+
+    // Else query the scope for the memory space to allocate for.
+    FailureOr<Attribute> memSpace =
+        genericOp.getScope().getAllocMemSpace(op->getContext());
+    if (failed(memSpace)) {
+      return failure();
+    }
+    return cast<BufferLikeType>(
+        getMemRefTypeWithStaticIdentityLayout(resultType, *memSpace));
+  }
+};
+
+struct LoopOpInterface
+    : public BufferizableOpInterface::ExternalModel<LoopOpInterface,
```

**Comment:**
```suggestion
    : BufferizableOpInterface::ExternalModel<LoopOpInterface,
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.cpp:164`

```diff
@@ -0,0 +1,327 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//===- BufferizationExternalModels.cpp -----------------------------------===//
+//
+// This file implements bufferization interfaces for PCF ops.
+//
+//===---------------------------------------------------------------------===//
+
+#include "iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.h"
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+using namespace mlir::bufferization;
+
+namespace {
+
+struct GenericOpInterface
+    : public BufferizableOpInterface::ExternalModel<GenericOpInterface,
+                                                    PCF::GenericOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // Parallel ops can be treated as though they never read.
+    return false;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // Generic ops must always be assumed to write to a tensor (init) operand.
+    return true;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    OpResult tiedResult = genericOp.getTiedResult(opOperand);
+    if (!tiedResult) {
+      return {};
+    }
+
+    return {{tiedResult, BufferRelation::Equivalent,
+             /*isDefinite=*/true}};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    Location loc = genericOp.getLoc();
+
+    SmallVector<Value> newInits;
+    newInits.reserve(genericOp.getInits().size());
+    for (auto init : genericOp.getInits()) {
+      if (isa<RankedTensorType>(init.getType())) {
+        FailureOr<Value> newInit = getBuffer(rewriter, init, options, state);
+        if (failed(newInit)) {
+          return op->emitError() << "failed to get init buffer";
+        }
+        newInits.push_back(*newInit);
+      } else {
+        newInits.push_back(init);
+      }
+    }
+
+    SmallVector<Type> newResultTypes;
+    for (auto result : genericOp.getResults()) {
+      if (isa<TensorType>(result.getType())) {
+        FailureOr<BufferLikeType> resultType =
+            bufferization::getBufferType(result, options, state);
+        if (failed(resultType)) {
+          return failure();
+        }
+        newResultTypes.push_back(*resultType);
+      } else {
+        newResultTypes.push_back(result.getType());
+      }
+    }
+
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, newResultTypes, genericOp.getScope(), newInits,
+        genericOp.getDynamicSizes(), genericOp.getIsTied(),
+        genericOp.getNumIterators(), genericOp.getSyncOnReturn());
+    newGenericOp.getRegion().takeBody(genericOp.getRegion());
+    newGenericOp.getInitializer().takeBody(genericOp.getInitializer());
+    replaceOpWithBufferizedValues(rewriter, op, newGenericOp.getResults());
+    return success();
+  }
+
+  FailureOr<BufferLikeType>
+  getBufferType(Operation *op, Value value, const BufferizationOptions &options,
+                const BufferizationState &state,
+                SmallVector<Value> &invocationStack) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+
+    // Block arguments are `pcf.sref`, so this must always be an opresult.
+    auto result = cast<OpResult>(value);
+    assert(result.getOwner() == op && "invalid value");
+
+    // If the result has a tied init, use that as the buffer type.
+    OpOperand *tiedInit = genericOp.getTiedInit(result.getResultNumber());
+    if (tiedInit) {
+      return bufferization::detail::asMemRefType(bufferization::getBufferType(
+          tiedInit->get(), options, state, invocationStack));
+    }
+
+    auto resultType = cast<RankedTensorType>(result.getType());
+
+    // Else query the scope for the memory space to allocate for.
+    FailureOr<Attribute> memSpace =
+        genericOp.getScope().getAllocMemSpace(op->getContext());
+    if (failed(memSpace)) {
+      return failure();
+    }
+    return cast<BufferLikeType>(
+        getMemRefTypeWithStaticIdentityLayout(resultType, *memSpace));
+  }
+};
+
+struct LoopOpInterface
+    : public BufferizableOpInterface::ExternalModel<LoopOpInterface,
+                                                    PCF::LoopOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // Parallel ops can be treated as though they never read.
+    return false;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // Generic ops must always be assumed to write to a tensor (init) operand.
+    return true;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    auto loopOp = cast<PCF::LoopOp>(op);
+    OpResult tiedResult = loopOp.getTiedResult(opOperand);
+    if (!tiedResult) {
+      return {};
+    }
+
+    return {{tiedResult, BufferRelation::Equivalent,
+             /*isDefinite=*/true}};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto loopOp = cast<PCF::LoopOp>(op);
+    Location loc = loopOp.getLoc();
+
+    SmallVector<Value> newInits;
+    newInits.reserve(loopOp.getInits().size());
+    for (auto init : loopOp.getInits()) {
```

**Comment:**
same here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.cpp:177`

```diff
@@ -0,0 +1,327 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//===- BufferizationExternalModels.cpp -----------------------------------===//
+//
+// This file implements bufferization interfaces for PCF ops.
+//
+//===---------------------------------------------------------------------===//
+
+#include "iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.h"
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+using namespace mlir::bufferization;
+
+namespace {
+
+struct GenericOpInterface
+    : public BufferizableOpInterface::ExternalModel<GenericOpInterface,
+                                                    PCF::GenericOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // Parallel ops can be treated as though they never read.
+    return false;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // Generic ops must always be assumed to write to a tensor (init) operand.
+    return true;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    OpResult tiedResult = genericOp.getTiedResult(opOperand);
+    if (!tiedResult) {
+      return {};
+    }
+
+    return {{tiedResult, BufferRelation::Equivalent,
+             /*isDefinite=*/true}};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    Location loc = genericOp.getLoc();
+
+    SmallVector<Value> newInits;
+    newInits.reserve(genericOp.getInits().size());
+    for (auto init : genericOp.getInits()) {
+      if (isa<RankedTensorType>(init.getType())) {
+        FailureOr<Value> newInit = getBuffer(rewriter, init, options, state);
+        if (failed(newInit)) {
+          return op->emitError() << "failed to get init buffer";
+        }
+        newInits.push_back(*newInit);
+      } else {
+        newInits.push_back(init);
+      }
+    }
+
+    SmallVector<Type> newResultTypes;
+    for (auto result : genericOp.getResults()) {
+      if (isa<TensorType>(result.getType())) {
+        FailureOr<BufferLikeType> resultType =
+            bufferization::getBufferType(result, options, state);
+        if (failed(resultType)) {
+          return failure();
+        }
+        newResultTypes.push_back(*resultType);
+      } else {
+        newResultTypes.push_back(result.getType());
+      }
+    }
+
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, newResultTypes, genericOp.getScope(), newInits,
+        genericOp.getDynamicSizes(), genericOp.getIsTied(),
+        genericOp.getNumIterators(), genericOp.getSyncOnReturn());
+    newGenericOp.getRegion().takeBody(genericOp.getRegion());
+    newGenericOp.getInitializer().takeBody(genericOp.getInitializer());
+    replaceOpWithBufferizedValues(rewriter, op, newGenericOp.getResults());
+    return success();
+  }
+
+  FailureOr<BufferLikeType>
+  getBufferType(Operation *op, Value value, const BufferizationOptions &options,
+                const BufferizationState &state,
+                SmallVector<Value> &invocationStack) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+
+    // Block arguments are `pcf.sref`, so this must always be an opresult.
+    auto result = cast<OpResult>(value);
+    assert(result.getOwner() == op && "invalid value");
+
+    // If the result has a tied init, use that as the buffer type.
+    OpOperand *tiedInit = genericOp.getTiedInit(result.getResultNumber());
+    if (tiedInit) {
+      return bufferization::detail::asMemRefType(bufferization::getBufferType(
+          tiedInit->get(), options, state, invocationStack));
+    }
+
+    auto resultType = cast<RankedTensorType>(result.getType());
+
+    // Else query the scope for the memory space to allocate for.
+    FailureOr<Attribute> memSpace =
+        genericOp.getScope().getAllocMemSpace(op->getContext());
+    if (failed(memSpace)) {
+      return failure();
+    }
+    return cast<BufferLikeType>(
+        getMemRefTypeWithStaticIdentityLayout(resultType, *memSpace));
+  }
+};
+
+struct LoopOpInterface
+    : public BufferizableOpInterface::ExternalModel<LoopOpInterface,
+                                                    PCF::LoopOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // Parallel ops can be treated as though they never read.
+    return false;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // Generic ops must always be assumed to write to a tensor (init) operand.
+    return true;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    auto loopOp = cast<PCF::LoopOp>(op);
+    OpResult tiedResult = loopOp.getTiedResult(opOperand);
+    if (!tiedResult) {
+      return {};
+    }
+
+    return {{tiedResult, BufferRelation::Equivalent,
+             /*isDefinite=*/true}};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto loopOp = cast<PCF::LoopOp>(op);
+    Location loc = loopOp.getLoc();
+
+    SmallVector<Value> newInits;
+    newInits.reserve(loopOp.getInits().size());
+    for (auto init : loopOp.getInits()) {
+      if (isa<RankedTensorType>(init.getType())) {
+        FailureOr<Value> newInit = getBuffer(rewriter, init, options, state);
+        if (failed(newInit)) {
+          return op->emitError() << "failed to get init buffer";
+        }
+        newInits.push_back(*newInit);
+      } else {
+        newInits.push_back(init);
+      }
+    }
+
+    SmallVector<Type> newResultTypes;
+    for (auto result : loopOp.getResults()) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.cpp:230`

```diff
@@ -0,0 +1,327 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//===- BufferizationExternalModels.cpp -----------------------------------===//
+//
+// This file implements bufferization interfaces for PCF ops.
+//
+//===---------------------------------------------------------------------===//
+
+#include "iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.h"
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+using namespace mlir::bufferization;
+
+namespace {
+
+struct GenericOpInterface
+    : public BufferizableOpInterface::ExternalModel<GenericOpInterface,
+                                                    PCF::GenericOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // Parallel ops can be treated as though they never read.
+    return false;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // Generic ops must always be assumed to write to a tensor (init) operand.
+    return true;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    OpResult tiedResult = genericOp.getTiedResult(opOperand);
+    if (!tiedResult) {
+      return {};
+    }
+
+    return {{tiedResult, BufferRelation::Equivalent,
+             /*isDefinite=*/true}};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    Location loc = genericOp.getLoc();
+
+    SmallVector<Value> newInits;
+    newInits.reserve(genericOp.getInits().size());
+    for (auto init : genericOp.getInits()) {
+      if (isa<RankedTensorType>(init.getType())) {
+        FailureOr<Value> newInit = getBuffer(rewriter, init, options, state);
+        if (failed(newInit)) {
+          return op->emitError() << "failed to get init buffer";
+        }
+        newInits.push_back(*newInit);
+      } else {
+        newInits.push_back(init);
+      }
+    }
+
+    SmallVector<Type> newResultTypes;
+    for (auto result : genericOp.getResults()) {
+      if (isa<TensorType>(result.getType())) {
+        FailureOr<BufferLikeType> resultType =
+            bufferization::getBufferType(result, options, state);
+        if (failed(resultType)) {
+          return failure();
+        }
+        newResultTypes.push_back(*resultType);
+      } else {
+        newResultTypes.push_back(result.getType());
+      }
+    }
+
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, newResultTypes, genericOp.getScope(), newInits,
+        genericOp.getDynamicSizes(), genericOp.getIsTied(),
+        genericOp.getNumIterators(), genericOp.getSyncOnReturn());
+    newGenericOp.getRegion().takeBody(genericOp.getRegion());
+    newGenericOp.getInitializer().takeBody(genericOp.getInitializer());
+    replaceOpWithBufferizedValues(rewriter, op, newGenericOp.getResults());
+    return success();
+  }
+
+  FailureOr<BufferLikeType>
+  getBufferType(Operation *op, Value value, const BufferizationOptions &options,
+                const BufferizationState &state,
+                SmallVector<Value> &invocationStack) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+
+    // Block arguments are `pcf.sref`, so this must always be an opresult.
+    auto result = cast<OpResult>(value);
+    assert(result.getOwner() == op && "invalid value");
+
+    // If the result has a tied init, use that as the buffer type.
+    OpOperand *tiedInit = genericOp.getTiedInit(result.getResultNumber());
+    if (tiedInit) {
+      return bufferization::detail::asMemRefType(bufferization::getBufferType(
+          tiedInit->get(), options, state, invocationStack));
+    }
+
+    auto resultType = cast<RankedTensorType>(result.getType());
+
+    // Else query the scope for the memory space to allocate for.
+    FailureOr<Attribute> memSpace =
+        genericOp.getScope().getAllocMemSpace(op->getContext());
+    if (failed(memSpace)) {
+      return failure();
+    }
+    return cast<BufferLikeType>(
+        getMemRefTypeWithStaticIdentityLayout(resultType, *memSpace));
+  }
+};
+
+struct LoopOpInterface
+    : public BufferizableOpInterface::ExternalModel<LoopOpInterface,
+                                                    PCF::LoopOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // Parallel ops can be treated as though they never read.
+    return false;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // Generic ops must always be assumed to write to a tensor (init) operand.
+    return true;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    auto loopOp = cast<PCF::LoopOp>(op);
+    OpResult tiedResult = loopOp.getTiedResult(opOperand);
+    if (!tiedResult) {
+      return {};
+    }
+
+    return {{tiedResult, BufferRelation::Equivalent,
+             /*isDefinite=*/true}};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto loopOp = cast<PCF::LoopOp>(op);
+    Location loc = loopOp.getLoc();
+
+    SmallVector<Value> newInits;
+    newInits.reserve(loopOp.getInits().size());
+    for (auto init : loopOp.getInits()) {
+      if (isa<RankedTensorType>(init.getType())) {
+        FailureOr<Value> newInit = getBuffer(rewriter, init, options, state);
+        if (failed(newInit)) {
+          return op->emitError() << "failed to get init buffer";
+        }
+        newInits.push_back(*newInit);
+      } else {
+        newInits.push_back(init);
+      }
+    }
+
+    SmallVector<Type> newResultTypes;
+    for (auto result : loopOp.getResults()) {
+      if (isa<TensorType>(result.getType())) {
+        FailureOr<BufferLikeType> resultType =
+            bufferization::getBufferType(result, options, state);
+        if (failed(resultType)) {
+          return failure();
+        }
+        newResultTypes.push_back(*resultType);
+      } else {
+        newResultTypes.push_back(result.getType());
+      }
+    }
+
+    auto newLoopOp = PCF::LoopOp::create(
+        rewriter, loc, newResultTypes, loopOp.getScope(), loopOp.getCount(),
+        newInits, loopOp.getDynamicSizes(), loopOp.getIsTied(),
+        loopOp.getSyncOnReturn());
+    newLoopOp.getRegion().takeBody(loopOp.getRegion());
+    replaceOpWithBufferizedValues(rewriter, op, newLoopOp.getResults());
+    return success();
+  }
+
+  FailureOr<BufferLikeType>
+  getBufferType(Operation *op, Value value, const BufferizationOptions &options,
+                const BufferizationState &state,
+                SmallVector<Value> &invocationStack) const {
+    auto loopOp = cast<PCF::LoopOp>(op);
+
+    // Block arguments are `pcf.sref`, so this must always be an opresult.
+    auto result = cast<OpResult>(value);
+    assert(result.getOwner() == op && "invalid value");
+
+    // If the result has a tied init, use that as the buffer type.
+    OpOperand *tiedInit = loopOp.getTiedInit(result.getResultNumber());
+    if (tiedInit) {
+      return bufferization::detail::asMemRefType(bufferization::getBufferType(
+          tiedInit->get(), options, state, invocationStack));
+    }
+
+    auto resultType = cast<RankedTensorType>(result.getType());
+
+    // Else query the scope for the memory space to allocate for.
+    FailureOr<Attribute> memSpace =
+        loopOp.getScope().getAllocMemSpace(op->getContext());
+    if (failed(memSpace)) {
+      return failure();
+    }
+    return cast<BufferLikeType>(
+        getMemRefTypeWithStaticIdentityLayout(resultType, *memSpace));
+  }
+};
+
+struct WriteSliceOpInterface
+    : public BufferizableOpInterface::ExternalModel<WriteSliceOpInterface,
```

**Comment:**
```suggestion
    : BufferizableOpInterface::ExternalModel<WriteSliceOpInterface,
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.cpp:267`

```diff
@@ -0,0 +1,327 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//===- BufferizationExternalModels.cpp -----------------------------------===//
+//
+// This file implements bufferization interfaces for PCF ops.
+//
+//===---------------------------------------------------------------------===//
+
+#include "iree/compiler/Codegen/Dialect/PCF/ExternalInterfaces/BufferizationExternalModels.h"
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+using namespace mlir::bufferization;
+
+namespace {
+
+struct GenericOpInterface
+    : public BufferizableOpInterface::ExternalModel<GenericOpInterface,
+                                                    PCF::GenericOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // Parallel ops can be treated as though they never read.
+    return false;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // Generic ops must always be assumed to write to a tensor (init) operand.
+    return true;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    OpResult tiedResult = genericOp.getTiedResult(opOperand);
+    if (!tiedResult) {
+      return {};
+    }
+
+    return {{tiedResult, BufferRelation::Equivalent,
+             /*isDefinite=*/true}};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+    Location loc = genericOp.getLoc();
+
+    SmallVector<Value> newInits;
+    newInits.reserve(genericOp.getInits().size());
+    for (auto init : genericOp.getInits()) {
+      if (isa<RankedTensorType>(init.getType())) {
+        FailureOr<Value> newInit = getBuffer(rewriter, init, options, state);
+        if (failed(newInit)) {
+          return op->emitError() << "failed to get init buffer";
+        }
+        newInits.push_back(*newInit);
+      } else {
+        newInits.push_back(init);
+      }
+    }
+
+    SmallVector<Type> newResultTypes;
+    for (auto result : genericOp.getResults()) {
+      if (isa<TensorType>(result.getType())) {
+        FailureOr<BufferLikeType> resultType =
+            bufferization::getBufferType(result, options, state);
+        if (failed(resultType)) {
+          return failure();
+        }
+        newResultTypes.push_back(*resultType);
+      } else {
+        newResultTypes.push_back(result.getType());
+      }
+    }
+
+    auto newGenericOp = PCF::GenericOp::create(
+        rewriter, loc, newResultTypes, genericOp.getScope(), newInits,
+        genericOp.getDynamicSizes(), genericOp.getIsTied(),
+        genericOp.getNumIterators(), genericOp.getSyncOnReturn());
+    newGenericOp.getRegion().takeBody(genericOp.getRegion());
+    newGenericOp.getInitializer().takeBody(genericOp.getInitializer());
+    replaceOpWithBufferizedValues(rewriter, op, newGenericOp.getResults());
+    return success();
+  }
+
+  FailureOr<BufferLikeType>
+  getBufferType(Operation *op, Value value, const BufferizationOptions &options,
+                const BufferizationState &state,
+                SmallVector<Value> &invocationStack) const {
+    auto genericOp = cast<PCF::GenericOp>(op);
+
+    // Block arguments are `pcf.sref`, so this must always be an opresult.
+    auto result = cast<OpResult>(value);
+    assert(result.getOwner() == op && "invalid value");
+
+    // If the result has a tied init, use that as the buffer type.
+    OpOperand *tiedInit = genericOp.getTiedInit(result.getResultNumber());
+    if (tiedInit) {
+      return bufferization::detail::asMemRefType(bufferization::getBufferType(
+          tiedInit->get(), options, state, invocationStack));
+    }
+
+    auto resultType = cast<RankedTensorType>(result.getType());
+
+    // Else query the scope for the memory space to allocate for.
+    FailureOr<Attribute> memSpace =
+        genericOp.getScope().getAllocMemSpace(op->getContext());
+    if (failed(memSpace)) {
+      return failure();
+    }
+    return cast<BufferLikeType>(
+        getMemRefTypeWithStaticIdentityLayout(resultType, *memSpace));
+  }
+};
+
+struct LoopOpInterface
+    : public BufferizableOpInterface::ExternalModel<LoopOpInterface,
+                                                    PCF::LoopOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // Parallel ops can be treated as though they never read.
+    return false;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // Generic ops must always be assumed to write to a tensor (init) operand.
+    return true;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    auto loopOp = cast<PCF::LoopOp>(op);
+    OpResult tiedResult = loopOp.getTiedResult(opOperand);
+    if (!tiedResult) {
+      return {};
+    }
+
+    return {{tiedResult, BufferRelation::Equivalent,
+             /*isDefinite=*/true}};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto loopOp = cast<PCF::LoopOp>(op);
+    Location loc = loopOp.getLoc();
+
+    SmallVector<Value> newInits;
+    newInits.reserve(loopOp.getInits().size());
+    for (auto init : loopOp.getInits()) {
+      if (isa<RankedTensorType>(init.getType())) {
+        FailureOr<Value> newInit = getBuffer(rewriter, init, options, state);
+        if (failed(newInit)) {
+          return op->emitError() << "failed to get init buffer";
+        }
+        newInits.push_back(*newInit);
+      } else {
+        newInits.push_back(init);
+      }
+    }
+
+    SmallVector<Type> newResultTypes;
+    for (auto result : loopOp.getResults()) {
+      if (isa<TensorType>(result.getType())) {
+        FailureOr<BufferLikeType> resultType =
+            bufferization::getBufferType(result, options, state);
+        if (failed(resultType)) {
+          return failure();
+        }
+        newResultTypes.push_back(*resultType);
+      } else {
+        newResultTypes.push_back(result.getType());
+      }
+    }
+
+    auto newLoopOp = PCF::LoopOp::create(
+        rewriter, loc, newResultTypes, loopOp.getScope(), loopOp.getCount(),
+        newInits, loopOp.getDynamicSizes(), loopOp.getIsTied(),
+        loopOp.getSyncOnReturn());
+    newLoopOp.getRegion().takeBody(loopOp.getRegion());
+    replaceOpWithBufferizedValues(rewriter, op, newLoopOp.getResults());
+    return success();
+  }
+
+  FailureOr<BufferLikeType>
+  getBufferType(Operation *op, Value value, const BufferizationOptions &options,
+                const BufferizationState &state,
+                SmallVector<Value> &invocationStack) const {
+    auto loopOp = cast<PCF::LoopOp>(op);
+
+    // Block arguments are `pcf.sref`, so this must always be an opresult.
+    auto result = cast<OpResult>(value);
+    assert(result.getOwner() == op && "invalid value");
+
+    // If the result has a tied init, use that as the buffer type.
+    OpOperand *tiedInit = loopOp.getTiedInit(result.getResultNumber());
+    if (tiedInit) {
+      return bufferization::detail::asMemRefType(bufferization::getBufferType(
+          tiedInit->get(), options, state, invocationStack));
+    }
+
+    auto resultType = cast<RankedTensorType>(result.getType());
+
+    // Else query the scope for the memory space to allocate for.
+    FailureOr<Attribute> memSpace =
+        loopOp.getScope().getAllocMemSpace(op->getContext());
+    if (failed(memSpace)) {
+      return failure();
+    }
+    return cast<BufferLikeType>(
+        getMemRefTypeWithStaticIdentityLayout(resultType, *memSpace));
+  }
+};
+
+struct WriteSliceOpInterface
+    : public BufferizableOpInterface::ExternalModel<WriteSliceOpInterface,
+                                                    PCF::WriteSliceOp> {
+  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
+                              const AnalysisState &state) const {
+    // The only valid tensor operand is the source which is always read.
+    return true;
+  }
+
+  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
+                               const AnalysisState &state) const {
+    // The only valid tensor operand is the source which is only read.
+    return false;
+  }
+
+  AliasingValueList getAliasingValues(Operation *op, OpOperand &opOperand,
+                                      const AnalysisState &state) const {
+    return {};
+  }
+
+  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
+                          const BufferizationOptions &options,
+                          BufferizationState &state) const {
+    auto writeOp = cast<PCF::WriteSliceOp>(op);
+
+    if (isa<RankedTensorType>(writeOp.getSourceType())) {
+      FailureOr<Value> newSrc =
+          getBuffer(rewriter, writeOp.getSource(), options, state);
+      if (failed(newSrc)) {
+        return failure();
+      }
+      writeOp.getSourceMutable().assign(*newSrc);
+    }
+    return success();
+  }
+};
+
+struct ReadSliceOpInterface
+    : public BufferizableOpInterface::ExternalModel<ReadSliceOpInterface,
```

**Comment:**
```suggestion
    : BufferizableOpInterface::ExternalModel<ReadSliceOpInterface,
```

---


---


## [PR #22804](https://github.com/iree-org/iree/pull/22804): [Codegen] Add PCF dialect

### Review Summary

**COMMENTED** (2025-12-04)

**APPROVED** (2025-12-04)

The implementation looks good; I haven't reviewed the op semantics carefully, so you probably want to wait for another review that focuses on this.

**COMMENTED** (2025-12-04)

**COMMENTED** (2025-12-04)

**COMMENTED** (2025-12-04)

**COMMENTED** (2025-12-06)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:69`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
```

**Comment:**
Nit: make this a declaration and move the implementation to the .cpp?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:112`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
+      SmallVector<OpFoldResult> result;
+      unsigned ctr = 0;
+      OpBuilder b(getContext());
+      for (int64_t i = 0, e = getResultType().getRank(); i < e; ++i) {
+        if (getResultType().isDynamicDim(i)) {
+          result.push_back(getDynamicSizes()[ctr++]);
+        } else {
+          result.push_back(b.getIndexAttr(getResultType().getShape()[i]));
+        }
+      }
+      return result;
+    }
+  }];
+
+  let hasVerifier = 1;
+}
+
+} // OpGroupAllocOps
+
+//===----------------------------------------------------------------------===//
+// ParallelOps
+//===----------------------------------------------------------------------===//
+
+def OpGroupParallelOps : OpDocGroup {
+  let summary = "Parallel execution ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupParallelOps in {
+
+def GenericOp : PCF_Op<"generic", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region across a set of workers at a specified scope. When
+    control flow reaches this op, workers with count equal to the native
+    parallelism of the scope are spawned and begin executing the region. The
```

**Comment:**
I can't parse this sentence. Do you mean that `nproc` workers will be spawned, or that some workers from a larger pool whose count is `nproc` will be spawned?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:135`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
+      SmallVector<OpFoldResult> result;
+      unsigned ctr = 0;
+      OpBuilder b(getContext());
+      for (int64_t i = 0, e = getResultType().getRank(); i < e; ++i) {
+        if (getResultType().isDynamicDim(i)) {
+          result.push_back(getDynamicSizes()[ctr++]);
+        } else {
+          result.push_back(b.getIndexAttr(getResultType().getShape()[i]));
+        }
+      }
+      return result;
+    }
+  }];
+
+  let hasVerifier = 1;
+}
+
+} // OpGroupAllocOps
+
+//===----------------------------------------------------------------------===//
+// ParallelOps
+//===----------------------------------------------------------------------===//
+
+def OpGroupParallelOps : OpDocGroup {
+  let summary = "Parallel execution ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupParallelOps in {
+
+def GenericOp : PCF_Op<"generic", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region across a set of workers at a specified scope. When
+    control flow reaches this op, workers with count equal to the native
+    parallelism of the scope are spawned and begin executing the region. The
+    scope is given by an attribute implementing the `ScopeAttr` interface and
+    is responsible for the semantics of all pcf primitives at the same scope.
+    Further details about scopes are included in the docs for the interface.
+
+    The optional `initialize` region is executed once when control flow first
+    reaches the op. Values yielded from the initializer become block arguments
+    available to the execute region. This is useful for setting up per-op
+    state that persists across all worker invocations.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all workers have returned. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with tied results:
+    ```mlir
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.generic scope(#foo.scope)
+        execute(%ref = %0)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // Each worker can read/write %ref
```

**Comment:**
```suggestion
        // Each worker can read/write %ref.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:122`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
+      SmallVector<OpFoldResult> result;
+      unsigned ctr = 0;
+      OpBuilder b(getContext());
+      for (int64_t i = 0, e = getResultType().getRank(); i < e; ++i) {
+        if (getResultType().isDynamicDim(i)) {
+          result.push_back(getDynamicSizes()[ctr++]);
+        } else {
+          result.push_back(b.getIndexAttr(getResultType().getShape()[i]));
+        }
+      }
+      return result;
+    }
+  }];
+
+  let hasVerifier = 1;
+}
+
+} // OpGroupAllocOps
+
+//===----------------------------------------------------------------------===//
+// ParallelOps
+//===----------------------------------------------------------------------===//
+
+def OpGroupParallelOps : OpDocGroup {
+  let summary = "Parallel execution ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupParallelOps in {
+
+def GenericOp : PCF_Op<"generic", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region across a set of workers at a specified scope. When
+    control flow reaches this op, workers with count equal to the native
+    parallelism of the scope are spawned and begin executing the region. The
+    scope is given by an attribute implementing the `ScopeAttr` interface and
+    is responsible for the semantics of all pcf primitives at the same scope.
+    Further details about scopes are included in the docs for the interface.
+
+    The optional `initialize` region is executed once when control flow first
+    reaches the op. Values yielded from the initializer become block arguments
+    available to the execute region. This is useful for setting up per-op
+    state that persists across all worker invocations.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all workers have returned. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with tied results:
+    ```mlir
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.generic scope(#foo.scope)
+        execute(%ref = %0)[%id: index, %num_workers: index]
```

**Comment:**
Why is the type of %0 omitted here? Is it always the same as the result type?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:149`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
+      SmallVector<OpFoldResult> result;
+      unsigned ctr = 0;
+      OpBuilder b(getContext());
+      for (int64_t i = 0, e = getResultType().getRank(); i < e; ++i) {
+        if (getResultType().isDynamicDim(i)) {
+          result.push_back(getDynamicSizes()[ctr++]);
+        } else {
+          result.push_back(b.getIndexAttr(getResultType().getShape()[i]));
+        }
+      }
+      return result;
+    }
+  }];
+
+  let hasVerifier = 1;
+}
+
+} // OpGroupAllocOps
+
+//===----------------------------------------------------------------------===//
+// ParallelOps
+//===----------------------------------------------------------------------===//
+
+def OpGroupParallelOps : OpDocGroup {
+  let summary = "Parallel execution ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupParallelOps in {
+
+def GenericOp : PCF_Op<"generic", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region across a set of workers at a specified scope. When
+    control flow reaches this op, workers with count equal to the native
+    parallelism of the scope are spawned and begin executing the region. The
+    scope is given by an attribute implementing the `ScopeAttr` interface and
+    is responsible for the semantics of all pcf primitives at the same scope.
+    Further details about scopes are included in the docs for the interface.
+
+    The optional `initialize` region is executed once when control flow first
+    reaches the op. Values yielded from the initializer become block arguments
+    available to the execute region. This is useful for setting up per-op
+    state that persists across all worker invocations.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all workers have returned. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with tied results:
+    ```mlir
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.generic scope(#foo.scope)
+        execute(%ref = %0)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // Each worker can read/write %ref
+        pcf.return
+      }
+    ```
+
+    Example with initializer:
+    ```mlir
+      %result = pcf.generic scope(#foo.scope)
+        initialize {
+          %scratch = pcf.alloc() : !pcf.sref<16xf32, #foo.scope>
+          pcf.yield %scratch : !pcf.sref<16xf32, #foo.scope>
+        } -> (%scratch_arg: !pcf.sref<16xf32, #foo.scope>)
+        execute(%ref = %init)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %scratch_arg is available here, initialized once
```

**Comment:**
```suggestion
        // %scratch_arg is available here, initialized once.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:160`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
+      SmallVector<OpFoldResult> result;
+      unsigned ctr = 0;
+      OpBuilder b(getContext());
+      for (int64_t i = 0, e = getResultType().getRank(); i < e; ++i) {
+        if (getResultType().isDynamicDim(i)) {
+          result.push_back(getDynamicSizes()[ctr++]);
+        } else {
+          result.push_back(b.getIndexAttr(getResultType().getShape()[i]));
+        }
+      }
+      return result;
+    }
+  }];
+
+  let hasVerifier = 1;
+}
+
+} // OpGroupAllocOps
+
+//===----------------------------------------------------------------------===//
+// ParallelOps
+//===----------------------------------------------------------------------===//
+
+def OpGroupParallelOps : OpDocGroup {
+  let summary = "Parallel execution ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupParallelOps in {
+
+def GenericOp : PCF_Op<"generic", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region across a set of workers at a specified scope. When
+    control flow reaches this op, workers with count equal to the native
+    parallelism of the scope are spawned and begin executing the region. The
+    scope is given by an attribute implementing the `ScopeAttr` interface and
+    is responsible for the semantics of all pcf primitives at the same scope.
+    Further details about scopes are included in the docs for the interface.
+
+    The optional `initialize` region is executed once when control flow first
+    reaches the op. Values yielded from the initializer become block arguments
+    available to the execute region. This is useful for setting up per-op
+    state that persists across all worker invocations.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all workers have returned. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with tied results:
+    ```mlir
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.generic scope(#foo.scope)
+        execute(%ref = %0)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // Each worker can read/write %ref
+        pcf.return
+      }
+    ```
+
+    Example with initializer:
+    ```mlir
+      %result = pcf.generic scope(#foo.scope)
+        initialize {
+          %scratch = pcf.alloc() : !pcf.sref<16xf32, #foo.scope>
+          pcf.yield %scratch : !pcf.sref<16xf32, #foo.scope>
+        } -> (%scratch_arg: !pcf.sref<16xf32, #foo.scope>)
+        execute(%ref = %init)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %scratch_arg is available here, initialized once
+        pcf.return
+      }
+    ```
+
+    Example with untied (allocated) results:
+    ```mlir
+      %d0, %d1 = ... : index
+      %result = pcf.generic scope(#foo.scope)
+        execute[%id: index, %num_workers: index]
+             : () -> (tensor<?x?xf32>{%d0, %d1}) {
+        // Result sref is allocated by the op, not tied to any init
```

**Comment:**
```suggestion
        // Result sref is allocated by the op, not tied to any init.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:225`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
+      SmallVector<OpFoldResult> result;
+      unsigned ctr = 0;
+      OpBuilder b(getContext());
+      for (int64_t i = 0, e = getResultType().getRank(); i < e; ++i) {
+        if (getResultType().isDynamicDim(i)) {
+          result.push_back(getDynamicSizes()[ctr++]);
+        } else {
+          result.push_back(b.getIndexAttr(getResultType().getShape()[i]));
+        }
+      }
+      return result;
+    }
+  }];
+
+  let hasVerifier = 1;
+}
+
+} // OpGroupAllocOps
+
+//===----------------------------------------------------------------------===//
+// ParallelOps
+//===----------------------------------------------------------------------===//
+
+def OpGroupParallelOps : OpDocGroup {
+  let summary = "Parallel execution ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupParallelOps in {
+
+def GenericOp : PCF_Op<"generic", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region across a set of workers at a specified scope. When
+    control flow reaches this op, workers with count equal to the native
+    parallelism of the scope are spawned and begin executing the region. The
+    scope is given by an attribute implementing the `ScopeAttr` interface and
+    is responsible for the semantics of all pcf primitives at the same scope.
+    Further details about scopes are included in the docs for the interface.
+
+    The optional `initialize` region is executed once when control flow first
+    reaches the op. Values yielded from the initializer become block arguments
+    available to the execute region. This is useful for setting up per-op
+    state that persists across all worker invocations.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all workers have returned. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with tied results:
+    ```mlir
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.generic scope(#foo.scope)
+        execute(%ref = %0)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // Each worker can read/write %ref
+        pcf.return
+      }
+    ```
+
+    Example with initializer:
+    ```mlir
+      %result = pcf.generic scope(#foo.scope)
+        initialize {
+          %scratch = pcf.alloc() : !pcf.sref<16xf32, #foo.scope>
+          pcf.yield %scratch : !pcf.sref<16xf32, #foo.scope>
+        } -> (%scratch_arg: !pcf.sref<16xf32, #foo.scope>)
+        execute(%ref = %init)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %scratch_arg is available here, initialized once
+        pcf.return
+      }
+    ```
+
+    Example with untied (allocated) results:
+    ```mlir
+      %d0, %d1 = ... : index
+      %result = pcf.generic scope(#foo.scope)
+        execute[%id: index, %num_workers: index]
+             : () -> (tensor<?x?xf32>{%d0, %d1}) {
+        // Result sref is allocated by the op, not tied to any init
+        pcf.return
+      }
+    ```
+  }];
+  let arguments = (ins
+    PCF_ScopeAttr:$scope,
+    Variadic<PCF_AnyTensorOrMemRef>:$inits,
+    Variadic<Index>:$dynamic_sizes,
+    ArrayProp<BoolProp>:$is_tied,
+    DefaultValuedProp<BoolProp, "false">:$sync_on_return,
+    IntProp<"int64_t">:$num_index_args,
+    IntProp<"int64_t">:$num_leading_args
+  );
+
+  let results = (outs Variadic<PCF_AnyTensorOrMemRef>:$results);
+  let regions = (region
+    MaxSizedRegion<1>:$initializer,
+    MinSizedRegion<1>:$region
+  );
+
+  let assemblyFormat = [{
+    (`sync` $sync_on_return^)?
+    `scope` `(` $scope `)`
+    (`initialize` $initializer^)?
+    custom<ParallelExecutionBody>($inits,
+                                  type($inits),
+                                  $dynamic_sizes,
+                                  type($results),
+                                  $is_tied,
+                                  $region,
+                                  $num_leading_args,
+                                  "true")
+    custom<InferNumIndexArgs>(ref($region), ref($num_leading_args), $num_index_args)
+    prop-dict attr-dict
+  }];
+  let hasVerifier = 1;
+
+  // The default builder does not add the proper body BBargs, roll our own.
+  let skipDefaultBuilders = 1;
+  let builders = [
+    // Builder with no return values.
+    OpBuilder<(ins "ScopeAttr":$scope, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with return types inferred from inits.
+    OpBuilder<(ins "ScopeAttr":$scope, "ValueRange":$inits,
+                   "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with no inits.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$dynamic_sizes, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with everything.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$inits, "ValueRange":$dynamic_sizes,
+                   "ArrayRef<bool>":$is_tied, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+  ];
+
+  let extraClassDeclaration = [{
+    int64_t getNumIterators() {
+      return getNumIndexArgs() / 2;
+    }
+    MutableArrayRef<BlockArgument> getLeadingArgs() {
+      return getRegion().getArguments().take_front(getNumLeadingArgs());
+    }
```

**Comment:**
Err, should we get mutable and non-mutable variants? Making the default mutable seems like a potential footgun.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:240`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
+      SmallVector<OpFoldResult> result;
+      unsigned ctr = 0;
+      OpBuilder b(getContext());
+      for (int64_t i = 0, e = getResultType().getRank(); i < e; ++i) {
+        if (getResultType().isDynamicDim(i)) {
+          result.push_back(getDynamicSizes()[ctr++]);
+        } else {
+          result.push_back(b.getIndexAttr(getResultType().getShape()[i]));
+        }
+      }
+      return result;
+    }
+  }];
+
+  let hasVerifier = 1;
+}
+
+} // OpGroupAllocOps
+
+//===----------------------------------------------------------------------===//
+// ParallelOps
+//===----------------------------------------------------------------------===//
+
+def OpGroupParallelOps : OpDocGroup {
+  let summary = "Parallel execution ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupParallelOps in {
+
+def GenericOp : PCF_Op<"generic", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region across a set of workers at a specified scope. When
+    control flow reaches this op, workers with count equal to the native
+    parallelism of the scope are spawned and begin executing the region. The
+    scope is given by an attribute implementing the `ScopeAttr` interface and
+    is responsible for the semantics of all pcf primitives at the same scope.
+    Further details about scopes are included in the docs for the interface.
+
+    The optional `initialize` region is executed once when control flow first
+    reaches the op. Values yielded from the initializer become block arguments
+    available to the execute region. This is useful for setting up per-op
+    state that persists across all worker invocations.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all workers have returned. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with tied results:
+    ```mlir
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.generic scope(#foo.scope)
+        execute(%ref = %0)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // Each worker can read/write %ref
+        pcf.return
+      }
+    ```
+
+    Example with initializer:
+    ```mlir
+      %result = pcf.generic scope(#foo.scope)
+        initialize {
+          %scratch = pcf.alloc() : !pcf.sref<16xf32, #foo.scope>
+          pcf.yield %scratch : !pcf.sref<16xf32, #foo.scope>
+        } -> (%scratch_arg: !pcf.sref<16xf32, #foo.scope>)
+        execute(%ref = %init)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %scratch_arg is available here, initialized once
+        pcf.return
+      }
+    ```
+
+    Example with untied (allocated) results:
+    ```mlir
+      %d0, %d1 = ... : index
+      %result = pcf.generic scope(#foo.scope)
+        execute[%id: index, %num_workers: index]
+             : () -> (tensor<?x?xf32>{%d0, %d1}) {
+        // Result sref is allocated by the op, not tied to any init
+        pcf.return
+      }
+    ```
+  }];
+  let arguments = (ins
+    PCF_ScopeAttr:$scope,
+    Variadic<PCF_AnyTensorOrMemRef>:$inits,
+    Variadic<Index>:$dynamic_sizes,
+    ArrayProp<BoolProp>:$is_tied,
+    DefaultValuedProp<BoolProp, "false">:$sync_on_return,
+    IntProp<"int64_t">:$num_index_args,
+    IntProp<"int64_t">:$num_leading_args
+  );
+
+  let results = (outs Variadic<PCF_AnyTensorOrMemRef>:$results);
+  let regions = (region
+    MaxSizedRegion<1>:$initializer,
+    MinSizedRegion<1>:$region
+  );
+
+  let assemblyFormat = [{
+    (`sync` $sync_on_return^)?
+    `scope` `(` $scope `)`
+    (`initialize` $initializer^)?
+    custom<ParallelExecutionBody>($inits,
+                                  type($inits),
+                                  $dynamic_sizes,
+                                  type($results),
+                                  $is_tied,
+                                  $region,
+                                  $num_leading_args,
+                                  "true")
+    custom<InferNumIndexArgs>(ref($region), ref($num_leading_args), $num_index_args)
+    prop-dict attr-dict
+  }];
+  let hasVerifier = 1;
+
+  // The default builder does not add the proper body BBargs, roll our own.
+  let skipDefaultBuilders = 1;
+  let builders = [
+    // Builder with no return values.
+    OpBuilder<(ins "ScopeAttr":$scope, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with return types inferred from inits.
+    OpBuilder<(ins "ScopeAttr":$scope, "ValueRange":$inits,
+                   "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with no inits.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$dynamic_sizes, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with everything.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$inits, "ValueRange":$dynamic_sizes,
+                   "ArrayRef<bool>":$is_tied, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+  ];
+
+  let extraClassDeclaration = [{
+    int64_t getNumIterators() {
+      return getNumIndexArgs() / 2;
+    }
+    MutableArrayRef<BlockArgument> getLeadingArgs() {
+      return getRegion().getArguments().take_front(getNumLeadingArgs());
+    }
+    MutableArrayRef<BlockArgument> getIdArgs() {
+      return getRegion().getArguments().take_back(getNumIndexArgs()).take_front(getNumIterators());
+    }
+    MutableArrayRef<BlockArgument> getCountArgs() {
+      return getRegion().getArguments().take_back(getNumIterators());
+    }
+    MutableArrayRef<BlockArgument> getIdAndCountArgs() {
+      return getRegion().getArguments().take_back(getNumIndexArgs());
+    }
+    MutableArrayRef<BlockArgument> getRegionRefArgs() {
+      return getRegion().getArguments().drop_front(getNumLeadingArgs()).take_front(getNumResults());
+    }
+
+    bool isRegionRefArg(BlockArgument b) {
+      assert(b.getOwner() == &getRegion().front() && "unexpected non-entry block arg");
```

**Comment:**
Move longer methods to .cpp?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:359`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
+      SmallVector<OpFoldResult> result;
+      unsigned ctr = 0;
+      OpBuilder b(getContext());
+      for (int64_t i = 0, e = getResultType().getRank(); i < e; ++i) {
+        if (getResultType().isDynamicDim(i)) {
+          result.push_back(getDynamicSizes()[ctr++]);
+        } else {
+          result.push_back(b.getIndexAttr(getResultType().getShape()[i]));
+        }
+      }
+      return result;
+    }
+  }];
+
+  let hasVerifier = 1;
+}
+
+} // OpGroupAllocOps
+
+//===----------------------------------------------------------------------===//
+// ParallelOps
+//===----------------------------------------------------------------------===//
+
+def OpGroupParallelOps : OpDocGroup {
+  let summary = "Parallel execution ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupParallelOps in {
+
+def GenericOp : PCF_Op<"generic", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region across a set of workers at a specified scope. When
+    control flow reaches this op, workers with count equal to the native
+    parallelism of the scope are spawned and begin executing the region. The
+    scope is given by an attribute implementing the `ScopeAttr` interface and
+    is responsible for the semantics of all pcf primitives at the same scope.
+    Further details about scopes are included in the docs for the interface.
+
+    The optional `initialize` region is executed once when control flow first
+    reaches the op. Values yielded from the initializer become block arguments
+    available to the execute region. This is useful for setting up per-op
+    state that persists across all worker invocations.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all workers have returned. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with tied results:
+    ```mlir
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.generic scope(#foo.scope)
+        execute(%ref = %0)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // Each worker can read/write %ref
+        pcf.return
+      }
+    ```
+
+    Example with initializer:
+    ```mlir
+      %result = pcf.generic scope(#foo.scope)
+        initialize {
+          %scratch = pcf.alloc() : !pcf.sref<16xf32, #foo.scope>
+          pcf.yield %scratch : !pcf.sref<16xf32, #foo.scope>
+        } -> (%scratch_arg: !pcf.sref<16xf32, #foo.scope>)
+        execute(%ref = %init)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %scratch_arg is available here, initialized once
+        pcf.return
+      }
+    ```
+
+    Example with untied (allocated) results:
+    ```mlir
+      %d0, %d1 = ... : index
+      %result = pcf.generic scope(#foo.scope)
+        execute[%id: index, %num_workers: index]
+             : () -> (tensor<?x?xf32>{%d0, %d1}) {
+        // Result sref is allocated by the op, not tied to any init
+        pcf.return
+      }
+    ```
+  }];
+  let arguments = (ins
+    PCF_ScopeAttr:$scope,
+    Variadic<PCF_AnyTensorOrMemRef>:$inits,
+    Variadic<Index>:$dynamic_sizes,
+    ArrayProp<BoolProp>:$is_tied,
+    DefaultValuedProp<BoolProp, "false">:$sync_on_return,
+    IntProp<"int64_t">:$num_index_args,
+    IntProp<"int64_t">:$num_leading_args
+  );
+
+  let results = (outs Variadic<PCF_AnyTensorOrMemRef>:$results);
+  let regions = (region
+    MaxSizedRegion<1>:$initializer,
+    MinSizedRegion<1>:$region
+  );
+
+  let assemblyFormat = [{
+    (`sync` $sync_on_return^)?
+    `scope` `(` $scope `)`
+    (`initialize` $initializer^)?
+    custom<ParallelExecutionBody>($inits,
+                                  type($inits),
+                                  $dynamic_sizes,
+                                  type($results),
+                                  $is_tied,
+                                  $region,
+                                  $num_leading_args,
+                                  "true")
+    custom<InferNumIndexArgs>(ref($region), ref($num_leading_args), $num_index_args)
+    prop-dict attr-dict
+  }];
+  let hasVerifier = 1;
+
+  // The default builder does not add the proper body BBargs, roll our own.
+  let skipDefaultBuilders = 1;
+  let builders = [
+    // Builder with no return values.
+    OpBuilder<(ins "ScopeAttr":$scope, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with return types inferred from inits.
+    OpBuilder<(ins "ScopeAttr":$scope, "ValueRange":$inits,
+                   "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with no inits.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$dynamic_sizes, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with everything.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$inits, "ValueRange":$dynamic_sizes,
+                   "ArrayRef<bool>":$is_tied, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+  ];
+
+  let extraClassDeclaration = [{
+    int64_t getNumIterators() {
+      return getNumIndexArgs() / 2;
+    }
+    MutableArrayRef<BlockArgument> getLeadingArgs() {
+      return getRegion().getArguments().take_front(getNumLeadingArgs());
+    }
+    MutableArrayRef<BlockArgument> getIdArgs() {
+      return getRegion().getArguments().take_back(getNumIndexArgs()).take_front(getNumIterators());
+    }
+    MutableArrayRef<BlockArgument> getCountArgs() {
+      return getRegion().getArguments().take_back(getNumIterators());
+    }
+    MutableArrayRef<BlockArgument> getIdAndCountArgs() {
+      return getRegion().getArguments().take_back(getNumIndexArgs());
+    }
+    MutableArrayRef<BlockArgument> getRegionRefArgs() {
+      return getRegion().getArguments().drop_front(getNumLeadingArgs()).take_front(getNumResults());
+    }
+
+    bool isRegionRefArg(BlockArgument b) {
+      assert(b.getOwner() == &getRegion().front() && "unexpected non-entry block arg");
+      int64_t rangeBegin = getNumLeadingArgs();
+      int64_t rangeEnd = getNumLeadingArgs() + getNumResults();
+      return b.getArgNumber() >= rangeBegin && b.getArgNumber() < rangeEnd;
+    }
+
+    bool isInitializedArg(BlockArgument b) {
+      assert(b.getOwner() == &getRegion().front() && "unexpected non-entry block arg");
+      return b.getArgNumber() < getNumLeadingArgs();
+    }
+
+    SmallVector<int64_t> getInitTiedResultIndices() {
+      SmallVector<int64_t> tiedResults;
+      for (auto [i, isTied] : llvm::enumerate(getIsTied())) {
+        if (isTied) {
+          tiedResults.push_back(i);
+        }
+      }
+      return tiedResults;
+    }
+
+    OpResult getTiedResult(OpOperand &operand) {
+      int64_t beginIndex = getInits().getBeginOperandIndex();
+      int64_t operandIndex = operand.getOperandNumber();
+      if (operandIndex < beginIndex || operandIndex >= getInits().size() + beginIndex) {
+        return OpResult();
+      }
+
+      int64_t initIndex = operandIndex - beginIndex;
+      for (auto [i, isTied] : llvm::enumerate(getIsTied())) {
+        if (isTied) {
+          if (initIndex == 0) {
+            return (*this)->getOpResult(i);
+          }
+          --initIndex;
+        }
+      }
+
+      return OpResult();
+    }
+
+    OpResult getTiedResult(BlockArgument b) {
+      assert(isRegionRefArg(b) && "unexpected non region ref arg");
+      return (*this)->getOpResult(b.getArgNumber() - getNumLeadingArgs());
+    }
+
+    OpOperand *getTiedInit(int64_t i) {
+      if (i < 0 || i >= getNumResults() || !getIsTied()[i]) {
+        return nullptr;
+      }
+
+      int64_t initIndex = llvm::count(getIsTied().take_front(i), true);
+      return &getInitsMutable()[initIndex];
+    }
+
+    ShapedType getResultType(int64_t i) {
+      return cast<ShapedType>(getResults()[i].getType());
+    }
+
+    ValueRange getResultDims(int64_t i) {
+      if (getIsTied()[i]) {
+        return {};
+      }
+
+      int64_t startIndex = 0;
+      for (auto [curr, isTied] : llvm::enumerate(getIsTied())) {
+        if (curr == i) {
+          break;
+        }
+        if (!isTied) {
+          startIndex += getResultType(curr).getNumDynamicDims();
+        }
+      }
+
+      return ValueRange(getDynamicSizes().slice(startIndex, startIndex + getResultType(i).getNumDynamicDims()));
+    }
+  }];
+}
+
+def LoopOp : PCF_Op<"loop", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+       DeclareOpInterfaceMethods<RegionBranchOpInterface>,
+       SingleBlockImplicitTerminator<"mlir::iree_compiler::IREE::PCF::ReturnOp">
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region for each point in the iteration space defined by the
+    `count` operands. Unlike `pcf.generic` which spawns workers equal to the
+    native parallelism of the scope, `pcf.loop` explicitly specifies the
+    iteration count and maps iterations to workers according to the scope's
+    scheduling policy.
+
+    When control flow reaches this op, the scope determines how to distribute
+    the iterations across available workers. The scope is given by an attribute
+    implementing the `ScopeAttr` interface. Further details about scopes are
+    included in the docs for the interface.
+
+    The execute region receives one index block argument per count operand,
+    representing the current iteration's coordinates in the iteration space.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all iterations have completed. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with 1D iteration:
+    ```mlir
+      %n = ... : index
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.loop scope(#foo.scope) count(%n)
+        execute(%ref = %0)[%id: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %id ranges from 0 to %n-1
```

**Comment:**
```suggestion
        // %id ranges from 0 to %n-1.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:370`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
+      SmallVector<OpFoldResult> result;
+      unsigned ctr = 0;
+      OpBuilder b(getContext());
+      for (int64_t i = 0, e = getResultType().getRank(); i < e; ++i) {
+        if (getResultType().isDynamicDim(i)) {
+          result.push_back(getDynamicSizes()[ctr++]);
+        } else {
+          result.push_back(b.getIndexAttr(getResultType().getShape()[i]));
+        }
+      }
+      return result;
+    }
+  }];
+
+  let hasVerifier = 1;
+}
+
+} // OpGroupAllocOps
+
+//===----------------------------------------------------------------------===//
+// ParallelOps
+//===----------------------------------------------------------------------===//
+
+def OpGroupParallelOps : OpDocGroup {
+  let summary = "Parallel execution ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupParallelOps in {
+
+def GenericOp : PCF_Op<"generic", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region across a set of workers at a specified scope. When
+    control flow reaches this op, workers with count equal to the native
+    parallelism of the scope are spawned and begin executing the region. The
+    scope is given by an attribute implementing the `ScopeAttr` interface and
+    is responsible for the semantics of all pcf primitives at the same scope.
+    Further details about scopes are included in the docs for the interface.
+
+    The optional `initialize` region is executed once when control flow first
+    reaches the op. Values yielded from the initializer become block arguments
+    available to the execute region. This is useful for setting up per-op
+    state that persists across all worker invocations.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all workers have returned. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with tied results:
+    ```mlir
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.generic scope(#foo.scope)
+        execute(%ref = %0)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // Each worker can read/write %ref
+        pcf.return
+      }
+    ```
+
+    Example with initializer:
+    ```mlir
+      %result = pcf.generic scope(#foo.scope)
+        initialize {
+          %scratch = pcf.alloc() : !pcf.sref<16xf32, #foo.scope>
+          pcf.yield %scratch : !pcf.sref<16xf32, #foo.scope>
+        } -> (%scratch_arg: !pcf.sref<16xf32, #foo.scope>)
+        execute(%ref = %init)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %scratch_arg is available here, initialized once
+        pcf.return
+      }
+    ```
+
+    Example with untied (allocated) results:
+    ```mlir
+      %d0, %d1 = ... : index
+      %result = pcf.generic scope(#foo.scope)
+        execute[%id: index, %num_workers: index]
+             : () -> (tensor<?x?xf32>{%d0, %d1}) {
+        // Result sref is allocated by the op, not tied to any init
+        pcf.return
+      }
+    ```
+  }];
+  let arguments = (ins
+    PCF_ScopeAttr:$scope,
+    Variadic<PCF_AnyTensorOrMemRef>:$inits,
+    Variadic<Index>:$dynamic_sizes,
+    ArrayProp<BoolProp>:$is_tied,
+    DefaultValuedProp<BoolProp, "false">:$sync_on_return,
+    IntProp<"int64_t">:$num_index_args,
+    IntProp<"int64_t">:$num_leading_args
+  );
+
+  let results = (outs Variadic<PCF_AnyTensorOrMemRef>:$results);
+  let regions = (region
+    MaxSizedRegion<1>:$initializer,
+    MinSizedRegion<1>:$region
+  );
+
+  let assemblyFormat = [{
+    (`sync` $sync_on_return^)?
+    `scope` `(` $scope `)`
+    (`initialize` $initializer^)?
+    custom<ParallelExecutionBody>($inits,
+                                  type($inits),
+                                  $dynamic_sizes,
+                                  type($results),
+                                  $is_tied,
+                                  $region,
+                                  $num_leading_args,
+                                  "true")
+    custom<InferNumIndexArgs>(ref($region), ref($num_leading_args), $num_index_args)
+    prop-dict attr-dict
+  }];
+  let hasVerifier = 1;
+
+  // The default builder does not add the proper body BBargs, roll our own.
+  let skipDefaultBuilders = 1;
+  let builders = [
+    // Builder with no return values.
+    OpBuilder<(ins "ScopeAttr":$scope, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with return types inferred from inits.
+    OpBuilder<(ins "ScopeAttr":$scope, "ValueRange":$inits,
+                   "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with no inits.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$dynamic_sizes, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with everything.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$inits, "ValueRange":$dynamic_sizes,
+                   "ArrayRef<bool>":$is_tied, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+  ];
+
+  let extraClassDeclaration = [{
+    int64_t getNumIterators() {
+      return getNumIndexArgs() / 2;
+    }
+    MutableArrayRef<BlockArgument> getLeadingArgs() {
+      return getRegion().getArguments().take_front(getNumLeadingArgs());
+    }
+    MutableArrayRef<BlockArgument> getIdArgs() {
+      return getRegion().getArguments().take_back(getNumIndexArgs()).take_front(getNumIterators());
+    }
+    MutableArrayRef<BlockArgument> getCountArgs() {
+      return getRegion().getArguments().take_back(getNumIterators());
+    }
+    MutableArrayRef<BlockArgument> getIdAndCountArgs() {
+      return getRegion().getArguments().take_back(getNumIndexArgs());
+    }
+    MutableArrayRef<BlockArgument> getRegionRefArgs() {
+      return getRegion().getArguments().drop_front(getNumLeadingArgs()).take_front(getNumResults());
+    }
+
+    bool isRegionRefArg(BlockArgument b) {
+      assert(b.getOwner() == &getRegion().front() && "unexpected non-entry block arg");
+      int64_t rangeBegin = getNumLeadingArgs();
+      int64_t rangeEnd = getNumLeadingArgs() + getNumResults();
+      return b.getArgNumber() >= rangeBegin && b.getArgNumber() < rangeEnd;
+    }
+
+    bool isInitializedArg(BlockArgument b) {
+      assert(b.getOwner() == &getRegion().front() && "unexpected non-entry block arg");
+      return b.getArgNumber() < getNumLeadingArgs();
+    }
+
+    SmallVector<int64_t> getInitTiedResultIndices() {
+      SmallVector<int64_t> tiedResults;
+      for (auto [i, isTied] : llvm::enumerate(getIsTied())) {
+        if (isTied) {
+          tiedResults.push_back(i);
+        }
+      }
+      return tiedResults;
+    }
+
+    OpResult getTiedResult(OpOperand &operand) {
+      int64_t beginIndex = getInits().getBeginOperandIndex();
+      int64_t operandIndex = operand.getOperandNumber();
+      if (operandIndex < beginIndex || operandIndex >= getInits().size() + beginIndex) {
+        return OpResult();
+      }
+
+      int64_t initIndex = operandIndex - beginIndex;
+      for (auto [i, isTied] : llvm::enumerate(getIsTied())) {
+        if (isTied) {
+          if (initIndex == 0) {
+            return (*this)->getOpResult(i);
+          }
+          --initIndex;
+        }
+      }
+
+      return OpResult();
+    }
+
+    OpResult getTiedResult(BlockArgument b) {
+      assert(isRegionRefArg(b) && "unexpected non region ref arg");
+      return (*this)->getOpResult(b.getArgNumber() - getNumLeadingArgs());
+    }
+
+    OpOperand *getTiedInit(int64_t i) {
+      if (i < 0 || i >= getNumResults() || !getIsTied()[i]) {
+        return nullptr;
+      }
+
+      int64_t initIndex = llvm::count(getIsTied().take_front(i), true);
+      return &getInitsMutable()[initIndex];
+    }
+
+    ShapedType getResultType(int64_t i) {
+      return cast<ShapedType>(getResults()[i].getType());
+    }
+
+    ValueRange getResultDims(int64_t i) {
+      if (getIsTied()[i]) {
+        return {};
+      }
+
+      int64_t startIndex = 0;
+      for (auto [curr, isTied] : llvm::enumerate(getIsTied())) {
+        if (curr == i) {
+          break;
+        }
+        if (!isTied) {
+          startIndex += getResultType(curr).getNumDynamicDims();
+        }
+      }
+
+      return ValueRange(getDynamicSizes().slice(startIndex, startIndex + getResultType(i).getNumDynamicDims()));
+    }
+  }];
+}
+
+def LoopOp : PCF_Op<"loop", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+       DeclareOpInterfaceMethods<RegionBranchOpInterface>,
+       SingleBlockImplicitTerminator<"mlir::iree_compiler::IREE::PCF::ReturnOp">
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region for each point in the iteration space defined by the
+    `count` operands. Unlike `pcf.generic` which spawns workers equal to the
+    native parallelism of the scope, `pcf.loop` explicitly specifies the
+    iteration count and maps iterations to workers according to the scope's
+    scheduling policy.
+
+    When control flow reaches this op, the scope determines how to distribute
+    the iterations across available workers. The scope is given by an attribute
+    implementing the `ScopeAttr` interface. Further details about scopes are
+    included in the docs for the interface.
+
+    The execute region receives one index block argument per count operand,
+    representing the current iteration's coordinates in the iteration space.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all iterations have completed. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with 1D iteration:
+    ```mlir
+      %n = ... : index
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.loop scope(#foo.scope) count(%n)
+        execute(%ref = %0)[%id: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %id ranges from 0 to %n-1
+        pcf.return
+      }
+    ```
+
+    Example with multi-dimensional iteration:
+    ```mlir
+      %m, %n = ... : index
+      %result = pcf.loop scope(#foo.scope) count(%m, %n)
+        execute(%ref = %init)[%i: index, %j: index]
+             : (!pcf.sref<?x?xf32, #foo.scope>) -> (tensor<?x?xf32>) {
+        // %i ranges from 0 to %m-1, %j ranges from 0 to %n-1
```

**Comment:**
```suggestion
        // %i ranges from 0 to %m-1, %j ranges from 0 to %n-1.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:434`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
+      SmallVector<OpFoldResult> result;
+      unsigned ctr = 0;
+      OpBuilder b(getContext());
+      for (int64_t i = 0, e = getResultType().getRank(); i < e; ++i) {
+        if (getResultType().isDynamicDim(i)) {
+          result.push_back(getDynamicSizes()[ctr++]);
+        } else {
+          result.push_back(b.getIndexAttr(getResultType().getShape()[i]));
+        }
+      }
+      return result;
+    }
+  }];
+
+  let hasVerifier = 1;
+}
+
+} // OpGroupAllocOps
+
+//===----------------------------------------------------------------------===//
+// ParallelOps
+//===----------------------------------------------------------------------===//
+
+def OpGroupParallelOps : OpDocGroup {
+  let summary = "Parallel execution ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupParallelOps in {
+
+def GenericOp : PCF_Op<"generic", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region across a set of workers at a specified scope. When
+    control flow reaches this op, workers with count equal to the native
+    parallelism of the scope are spawned and begin executing the region. The
+    scope is given by an attribute implementing the `ScopeAttr` interface and
+    is responsible for the semantics of all pcf primitives at the same scope.
+    Further details about scopes are included in the docs for the interface.
+
+    The optional `initialize` region is executed once when control flow first
+    reaches the op. Values yielded from the initializer become block arguments
+    available to the execute region. This is useful for setting up per-op
+    state that persists across all worker invocations.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all workers have returned. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with tied results:
+    ```mlir
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.generic scope(#foo.scope)
+        execute(%ref = %0)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // Each worker can read/write %ref
+        pcf.return
+      }
+    ```
+
+    Example with initializer:
+    ```mlir
+      %result = pcf.generic scope(#foo.scope)
+        initialize {
+          %scratch = pcf.alloc() : !pcf.sref<16xf32, #foo.scope>
+          pcf.yield %scratch : !pcf.sref<16xf32, #foo.scope>
+        } -> (%scratch_arg: !pcf.sref<16xf32, #foo.scope>)
+        execute(%ref = %init)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %scratch_arg is available here, initialized once
+        pcf.return
+      }
+    ```
+
+    Example with untied (allocated) results:
+    ```mlir
+      %d0, %d1 = ... : index
+      %result = pcf.generic scope(#foo.scope)
+        execute[%id: index, %num_workers: index]
+             : () -> (tensor<?x?xf32>{%d0, %d1}) {
+        // Result sref is allocated by the op, not tied to any init
+        pcf.return
+      }
+    ```
+  }];
+  let arguments = (ins
+    PCF_ScopeAttr:$scope,
+    Variadic<PCF_AnyTensorOrMemRef>:$inits,
+    Variadic<Index>:$dynamic_sizes,
+    ArrayProp<BoolProp>:$is_tied,
+    DefaultValuedProp<BoolProp, "false">:$sync_on_return,
+    IntProp<"int64_t">:$num_index_args,
+    IntProp<"int64_t">:$num_leading_args
+  );
+
+  let results = (outs Variadic<PCF_AnyTensorOrMemRef>:$results);
+  let regions = (region
+    MaxSizedRegion<1>:$initializer,
+    MinSizedRegion<1>:$region
+  );
+
+  let assemblyFormat = [{
+    (`sync` $sync_on_return^)?
+    `scope` `(` $scope `)`
+    (`initialize` $initializer^)?
+    custom<ParallelExecutionBody>($inits,
+                                  type($inits),
+                                  $dynamic_sizes,
+                                  type($results),
+                                  $is_tied,
+                                  $region,
+                                  $num_leading_args,
+                                  "true")
+    custom<InferNumIndexArgs>(ref($region), ref($num_leading_args), $num_index_args)
+    prop-dict attr-dict
+  }];
+  let hasVerifier = 1;
+
+  // The default builder does not add the proper body BBargs, roll our own.
+  let skipDefaultBuilders = 1;
+  let builders = [
+    // Builder with no return values.
+    OpBuilder<(ins "ScopeAttr":$scope, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with return types inferred from inits.
+    OpBuilder<(ins "ScopeAttr":$scope, "ValueRange":$inits,
+                   "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with no inits.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$dynamic_sizes, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with everything.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$inits, "ValueRange":$dynamic_sizes,
+                   "ArrayRef<bool>":$is_tied, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+  ];
+
+  let extraClassDeclaration = [{
+    int64_t getNumIterators() {
+      return getNumIndexArgs() / 2;
+    }
+    MutableArrayRef<BlockArgument> getLeadingArgs() {
+      return getRegion().getArguments().take_front(getNumLeadingArgs());
+    }
+    MutableArrayRef<BlockArgument> getIdArgs() {
+      return getRegion().getArguments().take_back(getNumIndexArgs()).take_front(getNumIterators());
+    }
+    MutableArrayRef<BlockArgument> getCountArgs() {
+      return getRegion().getArguments().take_back(getNumIterators());
+    }
+    MutableArrayRef<BlockArgument> getIdAndCountArgs() {
+      return getRegion().getArguments().take_back(getNumIndexArgs());
+    }
+    MutableArrayRef<BlockArgument> getRegionRefArgs() {
+      return getRegion().getArguments().drop_front(getNumLeadingArgs()).take_front(getNumResults());
+    }
+
+    bool isRegionRefArg(BlockArgument b) {
+      assert(b.getOwner() == &getRegion().front() && "unexpected non-entry block arg");
+      int64_t rangeBegin = getNumLeadingArgs();
+      int64_t rangeEnd = getNumLeadingArgs() + getNumResults();
+      return b.getArgNumber() >= rangeBegin && b.getArgNumber() < rangeEnd;
+    }
+
+    bool isInitializedArg(BlockArgument b) {
+      assert(b.getOwner() == &getRegion().front() && "unexpected non-entry block arg");
+      return b.getArgNumber() < getNumLeadingArgs();
+    }
+
+    SmallVector<int64_t> getInitTiedResultIndices() {
+      SmallVector<int64_t> tiedResults;
+      for (auto [i, isTied] : llvm::enumerate(getIsTied())) {
+        if (isTied) {
+          tiedResults.push_back(i);
+        }
+      }
+      return tiedResults;
+    }
+
+    OpResult getTiedResult(OpOperand &operand) {
+      int64_t beginIndex = getInits().getBeginOperandIndex();
+      int64_t operandIndex = operand.getOperandNumber();
+      if (operandIndex < beginIndex || operandIndex >= getInits().size() + beginIndex) {
+        return OpResult();
+      }
+
+      int64_t initIndex = operandIndex - beginIndex;
+      for (auto [i, isTied] : llvm::enumerate(getIsTied())) {
+        if (isTied) {
+          if (initIndex == 0) {
+            return (*this)->getOpResult(i);
+          }
+          --initIndex;
+        }
+      }
+
+      return OpResult();
+    }
+
+    OpResult getTiedResult(BlockArgument b) {
+      assert(isRegionRefArg(b) && "unexpected non region ref arg");
+      return (*this)->getOpResult(b.getArgNumber() - getNumLeadingArgs());
+    }
+
+    OpOperand *getTiedInit(int64_t i) {
+      if (i < 0 || i >= getNumResults() || !getIsTied()[i]) {
+        return nullptr;
+      }
+
+      int64_t initIndex = llvm::count(getIsTied().take_front(i), true);
+      return &getInitsMutable()[initIndex];
+    }
+
+    ShapedType getResultType(int64_t i) {
+      return cast<ShapedType>(getResults()[i].getType());
+    }
+
+    ValueRange getResultDims(int64_t i) {
+      if (getIsTied()[i]) {
+        return {};
+      }
+
+      int64_t startIndex = 0;
+      for (auto [curr, isTied] : llvm::enumerate(getIsTied())) {
+        if (curr == i) {
+          break;
+        }
+        if (!isTied) {
+          startIndex += getResultType(curr).getNumDynamicDims();
+        }
+      }
+
+      return ValueRange(getDynamicSizes().slice(startIndex, startIndex + getResultType(i).getNumDynamicDims()));
+    }
+  }];
+}
+
+def LoopOp : PCF_Op<"loop", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+       DeclareOpInterfaceMethods<RegionBranchOpInterface>,
+       SingleBlockImplicitTerminator<"mlir::iree_compiler::IREE::PCF::ReturnOp">
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region for each point in the iteration space defined by the
+    `count` operands. Unlike `pcf.generic` which spawns workers equal to the
+    native parallelism of the scope, `pcf.loop` explicitly specifies the
+    iteration count and maps iterations to workers according to the scope's
+    scheduling policy.
+
+    When control flow reaches this op, the scope determines how to distribute
+    the iterations across available workers. The scope is given by an attribute
+    implementing the `ScopeAttr` interface. Further details about scopes are
+    included in the docs for the interface.
+
+    The execute region receives one index block argument per count operand,
+    representing the current iteration's coordinates in the iteration space.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all iterations have completed. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with 1D iteration:
+    ```mlir
+      %n = ... : index
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.loop scope(#foo.scope) count(%n)
+        execute(%ref = %0)[%id: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %id ranges from 0 to %n-1
+        pcf.return
+      }
+    ```
+
+    Example with multi-dimensional iteration:
+    ```mlir
+      %m, %n = ... : index
+      %result = pcf.loop scope(#foo.scope) count(%m, %n)
+        execute(%ref = %init)[%i: index, %j: index]
+             : (!pcf.sref<?x?xf32, #foo.scope>) -> (tensor<?x?xf32>) {
+        // %i ranges from 0 to %m-1, %j ranges from 0 to %n-1
+        pcf.return
+      }
+    ```
+  }];
+  let arguments = (ins
+    PCF_ScopeAttr:$scope,
+    Variadic<Index>:$count,
+    Variadic<PCF_AnyTensorOrMemRef>:$inits,
+    Variadic<Index>:$dynamic_sizes,
+    ArrayProp<BoolProp>:$is_tied,
+    DefaultValuedProp<BoolProp, "false">:$sync_on_return
+  );
+
+  let results = (outs Variadic<PCF_AnyTensorOrMemRef>:$results);
+  let regions = (region AnyRegion:$region);
+
+  let assemblyFormat = [{
+    (`sync` $sync_on_return^)?
+    `scope` `(` $scope `)`
+    `count` `(` $count `)`
+    custom<ParallelExecutionBody>($inits,
+                                  type($inits),
+                                  $dynamic_sizes,
+                                  type($results),
+                                  $is_tied,
+                                  $region)
+    prop-dict attr-dict
+  }];
+  let hasVerifier = 1;
+
+  // The default builder does not add the proper body BBargs, roll our own.
+  let skipDefaultBuilders = 1;
+  let builders = [
+    // Builder with no return values.
+    OpBuilder<(ins "ScopeAttr":$scope, "ValueRange":$count,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with return types inferred from inits.
+    OpBuilder<(ins "ScopeAttr":$scope, "ValueRange":$count,
+                   "ValueRange":$inits,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with no inits.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$count,
+                   "ValueRange":$dynamic_sizes,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with everything.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$count, "ValueRange":$inits,
+                   "ValueRange":$dynamic_sizes, "ArrayRef<bool>":$is_tied,
+                   CArg<"bool", "false">:$sync_on_return)>,
+  ];
+
+  let extraClassDeclaration = [{
+    int64_t getNumIdArgs() {
+      return getCount().size();
+    }
+    MutableArrayRef<BlockArgument> getIdArgs() {
+      return getRegion().getArguments().take_back(getNumIdArgs());
+    }
+    MutableArrayRef<BlockArgument> getRegionRefArgs() {
+      return getRegion().getArguments().take_front(getNumResults());
+    }
+
+    SmallVector<int64_t> getInitTiedResultIndices() {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:427`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
+    dimensions in the result type must have corresponding dynamic size
+    operands. The allocation scope is determined by the scope attribute of
+    the result type.
+
+    Example:
+    ```mlir
+      %sref = pcf.alloc() : !pcf.sref<4x8xf32, #foo.scope>
+      %sref_dyn = pcf.alloc(%d0, %d1) : !pcf.sref<?x?xf32, #foo.scope>
+    ```
+  }];
+
+  let arguments = (ins Variadic<Index>:$dynamicSizes);
+  let results = (outs PCF_AnyShapedRef:$result);
+
+  let builders = [
+    OpBuilder<(ins "ShapedRefType":$srefType), [{
+      return build($_builder, $_state, srefType, {});
+    }]>
+  ];
+
+  let assemblyFormat = [{
+    `(`$dynamicSizes`)` attr-dict `:` type($result)
+  }];
+
+  let extraClassDeclaration = [{
+    ShapedRefType getResultType() {
+      return cast<ShapedRefType>(getResult().getType());
+    }
+    SmallVector<OpFoldResult> getMixedSizes() {
+      SmallVector<OpFoldResult> result;
+      unsigned ctr = 0;
+      OpBuilder b(getContext());
+      for (int64_t i = 0, e = getResultType().getRank(); i < e; ++i) {
+        if (getResultType().isDynamicDim(i)) {
+          result.push_back(getDynamicSizes()[ctr++]);
+        } else {
+          result.push_back(b.getIndexAttr(getResultType().getShape()[i]));
+        }
+      }
+      return result;
+    }
+  }];
+
+  let hasVerifier = 1;
+}
+
+} // OpGroupAllocOps
+
+//===----------------------------------------------------------------------===//
+// ParallelOps
+//===----------------------------------------------------------------------===//
+
+def OpGroupParallelOps : OpDocGroup {
+  let summary = "Parallel execution ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupParallelOps in {
+
+def GenericOp : PCF_Op<"generic", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region across a set of workers at a specified scope. When
+    control flow reaches this op, workers with count equal to the native
+    parallelism of the scope are spawned and begin executing the region. The
+    scope is given by an attribute implementing the `ScopeAttr` interface and
+    is responsible for the semantics of all pcf primitives at the same scope.
+    Further details about scopes are included in the docs for the interface.
+
+    The optional `initialize` region is executed once when control flow first
+    reaches the op. Values yielded from the initializer become block arguments
+    available to the execute region. This is useful for setting up per-op
+    state that persists across all worker invocations.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all workers have returned. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with tied results:
+    ```mlir
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.generic scope(#foo.scope)
+        execute(%ref = %0)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // Each worker can read/write %ref
+        pcf.return
+      }
+    ```
+
+    Example with initializer:
+    ```mlir
+      %result = pcf.generic scope(#foo.scope)
+        initialize {
+          %scratch = pcf.alloc() : !pcf.sref<16xf32, #foo.scope>
+          pcf.yield %scratch : !pcf.sref<16xf32, #foo.scope>
+        } -> (%scratch_arg: !pcf.sref<16xf32, #foo.scope>)
+        execute(%ref = %init)[%id: index, %num_workers: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %scratch_arg is available here, initialized once
+        pcf.return
+      }
+    ```
+
+    Example with untied (allocated) results:
+    ```mlir
+      %d0, %d1 = ... : index
+      %result = pcf.generic scope(#foo.scope)
+        execute[%id: index, %num_workers: index]
+             : () -> (tensor<?x?xf32>{%d0, %d1}) {
+        // Result sref is allocated by the op, not tied to any init
+        pcf.return
+      }
+    ```
+  }];
+  let arguments = (ins
+    PCF_ScopeAttr:$scope,
+    Variadic<PCF_AnyTensorOrMemRef>:$inits,
+    Variadic<Index>:$dynamic_sizes,
+    ArrayProp<BoolProp>:$is_tied,
+    DefaultValuedProp<BoolProp, "false">:$sync_on_return,
+    IntProp<"int64_t">:$num_index_args,
+    IntProp<"int64_t">:$num_leading_args
+  );
+
+  let results = (outs Variadic<PCF_AnyTensorOrMemRef>:$results);
+  let regions = (region
+    MaxSizedRegion<1>:$initializer,
+    MinSizedRegion<1>:$region
+  );
+
+  let assemblyFormat = [{
+    (`sync` $sync_on_return^)?
+    `scope` `(` $scope `)`
+    (`initialize` $initializer^)?
+    custom<ParallelExecutionBody>($inits,
+                                  type($inits),
+                                  $dynamic_sizes,
+                                  type($results),
+                                  $is_tied,
+                                  $region,
+                                  $num_leading_args,
+                                  "true")
+    custom<InferNumIndexArgs>(ref($region), ref($num_leading_args), $num_index_args)
+    prop-dict attr-dict
+  }];
+  let hasVerifier = 1;
+
+  // The default builder does not add the proper body BBargs, roll our own.
+  let skipDefaultBuilders = 1;
+  let builders = [
+    // Builder with no return values.
+    OpBuilder<(ins "ScopeAttr":$scope, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with return types inferred from inits.
+    OpBuilder<(ins "ScopeAttr":$scope, "ValueRange":$inits,
+                   "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with no inits.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$dynamic_sizes, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with everything.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$inits, "ValueRange":$dynamic_sizes,
+                   "ArrayRef<bool>":$is_tied, "int64_t":$num_iterators,
+                   CArg<"bool", "false">:$sync_on_return)>,
+  ];
+
+  let extraClassDeclaration = [{
+    int64_t getNumIterators() {
+      return getNumIndexArgs() / 2;
+    }
+    MutableArrayRef<BlockArgument> getLeadingArgs() {
+      return getRegion().getArguments().take_front(getNumLeadingArgs());
+    }
+    MutableArrayRef<BlockArgument> getIdArgs() {
+      return getRegion().getArguments().take_back(getNumIndexArgs()).take_front(getNumIterators());
+    }
+    MutableArrayRef<BlockArgument> getCountArgs() {
+      return getRegion().getArguments().take_back(getNumIterators());
+    }
+    MutableArrayRef<BlockArgument> getIdAndCountArgs() {
+      return getRegion().getArguments().take_back(getNumIndexArgs());
+    }
+    MutableArrayRef<BlockArgument> getRegionRefArgs() {
+      return getRegion().getArguments().drop_front(getNumLeadingArgs()).take_front(getNumResults());
+    }
+
+    bool isRegionRefArg(BlockArgument b) {
+      assert(b.getOwner() == &getRegion().front() && "unexpected non-entry block arg");
+      int64_t rangeBegin = getNumLeadingArgs();
+      int64_t rangeEnd = getNumLeadingArgs() + getNumResults();
+      return b.getArgNumber() >= rangeBegin && b.getArgNumber() < rangeEnd;
+    }
+
+    bool isInitializedArg(BlockArgument b) {
+      assert(b.getOwner() == &getRegion().front() && "unexpected non-entry block arg");
+      return b.getArgNumber() < getNumLeadingArgs();
+    }
+
+    SmallVector<int64_t> getInitTiedResultIndices() {
+      SmallVector<int64_t> tiedResults;
+      for (auto [i, isTied] : llvm::enumerate(getIsTied())) {
+        if (isTied) {
+          tiedResults.push_back(i);
+        }
+      }
+      return tiedResults;
+    }
+
+    OpResult getTiedResult(OpOperand &operand) {
+      int64_t beginIndex = getInits().getBeginOperandIndex();
+      int64_t operandIndex = operand.getOperandNumber();
+      if (operandIndex < beginIndex || operandIndex >= getInits().size() + beginIndex) {
+        return OpResult();
+      }
+
+      int64_t initIndex = operandIndex - beginIndex;
+      for (auto [i, isTied] : llvm::enumerate(getIsTied())) {
+        if (isTied) {
+          if (initIndex == 0) {
+            return (*this)->getOpResult(i);
+          }
+          --initIndex;
+        }
+      }
+
+      return OpResult();
+    }
+
+    OpResult getTiedResult(BlockArgument b) {
+      assert(isRegionRefArg(b) && "unexpected non region ref arg");
+      return (*this)->getOpResult(b.getArgNumber() - getNumLeadingArgs());
+    }
+
+    OpOperand *getTiedInit(int64_t i) {
+      if (i < 0 || i >= getNumResults() || !getIsTied()[i]) {
+        return nullptr;
+      }
+
+      int64_t initIndex = llvm::count(getIsTied().take_front(i), true);
+      return &getInitsMutable()[initIndex];
+    }
+
+    ShapedType getResultType(int64_t i) {
+      return cast<ShapedType>(getResults()[i].getType());
+    }
+
+    ValueRange getResultDims(int64_t i) {
+      if (getIsTied()[i]) {
+        return {};
+      }
+
+      int64_t startIndex = 0;
+      for (auto [curr, isTied] : llvm::enumerate(getIsTied())) {
+        if (curr == i) {
+          break;
+        }
+        if (!isTied) {
+          startIndex += getResultType(curr).getNumDynamicDims();
+        }
+      }
+
+      return ValueRange(getDynamicSizes().slice(startIndex, startIndex + getResultType(i).getNumDynamicDims()));
+    }
+  }];
+}
+
+def LoopOp : PCF_Op<"loop", [
+       AttrSizedOperandSegments,
+       AutomaticAllocationScope,
+       RecursiveMemoryEffects,
+       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmBlockArgumentNames"]>,
+       DeclareOpInterfaceMethods<RegionBranchOpInterface>,
+       SingleBlockImplicitTerminator<"mlir::iree_compiler::IREE::PCF::ReturnOp">
+     ]> {
+  let summary = [{
+    Execute a set of workers in parallel on a region.
+  }];
+  let description = [{
+    Executes a region for each point in the iteration space defined by the
+    `count` operands. Unlike `pcf.generic` which spawns workers equal to the
+    native parallelism of the scope, `pcf.loop` explicitly specifies the
+    iteration count and maps iterations to workers according to the scope's
+    scheduling policy.
+
+    When control flow reaches this op, the scope determines how to distribute
+    the iterations across available workers. The scope is given by an attribute
+    implementing the `ScopeAttr` interface. Further details about scopes are
+    included in the docs for the interface.
+
+    The execute region receives one index block argument per count operand,
+    representing the current iteration's coordinates in the iteration space.
+
+    Results are produced by snapshotting the value of each result's tied sref
+    once all iterations have completed. Results can either be:
+    1. Tied to initial values (tensor or memref) - the init value provides the
+       initial contents and the result captures the final state.
+    2. Allocated by the op itself - dynamic sizes must be provided for
+       untied results with dynamic dimensions.
+
+    Basic example with 1D iteration:
+    ```mlir
+      %n = ... : index
+      %0 = ... : tensor<4x8xf32>
+      %1 = pcf.loop scope(#foo.scope) count(%n)
+        execute(%ref = %0)[%id: index]
+             : (!pcf.sref<4x8xf32, #foo.scope>) -> (tensor<4x8xf32>) {
+        // %id ranges from 0 to %n-1
+        pcf.return
+      }
+    ```
+
+    Example with multi-dimensional iteration:
+    ```mlir
+      %m, %n = ... : index
+      %result = pcf.loop scope(#foo.scope) count(%m, %n)
+        execute(%ref = %init)[%i: index, %j: index]
+             : (!pcf.sref<?x?xf32, #foo.scope>) -> (tensor<?x?xf32>) {
+        // %i ranges from 0 to %m-1, %j ranges from 0 to %n-1
+        pcf.return
+      }
+    ```
+  }];
+  let arguments = (ins
+    PCF_ScopeAttr:$scope,
+    Variadic<Index>:$count,
+    Variadic<PCF_AnyTensorOrMemRef>:$inits,
+    Variadic<Index>:$dynamic_sizes,
+    ArrayProp<BoolProp>:$is_tied,
+    DefaultValuedProp<BoolProp, "false">:$sync_on_return
+  );
+
+  let results = (outs Variadic<PCF_AnyTensorOrMemRef>:$results);
+  let regions = (region AnyRegion:$region);
+
+  let assemblyFormat = [{
+    (`sync` $sync_on_return^)?
+    `scope` `(` $scope `)`
+    `count` `(` $count `)`
+    custom<ParallelExecutionBody>($inits,
+                                  type($inits),
+                                  $dynamic_sizes,
+                                  type($results),
+                                  $is_tied,
+                                  $region)
+    prop-dict attr-dict
+  }];
+  let hasVerifier = 1;
+
+  // The default builder does not add the proper body BBargs, roll our own.
+  let skipDefaultBuilders = 1;
+  let builders = [
+    // Builder with no return values.
+    OpBuilder<(ins "ScopeAttr":$scope, "ValueRange":$count,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with return types inferred from inits.
+    OpBuilder<(ins "ScopeAttr":$scope, "ValueRange":$count,
+                   "ValueRange":$inits,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with no inits.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$count,
+                   "ValueRange":$dynamic_sizes,
+                   CArg<"bool", "false">:$sync_on_return)>,
+    // Builder with everything.
+    OpBuilder<(ins "TypeRange":$result_types, "ScopeAttr":$scope,
+                   "ValueRange":$count, "ValueRange":$inits,
+                   "ValueRange":$dynamic_sizes, "ArrayRef<bool>":$is_tied,
+                   CArg<"bool", "false">:$sync_on_return)>,
+  ];
+
+  let extraClassDeclaration = [{
+    int64_t getNumIdArgs() {
+      return getCount().size();
+    }
+    MutableArrayRef<BlockArgument> getIdArgs() {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp:30`

```diff
@@ -0,0 +1,144 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/LogicalResult.h"
+
+#define GET_TYPEDEF_CLASSES
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp.inc" // IWYU pragma: keep
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// #pcf.sref<...>
+//===----------------------------------------------------------------------===//
+
+Type ShapedRefType::parse(AsmParser &parser) {
+  if (parser.parseLess()) {
+    return {};
+  }
+
+  llvm::SmallVector<int64_t> shape;
+  mlir::Type elementType;
+  Attribute scope;
+
+  auto shapeLoc = parser.getCurrentLocation();
```

**Comment:**
nit: spell out the type. Also below.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp:27`

```diff
@@ -0,0 +1,144 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/LogicalResult.h"
+
+#define GET_TYPEDEF_CLASSES
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp.inc" // IWYU pragma: keep
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// #pcf.sref<...>
+//===----------------------------------------------------------------------===//
+
+Type ShapedRefType::parse(AsmParser &parser) {
+  if (parser.parseLess()) {
+    return {};
+  }
+
+  llvm::SmallVector<int64_t> shape;
+  mlir::Type elementType;
```

**Comment:**
```suggestion
  SmallVector<int64_t> shape;
  Type elementType;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp:31`

```diff
@@ -0,0 +1,144 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/LogicalResult.h"
+
+#define GET_TYPEDEF_CLASSES
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp.inc" // IWYU pragma: keep
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// #pcf.sref<...>
+//===----------------------------------------------------------------------===//
+
+Type ShapedRefType::parse(AsmParser &parser) {
+  if (parser.parseLess()) {
+    return {};
+  }
+
+  llvm::SmallVector<int64_t> shape;
+  mlir::Type elementType;
+  Attribute scope;
+
+  auto shapeLoc = parser.getCurrentLocation();
+  if (mlir::failed(parser.parseDimensionList(shape))) {
```

**Comment:**
```suggestion
  if (failed(parser.parseDimensionList(shape))) {
```
also elsewhere below

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp:92`

```diff
@@ -0,0 +1,144 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/LogicalResult.h"
+
+#define GET_TYPEDEF_CLASSES
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp.inc" // IWYU pragma: keep
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// #pcf.sref<...>
+//===----------------------------------------------------------------------===//
+
+Type ShapedRefType::parse(AsmParser &parser) {
+  if (parser.parseLess()) {
+    return {};
+  }
+
+  llvm::SmallVector<int64_t> shape;
+  mlir::Type elementType;
+  Attribute scope;
+
+  auto shapeLoc = parser.getCurrentLocation();
+  if (mlir::failed(parser.parseDimensionList(shape))) {
+    parser.emitError(shapeLoc, "failed to parse parameter 'shape'");
+    return {};
+  }
+
+  auto elemTypeLoc = parser.getCurrentLocation();
+  if (mlir::failed(parser.parseType(elementType))) {
+    parser.emitError(elemTypeLoc, "failed to parse parameter 'elementType'");
+    return {};
+  }
+
+  auto commaLoc = parser.getCurrentLocation();
+  if (mlir::failed(parser.parseComma())) {
+    parser.emitError(commaLoc, "expected comma after 'elementType'");
+    return {};
+  }
+
+  Attribute syncScope;
+  if (mlir::succeeded(parser.parseOptionalKeyword("sync"))) {
+    if (mlir::failed(parser.parseLParen())) {
+      return {};
+    }
+
+    auto scopeLoc = parser.getCurrentLocation();
+    if (mlir::failed(parser.parseAttribute(scope))) {
+      parser.emitError(scopeLoc, "failed to parse parameter 'scope'");
+      return {};
+    }
+
+    // Special parsing for SyncOnParentAttr sync scope.
+    syncScope = SyncOnParentAttr::get(parser.getContext());
+    if (mlir::failed(parser.parseRParen())) {
+      return {};
+    }
+  } else {
+    auto scopeLoc = parser.getCurrentLocation();
+    if (mlir::failed(parser.parseAttribute(scope))) {
+      parser.emitError(scopeLoc, "failed to parse parameter 'scope'");
+      return {};
+    }
+
+    if (!isa<ScopeAttr>(scope)) {
+      parser.emitError(scopeLoc, "expected 'scope' parameter ")
+          << scope << " to implement 'ScopeAttrInterface'";
+      return {};
+    }
+
+    if (mlir::succeeded(parser.parseOptionalComma())) {
+      auto syncLoc = parser.getCurrentLocation();
+      if (failed(parser.parseAttribute(syncScope))) {
+        parser.emitError(syncLoc, "failed to parse parameter 'sync_scope'");
+        return {};
+      }
+    }
+  }
+
+  if (parser.parseGreater()) {
+    return {};
+  }
+
+  MLIRContext *context = parser.getContext();
+  return ShapedRefType::get(context, shape, elementType, cast<ScopeAttr>(scope),
```

**Comment:**
nit
```suggestion
  return ShapedRefType::get(parser.getContext(), shape, elementType, cast<ScopeAttr>(scope),
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp:101`

```diff
@@ -0,0 +1,144 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/LogicalResult.h"
+
+#define GET_TYPEDEF_CLASSES
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp.inc" // IWYU pragma: keep
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// #pcf.sref<...>
+//===----------------------------------------------------------------------===//
+
+Type ShapedRefType::parse(AsmParser &parser) {
+  if (parser.parseLess()) {
+    return {};
+  }
+
+  llvm::SmallVector<int64_t> shape;
+  mlir::Type elementType;
+  Attribute scope;
+
+  auto shapeLoc = parser.getCurrentLocation();
+  if (mlir::failed(parser.parseDimensionList(shape))) {
+    parser.emitError(shapeLoc, "failed to parse parameter 'shape'");
+    return {};
+  }
+
+  auto elemTypeLoc = parser.getCurrentLocation();
+  if (mlir::failed(parser.parseType(elementType))) {
+    parser.emitError(elemTypeLoc, "failed to parse parameter 'elementType'");
+    return {};
+  }
+
+  auto commaLoc = parser.getCurrentLocation();
+  if (mlir::failed(parser.parseComma())) {
+    parser.emitError(commaLoc, "expected comma after 'elementType'");
+    return {};
+  }
+
+  Attribute syncScope;
+  if (mlir::succeeded(parser.parseOptionalKeyword("sync"))) {
+    if (mlir::failed(parser.parseLParen())) {
+      return {};
+    }
+
+    auto scopeLoc = parser.getCurrentLocation();
+    if (mlir::failed(parser.parseAttribute(scope))) {
+      parser.emitError(scopeLoc, "failed to parse parameter 'scope'");
+      return {};
+    }
+
+    // Special parsing for SyncOnParentAttr sync scope.
+    syncScope = SyncOnParentAttr::get(parser.getContext());
+    if (mlir::failed(parser.parseRParen())) {
+      return {};
+    }
+  } else {
+    auto scopeLoc = parser.getCurrentLocation();
+    if (mlir::failed(parser.parseAttribute(scope))) {
+      parser.emitError(scopeLoc, "failed to parse parameter 'scope'");
+      return {};
+    }
+
+    if (!isa<ScopeAttr>(scope)) {
+      parser.emitError(scopeLoc, "expected 'scope' parameter ")
+          << scope << " to implement 'ScopeAttrInterface'";
+      return {};
+    }
+
+    if (mlir::succeeded(parser.parseOptionalComma())) {
+      auto syncLoc = parser.getCurrentLocation();
+      if (failed(parser.parseAttribute(syncScope))) {
+        parser.emitError(syncLoc, "failed to parse parameter 'sync_scope'");
+        return {};
+      }
+    }
+  }
+
+  if (parser.parseGreater()) {
+    return {};
+  }
+
+  MLIRContext *context = parser.getContext();
+  return ShapedRefType::get(context, shape, elementType, cast<ScopeAttr>(scope),
+                            syncScope);
+}
+
+void ShapedRefType::print(AsmPrinter &printer) const {
+  printer << "<";
+
+  auto shape = getShape();
+  for (int64_t dim : shape) {
+    if (mlir::ShapedType::isDynamic(dim))
```

**Comment:**
also here: we don't need the mlir namespace prefix

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp:99`

```diff
@@ -0,0 +1,144 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFAttrs.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "llvm/Support/LogicalResult.h"
+
+#define GET_TYPEDEF_CLASSES
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.cpp.inc" // IWYU pragma: keep
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// #pcf.sref<...>
+//===----------------------------------------------------------------------===//
+
+Type ShapedRefType::parse(AsmParser &parser) {
+  if (parser.parseLess()) {
+    return {};
+  }
+
+  llvm::SmallVector<int64_t> shape;
+  mlir::Type elementType;
+  Attribute scope;
+
+  auto shapeLoc = parser.getCurrentLocation();
+  if (mlir::failed(parser.parseDimensionList(shape))) {
+    parser.emitError(shapeLoc, "failed to parse parameter 'shape'");
+    return {};
+  }
+
+  auto elemTypeLoc = parser.getCurrentLocation();
+  if (mlir::failed(parser.parseType(elementType))) {
+    parser.emitError(elemTypeLoc, "failed to parse parameter 'elementType'");
+    return {};
+  }
+
+  auto commaLoc = parser.getCurrentLocation();
+  if (mlir::failed(parser.parseComma())) {
+    parser.emitError(commaLoc, "expected comma after 'elementType'");
+    return {};
+  }
+
+  Attribute syncScope;
+  if (mlir::succeeded(parser.parseOptionalKeyword("sync"))) {
+    if (mlir::failed(parser.parseLParen())) {
+      return {};
+    }
+
+    auto scopeLoc = parser.getCurrentLocation();
+    if (mlir::failed(parser.parseAttribute(scope))) {
+      parser.emitError(scopeLoc, "failed to parse parameter 'scope'");
+      return {};
+    }
+
+    // Special parsing for SyncOnParentAttr sync scope.
+    syncScope = SyncOnParentAttr::get(parser.getContext());
+    if (mlir::failed(parser.parseRParen())) {
+      return {};
+    }
+  } else {
+    auto scopeLoc = parser.getCurrentLocation();
+    if (mlir::failed(parser.parseAttribute(scope))) {
+      parser.emitError(scopeLoc, "failed to parse parameter 'scope'");
+      return {};
+    }
+
+    if (!isa<ScopeAttr>(scope)) {
+      parser.emitError(scopeLoc, "expected 'scope' parameter ")
+          << scope << " to implement 'ScopeAttrInterface'";
+      return {};
+    }
+
+    if (mlir::succeeded(parser.parseOptionalComma())) {
+      auto syncLoc = parser.getCurrentLocation();
+      if (failed(parser.parseAttribute(syncScope))) {
+        parser.emitError(syncLoc, "failed to parse parameter 'sync_scope'");
+        return {};
+      }
+    }
+  }
+
+  if (parser.parseGreater()) {
+    return {};
+  }
+
+  MLIRContext *context = parser.getContext();
+  return ShapedRefType::get(context, shape, elementType, cast<ScopeAttr>(scope),
+                            syncScope);
+}
+
+void ShapedRefType::print(AsmPrinter &printer) const {
+  printer << "<";
+
+  auto shape = getShape();
```

**Comment:**
type

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.cpp:576`

```diff
@@ -0,0 +1,952 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include <numeric>
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/Dialect/Utils/StaticValueUtils.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/TypeUtilities.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// AllocOps
+//===----------------------------------------------------------------------===//
+
+LogicalResult AllocOp::verify() {
+  if (getDynamicSizes().size() != getResultType().getNumDynamicDims()) {
+    return emitOpError(
+        "dimension operand count does not equal sref dynamic dimension count");
+  }
+
+  // TODO: Restrict legal parents for this op.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// StructuralOps
+//===----------------------------------------------------------------------===//
+
+template <typename OpTy>
+static LogicalResult verifyParallelBodyOp(OpTy op, int64_t numLeadingArgs,
+                                          int64_t numIndexBodyArgs,
+                                          ArrayRef<BlockArgument> indexArgs) {
+  // Verify tied/token array lengths.
+  ArrayRef<bool> isTied = op.getIsTied();
+  int64_t numResults = op.getNumResults();
+  if (isTied.size() != numResults) {
+    return op.emitOpError(
+               "`is_tied` mask length expected to match number of results ")
+           << numResults;
+  }
+
+  int64_t numInits =
+      std::accumulate(isTied.begin(), isTied.end(), (int64_t)(0));
+  if (op.getInits().size() != numInits) {
+    return op.emitOpError("number of inits ")
+           << op.getInits().size()
+           << " does not match the number of results marked as tied "
+           << numInits;
+  }
+
+  if (op.getRegion().getArguments().size() !=
+      numLeadingArgs + numResults + numIndexBodyArgs) {
+    return op.emitOpError("expected region to have |numLeadingArgs| + "
+                          "|numIndexArgs| + |numResults| "
+                          "total arguments");
+  }
+
+  for (BlockArgument countArg : indexArgs) {
+    if (!countArg.getType().isIndex()) {
+      return op.emitOpError(
+          "expected index type for thread count/id region arguments");
+    }
+  }
+
+  PCF::ScopeAttr scope = op.getScope();
+  int64_t currIsTiedIndex = 0;
+  int64_t currResultIndex = 0;
+  for (auto [resultType, refArg, isTied] : llvm::zip_equal(
+           op.getResultTypes(), op.getRegionRefArgs(), op.getIsTied())) {
+    auto srefType = dyn_cast<PCF::ShapedRefType>(refArg.getType());
+    if (!srefType || srefType.getScope() != scope) {
+      return op.emitOpError(
+                 "expected region ref argument to be of type !pcf.sref "
+                 "with scope ")
+             << scope;
+    }
+    if (!srefType.isParentScopeOnlySync() && srefType.getSyncScope()) {
+      return op.emitOpError(
+          "expected region ref argument to have none or parent sync scope");
+    }
+
+    // Traits guarantee this cast to be valid.
+    auto shapedResultType = cast<ShapedType>(resultType);
+    if (shapedResultType.getShape() != srefType.getShape()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " with type " << srefType
+             << " shape mismatch with tied result of type " << resultType;
+    }
+
+    if (shapedResultType.getElementType() != srefType.getElementType()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " element type mismatch of "
+             << srefType.getElementType() << " vs "
+             << shapedResultType.getElementType();
+    }
+
+    if (isTied) {
+      Value init = op.getInits()[currIsTiedIndex];
+      if (init.getType() != resultType) {
+        return op.emitOpError("tied init at index ")
+               << currIsTiedIndex << " does not match the type " << resultType
+               << " at result index " << currResultIndex;
+      }
+      ++currIsTiedIndex;
+    }
+    ++currResultIndex;
+  }
+  return success();
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body, int64_t &numLeadingArgs, bool parseOptionalLeadingArgs) {
+  SmallVector<OpAsmParser::Argument> regionLeadingArgs;
+  if (parseOptionalLeadingArgs) {
+    if (succeeded(parser.parseOptionalArrow())) {
+      if (failed(parser.parseArgumentList(regionLeadingArgs,
+                                          OpAsmParser::Delimiter::Paren,
+                                          /*allowType=*/true))) {
+        return failure();
+      }
+    }
+    numLeadingArgs = regionLeadingArgs.size();
+  }
+  if (failed(parser.parseKeyword("execute")))
+    return failure();
+  SmallVector<OpAsmParser::Argument> regionRefArgs;
+  if (succeeded(parser.parseOptionalLParen())) {
+    do {
+      // Reserve entries in the lists.
+      regionRefArgs.emplace_back();
+      if (failed(parser.parseArgument(regionRefArgs.back(),
+                                      /*allowType=*/false,
+                                      /*allowAttrs=*/true))) {
+        return failure();
+      }
+
+      // Parse the tied init if present.
+      if (succeeded(parser.parseOptionalEqual())) {
+        inits.emplace_back();
+        if (failed(parser.parseOperand(inits.back()))) {
+          return failure();
+        }
+        isTied.push_back(true);
+      } else {
+        isTied.push_back(false);
+      }
+    } while (succeeded(parser.parseOptionalComma()));
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  SmallVector<OpAsmParser::Argument> indexArgs;
+  if (failed(parser.parseLSquare())) {
+    return failure();
+  }
+
+  if (failed(parser.parseArgumentList(
+          indexArgs, /*delimiter=*/OpAsmParser::Delimiter::None,
+          /*allowType=*/true, /*allowAttrs=*/true))) {
+    return failure();
+  }
+
+  if (failed(parser.parseRSquare())) {
+    return failure();
+  }
+
+  // If there is at least one region arg the arg types and op result types need
+  // to be parsed.
+  if (!regionRefArgs.empty()) {
+    if (failed(parser.parseColon()) || failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    // Parse all types except the last followed by commas.
+    for (OpAsmParser::Argument &arg :
+         MutableArrayRef<OpAsmParser::Argument>(regionRefArgs.begin(),
+                                                regionRefArgs.end())
+             .drop_back()) {
+      if (failed(parser.parseType(arg.type)) || failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    // Parse the last type.
+    if (failed(parser.parseType(regionRefArgs.back().type))) {
+      return failure();
+    }
+
+    if (failed(parser.parseRParen()) || failed(parser.parseArrow()) ||
+        failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    int64_t numResults = isTied.size();
+    resultTypes.resize(numResults);
+    for (auto [i, isTied] : llvm::enumerate(isTied)) {
+      if (failed(parser.parseType(resultTypes[i]))) {
+        return failure();
+      }
+
+      auto shapedType = dyn_cast<ShapedType>(resultTypes[i]);
+      if (!shapedType) {
+        return failure();
+      }
+
+      if (isTied) {
+        initTypes.push_back(resultTypes[i]);
+      } else if (succeeded(parser.parseOptionalLBrace())) {
+        // Only parse dynamic dims for non-tied operands.
+        SmallVector<OpAsmParser::UnresolvedOperand> dims;
+        if (failed(parser.parseOperandList(dims))) {
+          return failure();
+        }
+        size_t numDynamicDims = shapedType.getNumDynamicDims();
+        if (dims.size() != numDynamicDims) {
+          return failure();
+        }
+        if (failed(parser.parseRBrace())) {
+          return failure();
+        }
+        dynamicSizes.append(dims);
+      }
+
+      if (i < numResults - 1 && failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  // The stored argument order is:
+  // (initialized vals) (result tied refs) (num threads).
+  SmallVector<OpAsmParser::Argument> args;
+  args.append(regionLeadingArgs);
+  args.append(regionRefArgs);
+  args.append(indexArgs);
+  return parser.parseRegion(body, args, /*enableNameShadowing=*/false);
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body) {
+  int64_t numLeadingArgs = 0;
+  return parseParallelExecutionBody(parser, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, numLeadingArgs,
+                                    false);
+}
+
+static void printParallelExecutionBody(
+    OpAsmPrinter &p, Operation *op, OperandRange inits, TypeRange initTypes,
+    OperandRange dynamicSizes, TypeRange resultTypes, ArrayRef<bool> isTied,
+    Region &body, int64_t numLeadingArgs, bool printOptionalLeadingArgs) {
+  if (printOptionalLeadingArgs && numLeadingArgs > 0) {
+    p << "-> (";
+    MutableArrayRef<BlockArgument> leadingArgRange =
+        body.getArguments().take_front(numLeadingArgs);
+    llvm::interleaveComma(leadingArgRange, p, [&](BlockArgument arg) {
+      p.printRegionArgument(arg);
+    });
+    p << ")";
+  }
+
+  p.printNewline();
+  p << "  execute";
+
+  int64_t numResults = resultTypes.size();
+  int64_t numIndexArgs = body.getNumArguments() - numResults - numLeadingArgs;
+  MutableArrayRef<BlockArgument> threadCountArgRange =
+      body.getArguments().take_back(numIndexArgs);
+  MutableArrayRef<BlockArgument> refArgRange =
+      body.getArguments().drop_back(numIndexArgs).take_back(numResults);
+
+  if (numResults != 0) {
+    p << "(";
+    int64_t currInitIndex = 0;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      p << refArgRange[i];
+      if (isTied[i]) {
+        p << " = ";
+        p << inits[currInitIndex];
+        ++currInitIndex;
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ")";
+  }
+  p << "[";
+  llvm::interleaveComma(threadCountArgRange, p,
+                        [&](BlockArgument arg) { p.printRegionArgument(arg); });
+  p << "]";
+
+  // Now print the function type.
+  if (numResults != 0) {
+    p.printNewline();
+    // Whitespace to line up parentheses.
+    //   |--execute(
+    //   |--_____: (
+    p << "       : (";
+    llvm::interleaveComma(refArgRange, p,
+                          [&](BlockArgument arg) { p << arg.getType(); });
+    p << ")";
+    p.printNewline();
+    //   |--execute(
+    //   |--____-> (
+    p << "      -> (";
+    OperandRange currSizes = dynamicSizes;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      ShapedType resultType = cast<ShapedType>(resultTypes[i]);
+      bool isResultTied = isTied[i];
+      p << resultType;
+      if (!isResultTied && !resultType.hasStaticShape()) {
+        int64_t numDynamicDims = resultType.getNumDynamicDims();
+        p << "{";
+        llvm::interleaveComma(currSizes.take_front(numDynamicDims), p,
+                              [&](Value dim) { p << dim; });
+        currSizes = currSizes.drop_front(numDynamicDims);
+        p << "}";
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ") ";
+  } else {
+    // Print a space before the region brace if there are no loop results.
+    p << " ";
+  }
+  p.printRegion(body, /*printEntryBlockArgs=*/false,
+                /*printBlockTerminators=*/true);
+}
+
+static void printParallelExecutionBody(OpAsmPrinter &p, Operation *op,
+                                       OperandRange inits, TypeRange initTypes,
+                                       OperandRange dynamicSizes,
+                                       TypeRange resultTypes,
+                                       ArrayRef<bool> isTied, Region &body) {
+  return printParallelExecutionBody(p, op, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, 0, false);
+}
+
+//===----------------------------------------------------------------------===//
+// GenericOp
+//===----------------------------------------------------------------------===//
+
+static ParseResult parseInferNumIndexArgs(OpAsmParser &parser, Region &body,
+                                          int64_t &numLeadingArgs,
+                                          int64_t &numIndexArgs) {
+  numIndexArgs = 0;
+  for (BlockArgument bbArg :
+       llvm::reverse(body.getArguments().drop_front(numLeadingArgs))) {
+    if (!bbArg.getType().isIndex()) {
+      return success();
+    }
+    ++numIndexArgs;
+  }
+  return success();
+}
+
+static void printInferNumIndexArgs(OpAsmPrinter &, Operation *, Region &,
+                                   int64_t &, int64_t) {
+  // Nothing to do. The number of count args gets parsed solely from the region.
+}
+
+void GenericOp::getAsmBlockArgumentNames(Region &region,
+                                         OpAsmSetValueNameFn setNameFn) {
+  if (&region == &getInitializer()) {
+    return;
+  }
+
+  assert(&region == &getRegion() && "Unexpected region");
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getCountArgs()) {
+    setNameFn(v, "count");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult GenericOp::verify() {
+  if (getNumIndexArgs() % 2 != 0) {
+    return emitOpError("expected even number of id + count args");
+  }
+  if (getRegion().front().getNumArguments() < getNumIndexArgs()) {
+    return emitOpError(
+        "fewer body arguments than specified number of counts/ids.");
+  }
+  return verifyParallelBodyOp(*this, getNumLeadingArgs(), getNumIndexArgs(),
+                              getIdAndCountArgs());
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, int64_t numIterators,
+                      bool syncOnReturn) {
+  GenericOp::build(b, result, TypeRange(), scope, ArrayRef<Value>{},
+                   ArrayRef<Value>{}, ArrayRef<bool>{}, numIterators,
+                   syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, ValueRange inits, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  GenericOp::build(b, result, resultTypes, scope, inits, ArrayRef<Value>{},
+                   isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope,
+                      ValueRange dynamicSizes, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  GenericOp::build(b, result, resultTypes, scope, ArrayRef<Value>{},
+                   dynamicSizes, isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope, ValueRange inits,
+                      ValueRange dynamicSizes, ArrayRef<bool> isTied,
+                      int64_t numIterators, bool syncOnReturn) {
+
+  result.addAttribute(GenericOp::getScopeAttrName(result.name), scope);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+  inherentAttrs.setNumIndexArgs(2 * numIterators);
+
+  // Add the initializer region.
+  result.addRegion();
+
+  // Add the main region.
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count/id args.
+  Type indexType = b.getIndexType();
+  for (int64_t i = 0; i < 2 * numIterators; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// LoopOp
+//===----------------------------------------------------------------------===//
+
+void LoopOp::getAsmBlockArgumentNames(Region &region,
+                                      OpAsmSetValueNameFn setNameFn) {
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult LoopOp::verify() {
+  if (getCount().empty()) {
+    return emitOpError("expected at least one iteration count argument");
+  }
+  if (getBody()->getNumArguments() < getNumIdArgs()) {
+    return emitOpError("fewer body arguments than specified number of ids.");
+  }
+  return verifyParallelBodyOp(*this, /*numLeadingArgs=*/0, getNumIdArgs(),
+                              getIdArgs());
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, bool syncOnReturn) {
+  LoopOp::build(b, result, TypeRange(), scope, count, ArrayRef<Value>{},
+                ArrayRef<Value>{}, ArrayRef<bool>{}, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, ValueRange inits,
+                   bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  LoopOp::build(b, result, resultTypes, scope, count, inits, ArrayRef<Value>{},
+                isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange dynamicSizes, bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  LoopOp::build(b, result, resultTypes, scope, count, ArrayRef<Value>{},
+                dynamicSizes, isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange inits, ValueRange dynamicSizes,
+                   ArrayRef<bool> isTied, bool syncOnReturn) {
+
+  result.addAttribute(LoopOp::getScopeAttrName(result.name), scope);
+  result.addOperands(count);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(count.size()),
+                              static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count args.
+  Type indexType = b.getIndexType();
+  int64_t numCountArgs = count.size() == 0 ? 1 : count.size();
```

**Comment:**
```suggestion
  int64_t numCountArgs = count.empty() ? 1 : count.size();
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.cpp:667`

```diff
@@ -0,0 +1,952 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include <numeric>
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/Dialect/Utils/StaticValueUtils.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/TypeUtilities.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// AllocOps
+//===----------------------------------------------------------------------===//
+
+LogicalResult AllocOp::verify() {
+  if (getDynamicSizes().size() != getResultType().getNumDynamicDims()) {
+    return emitOpError(
+        "dimension operand count does not equal sref dynamic dimension count");
+  }
+
+  // TODO: Restrict legal parents for this op.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// StructuralOps
+//===----------------------------------------------------------------------===//
+
+template <typename OpTy>
+static LogicalResult verifyParallelBodyOp(OpTy op, int64_t numLeadingArgs,
+                                          int64_t numIndexBodyArgs,
+                                          ArrayRef<BlockArgument> indexArgs) {
+  // Verify tied/token array lengths.
+  ArrayRef<bool> isTied = op.getIsTied();
+  int64_t numResults = op.getNumResults();
+  if (isTied.size() != numResults) {
+    return op.emitOpError(
+               "`is_tied` mask length expected to match number of results ")
+           << numResults;
+  }
+
+  int64_t numInits =
+      std::accumulate(isTied.begin(), isTied.end(), (int64_t)(0));
+  if (op.getInits().size() != numInits) {
+    return op.emitOpError("number of inits ")
+           << op.getInits().size()
+           << " does not match the number of results marked as tied "
+           << numInits;
+  }
+
+  if (op.getRegion().getArguments().size() !=
+      numLeadingArgs + numResults + numIndexBodyArgs) {
+    return op.emitOpError("expected region to have |numLeadingArgs| + "
+                          "|numIndexArgs| + |numResults| "
+                          "total arguments");
+  }
+
+  for (BlockArgument countArg : indexArgs) {
+    if (!countArg.getType().isIndex()) {
+      return op.emitOpError(
+          "expected index type for thread count/id region arguments");
+    }
+  }
+
+  PCF::ScopeAttr scope = op.getScope();
+  int64_t currIsTiedIndex = 0;
+  int64_t currResultIndex = 0;
+  for (auto [resultType, refArg, isTied] : llvm::zip_equal(
+           op.getResultTypes(), op.getRegionRefArgs(), op.getIsTied())) {
+    auto srefType = dyn_cast<PCF::ShapedRefType>(refArg.getType());
+    if (!srefType || srefType.getScope() != scope) {
+      return op.emitOpError(
+                 "expected region ref argument to be of type !pcf.sref "
+                 "with scope ")
+             << scope;
+    }
+    if (!srefType.isParentScopeOnlySync() && srefType.getSyncScope()) {
+      return op.emitOpError(
+          "expected region ref argument to have none or parent sync scope");
+    }
+
+    // Traits guarantee this cast to be valid.
+    auto shapedResultType = cast<ShapedType>(resultType);
+    if (shapedResultType.getShape() != srefType.getShape()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " with type " << srefType
+             << " shape mismatch with tied result of type " << resultType;
+    }
+
+    if (shapedResultType.getElementType() != srefType.getElementType()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " element type mismatch of "
+             << srefType.getElementType() << " vs "
+             << shapedResultType.getElementType();
+    }
+
+    if (isTied) {
+      Value init = op.getInits()[currIsTiedIndex];
+      if (init.getType() != resultType) {
+        return op.emitOpError("tied init at index ")
+               << currIsTiedIndex << " does not match the type " << resultType
+               << " at result index " << currResultIndex;
+      }
+      ++currIsTiedIndex;
+    }
+    ++currResultIndex;
+  }
+  return success();
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body, int64_t &numLeadingArgs, bool parseOptionalLeadingArgs) {
+  SmallVector<OpAsmParser::Argument> regionLeadingArgs;
+  if (parseOptionalLeadingArgs) {
+    if (succeeded(parser.parseOptionalArrow())) {
+      if (failed(parser.parseArgumentList(regionLeadingArgs,
+                                          OpAsmParser::Delimiter::Paren,
+                                          /*allowType=*/true))) {
+        return failure();
+      }
+    }
+    numLeadingArgs = regionLeadingArgs.size();
+  }
+  if (failed(parser.parseKeyword("execute")))
+    return failure();
+  SmallVector<OpAsmParser::Argument> regionRefArgs;
+  if (succeeded(parser.parseOptionalLParen())) {
+    do {
+      // Reserve entries in the lists.
+      regionRefArgs.emplace_back();
+      if (failed(parser.parseArgument(regionRefArgs.back(),
+                                      /*allowType=*/false,
+                                      /*allowAttrs=*/true))) {
+        return failure();
+      }
+
+      // Parse the tied init if present.
+      if (succeeded(parser.parseOptionalEqual())) {
+        inits.emplace_back();
+        if (failed(parser.parseOperand(inits.back()))) {
+          return failure();
+        }
+        isTied.push_back(true);
+      } else {
+        isTied.push_back(false);
+      }
+    } while (succeeded(parser.parseOptionalComma()));
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  SmallVector<OpAsmParser::Argument> indexArgs;
+  if (failed(parser.parseLSquare())) {
+    return failure();
+  }
+
+  if (failed(parser.parseArgumentList(
+          indexArgs, /*delimiter=*/OpAsmParser::Delimiter::None,
+          /*allowType=*/true, /*allowAttrs=*/true))) {
+    return failure();
+  }
+
+  if (failed(parser.parseRSquare())) {
+    return failure();
+  }
+
+  // If there is at least one region arg the arg types and op result types need
+  // to be parsed.
+  if (!regionRefArgs.empty()) {
+    if (failed(parser.parseColon()) || failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    // Parse all types except the last followed by commas.
+    for (OpAsmParser::Argument &arg :
+         MutableArrayRef<OpAsmParser::Argument>(regionRefArgs.begin(),
+                                                regionRefArgs.end())
+             .drop_back()) {
+      if (failed(parser.parseType(arg.type)) || failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    // Parse the last type.
+    if (failed(parser.parseType(regionRefArgs.back().type))) {
+      return failure();
+    }
+
+    if (failed(parser.parseRParen()) || failed(parser.parseArrow()) ||
+        failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    int64_t numResults = isTied.size();
+    resultTypes.resize(numResults);
+    for (auto [i, isTied] : llvm::enumerate(isTied)) {
+      if (failed(parser.parseType(resultTypes[i]))) {
+        return failure();
+      }
+
+      auto shapedType = dyn_cast<ShapedType>(resultTypes[i]);
+      if (!shapedType) {
+        return failure();
+      }
+
+      if (isTied) {
+        initTypes.push_back(resultTypes[i]);
+      } else if (succeeded(parser.parseOptionalLBrace())) {
+        // Only parse dynamic dims for non-tied operands.
+        SmallVector<OpAsmParser::UnresolvedOperand> dims;
+        if (failed(parser.parseOperandList(dims))) {
+          return failure();
+        }
+        size_t numDynamicDims = shapedType.getNumDynamicDims();
+        if (dims.size() != numDynamicDims) {
+          return failure();
+        }
+        if (failed(parser.parseRBrace())) {
+          return failure();
+        }
+        dynamicSizes.append(dims);
+      }
+
+      if (i < numResults - 1 && failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  // The stored argument order is:
+  // (initialized vals) (result tied refs) (num threads).
+  SmallVector<OpAsmParser::Argument> args;
+  args.append(regionLeadingArgs);
+  args.append(regionRefArgs);
+  args.append(indexArgs);
+  return parser.parseRegion(body, args, /*enableNameShadowing=*/false);
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body) {
+  int64_t numLeadingArgs = 0;
+  return parseParallelExecutionBody(parser, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, numLeadingArgs,
+                                    false);
+}
+
+static void printParallelExecutionBody(
+    OpAsmPrinter &p, Operation *op, OperandRange inits, TypeRange initTypes,
+    OperandRange dynamicSizes, TypeRange resultTypes, ArrayRef<bool> isTied,
+    Region &body, int64_t numLeadingArgs, bool printOptionalLeadingArgs) {
+  if (printOptionalLeadingArgs && numLeadingArgs > 0) {
+    p << "-> (";
+    MutableArrayRef<BlockArgument> leadingArgRange =
+        body.getArguments().take_front(numLeadingArgs);
+    llvm::interleaveComma(leadingArgRange, p, [&](BlockArgument arg) {
+      p.printRegionArgument(arg);
+    });
+    p << ")";
+  }
+
+  p.printNewline();
+  p << "  execute";
+
+  int64_t numResults = resultTypes.size();
+  int64_t numIndexArgs = body.getNumArguments() - numResults - numLeadingArgs;
+  MutableArrayRef<BlockArgument> threadCountArgRange =
+      body.getArguments().take_back(numIndexArgs);
+  MutableArrayRef<BlockArgument> refArgRange =
+      body.getArguments().drop_back(numIndexArgs).take_back(numResults);
+
+  if (numResults != 0) {
+    p << "(";
+    int64_t currInitIndex = 0;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      p << refArgRange[i];
+      if (isTied[i]) {
+        p << " = ";
+        p << inits[currInitIndex];
+        ++currInitIndex;
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ")";
+  }
+  p << "[";
+  llvm::interleaveComma(threadCountArgRange, p,
+                        [&](BlockArgument arg) { p.printRegionArgument(arg); });
+  p << "]";
+
+  // Now print the function type.
+  if (numResults != 0) {
+    p.printNewline();
+    // Whitespace to line up parentheses.
+    //   |--execute(
+    //   |--_____: (
+    p << "       : (";
+    llvm::interleaveComma(refArgRange, p,
+                          [&](BlockArgument arg) { p << arg.getType(); });
+    p << ")";
+    p.printNewline();
+    //   |--execute(
+    //   |--____-> (
+    p << "      -> (";
+    OperandRange currSizes = dynamicSizes;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      ShapedType resultType = cast<ShapedType>(resultTypes[i]);
+      bool isResultTied = isTied[i];
+      p << resultType;
+      if (!isResultTied && !resultType.hasStaticShape()) {
+        int64_t numDynamicDims = resultType.getNumDynamicDims();
+        p << "{";
+        llvm::interleaveComma(currSizes.take_front(numDynamicDims), p,
+                              [&](Value dim) { p << dim; });
+        currSizes = currSizes.drop_front(numDynamicDims);
+        p << "}";
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ") ";
+  } else {
+    // Print a space before the region brace if there are no loop results.
+    p << " ";
+  }
+  p.printRegion(body, /*printEntryBlockArgs=*/false,
+                /*printBlockTerminators=*/true);
+}
+
+static void printParallelExecutionBody(OpAsmPrinter &p, Operation *op,
+                                       OperandRange inits, TypeRange initTypes,
+                                       OperandRange dynamicSizes,
+                                       TypeRange resultTypes,
+                                       ArrayRef<bool> isTied, Region &body) {
+  return printParallelExecutionBody(p, op, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, 0, false);
+}
+
+//===----------------------------------------------------------------------===//
+// GenericOp
+//===----------------------------------------------------------------------===//
+
+static ParseResult parseInferNumIndexArgs(OpAsmParser &parser, Region &body,
+                                          int64_t &numLeadingArgs,
+                                          int64_t &numIndexArgs) {
+  numIndexArgs = 0;
+  for (BlockArgument bbArg :
+       llvm::reverse(body.getArguments().drop_front(numLeadingArgs))) {
+    if (!bbArg.getType().isIndex()) {
+      return success();
+    }
+    ++numIndexArgs;
+  }
+  return success();
+}
+
+static void printInferNumIndexArgs(OpAsmPrinter &, Operation *, Region &,
+                                   int64_t &, int64_t) {
+  // Nothing to do. The number of count args gets parsed solely from the region.
+}
+
+void GenericOp::getAsmBlockArgumentNames(Region &region,
+                                         OpAsmSetValueNameFn setNameFn) {
+  if (&region == &getInitializer()) {
+    return;
+  }
+
+  assert(&region == &getRegion() && "Unexpected region");
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getCountArgs()) {
+    setNameFn(v, "count");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult GenericOp::verify() {
+  if (getNumIndexArgs() % 2 != 0) {
+    return emitOpError("expected even number of id + count args");
+  }
+  if (getRegion().front().getNumArguments() < getNumIndexArgs()) {
+    return emitOpError(
+        "fewer body arguments than specified number of counts/ids.");
+  }
+  return verifyParallelBodyOp(*this, getNumLeadingArgs(), getNumIndexArgs(),
+                              getIdAndCountArgs());
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, int64_t numIterators,
+                      bool syncOnReturn) {
+  GenericOp::build(b, result, TypeRange(), scope, ArrayRef<Value>{},
+                   ArrayRef<Value>{}, ArrayRef<bool>{}, numIterators,
+                   syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, ValueRange inits, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  GenericOp::build(b, result, resultTypes, scope, inits, ArrayRef<Value>{},
+                   isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope,
+                      ValueRange dynamicSizes, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  GenericOp::build(b, result, resultTypes, scope, ArrayRef<Value>{},
+                   dynamicSizes, isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope, ValueRange inits,
+                      ValueRange dynamicSizes, ArrayRef<bool> isTied,
+                      int64_t numIterators, bool syncOnReturn) {
+
+  result.addAttribute(GenericOp::getScopeAttrName(result.name), scope);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+  inherentAttrs.setNumIndexArgs(2 * numIterators);
+
+  // Add the initializer region.
+  result.addRegion();
+
+  // Add the main region.
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count/id args.
+  Type indexType = b.getIndexType();
+  for (int64_t i = 0; i < 2 * numIterators; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// LoopOp
+//===----------------------------------------------------------------------===//
+
+void LoopOp::getAsmBlockArgumentNames(Region &region,
+                                      OpAsmSetValueNameFn setNameFn) {
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult LoopOp::verify() {
+  if (getCount().empty()) {
+    return emitOpError("expected at least one iteration count argument");
+  }
+  if (getBody()->getNumArguments() < getNumIdArgs()) {
+    return emitOpError("fewer body arguments than specified number of ids.");
+  }
+  return verifyParallelBodyOp(*this, /*numLeadingArgs=*/0, getNumIdArgs(),
+                              getIdArgs());
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, bool syncOnReturn) {
+  LoopOp::build(b, result, TypeRange(), scope, count, ArrayRef<Value>{},
+                ArrayRef<Value>{}, ArrayRef<bool>{}, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, ValueRange inits,
+                   bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  LoopOp::build(b, result, resultTypes, scope, count, inits, ArrayRef<Value>{},
+                isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange dynamicSizes, bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  LoopOp::build(b, result, resultTypes, scope, count, ArrayRef<Value>{},
+                dynamicSizes, isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange inits, ValueRange dynamicSizes,
+                   ArrayRef<bool> isTied, bool syncOnReturn) {
+
+  result.addAttribute(LoopOp::getScopeAttrName(result.name), scope);
+  result.addOperands(count);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(count.size()),
+                              static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count args.
+  Type indexType = b.getIndexType();
+  int64_t numCountArgs = count.size() == 0 ? 1 : count.size();
+  for (int64_t i = 0; i < numCountArgs; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+void LoopOp::getSuccessorRegions(RegionBranchPoint point,
+                                 SmallVectorImpl<RegionSuccessor> &regions) {
+  // If the predecessor is the GenericOp, branch into the body.
+  if (point.isParent()) {
+    regions.push_back(RegionSuccessor(&getRegion()));
+    return;
+  }
+
+  // Otherwise, the region branches back to the parent operation.
+  regions.push_back(RegionSuccessor(getOperation(), getResults()));
+}
+
+//===----------------------------------------------------------------------===//
+// Control Flow Ops
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// BranchCondReturnOp
+//===----------------------------------------------------------------------===//
+
+void BranchCondReturnOp::setDest(Block *block) { return setSuccessor(block); }
+
+void BranchCondReturnOp::eraseOperand(unsigned index) {
+  (*this)->eraseOperand(index);
+}
+
+SuccessorOperands BranchCondReturnOp::getSuccessorOperands(unsigned index) {
+  assert(index == 0 && "invalid successor index");
+  // Single index operand produced by this op.
+  return SuccessorOperands(getDestOperandsMutable());
+}
+
+Block *
+BranchCondReturnOp::getSuccessorForOperands(ArrayRef<Attribute> operands) {
+  if (IntegerAttr condAttr =
+          llvm::dyn_cast_or_null<IntegerAttr>(operands.front())) {
+    return condAttr.getValue().isOne() ? nullptr : getDest();
+  }
+  return nullptr;
+}
+
+//===----------------------------------------------------------------------===//
+// WriteOps
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// ParallelInsertSliceOp
+//===----------------------------------------------------------------------===//
+
+// Build a WriteSliceOp with mixed static and dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<OpFoldResult> offsets,
+                         ArrayRef<OpFoldResult> sizes,
+                         ArrayRef<OpFoldResult> strides,
+                         ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, {}, source, dest, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+/// Build an WriteSliceOp with mixed static and dynamic entries
+/// packed into a Range vector.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<Range> ranges,
+                         ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, source, dest, offsets, sizes, strides, attrs);
+}
+
+// Build a WriteSliceOp with dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ValueRange offsets, ValueRange sizes,
+                         ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
```

**Comment:**
Use map_to_vector. You can do `auto offsetValues = llvm::map_to_vector(offsets, llvm::StaticCastTo<OpFoldResult>);`

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.cpp:707`

```diff
@@ -0,0 +1,952 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include <numeric>
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/Dialect/Utils/StaticValueUtils.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/TypeUtilities.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// AllocOps
+//===----------------------------------------------------------------------===//
+
+LogicalResult AllocOp::verify() {
+  if (getDynamicSizes().size() != getResultType().getNumDynamicDims()) {
+    return emitOpError(
+        "dimension operand count does not equal sref dynamic dimension count");
+  }
+
+  // TODO: Restrict legal parents for this op.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// StructuralOps
+//===----------------------------------------------------------------------===//
+
+template <typename OpTy>
+static LogicalResult verifyParallelBodyOp(OpTy op, int64_t numLeadingArgs,
+                                          int64_t numIndexBodyArgs,
+                                          ArrayRef<BlockArgument> indexArgs) {
+  // Verify tied/token array lengths.
+  ArrayRef<bool> isTied = op.getIsTied();
+  int64_t numResults = op.getNumResults();
+  if (isTied.size() != numResults) {
+    return op.emitOpError(
+               "`is_tied` mask length expected to match number of results ")
+           << numResults;
+  }
+
+  int64_t numInits =
+      std::accumulate(isTied.begin(), isTied.end(), (int64_t)(0));
+  if (op.getInits().size() != numInits) {
+    return op.emitOpError("number of inits ")
+           << op.getInits().size()
+           << " does not match the number of results marked as tied "
+           << numInits;
+  }
+
+  if (op.getRegion().getArguments().size() !=
+      numLeadingArgs + numResults + numIndexBodyArgs) {
+    return op.emitOpError("expected region to have |numLeadingArgs| + "
+                          "|numIndexArgs| + |numResults| "
+                          "total arguments");
+  }
+
+  for (BlockArgument countArg : indexArgs) {
+    if (!countArg.getType().isIndex()) {
+      return op.emitOpError(
+          "expected index type for thread count/id region arguments");
+    }
+  }
+
+  PCF::ScopeAttr scope = op.getScope();
+  int64_t currIsTiedIndex = 0;
+  int64_t currResultIndex = 0;
+  for (auto [resultType, refArg, isTied] : llvm::zip_equal(
+           op.getResultTypes(), op.getRegionRefArgs(), op.getIsTied())) {
+    auto srefType = dyn_cast<PCF::ShapedRefType>(refArg.getType());
+    if (!srefType || srefType.getScope() != scope) {
+      return op.emitOpError(
+                 "expected region ref argument to be of type !pcf.sref "
+                 "with scope ")
+             << scope;
+    }
+    if (!srefType.isParentScopeOnlySync() && srefType.getSyncScope()) {
+      return op.emitOpError(
+          "expected region ref argument to have none or parent sync scope");
+    }
+
+    // Traits guarantee this cast to be valid.
+    auto shapedResultType = cast<ShapedType>(resultType);
+    if (shapedResultType.getShape() != srefType.getShape()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " with type " << srefType
+             << " shape mismatch with tied result of type " << resultType;
+    }
+
+    if (shapedResultType.getElementType() != srefType.getElementType()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " element type mismatch of "
+             << srefType.getElementType() << " vs "
+             << shapedResultType.getElementType();
+    }
+
+    if (isTied) {
+      Value init = op.getInits()[currIsTiedIndex];
+      if (init.getType() != resultType) {
+        return op.emitOpError("tied init at index ")
+               << currIsTiedIndex << " does not match the type " << resultType
+               << " at result index " << currResultIndex;
+      }
+      ++currIsTiedIndex;
+    }
+    ++currResultIndex;
+  }
+  return success();
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body, int64_t &numLeadingArgs, bool parseOptionalLeadingArgs) {
+  SmallVector<OpAsmParser::Argument> regionLeadingArgs;
+  if (parseOptionalLeadingArgs) {
+    if (succeeded(parser.parseOptionalArrow())) {
+      if (failed(parser.parseArgumentList(regionLeadingArgs,
+                                          OpAsmParser::Delimiter::Paren,
+                                          /*allowType=*/true))) {
+        return failure();
+      }
+    }
+    numLeadingArgs = regionLeadingArgs.size();
+  }
+  if (failed(parser.parseKeyword("execute")))
+    return failure();
+  SmallVector<OpAsmParser::Argument> regionRefArgs;
+  if (succeeded(parser.parseOptionalLParen())) {
+    do {
+      // Reserve entries in the lists.
+      regionRefArgs.emplace_back();
+      if (failed(parser.parseArgument(regionRefArgs.back(),
+                                      /*allowType=*/false,
+                                      /*allowAttrs=*/true))) {
+        return failure();
+      }
+
+      // Parse the tied init if present.
+      if (succeeded(parser.parseOptionalEqual())) {
+        inits.emplace_back();
+        if (failed(parser.parseOperand(inits.back()))) {
+          return failure();
+        }
+        isTied.push_back(true);
+      } else {
+        isTied.push_back(false);
+      }
+    } while (succeeded(parser.parseOptionalComma()));
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  SmallVector<OpAsmParser::Argument> indexArgs;
+  if (failed(parser.parseLSquare())) {
+    return failure();
+  }
+
+  if (failed(parser.parseArgumentList(
+          indexArgs, /*delimiter=*/OpAsmParser::Delimiter::None,
+          /*allowType=*/true, /*allowAttrs=*/true))) {
+    return failure();
+  }
+
+  if (failed(parser.parseRSquare())) {
+    return failure();
+  }
+
+  // If there is at least one region arg the arg types and op result types need
+  // to be parsed.
+  if (!regionRefArgs.empty()) {
+    if (failed(parser.parseColon()) || failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    // Parse all types except the last followed by commas.
+    for (OpAsmParser::Argument &arg :
+         MutableArrayRef<OpAsmParser::Argument>(regionRefArgs.begin(),
+                                                regionRefArgs.end())
+             .drop_back()) {
+      if (failed(parser.parseType(arg.type)) || failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    // Parse the last type.
+    if (failed(parser.parseType(regionRefArgs.back().type))) {
+      return failure();
+    }
+
+    if (failed(parser.parseRParen()) || failed(parser.parseArrow()) ||
+        failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    int64_t numResults = isTied.size();
+    resultTypes.resize(numResults);
+    for (auto [i, isTied] : llvm::enumerate(isTied)) {
+      if (failed(parser.parseType(resultTypes[i]))) {
+        return failure();
+      }
+
+      auto shapedType = dyn_cast<ShapedType>(resultTypes[i]);
+      if (!shapedType) {
+        return failure();
+      }
+
+      if (isTied) {
+        initTypes.push_back(resultTypes[i]);
+      } else if (succeeded(parser.parseOptionalLBrace())) {
+        // Only parse dynamic dims for non-tied operands.
+        SmallVector<OpAsmParser::UnresolvedOperand> dims;
+        if (failed(parser.parseOperandList(dims))) {
+          return failure();
+        }
+        size_t numDynamicDims = shapedType.getNumDynamicDims();
+        if (dims.size() != numDynamicDims) {
+          return failure();
+        }
+        if (failed(parser.parseRBrace())) {
+          return failure();
+        }
+        dynamicSizes.append(dims);
+      }
+
+      if (i < numResults - 1 && failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  // The stored argument order is:
+  // (initialized vals) (result tied refs) (num threads).
+  SmallVector<OpAsmParser::Argument> args;
+  args.append(regionLeadingArgs);
+  args.append(regionRefArgs);
+  args.append(indexArgs);
+  return parser.parseRegion(body, args, /*enableNameShadowing=*/false);
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body) {
+  int64_t numLeadingArgs = 0;
+  return parseParallelExecutionBody(parser, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, numLeadingArgs,
+                                    false);
+}
+
+static void printParallelExecutionBody(
+    OpAsmPrinter &p, Operation *op, OperandRange inits, TypeRange initTypes,
+    OperandRange dynamicSizes, TypeRange resultTypes, ArrayRef<bool> isTied,
+    Region &body, int64_t numLeadingArgs, bool printOptionalLeadingArgs) {
+  if (printOptionalLeadingArgs && numLeadingArgs > 0) {
+    p << "-> (";
+    MutableArrayRef<BlockArgument> leadingArgRange =
+        body.getArguments().take_front(numLeadingArgs);
+    llvm::interleaveComma(leadingArgRange, p, [&](BlockArgument arg) {
+      p.printRegionArgument(arg);
+    });
+    p << ")";
+  }
+
+  p.printNewline();
+  p << "  execute";
+
+  int64_t numResults = resultTypes.size();
+  int64_t numIndexArgs = body.getNumArguments() - numResults - numLeadingArgs;
+  MutableArrayRef<BlockArgument> threadCountArgRange =
+      body.getArguments().take_back(numIndexArgs);
+  MutableArrayRef<BlockArgument> refArgRange =
+      body.getArguments().drop_back(numIndexArgs).take_back(numResults);
+
+  if (numResults != 0) {
+    p << "(";
+    int64_t currInitIndex = 0;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      p << refArgRange[i];
+      if (isTied[i]) {
+        p << " = ";
+        p << inits[currInitIndex];
+        ++currInitIndex;
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ")";
+  }
+  p << "[";
+  llvm::interleaveComma(threadCountArgRange, p,
+                        [&](BlockArgument arg) { p.printRegionArgument(arg); });
+  p << "]";
+
+  // Now print the function type.
+  if (numResults != 0) {
+    p.printNewline();
+    // Whitespace to line up parentheses.
+    //   |--execute(
+    //   |--_____: (
+    p << "       : (";
+    llvm::interleaveComma(refArgRange, p,
+                          [&](BlockArgument arg) { p << arg.getType(); });
+    p << ")";
+    p.printNewline();
+    //   |--execute(
+    //   |--____-> (
+    p << "      -> (";
+    OperandRange currSizes = dynamicSizes;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      ShapedType resultType = cast<ShapedType>(resultTypes[i]);
+      bool isResultTied = isTied[i];
+      p << resultType;
+      if (!isResultTied && !resultType.hasStaticShape()) {
+        int64_t numDynamicDims = resultType.getNumDynamicDims();
+        p << "{";
+        llvm::interleaveComma(currSizes.take_front(numDynamicDims), p,
+                              [&](Value dim) { p << dim; });
+        currSizes = currSizes.drop_front(numDynamicDims);
+        p << "}";
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ") ";
+  } else {
+    // Print a space before the region brace if there are no loop results.
+    p << " ";
+  }
+  p.printRegion(body, /*printEntryBlockArgs=*/false,
+                /*printBlockTerminators=*/true);
+}
+
+static void printParallelExecutionBody(OpAsmPrinter &p, Operation *op,
+                                       OperandRange inits, TypeRange initTypes,
+                                       OperandRange dynamicSizes,
+                                       TypeRange resultTypes,
+                                       ArrayRef<bool> isTied, Region &body) {
+  return printParallelExecutionBody(p, op, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, 0, false);
+}
+
+//===----------------------------------------------------------------------===//
+// GenericOp
+//===----------------------------------------------------------------------===//
+
+static ParseResult parseInferNumIndexArgs(OpAsmParser &parser, Region &body,
+                                          int64_t &numLeadingArgs,
+                                          int64_t &numIndexArgs) {
+  numIndexArgs = 0;
+  for (BlockArgument bbArg :
+       llvm::reverse(body.getArguments().drop_front(numLeadingArgs))) {
+    if (!bbArg.getType().isIndex()) {
+      return success();
+    }
+    ++numIndexArgs;
+  }
+  return success();
+}
+
+static void printInferNumIndexArgs(OpAsmPrinter &, Operation *, Region &,
+                                   int64_t &, int64_t) {
+  // Nothing to do. The number of count args gets parsed solely from the region.
+}
+
+void GenericOp::getAsmBlockArgumentNames(Region &region,
+                                         OpAsmSetValueNameFn setNameFn) {
+  if (&region == &getInitializer()) {
+    return;
+  }
+
+  assert(&region == &getRegion() && "Unexpected region");
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getCountArgs()) {
+    setNameFn(v, "count");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult GenericOp::verify() {
+  if (getNumIndexArgs() % 2 != 0) {
+    return emitOpError("expected even number of id + count args");
+  }
+  if (getRegion().front().getNumArguments() < getNumIndexArgs()) {
+    return emitOpError(
+        "fewer body arguments than specified number of counts/ids.");
+  }
+  return verifyParallelBodyOp(*this, getNumLeadingArgs(), getNumIndexArgs(),
+                              getIdAndCountArgs());
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, int64_t numIterators,
+                      bool syncOnReturn) {
+  GenericOp::build(b, result, TypeRange(), scope, ArrayRef<Value>{},
+                   ArrayRef<Value>{}, ArrayRef<bool>{}, numIterators,
+                   syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, ValueRange inits, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  GenericOp::build(b, result, resultTypes, scope, inits, ArrayRef<Value>{},
+                   isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope,
+                      ValueRange dynamicSizes, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  GenericOp::build(b, result, resultTypes, scope, ArrayRef<Value>{},
+                   dynamicSizes, isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope, ValueRange inits,
+                      ValueRange dynamicSizes, ArrayRef<bool> isTied,
+                      int64_t numIterators, bool syncOnReturn) {
+
+  result.addAttribute(GenericOp::getScopeAttrName(result.name), scope);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+  inherentAttrs.setNumIndexArgs(2 * numIterators);
+
+  // Add the initializer region.
+  result.addRegion();
+
+  // Add the main region.
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count/id args.
+  Type indexType = b.getIndexType();
+  for (int64_t i = 0; i < 2 * numIterators; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// LoopOp
+//===----------------------------------------------------------------------===//
+
+void LoopOp::getAsmBlockArgumentNames(Region &region,
+                                      OpAsmSetValueNameFn setNameFn) {
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult LoopOp::verify() {
+  if (getCount().empty()) {
+    return emitOpError("expected at least one iteration count argument");
+  }
+  if (getBody()->getNumArguments() < getNumIdArgs()) {
+    return emitOpError("fewer body arguments than specified number of ids.");
+  }
+  return verifyParallelBodyOp(*this, /*numLeadingArgs=*/0, getNumIdArgs(),
+                              getIdArgs());
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, bool syncOnReturn) {
+  LoopOp::build(b, result, TypeRange(), scope, count, ArrayRef<Value>{},
+                ArrayRef<Value>{}, ArrayRef<bool>{}, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, ValueRange inits,
+                   bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  LoopOp::build(b, result, resultTypes, scope, count, inits, ArrayRef<Value>{},
+                isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange dynamicSizes, bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  LoopOp::build(b, result, resultTypes, scope, count, ArrayRef<Value>{},
+                dynamicSizes, isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange inits, ValueRange dynamicSizes,
+                   ArrayRef<bool> isTied, bool syncOnReturn) {
+
+  result.addAttribute(LoopOp::getScopeAttrName(result.name), scope);
+  result.addOperands(count);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(count.size()),
+                              static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count args.
+  Type indexType = b.getIndexType();
+  int64_t numCountArgs = count.size() == 0 ? 1 : count.size();
+  for (int64_t i = 0; i < numCountArgs; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+void LoopOp::getSuccessorRegions(RegionBranchPoint point,
+                                 SmallVectorImpl<RegionSuccessor> &regions) {
+  // If the predecessor is the GenericOp, branch into the body.
+  if (point.isParent()) {
+    regions.push_back(RegionSuccessor(&getRegion()));
+    return;
+  }
+
+  // Otherwise, the region branches back to the parent operation.
+  regions.push_back(RegionSuccessor(getOperation(), getResults()));
+}
+
+//===----------------------------------------------------------------------===//
+// Control Flow Ops
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// BranchCondReturnOp
+//===----------------------------------------------------------------------===//
+
+void BranchCondReturnOp::setDest(Block *block) { return setSuccessor(block); }
+
+void BranchCondReturnOp::eraseOperand(unsigned index) {
+  (*this)->eraseOperand(index);
+}
+
+SuccessorOperands BranchCondReturnOp::getSuccessorOperands(unsigned index) {
+  assert(index == 0 && "invalid successor index");
+  // Single index operand produced by this op.
+  return SuccessorOperands(getDestOperandsMutable());
+}
+
+Block *
+BranchCondReturnOp::getSuccessorForOperands(ArrayRef<Attribute> operands) {
+  if (IntegerAttr condAttr =
+          llvm::dyn_cast_or_null<IntegerAttr>(operands.front())) {
+    return condAttr.getValue().isOne() ? nullptr : getDest();
+  }
+  return nullptr;
+}
+
+//===----------------------------------------------------------------------===//
+// WriteOps
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// ParallelInsertSliceOp
+//===----------------------------------------------------------------------===//
+
+// Build a WriteSliceOp with mixed static and dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<OpFoldResult> offsets,
+                         ArrayRef<OpFoldResult> sizes,
+                         ArrayRef<OpFoldResult> strides,
+                         ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, {}, source, dest, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+/// Build an WriteSliceOp with mixed static and dynamic entries
+/// packed into a Range vector.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<Range> ranges,
+                         ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, source, dest, offsets, sizes, strides, attrs);
+}
+
+// Build a WriteSliceOp with dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ValueRange offsets, ValueRange sizes,
+                         ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, source, dest, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// ReadSliceOp
+//===----------------------------------------------------------------------===//
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.cpp:747`

```diff
@@ -0,0 +1,952 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include <numeric>
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/Dialect/Utils/StaticValueUtils.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/TypeUtilities.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// AllocOps
+//===----------------------------------------------------------------------===//
+
+LogicalResult AllocOp::verify() {
+  if (getDynamicSizes().size() != getResultType().getNumDynamicDims()) {
+    return emitOpError(
+        "dimension operand count does not equal sref dynamic dimension count");
+  }
+
+  // TODO: Restrict legal parents for this op.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// StructuralOps
+//===----------------------------------------------------------------------===//
+
+template <typename OpTy>
+static LogicalResult verifyParallelBodyOp(OpTy op, int64_t numLeadingArgs,
+                                          int64_t numIndexBodyArgs,
+                                          ArrayRef<BlockArgument> indexArgs) {
+  // Verify tied/token array lengths.
+  ArrayRef<bool> isTied = op.getIsTied();
+  int64_t numResults = op.getNumResults();
+  if (isTied.size() != numResults) {
+    return op.emitOpError(
+               "`is_tied` mask length expected to match number of results ")
+           << numResults;
+  }
+
+  int64_t numInits =
+      std::accumulate(isTied.begin(), isTied.end(), (int64_t)(0));
+  if (op.getInits().size() != numInits) {
+    return op.emitOpError("number of inits ")
+           << op.getInits().size()
+           << " does not match the number of results marked as tied "
+           << numInits;
+  }
+
+  if (op.getRegion().getArguments().size() !=
+      numLeadingArgs + numResults + numIndexBodyArgs) {
+    return op.emitOpError("expected region to have |numLeadingArgs| + "
+                          "|numIndexArgs| + |numResults| "
+                          "total arguments");
+  }
+
+  for (BlockArgument countArg : indexArgs) {
+    if (!countArg.getType().isIndex()) {
+      return op.emitOpError(
+          "expected index type for thread count/id region arguments");
+    }
+  }
+
+  PCF::ScopeAttr scope = op.getScope();
+  int64_t currIsTiedIndex = 0;
+  int64_t currResultIndex = 0;
+  for (auto [resultType, refArg, isTied] : llvm::zip_equal(
+           op.getResultTypes(), op.getRegionRefArgs(), op.getIsTied())) {
+    auto srefType = dyn_cast<PCF::ShapedRefType>(refArg.getType());
+    if (!srefType || srefType.getScope() != scope) {
+      return op.emitOpError(
+                 "expected region ref argument to be of type !pcf.sref "
+                 "with scope ")
+             << scope;
+    }
+    if (!srefType.isParentScopeOnlySync() && srefType.getSyncScope()) {
+      return op.emitOpError(
+          "expected region ref argument to have none or parent sync scope");
+    }
+
+    // Traits guarantee this cast to be valid.
+    auto shapedResultType = cast<ShapedType>(resultType);
+    if (shapedResultType.getShape() != srefType.getShape()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " with type " << srefType
+             << " shape mismatch with tied result of type " << resultType;
+    }
+
+    if (shapedResultType.getElementType() != srefType.getElementType()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " element type mismatch of "
+             << srefType.getElementType() << " vs "
+             << shapedResultType.getElementType();
+    }
+
+    if (isTied) {
+      Value init = op.getInits()[currIsTiedIndex];
+      if (init.getType() != resultType) {
+        return op.emitOpError("tied init at index ")
+               << currIsTiedIndex << " does not match the type " << resultType
+               << " at result index " << currResultIndex;
+      }
+      ++currIsTiedIndex;
+    }
+    ++currResultIndex;
+  }
+  return success();
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body, int64_t &numLeadingArgs, bool parseOptionalLeadingArgs) {
+  SmallVector<OpAsmParser::Argument> regionLeadingArgs;
+  if (parseOptionalLeadingArgs) {
+    if (succeeded(parser.parseOptionalArrow())) {
+      if (failed(parser.parseArgumentList(regionLeadingArgs,
+                                          OpAsmParser::Delimiter::Paren,
+                                          /*allowType=*/true))) {
+        return failure();
+      }
+    }
+    numLeadingArgs = regionLeadingArgs.size();
+  }
+  if (failed(parser.parseKeyword("execute")))
+    return failure();
+  SmallVector<OpAsmParser::Argument> regionRefArgs;
+  if (succeeded(parser.parseOptionalLParen())) {
+    do {
+      // Reserve entries in the lists.
+      regionRefArgs.emplace_back();
+      if (failed(parser.parseArgument(regionRefArgs.back(),
+                                      /*allowType=*/false,
+                                      /*allowAttrs=*/true))) {
+        return failure();
+      }
+
+      // Parse the tied init if present.
+      if (succeeded(parser.parseOptionalEqual())) {
+        inits.emplace_back();
+        if (failed(parser.parseOperand(inits.back()))) {
+          return failure();
+        }
+        isTied.push_back(true);
+      } else {
+        isTied.push_back(false);
+      }
+    } while (succeeded(parser.parseOptionalComma()));
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  SmallVector<OpAsmParser::Argument> indexArgs;
+  if (failed(parser.parseLSquare())) {
+    return failure();
+  }
+
+  if (failed(parser.parseArgumentList(
+          indexArgs, /*delimiter=*/OpAsmParser::Delimiter::None,
+          /*allowType=*/true, /*allowAttrs=*/true))) {
+    return failure();
+  }
+
+  if (failed(parser.parseRSquare())) {
+    return failure();
+  }
+
+  // If there is at least one region arg the arg types and op result types need
+  // to be parsed.
+  if (!regionRefArgs.empty()) {
+    if (failed(parser.parseColon()) || failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    // Parse all types except the last followed by commas.
+    for (OpAsmParser::Argument &arg :
+         MutableArrayRef<OpAsmParser::Argument>(regionRefArgs.begin(),
+                                                regionRefArgs.end())
+             .drop_back()) {
+      if (failed(parser.parseType(arg.type)) || failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    // Parse the last type.
+    if (failed(parser.parseType(regionRefArgs.back().type))) {
+      return failure();
+    }
+
+    if (failed(parser.parseRParen()) || failed(parser.parseArrow()) ||
+        failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    int64_t numResults = isTied.size();
+    resultTypes.resize(numResults);
+    for (auto [i, isTied] : llvm::enumerate(isTied)) {
+      if (failed(parser.parseType(resultTypes[i]))) {
+        return failure();
+      }
+
+      auto shapedType = dyn_cast<ShapedType>(resultTypes[i]);
+      if (!shapedType) {
+        return failure();
+      }
+
+      if (isTied) {
+        initTypes.push_back(resultTypes[i]);
+      } else if (succeeded(parser.parseOptionalLBrace())) {
+        // Only parse dynamic dims for non-tied operands.
+        SmallVector<OpAsmParser::UnresolvedOperand> dims;
+        if (failed(parser.parseOperandList(dims))) {
+          return failure();
+        }
+        size_t numDynamicDims = shapedType.getNumDynamicDims();
+        if (dims.size() != numDynamicDims) {
+          return failure();
+        }
+        if (failed(parser.parseRBrace())) {
+          return failure();
+        }
+        dynamicSizes.append(dims);
+      }
+
+      if (i < numResults - 1 && failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  // The stored argument order is:
+  // (initialized vals) (result tied refs) (num threads).
+  SmallVector<OpAsmParser::Argument> args;
+  args.append(regionLeadingArgs);
+  args.append(regionRefArgs);
+  args.append(indexArgs);
+  return parser.parseRegion(body, args, /*enableNameShadowing=*/false);
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body) {
+  int64_t numLeadingArgs = 0;
+  return parseParallelExecutionBody(parser, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, numLeadingArgs,
+                                    false);
+}
+
+static void printParallelExecutionBody(
+    OpAsmPrinter &p, Operation *op, OperandRange inits, TypeRange initTypes,
+    OperandRange dynamicSizes, TypeRange resultTypes, ArrayRef<bool> isTied,
+    Region &body, int64_t numLeadingArgs, bool printOptionalLeadingArgs) {
+  if (printOptionalLeadingArgs && numLeadingArgs > 0) {
+    p << "-> (";
+    MutableArrayRef<BlockArgument> leadingArgRange =
+        body.getArguments().take_front(numLeadingArgs);
+    llvm::interleaveComma(leadingArgRange, p, [&](BlockArgument arg) {
+      p.printRegionArgument(arg);
+    });
+    p << ")";
+  }
+
+  p.printNewline();
+  p << "  execute";
+
+  int64_t numResults = resultTypes.size();
+  int64_t numIndexArgs = body.getNumArguments() - numResults - numLeadingArgs;
+  MutableArrayRef<BlockArgument> threadCountArgRange =
+      body.getArguments().take_back(numIndexArgs);
+  MutableArrayRef<BlockArgument> refArgRange =
+      body.getArguments().drop_back(numIndexArgs).take_back(numResults);
+
+  if (numResults != 0) {
+    p << "(";
+    int64_t currInitIndex = 0;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      p << refArgRange[i];
+      if (isTied[i]) {
+        p << " = ";
+        p << inits[currInitIndex];
+        ++currInitIndex;
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ")";
+  }
+  p << "[";
+  llvm::interleaveComma(threadCountArgRange, p,
+                        [&](BlockArgument arg) { p.printRegionArgument(arg); });
+  p << "]";
+
+  // Now print the function type.
+  if (numResults != 0) {
+    p.printNewline();
+    // Whitespace to line up parentheses.
+    //   |--execute(
+    //   |--_____: (
+    p << "       : (";
+    llvm::interleaveComma(refArgRange, p,
+                          [&](BlockArgument arg) { p << arg.getType(); });
+    p << ")";
+    p.printNewline();
+    //   |--execute(
+    //   |--____-> (
+    p << "      -> (";
+    OperandRange currSizes = dynamicSizes;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      ShapedType resultType = cast<ShapedType>(resultTypes[i]);
+      bool isResultTied = isTied[i];
+      p << resultType;
+      if (!isResultTied && !resultType.hasStaticShape()) {
+        int64_t numDynamicDims = resultType.getNumDynamicDims();
+        p << "{";
+        llvm::interleaveComma(currSizes.take_front(numDynamicDims), p,
+                              [&](Value dim) { p << dim; });
+        currSizes = currSizes.drop_front(numDynamicDims);
+        p << "}";
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ") ";
+  } else {
+    // Print a space before the region brace if there are no loop results.
+    p << " ";
+  }
+  p.printRegion(body, /*printEntryBlockArgs=*/false,
+                /*printBlockTerminators=*/true);
+}
+
+static void printParallelExecutionBody(OpAsmPrinter &p, Operation *op,
+                                       OperandRange inits, TypeRange initTypes,
+                                       OperandRange dynamicSizes,
+                                       TypeRange resultTypes,
+                                       ArrayRef<bool> isTied, Region &body) {
+  return printParallelExecutionBody(p, op, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, 0, false);
+}
+
+//===----------------------------------------------------------------------===//
+// GenericOp
+//===----------------------------------------------------------------------===//
+
+static ParseResult parseInferNumIndexArgs(OpAsmParser &parser, Region &body,
+                                          int64_t &numLeadingArgs,
+                                          int64_t &numIndexArgs) {
+  numIndexArgs = 0;
+  for (BlockArgument bbArg :
+       llvm::reverse(body.getArguments().drop_front(numLeadingArgs))) {
+    if (!bbArg.getType().isIndex()) {
+      return success();
+    }
+    ++numIndexArgs;
+  }
+  return success();
+}
+
+static void printInferNumIndexArgs(OpAsmPrinter &, Operation *, Region &,
+                                   int64_t &, int64_t) {
+  // Nothing to do. The number of count args gets parsed solely from the region.
+}
+
+void GenericOp::getAsmBlockArgumentNames(Region &region,
+                                         OpAsmSetValueNameFn setNameFn) {
+  if (&region == &getInitializer()) {
+    return;
+  }
+
+  assert(&region == &getRegion() && "Unexpected region");
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getCountArgs()) {
+    setNameFn(v, "count");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult GenericOp::verify() {
+  if (getNumIndexArgs() % 2 != 0) {
+    return emitOpError("expected even number of id + count args");
+  }
+  if (getRegion().front().getNumArguments() < getNumIndexArgs()) {
+    return emitOpError(
+        "fewer body arguments than specified number of counts/ids.");
+  }
+  return verifyParallelBodyOp(*this, getNumLeadingArgs(), getNumIndexArgs(),
+                              getIdAndCountArgs());
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, int64_t numIterators,
+                      bool syncOnReturn) {
+  GenericOp::build(b, result, TypeRange(), scope, ArrayRef<Value>{},
+                   ArrayRef<Value>{}, ArrayRef<bool>{}, numIterators,
+                   syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, ValueRange inits, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  GenericOp::build(b, result, resultTypes, scope, inits, ArrayRef<Value>{},
+                   isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope,
+                      ValueRange dynamicSizes, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  GenericOp::build(b, result, resultTypes, scope, ArrayRef<Value>{},
+                   dynamicSizes, isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope, ValueRange inits,
+                      ValueRange dynamicSizes, ArrayRef<bool> isTied,
+                      int64_t numIterators, bool syncOnReturn) {
+
+  result.addAttribute(GenericOp::getScopeAttrName(result.name), scope);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+  inherentAttrs.setNumIndexArgs(2 * numIterators);
+
+  // Add the initializer region.
+  result.addRegion();
+
+  // Add the main region.
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count/id args.
+  Type indexType = b.getIndexType();
+  for (int64_t i = 0; i < 2 * numIterators; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// LoopOp
+//===----------------------------------------------------------------------===//
+
+void LoopOp::getAsmBlockArgumentNames(Region &region,
+                                      OpAsmSetValueNameFn setNameFn) {
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult LoopOp::verify() {
+  if (getCount().empty()) {
+    return emitOpError("expected at least one iteration count argument");
+  }
+  if (getBody()->getNumArguments() < getNumIdArgs()) {
+    return emitOpError("fewer body arguments than specified number of ids.");
+  }
+  return verifyParallelBodyOp(*this, /*numLeadingArgs=*/0, getNumIdArgs(),
+                              getIdArgs());
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, bool syncOnReturn) {
+  LoopOp::build(b, result, TypeRange(), scope, count, ArrayRef<Value>{},
+                ArrayRef<Value>{}, ArrayRef<bool>{}, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, ValueRange inits,
+                   bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  LoopOp::build(b, result, resultTypes, scope, count, inits, ArrayRef<Value>{},
+                isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange dynamicSizes, bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  LoopOp::build(b, result, resultTypes, scope, count, ArrayRef<Value>{},
+                dynamicSizes, isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange inits, ValueRange dynamicSizes,
+                   ArrayRef<bool> isTied, bool syncOnReturn) {
+
+  result.addAttribute(LoopOp::getScopeAttrName(result.name), scope);
+  result.addOperands(count);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(count.size()),
+                              static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count args.
+  Type indexType = b.getIndexType();
+  int64_t numCountArgs = count.size() == 0 ? 1 : count.size();
+  for (int64_t i = 0; i < numCountArgs; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+void LoopOp::getSuccessorRegions(RegionBranchPoint point,
+                                 SmallVectorImpl<RegionSuccessor> &regions) {
+  // If the predecessor is the GenericOp, branch into the body.
+  if (point.isParent()) {
+    regions.push_back(RegionSuccessor(&getRegion()));
+    return;
+  }
+
+  // Otherwise, the region branches back to the parent operation.
+  regions.push_back(RegionSuccessor(getOperation(), getResults()));
+}
+
+//===----------------------------------------------------------------------===//
+// Control Flow Ops
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// BranchCondReturnOp
+//===----------------------------------------------------------------------===//
+
+void BranchCondReturnOp::setDest(Block *block) { return setSuccessor(block); }
+
+void BranchCondReturnOp::eraseOperand(unsigned index) {
+  (*this)->eraseOperand(index);
+}
+
+SuccessorOperands BranchCondReturnOp::getSuccessorOperands(unsigned index) {
+  assert(index == 0 && "invalid successor index");
+  // Single index operand produced by this op.
+  return SuccessorOperands(getDestOperandsMutable());
+}
+
+Block *
+BranchCondReturnOp::getSuccessorForOperands(ArrayRef<Attribute> operands) {
+  if (IntegerAttr condAttr =
+          llvm::dyn_cast_or_null<IntegerAttr>(operands.front())) {
+    return condAttr.getValue().isOne() ? nullptr : getDest();
+  }
+  return nullptr;
+}
+
+//===----------------------------------------------------------------------===//
+// WriteOps
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// ParallelInsertSliceOp
+//===----------------------------------------------------------------------===//
+
+// Build a WriteSliceOp with mixed static and dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<OpFoldResult> offsets,
+                         ArrayRef<OpFoldResult> sizes,
+                         ArrayRef<OpFoldResult> strides,
+                         ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, {}, source, dest, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+/// Build an WriteSliceOp with mixed static and dynamic entries
+/// packed into a Range vector.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<Range> ranges,
+                         ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, source, dest, offsets, sizes, strides, attrs);
+}
+
+// Build a WriteSliceOp with dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ValueRange offsets, ValueRange sizes,
+                         ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, source, dest, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// ReadSliceOp
+//===----------------------------------------------------------------------===//
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// GetMemrefOp
+//===----------------------------------------------------------------------===//
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.cpp:812`

```diff
@@ -0,0 +1,952 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include <numeric>
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/Dialect/Utils/StaticValueUtils.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/TypeUtilities.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// AllocOps
+//===----------------------------------------------------------------------===//
+
+LogicalResult AllocOp::verify() {
+  if (getDynamicSizes().size() != getResultType().getNumDynamicDims()) {
+    return emitOpError(
+        "dimension operand count does not equal sref dynamic dimension count");
+  }
+
+  // TODO: Restrict legal parents for this op.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// StructuralOps
+//===----------------------------------------------------------------------===//
+
+template <typename OpTy>
+static LogicalResult verifyParallelBodyOp(OpTy op, int64_t numLeadingArgs,
+                                          int64_t numIndexBodyArgs,
+                                          ArrayRef<BlockArgument> indexArgs) {
+  // Verify tied/token array lengths.
+  ArrayRef<bool> isTied = op.getIsTied();
+  int64_t numResults = op.getNumResults();
+  if (isTied.size() != numResults) {
+    return op.emitOpError(
+               "`is_tied` mask length expected to match number of results ")
+           << numResults;
+  }
+
+  int64_t numInits =
+      std::accumulate(isTied.begin(), isTied.end(), (int64_t)(0));
+  if (op.getInits().size() != numInits) {
+    return op.emitOpError("number of inits ")
+           << op.getInits().size()
+           << " does not match the number of results marked as tied "
+           << numInits;
+  }
+
+  if (op.getRegion().getArguments().size() !=
+      numLeadingArgs + numResults + numIndexBodyArgs) {
+    return op.emitOpError("expected region to have |numLeadingArgs| + "
+                          "|numIndexArgs| + |numResults| "
+                          "total arguments");
+  }
+
+  for (BlockArgument countArg : indexArgs) {
+    if (!countArg.getType().isIndex()) {
+      return op.emitOpError(
+          "expected index type for thread count/id region arguments");
+    }
+  }
+
+  PCF::ScopeAttr scope = op.getScope();
+  int64_t currIsTiedIndex = 0;
+  int64_t currResultIndex = 0;
+  for (auto [resultType, refArg, isTied] : llvm::zip_equal(
+           op.getResultTypes(), op.getRegionRefArgs(), op.getIsTied())) {
+    auto srefType = dyn_cast<PCF::ShapedRefType>(refArg.getType());
+    if (!srefType || srefType.getScope() != scope) {
+      return op.emitOpError(
+                 "expected region ref argument to be of type !pcf.sref "
+                 "with scope ")
+             << scope;
+    }
+    if (!srefType.isParentScopeOnlySync() && srefType.getSyncScope()) {
+      return op.emitOpError(
+          "expected region ref argument to have none or parent sync scope");
+    }
+
+    // Traits guarantee this cast to be valid.
+    auto shapedResultType = cast<ShapedType>(resultType);
+    if (shapedResultType.getShape() != srefType.getShape()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " with type " << srefType
+             << " shape mismatch with tied result of type " << resultType;
+    }
+
+    if (shapedResultType.getElementType() != srefType.getElementType()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " element type mismatch of "
+             << srefType.getElementType() << " vs "
+             << shapedResultType.getElementType();
+    }
+
+    if (isTied) {
+      Value init = op.getInits()[currIsTiedIndex];
+      if (init.getType() != resultType) {
+        return op.emitOpError("tied init at index ")
+               << currIsTiedIndex << " does not match the type " << resultType
+               << " at result index " << currResultIndex;
+      }
+      ++currIsTiedIndex;
+    }
+    ++currResultIndex;
+  }
+  return success();
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body, int64_t &numLeadingArgs, bool parseOptionalLeadingArgs) {
+  SmallVector<OpAsmParser::Argument> regionLeadingArgs;
+  if (parseOptionalLeadingArgs) {
+    if (succeeded(parser.parseOptionalArrow())) {
+      if (failed(parser.parseArgumentList(regionLeadingArgs,
+                                          OpAsmParser::Delimiter::Paren,
+                                          /*allowType=*/true))) {
+        return failure();
+      }
+    }
+    numLeadingArgs = regionLeadingArgs.size();
+  }
+  if (failed(parser.parseKeyword("execute")))
+    return failure();
+  SmallVector<OpAsmParser::Argument> regionRefArgs;
+  if (succeeded(parser.parseOptionalLParen())) {
+    do {
+      // Reserve entries in the lists.
+      regionRefArgs.emplace_back();
+      if (failed(parser.parseArgument(regionRefArgs.back(),
+                                      /*allowType=*/false,
+                                      /*allowAttrs=*/true))) {
+        return failure();
+      }
+
+      // Parse the tied init if present.
+      if (succeeded(parser.parseOptionalEqual())) {
+        inits.emplace_back();
+        if (failed(parser.parseOperand(inits.back()))) {
+          return failure();
+        }
+        isTied.push_back(true);
+      } else {
+        isTied.push_back(false);
+      }
+    } while (succeeded(parser.parseOptionalComma()));
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  SmallVector<OpAsmParser::Argument> indexArgs;
+  if (failed(parser.parseLSquare())) {
+    return failure();
+  }
+
+  if (failed(parser.parseArgumentList(
+          indexArgs, /*delimiter=*/OpAsmParser::Delimiter::None,
+          /*allowType=*/true, /*allowAttrs=*/true))) {
+    return failure();
+  }
+
+  if (failed(parser.parseRSquare())) {
+    return failure();
+  }
+
+  // If there is at least one region arg the arg types and op result types need
+  // to be parsed.
+  if (!regionRefArgs.empty()) {
+    if (failed(parser.parseColon()) || failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    // Parse all types except the last followed by commas.
+    for (OpAsmParser::Argument &arg :
+         MutableArrayRef<OpAsmParser::Argument>(regionRefArgs.begin(),
+                                                regionRefArgs.end())
+             .drop_back()) {
+      if (failed(parser.parseType(arg.type)) || failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    // Parse the last type.
+    if (failed(parser.parseType(regionRefArgs.back().type))) {
+      return failure();
+    }
+
+    if (failed(parser.parseRParen()) || failed(parser.parseArrow()) ||
+        failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    int64_t numResults = isTied.size();
+    resultTypes.resize(numResults);
+    for (auto [i, isTied] : llvm::enumerate(isTied)) {
+      if (failed(parser.parseType(resultTypes[i]))) {
+        return failure();
+      }
+
+      auto shapedType = dyn_cast<ShapedType>(resultTypes[i]);
+      if (!shapedType) {
+        return failure();
+      }
+
+      if (isTied) {
+        initTypes.push_back(resultTypes[i]);
+      } else if (succeeded(parser.parseOptionalLBrace())) {
+        // Only parse dynamic dims for non-tied operands.
+        SmallVector<OpAsmParser::UnresolvedOperand> dims;
+        if (failed(parser.parseOperandList(dims))) {
+          return failure();
+        }
+        size_t numDynamicDims = shapedType.getNumDynamicDims();
+        if (dims.size() != numDynamicDims) {
+          return failure();
+        }
+        if (failed(parser.parseRBrace())) {
+          return failure();
+        }
+        dynamicSizes.append(dims);
+      }
+
+      if (i < numResults - 1 && failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  // The stored argument order is:
+  // (initialized vals) (result tied refs) (num threads).
+  SmallVector<OpAsmParser::Argument> args;
+  args.append(regionLeadingArgs);
+  args.append(regionRefArgs);
+  args.append(indexArgs);
+  return parser.parseRegion(body, args, /*enableNameShadowing=*/false);
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body) {
+  int64_t numLeadingArgs = 0;
+  return parseParallelExecutionBody(parser, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, numLeadingArgs,
+                                    false);
+}
+
+static void printParallelExecutionBody(
+    OpAsmPrinter &p, Operation *op, OperandRange inits, TypeRange initTypes,
+    OperandRange dynamicSizes, TypeRange resultTypes, ArrayRef<bool> isTied,
+    Region &body, int64_t numLeadingArgs, bool printOptionalLeadingArgs) {
+  if (printOptionalLeadingArgs && numLeadingArgs > 0) {
+    p << "-> (";
+    MutableArrayRef<BlockArgument> leadingArgRange =
+        body.getArguments().take_front(numLeadingArgs);
+    llvm::interleaveComma(leadingArgRange, p, [&](BlockArgument arg) {
+      p.printRegionArgument(arg);
+    });
+    p << ")";
+  }
+
+  p.printNewline();
+  p << "  execute";
+
+  int64_t numResults = resultTypes.size();
+  int64_t numIndexArgs = body.getNumArguments() - numResults - numLeadingArgs;
+  MutableArrayRef<BlockArgument> threadCountArgRange =
+      body.getArguments().take_back(numIndexArgs);
+  MutableArrayRef<BlockArgument> refArgRange =
+      body.getArguments().drop_back(numIndexArgs).take_back(numResults);
+
+  if (numResults != 0) {
+    p << "(";
+    int64_t currInitIndex = 0;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      p << refArgRange[i];
+      if (isTied[i]) {
+        p << " = ";
+        p << inits[currInitIndex];
+        ++currInitIndex;
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ")";
+  }
+  p << "[";
+  llvm::interleaveComma(threadCountArgRange, p,
+                        [&](BlockArgument arg) { p.printRegionArgument(arg); });
+  p << "]";
+
+  // Now print the function type.
+  if (numResults != 0) {
+    p.printNewline();
+    // Whitespace to line up parentheses.
+    //   |--execute(
+    //   |--_____: (
+    p << "       : (";
+    llvm::interleaveComma(refArgRange, p,
+                          [&](BlockArgument arg) { p << arg.getType(); });
+    p << ")";
+    p.printNewline();
+    //   |--execute(
+    //   |--____-> (
+    p << "      -> (";
+    OperandRange currSizes = dynamicSizes;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      ShapedType resultType = cast<ShapedType>(resultTypes[i]);
+      bool isResultTied = isTied[i];
+      p << resultType;
+      if (!isResultTied && !resultType.hasStaticShape()) {
+        int64_t numDynamicDims = resultType.getNumDynamicDims();
+        p << "{";
+        llvm::interleaveComma(currSizes.take_front(numDynamicDims), p,
+                              [&](Value dim) { p << dim; });
+        currSizes = currSizes.drop_front(numDynamicDims);
+        p << "}";
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ") ";
+  } else {
+    // Print a space before the region brace if there are no loop results.
+    p << " ";
+  }
+  p.printRegion(body, /*printEntryBlockArgs=*/false,
+                /*printBlockTerminators=*/true);
+}
+
+static void printParallelExecutionBody(OpAsmPrinter &p, Operation *op,
+                                       OperandRange inits, TypeRange initTypes,
+                                       OperandRange dynamicSizes,
+                                       TypeRange resultTypes,
+                                       ArrayRef<bool> isTied, Region &body) {
+  return printParallelExecutionBody(p, op, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, 0, false);
+}
+
+//===----------------------------------------------------------------------===//
+// GenericOp
+//===----------------------------------------------------------------------===//
+
+static ParseResult parseInferNumIndexArgs(OpAsmParser &parser, Region &body,
+                                          int64_t &numLeadingArgs,
+                                          int64_t &numIndexArgs) {
+  numIndexArgs = 0;
+  for (BlockArgument bbArg :
+       llvm::reverse(body.getArguments().drop_front(numLeadingArgs))) {
+    if (!bbArg.getType().isIndex()) {
+      return success();
+    }
+    ++numIndexArgs;
+  }
+  return success();
+}
+
+static void printInferNumIndexArgs(OpAsmPrinter &, Operation *, Region &,
+                                   int64_t &, int64_t) {
+  // Nothing to do. The number of count args gets parsed solely from the region.
+}
+
+void GenericOp::getAsmBlockArgumentNames(Region &region,
+                                         OpAsmSetValueNameFn setNameFn) {
+  if (&region == &getInitializer()) {
+    return;
+  }
+
+  assert(&region == &getRegion() && "Unexpected region");
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getCountArgs()) {
+    setNameFn(v, "count");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult GenericOp::verify() {
+  if (getNumIndexArgs() % 2 != 0) {
+    return emitOpError("expected even number of id + count args");
+  }
+  if (getRegion().front().getNumArguments() < getNumIndexArgs()) {
+    return emitOpError(
+        "fewer body arguments than specified number of counts/ids.");
+  }
+  return verifyParallelBodyOp(*this, getNumLeadingArgs(), getNumIndexArgs(),
+                              getIdAndCountArgs());
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, int64_t numIterators,
+                      bool syncOnReturn) {
+  GenericOp::build(b, result, TypeRange(), scope, ArrayRef<Value>{},
+                   ArrayRef<Value>{}, ArrayRef<bool>{}, numIterators,
+                   syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, ValueRange inits, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  GenericOp::build(b, result, resultTypes, scope, inits, ArrayRef<Value>{},
+                   isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope,
+                      ValueRange dynamicSizes, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  GenericOp::build(b, result, resultTypes, scope, ArrayRef<Value>{},
+                   dynamicSizes, isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope, ValueRange inits,
+                      ValueRange dynamicSizes, ArrayRef<bool> isTied,
+                      int64_t numIterators, bool syncOnReturn) {
+
+  result.addAttribute(GenericOp::getScopeAttrName(result.name), scope);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+  inherentAttrs.setNumIndexArgs(2 * numIterators);
+
+  // Add the initializer region.
+  result.addRegion();
+
+  // Add the main region.
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count/id args.
+  Type indexType = b.getIndexType();
+  for (int64_t i = 0; i < 2 * numIterators; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// LoopOp
+//===----------------------------------------------------------------------===//
+
+void LoopOp::getAsmBlockArgumentNames(Region &region,
+                                      OpAsmSetValueNameFn setNameFn) {
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult LoopOp::verify() {
+  if (getCount().empty()) {
+    return emitOpError("expected at least one iteration count argument");
+  }
+  if (getBody()->getNumArguments() < getNumIdArgs()) {
+    return emitOpError("fewer body arguments than specified number of ids.");
+  }
+  return verifyParallelBodyOp(*this, /*numLeadingArgs=*/0, getNumIdArgs(),
+                              getIdArgs());
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, bool syncOnReturn) {
+  LoopOp::build(b, result, TypeRange(), scope, count, ArrayRef<Value>{},
+                ArrayRef<Value>{}, ArrayRef<bool>{}, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, ValueRange inits,
+                   bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  LoopOp::build(b, result, resultTypes, scope, count, inits, ArrayRef<Value>{},
+                isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange dynamicSizes, bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  LoopOp::build(b, result, resultTypes, scope, count, ArrayRef<Value>{},
+                dynamicSizes, isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange inits, ValueRange dynamicSizes,
+                   ArrayRef<bool> isTied, bool syncOnReturn) {
+
+  result.addAttribute(LoopOp::getScopeAttrName(result.name), scope);
+  result.addOperands(count);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(count.size()),
+                              static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count args.
+  Type indexType = b.getIndexType();
+  int64_t numCountArgs = count.size() == 0 ? 1 : count.size();
+  for (int64_t i = 0; i < numCountArgs; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+void LoopOp::getSuccessorRegions(RegionBranchPoint point,
+                                 SmallVectorImpl<RegionSuccessor> &regions) {
+  // If the predecessor is the GenericOp, branch into the body.
+  if (point.isParent()) {
+    regions.push_back(RegionSuccessor(&getRegion()));
+    return;
+  }
+
+  // Otherwise, the region branches back to the parent operation.
+  regions.push_back(RegionSuccessor(getOperation(), getResults()));
+}
+
+//===----------------------------------------------------------------------===//
+// Control Flow Ops
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// BranchCondReturnOp
+//===----------------------------------------------------------------------===//
+
+void BranchCondReturnOp::setDest(Block *block) { return setSuccessor(block); }
+
+void BranchCondReturnOp::eraseOperand(unsigned index) {
+  (*this)->eraseOperand(index);
+}
+
+SuccessorOperands BranchCondReturnOp::getSuccessorOperands(unsigned index) {
+  assert(index == 0 && "invalid successor index");
+  // Single index operand produced by this op.
+  return SuccessorOperands(getDestOperandsMutable());
+}
+
+Block *
+BranchCondReturnOp::getSuccessorForOperands(ArrayRef<Attribute> operands) {
+  if (IntegerAttr condAttr =
+          llvm::dyn_cast_or_null<IntegerAttr>(operands.front())) {
+    return condAttr.getValue().isOne() ? nullptr : getDest();
+  }
+  return nullptr;
+}
+
+//===----------------------------------------------------------------------===//
+// WriteOps
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// ParallelInsertSliceOp
+//===----------------------------------------------------------------------===//
+
+// Build a WriteSliceOp with mixed static and dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<OpFoldResult> offsets,
+                         ArrayRef<OpFoldResult> sizes,
+                         ArrayRef<OpFoldResult> strides,
+                         ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, {}, source, dest, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+/// Build an WriteSliceOp with mixed static and dynamic entries
+/// packed into a Range vector.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<Range> ranges,
+                         ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, source, dest, offsets, sizes, strides, attrs);
+}
+
+// Build a WriteSliceOp with dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ValueRange offsets, ValueRange sizes,
+                         ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, source, dest, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// ReadSliceOp
+//===----------------------------------------------------------------------===//
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// GetMemrefOp
+//===----------------------------------------------------------------------===//
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// Folders
+//===----------------------------------------------------------------------===//
+
+LogicalResult WriteSliceOp::fold(FoldAdaptor adaptor,
+                                 SmallVectorImpl<OpFoldResult> &results) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        stride = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  if (!changed) {
+    return failure();
+  }
+
+  // Dispatch back to static/dynamic.
+  SmallVector<int64_t> staticOffsets, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicStrides;
+  dispatchIndexOpFoldResults(mixedOffsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(mixedStrides, dynamicStrides, staticStrides);
+
+  // Update the op's attributes in-place.
+  setStaticOffsetsAttr(builder.getDenseI64ArrayAttr(staticOffsets));
+  setStaticStridesAttr(builder.getDenseI64ArrayAttr(staticStrides));
+  getOffsetsMutable().assign(dynamicOffsets);
+  getStridesMutable().assign(dynamicStrides);
+
+  return success();
+}
+
+OpFoldResult ReadSliceOp::fold(FoldAdaptor adaptor) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
```

**Comment:**
type

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.cpp:823`

```diff
@@ -0,0 +1,952 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include <numeric>
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/Dialect/Utils/StaticValueUtils.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/TypeUtilities.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// AllocOps
+//===----------------------------------------------------------------------===//
+
+LogicalResult AllocOp::verify() {
+  if (getDynamicSizes().size() != getResultType().getNumDynamicDims()) {
+    return emitOpError(
+        "dimension operand count does not equal sref dynamic dimension count");
+  }
+
+  // TODO: Restrict legal parents for this op.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// StructuralOps
+//===----------------------------------------------------------------------===//
+
+template <typename OpTy>
+static LogicalResult verifyParallelBodyOp(OpTy op, int64_t numLeadingArgs,
+                                          int64_t numIndexBodyArgs,
+                                          ArrayRef<BlockArgument> indexArgs) {
+  // Verify tied/token array lengths.
+  ArrayRef<bool> isTied = op.getIsTied();
+  int64_t numResults = op.getNumResults();
+  if (isTied.size() != numResults) {
+    return op.emitOpError(
+               "`is_tied` mask length expected to match number of results ")
+           << numResults;
+  }
+
+  int64_t numInits =
+      std::accumulate(isTied.begin(), isTied.end(), (int64_t)(0));
+  if (op.getInits().size() != numInits) {
+    return op.emitOpError("number of inits ")
+           << op.getInits().size()
+           << " does not match the number of results marked as tied "
+           << numInits;
+  }
+
+  if (op.getRegion().getArguments().size() !=
+      numLeadingArgs + numResults + numIndexBodyArgs) {
+    return op.emitOpError("expected region to have |numLeadingArgs| + "
+                          "|numIndexArgs| + |numResults| "
+                          "total arguments");
+  }
+
+  for (BlockArgument countArg : indexArgs) {
+    if (!countArg.getType().isIndex()) {
+      return op.emitOpError(
+          "expected index type for thread count/id region arguments");
+    }
+  }
+
+  PCF::ScopeAttr scope = op.getScope();
+  int64_t currIsTiedIndex = 0;
+  int64_t currResultIndex = 0;
+  for (auto [resultType, refArg, isTied] : llvm::zip_equal(
+           op.getResultTypes(), op.getRegionRefArgs(), op.getIsTied())) {
+    auto srefType = dyn_cast<PCF::ShapedRefType>(refArg.getType());
+    if (!srefType || srefType.getScope() != scope) {
+      return op.emitOpError(
+                 "expected region ref argument to be of type !pcf.sref "
+                 "with scope ")
+             << scope;
+    }
+    if (!srefType.isParentScopeOnlySync() && srefType.getSyncScope()) {
+      return op.emitOpError(
+          "expected region ref argument to have none or parent sync scope");
+    }
+
+    // Traits guarantee this cast to be valid.
+    auto shapedResultType = cast<ShapedType>(resultType);
+    if (shapedResultType.getShape() != srefType.getShape()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " with type " << srefType
+             << " shape mismatch with tied result of type " << resultType;
+    }
+
+    if (shapedResultType.getElementType() != srefType.getElementType()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " element type mismatch of "
+             << srefType.getElementType() << " vs "
+             << shapedResultType.getElementType();
+    }
+
+    if (isTied) {
+      Value init = op.getInits()[currIsTiedIndex];
+      if (init.getType() != resultType) {
+        return op.emitOpError("tied init at index ")
+               << currIsTiedIndex << " does not match the type " << resultType
+               << " at result index " << currResultIndex;
+      }
+      ++currIsTiedIndex;
+    }
+    ++currResultIndex;
+  }
+  return success();
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body, int64_t &numLeadingArgs, bool parseOptionalLeadingArgs) {
+  SmallVector<OpAsmParser::Argument> regionLeadingArgs;
+  if (parseOptionalLeadingArgs) {
+    if (succeeded(parser.parseOptionalArrow())) {
+      if (failed(parser.parseArgumentList(regionLeadingArgs,
+                                          OpAsmParser::Delimiter::Paren,
+                                          /*allowType=*/true))) {
+        return failure();
+      }
+    }
+    numLeadingArgs = regionLeadingArgs.size();
+  }
+  if (failed(parser.parseKeyword("execute")))
+    return failure();
+  SmallVector<OpAsmParser::Argument> regionRefArgs;
+  if (succeeded(parser.parseOptionalLParen())) {
+    do {
+      // Reserve entries in the lists.
+      regionRefArgs.emplace_back();
+      if (failed(parser.parseArgument(regionRefArgs.back(),
+                                      /*allowType=*/false,
+                                      /*allowAttrs=*/true))) {
+        return failure();
+      }
+
+      // Parse the tied init if present.
+      if (succeeded(parser.parseOptionalEqual())) {
+        inits.emplace_back();
+        if (failed(parser.parseOperand(inits.back()))) {
+          return failure();
+        }
+        isTied.push_back(true);
+      } else {
+        isTied.push_back(false);
+      }
+    } while (succeeded(parser.parseOptionalComma()));
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  SmallVector<OpAsmParser::Argument> indexArgs;
+  if (failed(parser.parseLSquare())) {
+    return failure();
+  }
+
+  if (failed(parser.parseArgumentList(
+          indexArgs, /*delimiter=*/OpAsmParser::Delimiter::None,
+          /*allowType=*/true, /*allowAttrs=*/true))) {
+    return failure();
+  }
+
+  if (failed(parser.parseRSquare())) {
+    return failure();
+  }
+
+  // If there is at least one region arg the arg types and op result types need
+  // to be parsed.
+  if (!regionRefArgs.empty()) {
+    if (failed(parser.parseColon()) || failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    // Parse all types except the last followed by commas.
+    for (OpAsmParser::Argument &arg :
+         MutableArrayRef<OpAsmParser::Argument>(regionRefArgs.begin(),
+                                                regionRefArgs.end())
+             .drop_back()) {
+      if (failed(parser.parseType(arg.type)) || failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    // Parse the last type.
+    if (failed(parser.parseType(regionRefArgs.back().type))) {
+      return failure();
+    }
+
+    if (failed(parser.parseRParen()) || failed(parser.parseArrow()) ||
+        failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    int64_t numResults = isTied.size();
+    resultTypes.resize(numResults);
+    for (auto [i, isTied] : llvm::enumerate(isTied)) {
+      if (failed(parser.parseType(resultTypes[i]))) {
+        return failure();
+      }
+
+      auto shapedType = dyn_cast<ShapedType>(resultTypes[i]);
+      if (!shapedType) {
+        return failure();
+      }
+
+      if (isTied) {
+        initTypes.push_back(resultTypes[i]);
+      } else if (succeeded(parser.parseOptionalLBrace())) {
+        // Only parse dynamic dims for non-tied operands.
+        SmallVector<OpAsmParser::UnresolvedOperand> dims;
+        if (failed(parser.parseOperandList(dims))) {
+          return failure();
+        }
+        size_t numDynamicDims = shapedType.getNumDynamicDims();
+        if (dims.size() != numDynamicDims) {
+          return failure();
+        }
+        if (failed(parser.parseRBrace())) {
+          return failure();
+        }
+        dynamicSizes.append(dims);
+      }
+
+      if (i < numResults - 1 && failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  // The stored argument order is:
+  // (initialized vals) (result tied refs) (num threads).
+  SmallVector<OpAsmParser::Argument> args;
+  args.append(regionLeadingArgs);
+  args.append(regionRefArgs);
+  args.append(indexArgs);
+  return parser.parseRegion(body, args, /*enableNameShadowing=*/false);
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body) {
+  int64_t numLeadingArgs = 0;
+  return parseParallelExecutionBody(parser, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, numLeadingArgs,
+                                    false);
+}
+
+static void printParallelExecutionBody(
+    OpAsmPrinter &p, Operation *op, OperandRange inits, TypeRange initTypes,
+    OperandRange dynamicSizes, TypeRange resultTypes, ArrayRef<bool> isTied,
+    Region &body, int64_t numLeadingArgs, bool printOptionalLeadingArgs) {
+  if (printOptionalLeadingArgs && numLeadingArgs > 0) {
+    p << "-> (";
+    MutableArrayRef<BlockArgument> leadingArgRange =
+        body.getArguments().take_front(numLeadingArgs);
+    llvm::interleaveComma(leadingArgRange, p, [&](BlockArgument arg) {
+      p.printRegionArgument(arg);
+    });
+    p << ")";
+  }
+
+  p.printNewline();
+  p << "  execute";
+
+  int64_t numResults = resultTypes.size();
+  int64_t numIndexArgs = body.getNumArguments() - numResults - numLeadingArgs;
+  MutableArrayRef<BlockArgument> threadCountArgRange =
+      body.getArguments().take_back(numIndexArgs);
+  MutableArrayRef<BlockArgument> refArgRange =
+      body.getArguments().drop_back(numIndexArgs).take_back(numResults);
+
+  if (numResults != 0) {
+    p << "(";
+    int64_t currInitIndex = 0;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      p << refArgRange[i];
+      if (isTied[i]) {
+        p << " = ";
+        p << inits[currInitIndex];
+        ++currInitIndex;
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ")";
+  }
+  p << "[";
+  llvm::interleaveComma(threadCountArgRange, p,
+                        [&](BlockArgument arg) { p.printRegionArgument(arg); });
+  p << "]";
+
+  // Now print the function type.
+  if (numResults != 0) {
+    p.printNewline();
+    // Whitespace to line up parentheses.
+    //   |--execute(
+    //   |--_____: (
+    p << "       : (";
+    llvm::interleaveComma(refArgRange, p,
+                          [&](BlockArgument arg) { p << arg.getType(); });
+    p << ")";
+    p.printNewline();
+    //   |--execute(
+    //   |--____-> (
+    p << "      -> (";
+    OperandRange currSizes = dynamicSizes;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      ShapedType resultType = cast<ShapedType>(resultTypes[i]);
+      bool isResultTied = isTied[i];
+      p << resultType;
+      if (!isResultTied && !resultType.hasStaticShape()) {
+        int64_t numDynamicDims = resultType.getNumDynamicDims();
+        p << "{";
+        llvm::interleaveComma(currSizes.take_front(numDynamicDims), p,
+                              [&](Value dim) { p << dim; });
+        currSizes = currSizes.drop_front(numDynamicDims);
+        p << "}";
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ") ";
+  } else {
+    // Print a space before the region brace if there are no loop results.
+    p << " ";
+  }
+  p.printRegion(body, /*printEntryBlockArgs=*/false,
+                /*printBlockTerminators=*/true);
+}
+
+static void printParallelExecutionBody(OpAsmPrinter &p, Operation *op,
+                                       OperandRange inits, TypeRange initTypes,
+                                       OperandRange dynamicSizes,
+                                       TypeRange resultTypes,
+                                       ArrayRef<bool> isTied, Region &body) {
+  return printParallelExecutionBody(p, op, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, 0, false);
+}
+
+//===----------------------------------------------------------------------===//
+// GenericOp
+//===----------------------------------------------------------------------===//
+
+static ParseResult parseInferNumIndexArgs(OpAsmParser &parser, Region &body,
+                                          int64_t &numLeadingArgs,
+                                          int64_t &numIndexArgs) {
+  numIndexArgs = 0;
+  for (BlockArgument bbArg :
+       llvm::reverse(body.getArguments().drop_front(numLeadingArgs))) {
+    if (!bbArg.getType().isIndex()) {
+      return success();
+    }
+    ++numIndexArgs;
+  }
+  return success();
+}
+
+static void printInferNumIndexArgs(OpAsmPrinter &, Operation *, Region &,
+                                   int64_t &, int64_t) {
+  // Nothing to do. The number of count args gets parsed solely from the region.
+}
+
+void GenericOp::getAsmBlockArgumentNames(Region &region,
+                                         OpAsmSetValueNameFn setNameFn) {
+  if (&region == &getInitializer()) {
+    return;
+  }
+
+  assert(&region == &getRegion() && "Unexpected region");
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getCountArgs()) {
+    setNameFn(v, "count");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult GenericOp::verify() {
+  if (getNumIndexArgs() % 2 != 0) {
+    return emitOpError("expected even number of id + count args");
+  }
+  if (getRegion().front().getNumArguments() < getNumIndexArgs()) {
+    return emitOpError(
+        "fewer body arguments than specified number of counts/ids.");
+  }
+  return verifyParallelBodyOp(*this, getNumLeadingArgs(), getNumIndexArgs(),
+                              getIdAndCountArgs());
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, int64_t numIterators,
+                      bool syncOnReturn) {
+  GenericOp::build(b, result, TypeRange(), scope, ArrayRef<Value>{},
+                   ArrayRef<Value>{}, ArrayRef<bool>{}, numIterators,
+                   syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, ValueRange inits, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  GenericOp::build(b, result, resultTypes, scope, inits, ArrayRef<Value>{},
+                   isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope,
+                      ValueRange dynamicSizes, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  GenericOp::build(b, result, resultTypes, scope, ArrayRef<Value>{},
+                   dynamicSizes, isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope, ValueRange inits,
+                      ValueRange dynamicSizes, ArrayRef<bool> isTied,
+                      int64_t numIterators, bool syncOnReturn) {
+
+  result.addAttribute(GenericOp::getScopeAttrName(result.name), scope);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+  inherentAttrs.setNumIndexArgs(2 * numIterators);
+
+  // Add the initializer region.
+  result.addRegion();
+
+  // Add the main region.
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count/id args.
+  Type indexType = b.getIndexType();
+  for (int64_t i = 0; i < 2 * numIterators; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// LoopOp
+//===----------------------------------------------------------------------===//
+
+void LoopOp::getAsmBlockArgumentNames(Region &region,
+                                      OpAsmSetValueNameFn setNameFn) {
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult LoopOp::verify() {
+  if (getCount().empty()) {
+    return emitOpError("expected at least one iteration count argument");
+  }
+  if (getBody()->getNumArguments() < getNumIdArgs()) {
+    return emitOpError("fewer body arguments than specified number of ids.");
+  }
+  return verifyParallelBodyOp(*this, /*numLeadingArgs=*/0, getNumIdArgs(),
+                              getIdArgs());
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, bool syncOnReturn) {
+  LoopOp::build(b, result, TypeRange(), scope, count, ArrayRef<Value>{},
+                ArrayRef<Value>{}, ArrayRef<bool>{}, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, ValueRange inits,
+                   bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  LoopOp::build(b, result, resultTypes, scope, count, inits, ArrayRef<Value>{},
+                isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange dynamicSizes, bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  LoopOp::build(b, result, resultTypes, scope, count, ArrayRef<Value>{},
+                dynamicSizes, isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange inits, ValueRange dynamicSizes,
+                   ArrayRef<bool> isTied, bool syncOnReturn) {
+
+  result.addAttribute(LoopOp::getScopeAttrName(result.name), scope);
+  result.addOperands(count);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(count.size()),
+                              static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count args.
+  Type indexType = b.getIndexType();
+  int64_t numCountArgs = count.size() == 0 ? 1 : count.size();
+  for (int64_t i = 0; i < numCountArgs; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+void LoopOp::getSuccessorRegions(RegionBranchPoint point,
+                                 SmallVectorImpl<RegionSuccessor> &regions) {
+  // If the predecessor is the GenericOp, branch into the body.
+  if (point.isParent()) {
+    regions.push_back(RegionSuccessor(&getRegion()));
+    return;
+  }
+
+  // Otherwise, the region branches back to the parent operation.
+  regions.push_back(RegionSuccessor(getOperation(), getResults()));
+}
+
+//===----------------------------------------------------------------------===//
+// Control Flow Ops
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// BranchCondReturnOp
+//===----------------------------------------------------------------------===//
+
+void BranchCondReturnOp::setDest(Block *block) { return setSuccessor(block); }
+
+void BranchCondReturnOp::eraseOperand(unsigned index) {
+  (*this)->eraseOperand(index);
+}
+
+SuccessorOperands BranchCondReturnOp::getSuccessorOperands(unsigned index) {
+  assert(index == 0 && "invalid successor index");
+  // Single index operand produced by this op.
+  return SuccessorOperands(getDestOperandsMutable());
+}
+
+Block *
+BranchCondReturnOp::getSuccessorForOperands(ArrayRef<Attribute> operands) {
+  if (IntegerAttr condAttr =
+          llvm::dyn_cast_or_null<IntegerAttr>(operands.front())) {
+    return condAttr.getValue().isOne() ? nullptr : getDest();
+  }
+  return nullptr;
+}
+
+//===----------------------------------------------------------------------===//
+// WriteOps
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// ParallelInsertSliceOp
+//===----------------------------------------------------------------------===//
+
+// Build a WriteSliceOp with mixed static and dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<OpFoldResult> offsets,
+                         ArrayRef<OpFoldResult> sizes,
+                         ArrayRef<OpFoldResult> strides,
+                         ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, {}, source, dest, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+/// Build an WriteSliceOp with mixed static and dynamic entries
+/// packed into a Range vector.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<Range> ranges,
+                         ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, source, dest, offsets, sizes, strides, attrs);
+}
+
+// Build a WriteSliceOp with dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ValueRange offsets, ValueRange sizes,
+                         ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, source, dest, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// ReadSliceOp
+//===----------------------------------------------------------------------===//
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// GetMemrefOp
+//===----------------------------------------------------------------------===//
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// Folders
+//===----------------------------------------------------------------------===//
+
+LogicalResult WriteSliceOp::fold(FoldAdaptor adaptor,
+                                 SmallVectorImpl<OpFoldResult> &results) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        stride = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  if (!changed) {
+    return failure();
+  }
+
+  // Dispatch back to static/dynamic.
+  SmallVector<int64_t> staticOffsets, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicStrides;
+  dispatchIndexOpFoldResults(mixedOffsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(mixedStrides, dynamicStrides, staticStrides);
+
+  // Update the op's attributes in-place.
+  setStaticOffsetsAttr(builder.getDenseI64ArrayAttr(staticOffsets));
+  setStaticStridesAttr(builder.getDenseI64ArrayAttr(staticStrides));
+  getOffsetsMutable().assign(dynamicOffsets);
+  getStridesMutable().assign(dynamicStrides);
+
+  return success();
+}
+
+OpFoldResult ReadSliceOp::fold(FoldAdaptor adaptor) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
```

**Comment:**
type

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.cpp:815`

```diff
@@ -0,0 +1,952 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include <numeric>
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/Dialect/Utils/StaticValueUtils.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/TypeUtilities.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// AllocOps
+//===----------------------------------------------------------------------===//
+
+LogicalResult AllocOp::verify() {
+  if (getDynamicSizes().size() != getResultType().getNumDynamicDims()) {
+    return emitOpError(
+        "dimension operand count does not equal sref dynamic dimension count");
+  }
+
+  // TODO: Restrict legal parents for this op.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// StructuralOps
+//===----------------------------------------------------------------------===//
+
+template <typename OpTy>
+static LogicalResult verifyParallelBodyOp(OpTy op, int64_t numLeadingArgs,
+                                          int64_t numIndexBodyArgs,
+                                          ArrayRef<BlockArgument> indexArgs) {
+  // Verify tied/token array lengths.
+  ArrayRef<bool> isTied = op.getIsTied();
+  int64_t numResults = op.getNumResults();
+  if (isTied.size() != numResults) {
+    return op.emitOpError(
+               "`is_tied` mask length expected to match number of results ")
+           << numResults;
+  }
+
+  int64_t numInits =
+      std::accumulate(isTied.begin(), isTied.end(), (int64_t)(0));
+  if (op.getInits().size() != numInits) {
+    return op.emitOpError("number of inits ")
+           << op.getInits().size()
+           << " does not match the number of results marked as tied "
+           << numInits;
+  }
+
+  if (op.getRegion().getArguments().size() !=
+      numLeadingArgs + numResults + numIndexBodyArgs) {
+    return op.emitOpError("expected region to have |numLeadingArgs| + "
+                          "|numIndexArgs| + |numResults| "
+                          "total arguments");
+  }
+
+  for (BlockArgument countArg : indexArgs) {
+    if (!countArg.getType().isIndex()) {
+      return op.emitOpError(
+          "expected index type for thread count/id region arguments");
+    }
+  }
+
+  PCF::ScopeAttr scope = op.getScope();
+  int64_t currIsTiedIndex = 0;
+  int64_t currResultIndex = 0;
+  for (auto [resultType, refArg, isTied] : llvm::zip_equal(
+           op.getResultTypes(), op.getRegionRefArgs(), op.getIsTied())) {
+    auto srefType = dyn_cast<PCF::ShapedRefType>(refArg.getType());
+    if (!srefType || srefType.getScope() != scope) {
+      return op.emitOpError(
+                 "expected region ref argument to be of type !pcf.sref "
+                 "with scope ")
+             << scope;
+    }
+    if (!srefType.isParentScopeOnlySync() && srefType.getSyncScope()) {
+      return op.emitOpError(
+          "expected region ref argument to have none or parent sync scope");
+    }
+
+    // Traits guarantee this cast to be valid.
+    auto shapedResultType = cast<ShapedType>(resultType);
+    if (shapedResultType.getShape() != srefType.getShape()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " with type " << srefType
+             << " shape mismatch with tied result of type " << resultType;
+    }
+
+    if (shapedResultType.getElementType() != srefType.getElementType()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " element type mismatch of "
+             << srefType.getElementType() << " vs "
+             << shapedResultType.getElementType();
+    }
+
+    if (isTied) {
+      Value init = op.getInits()[currIsTiedIndex];
+      if (init.getType() != resultType) {
+        return op.emitOpError("tied init at index ")
+               << currIsTiedIndex << " does not match the type " << resultType
+               << " at result index " << currResultIndex;
+      }
+      ++currIsTiedIndex;
+    }
+    ++currResultIndex;
+  }
+  return success();
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body, int64_t &numLeadingArgs, bool parseOptionalLeadingArgs) {
+  SmallVector<OpAsmParser::Argument> regionLeadingArgs;
+  if (parseOptionalLeadingArgs) {
+    if (succeeded(parser.parseOptionalArrow())) {
+      if (failed(parser.parseArgumentList(regionLeadingArgs,
+                                          OpAsmParser::Delimiter::Paren,
+                                          /*allowType=*/true))) {
+        return failure();
+      }
+    }
+    numLeadingArgs = regionLeadingArgs.size();
+  }
+  if (failed(parser.parseKeyword("execute")))
+    return failure();
+  SmallVector<OpAsmParser::Argument> regionRefArgs;
+  if (succeeded(parser.parseOptionalLParen())) {
+    do {
+      // Reserve entries in the lists.
+      regionRefArgs.emplace_back();
+      if (failed(parser.parseArgument(regionRefArgs.back(),
+                                      /*allowType=*/false,
+                                      /*allowAttrs=*/true))) {
+        return failure();
+      }
+
+      // Parse the tied init if present.
+      if (succeeded(parser.parseOptionalEqual())) {
+        inits.emplace_back();
+        if (failed(parser.parseOperand(inits.back()))) {
+          return failure();
+        }
+        isTied.push_back(true);
+      } else {
+        isTied.push_back(false);
+      }
+    } while (succeeded(parser.parseOptionalComma()));
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  SmallVector<OpAsmParser::Argument> indexArgs;
+  if (failed(parser.parseLSquare())) {
+    return failure();
+  }
+
+  if (failed(parser.parseArgumentList(
+          indexArgs, /*delimiter=*/OpAsmParser::Delimiter::None,
+          /*allowType=*/true, /*allowAttrs=*/true))) {
+    return failure();
+  }
+
+  if (failed(parser.parseRSquare())) {
+    return failure();
+  }
+
+  // If there is at least one region arg the arg types and op result types need
+  // to be parsed.
+  if (!regionRefArgs.empty()) {
+    if (failed(parser.parseColon()) || failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    // Parse all types except the last followed by commas.
+    for (OpAsmParser::Argument &arg :
+         MutableArrayRef<OpAsmParser::Argument>(regionRefArgs.begin(),
+                                                regionRefArgs.end())
+             .drop_back()) {
+      if (failed(parser.parseType(arg.type)) || failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    // Parse the last type.
+    if (failed(parser.parseType(regionRefArgs.back().type))) {
+      return failure();
+    }
+
+    if (failed(parser.parseRParen()) || failed(parser.parseArrow()) ||
+        failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    int64_t numResults = isTied.size();
+    resultTypes.resize(numResults);
+    for (auto [i, isTied] : llvm::enumerate(isTied)) {
+      if (failed(parser.parseType(resultTypes[i]))) {
+        return failure();
+      }
+
+      auto shapedType = dyn_cast<ShapedType>(resultTypes[i]);
+      if (!shapedType) {
+        return failure();
+      }
+
+      if (isTied) {
+        initTypes.push_back(resultTypes[i]);
+      } else if (succeeded(parser.parseOptionalLBrace())) {
+        // Only parse dynamic dims for non-tied operands.
+        SmallVector<OpAsmParser::UnresolvedOperand> dims;
+        if (failed(parser.parseOperandList(dims))) {
+          return failure();
+        }
+        size_t numDynamicDims = shapedType.getNumDynamicDims();
+        if (dims.size() != numDynamicDims) {
+          return failure();
+        }
+        if (failed(parser.parseRBrace())) {
+          return failure();
+        }
+        dynamicSizes.append(dims);
+      }
+
+      if (i < numResults - 1 && failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  // The stored argument order is:
+  // (initialized vals) (result tied refs) (num threads).
+  SmallVector<OpAsmParser::Argument> args;
+  args.append(regionLeadingArgs);
+  args.append(regionRefArgs);
+  args.append(indexArgs);
+  return parser.parseRegion(body, args, /*enableNameShadowing=*/false);
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body) {
+  int64_t numLeadingArgs = 0;
+  return parseParallelExecutionBody(parser, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, numLeadingArgs,
+                                    false);
+}
+
+static void printParallelExecutionBody(
+    OpAsmPrinter &p, Operation *op, OperandRange inits, TypeRange initTypes,
+    OperandRange dynamicSizes, TypeRange resultTypes, ArrayRef<bool> isTied,
+    Region &body, int64_t numLeadingArgs, bool printOptionalLeadingArgs) {
+  if (printOptionalLeadingArgs && numLeadingArgs > 0) {
+    p << "-> (";
+    MutableArrayRef<BlockArgument> leadingArgRange =
+        body.getArguments().take_front(numLeadingArgs);
+    llvm::interleaveComma(leadingArgRange, p, [&](BlockArgument arg) {
+      p.printRegionArgument(arg);
+    });
+    p << ")";
+  }
+
+  p.printNewline();
+  p << "  execute";
+
+  int64_t numResults = resultTypes.size();
+  int64_t numIndexArgs = body.getNumArguments() - numResults - numLeadingArgs;
+  MutableArrayRef<BlockArgument> threadCountArgRange =
+      body.getArguments().take_back(numIndexArgs);
+  MutableArrayRef<BlockArgument> refArgRange =
+      body.getArguments().drop_back(numIndexArgs).take_back(numResults);
+
+  if (numResults != 0) {
+    p << "(";
+    int64_t currInitIndex = 0;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      p << refArgRange[i];
+      if (isTied[i]) {
+        p << " = ";
+        p << inits[currInitIndex];
+        ++currInitIndex;
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ")";
+  }
+  p << "[";
+  llvm::interleaveComma(threadCountArgRange, p,
+                        [&](BlockArgument arg) { p.printRegionArgument(arg); });
+  p << "]";
+
+  // Now print the function type.
+  if (numResults != 0) {
+    p.printNewline();
+    // Whitespace to line up parentheses.
+    //   |--execute(
+    //   |--_____: (
+    p << "       : (";
+    llvm::interleaveComma(refArgRange, p,
+                          [&](BlockArgument arg) { p << arg.getType(); });
+    p << ")";
+    p.printNewline();
+    //   |--execute(
+    //   |--____-> (
+    p << "      -> (";
+    OperandRange currSizes = dynamicSizes;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      ShapedType resultType = cast<ShapedType>(resultTypes[i]);
+      bool isResultTied = isTied[i];
+      p << resultType;
+      if (!isResultTied && !resultType.hasStaticShape()) {
+        int64_t numDynamicDims = resultType.getNumDynamicDims();
+        p << "{";
+        llvm::interleaveComma(currSizes.take_front(numDynamicDims), p,
+                              [&](Value dim) { p << dim; });
+        currSizes = currSizes.drop_front(numDynamicDims);
+        p << "}";
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ") ";
+  } else {
+    // Print a space before the region brace if there are no loop results.
+    p << " ";
+  }
+  p.printRegion(body, /*printEntryBlockArgs=*/false,
+                /*printBlockTerminators=*/true);
+}
+
+static void printParallelExecutionBody(OpAsmPrinter &p, Operation *op,
+                                       OperandRange inits, TypeRange initTypes,
+                                       OperandRange dynamicSizes,
+                                       TypeRange resultTypes,
+                                       ArrayRef<bool> isTied, Region &body) {
+  return printParallelExecutionBody(p, op, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, 0, false);
+}
+
+//===----------------------------------------------------------------------===//
+// GenericOp
+//===----------------------------------------------------------------------===//
+
+static ParseResult parseInferNumIndexArgs(OpAsmParser &parser, Region &body,
+                                          int64_t &numLeadingArgs,
+                                          int64_t &numIndexArgs) {
+  numIndexArgs = 0;
+  for (BlockArgument bbArg :
+       llvm::reverse(body.getArguments().drop_front(numLeadingArgs))) {
+    if (!bbArg.getType().isIndex()) {
+      return success();
+    }
+    ++numIndexArgs;
+  }
+  return success();
+}
+
+static void printInferNumIndexArgs(OpAsmPrinter &, Operation *, Region &,
+                                   int64_t &, int64_t) {
+  // Nothing to do. The number of count args gets parsed solely from the region.
+}
+
+void GenericOp::getAsmBlockArgumentNames(Region &region,
+                                         OpAsmSetValueNameFn setNameFn) {
+  if (&region == &getInitializer()) {
+    return;
+  }
+
+  assert(&region == &getRegion() && "Unexpected region");
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getCountArgs()) {
+    setNameFn(v, "count");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult GenericOp::verify() {
+  if (getNumIndexArgs() % 2 != 0) {
+    return emitOpError("expected even number of id + count args");
+  }
+  if (getRegion().front().getNumArguments() < getNumIndexArgs()) {
+    return emitOpError(
+        "fewer body arguments than specified number of counts/ids.");
+  }
+  return verifyParallelBodyOp(*this, getNumLeadingArgs(), getNumIndexArgs(),
+                              getIdAndCountArgs());
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, int64_t numIterators,
+                      bool syncOnReturn) {
+  GenericOp::build(b, result, TypeRange(), scope, ArrayRef<Value>{},
+                   ArrayRef<Value>{}, ArrayRef<bool>{}, numIterators,
+                   syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, ValueRange inits, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  GenericOp::build(b, result, resultTypes, scope, inits, ArrayRef<Value>{},
+                   isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope,
+                      ValueRange dynamicSizes, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  GenericOp::build(b, result, resultTypes, scope, ArrayRef<Value>{},
+                   dynamicSizes, isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope, ValueRange inits,
+                      ValueRange dynamicSizes, ArrayRef<bool> isTied,
+                      int64_t numIterators, bool syncOnReturn) {
+
+  result.addAttribute(GenericOp::getScopeAttrName(result.name), scope);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+  inherentAttrs.setNumIndexArgs(2 * numIterators);
+
+  // Add the initializer region.
+  result.addRegion();
+
+  // Add the main region.
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count/id args.
+  Type indexType = b.getIndexType();
+  for (int64_t i = 0; i < 2 * numIterators; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// LoopOp
+//===----------------------------------------------------------------------===//
+
+void LoopOp::getAsmBlockArgumentNames(Region &region,
+                                      OpAsmSetValueNameFn setNameFn) {
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult LoopOp::verify() {
+  if (getCount().empty()) {
+    return emitOpError("expected at least one iteration count argument");
+  }
+  if (getBody()->getNumArguments() < getNumIdArgs()) {
+    return emitOpError("fewer body arguments than specified number of ids.");
+  }
+  return verifyParallelBodyOp(*this, /*numLeadingArgs=*/0, getNumIdArgs(),
+                              getIdArgs());
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, bool syncOnReturn) {
+  LoopOp::build(b, result, TypeRange(), scope, count, ArrayRef<Value>{},
+                ArrayRef<Value>{}, ArrayRef<bool>{}, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, ValueRange inits,
+                   bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  LoopOp::build(b, result, resultTypes, scope, count, inits, ArrayRef<Value>{},
+                isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange dynamicSizes, bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  LoopOp::build(b, result, resultTypes, scope, count, ArrayRef<Value>{},
+                dynamicSizes, isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange inits, ValueRange dynamicSizes,
+                   ArrayRef<bool> isTied, bool syncOnReturn) {
+
+  result.addAttribute(LoopOp::getScopeAttrName(result.name), scope);
+  result.addOperands(count);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(count.size()),
+                              static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count args.
+  Type indexType = b.getIndexType();
+  int64_t numCountArgs = count.size() == 0 ? 1 : count.size();
+  for (int64_t i = 0; i < numCountArgs; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+void LoopOp::getSuccessorRegions(RegionBranchPoint point,
+                                 SmallVectorImpl<RegionSuccessor> &regions) {
+  // If the predecessor is the GenericOp, branch into the body.
+  if (point.isParent()) {
+    regions.push_back(RegionSuccessor(&getRegion()));
+    return;
+  }
+
+  // Otherwise, the region branches back to the parent operation.
+  regions.push_back(RegionSuccessor(getOperation(), getResults()));
+}
+
+//===----------------------------------------------------------------------===//
+// Control Flow Ops
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// BranchCondReturnOp
+//===----------------------------------------------------------------------===//
+
+void BranchCondReturnOp::setDest(Block *block) { return setSuccessor(block); }
+
+void BranchCondReturnOp::eraseOperand(unsigned index) {
+  (*this)->eraseOperand(index);
+}
+
+SuccessorOperands BranchCondReturnOp::getSuccessorOperands(unsigned index) {
+  assert(index == 0 && "invalid successor index");
+  // Single index operand produced by this op.
+  return SuccessorOperands(getDestOperandsMutable());
+}
+
+Block *
+BranchCondReturnOp::getSuccessorForOperands(ArrayRef<Attribute> operands) {
+  if (IntegerAttr condAttr =
+          llvm::dyn_cast_or_null<IntegerAttr>(operands.front())) {
+    return condAttr.getValue().isOne() ? nullptr : getDest();
+  }
+  return nullptr;
+}
+
+//===----------------------------------------------------------------------===//
+// WriteOps
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// ParallelInsertSliceOp
+//===----------------------------------------------------------------------===//
+
+// Build a WriteSliceOp with mixed static and dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<OpFoldResult> offsets,
+                         ArrayRef<OpFoldResult> sizes,
+                         ArrayRef<OpFoldResult> strides,
+                         ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, {}, source, dest, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+/// Build an WriteSliceOp with mixed static and dynamic entries
+/// packed into a Range vector.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<Range> ranges,
+                         ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, source, dest, offsets, sizes, strides, attrs);
+}
+
+// Build a WriteSliceOp with dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ValueRange offsets, ValueRange sizes,
+                         ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, source, dest, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// ReadSliceOp
+//===----------------------------------------------------------------------===//
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// GetMemrefOp
+//===----------------------------------------------------------------------===//
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// Folders
+//===----------------------------------------------------------------------===//
+
+LogicalResult WriteSliceOp::fold(FoldAdaptor adaptor,
+                                 SmallVectorImpl<OpFoldResult> &results) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        stride = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  if (!changed) {
+    return failure();
+  }
+
+  // Dispatch back to static/dynamic.
+  SmallVector<int64_t> staticOffsets, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicStrides;
+  dispatchIndexOpFoldResults(mixedOffsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(mixedStrides, dynamicStrides, staticStrides);
+
+  // Update the op's attributes in-place.
+  setStaticOffsetsAttr(builder.getDenseI64ArrayAttr(staticOffsets));
+  setStaticStridesAttr(builder.getDenseI64ArrayAttr(staticStrides));
+  getOffsetsMutable().assign(dynamicOffsets);
+  getStridesMutable().assign(dynamicStrides);
+
+  return success();
+}
+
+OpFoldResult ReadSliceOp::fold(FoldAdaptor adaptor) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
```

**Comment:**
```suggestion
      if (std::optional<int64_t> constantValue = getConstantIntValue(value)) {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.cpp:826`

```diff
@@ -0,0 +1,952 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include <numeric>
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/Dialect/Utils/StaticValueUtils.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/TypeUtilities.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// AllocOps
+//===----------------------------------------------------------------------===//
+
+LogicalResult AllocOp::verify() {
+  if (getDynamicSizes().size() != getResultType().getNumDynamicDims()) {
+    return emitOpError(
+        "dimension operand count does not equal sref dynamic dimension count");
+  }
+
+  // TODO: Restrict legal parents for this op.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// StructuralOps
+//===----------------------------------------------------------------------===//
+
+template <typename OpTy>
+static LogicalResult verifyParallelBodyOp(OpTy op, int64_t numLeadingArgs,
+                                          int64_t numIndexBodyArgs,
+                                          ArrayRef<BlockArgument> indexArgs) {
+  // Verify tied/token array lengths.
+  ArrayRef<bool> isTied = op.getIsTied();
+  int64_t numResults = op.getNumResults();
+  if (isTied.size() != numResults) {
+    return op.emitOpError(
+               "`is_tied` mask length expected to match number of results ")
+           << numResults;
+  }
+
+  int64_t numInits =
+      std::accumulate(isTied.begin(), isTied.end(), (int64_t)(0));
+  if (op.getInits().size() != numInits) {
+    return op.emitOpError("number of inits ")
+           << op.getInits().size()
+           << " does not match the number of results marked as tied "
+           << numInits;
+  }
+
+  if (op.getRegion().getArguments().size() !=
+      numLeadingArgs + numResults + numIndexBodyArgs) {
+    return op.emitOpError("expected region to have |numLeadingArgs| + "
+                          "|numIndexArgs| + |numResults| "
+                          "total arguments");
+  }
+
+  for (BlockArgument countArg : indexArgs) {
+    if (!countArg.getType().isIndex()) {
+      return op.emitOpError(
+          "expected index type for thread count/id region arguments");
+    }
+  }
+
+  PCF::ScopeAttr scope = op.getScope();
+  int64_t currIsTiedIndex = 0;
+  int64_t currResultIndex = 0;
+  for (auto [resultType, refArg, isTied] : llvm::zip_equal(
+           op.getResultTypes(), op.getRegionRefArgs(), op.getIsTied())) {
+    auto srefType = dyn_cast<PCF::ShapedRefType>(refArg.getType());
+    if (!srefType || srefType.getScope() != scope) {
+      return op.emitOpError(
+                 "expected region ref argument to be of type !pcf.sref "
+                 "with scope ")
+             << scope;
+    }
+    if (!srefType.isParentScopeOnlySync() && srefType.getSyncScope()) {
+      return op.emitOpError(
+          "expected region ref argument to have none or parent sync scope");
+    }
+
+    // Traits guarantee this cast to be valid.
+    auto shapedResultType = cast<ShapedType>(resultType);
+    if (shapedResultType.getShape() != srefType.getShape()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " with type " << srefType
+             << " shape mismatch with tied result of type " << resultType;
+    }
+
+    if (shapedResultType.getElementType() != srefType.getElementType()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " element type mismatch of "
+             << srefType.getElementType() << " vs "
+             << shapedResultType.getElementType();
+    }
+
+    if (isTied) {
+      Value init = op.getInits()[currIsTiedIndex];
+      if (init.getType() != resultType) {
+        return op.emitOpError("tied init at index ")
+               << currIsTiedIndex << " does not match the type " << resultType
+               << " at result index " << currResultIndex;
+      }
+      ++currIsTiedIndex;
+    }
+    ++currResultIndex;
+  }
+  return success();
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body, int64_t &numLeadingArgs, bool parseOptionalLeadingArgs) {
+  SmallVector<OpAsmParser::Argument> regionLeadingArgs;
+  if (parseOptionalLeadingArgs) {
+    if (succeeded(parser.parseOptionalArrow())) {
+      if (failed(parser.parseArgumentList(regionLeadingArgs,
+                                          OpAsmParser::Delimiter::Paren,
+                                          /*allowType=*/true))) {
+        return failure();
+      }
+    }
+    numLeadingArgs = regionLeadingArgs.size();
+  }
+  if (failed(parser.parseKeyword("execute")))
+    return failure();
+  SmallVector<OpAsmParser::Argument> regionRefArgs;
+  if (succeeded(parser.parseOptionalLParen())) {
+    do {
+      // Reserve entries in the lists.
+      regionRefArgs.emplace_back();
+      if (failed(parser.parseArgument(regionRefArgs.back(),
+                                      /*allowType=*/false,
+                                      /*allowAttrs=*/true))) {
+        return failure();
+      }
+
+      // Parse the tied init if present.
+      if (succeeded(parser.parseOptionalEqual())) {
+        inits.emplace_back();
+        if (failed(parser.parseOperand(inits.back()))) {
+          return failure();
+        }
+        isTied.push_back(true);
+      } else {
+        isTied.push_back(false);
+      }
+    } while (succeeded(parser.parseOptionalComma()));
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  SmallVector<OpAsmParser::Argument> indexArgs;
+  if (failed(parser.parseLSquare())) {
+    return failure();
+  }
+
+  if (failed(parser.parseArgumentList(
+          indexArgs, /*delimiter=*/OpAsmParser::Delimiter::None,
+          /*allowType=*/true, /*allowAttrs=*/true))) {
+    return failure();
+  }
+
+  if (failed(parser.parseRSquare())) {
+    return failure();
+  }
+
+  // If there is at least one region arg the arg types and op result types need
+  // to be parsed.
+  if (!regionRefArgs.empty()) {
+    if (failed(parser.parseColon()) || failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    // Parse all types except the last followed by commas.
+    for (OpAsmParser::Argument &arg :
+         MutableArrayRef<OpAsmParser::Argument>(regionRefArgs.begin(),
+                                                regionRefArgs.end())
+             .drop_back()) {
+      if (failed(parser.parseType(arg.type)) || failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    // Parse the last type.
+    if (failed(parser.parseType(regionRefArgs.back().type))) {
+      return failure();
+    }
+
+    if (failed(parser.parseRParen()) || failed(parser.parseArrow()) ||
+        failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    int64_t numResults = isTied.size();
+    resultTypes.resize(numResults);
+    for (auto [i, isTied] : llvm::enumerate(isTied)) {
+      if (failed(parser.parseType(resultTypes[i]))) {
+        return failure();
+      }
+
+      auto shapedType = dyn_cast<ShapedType>(resultTypes[i]);
+      if (!shapedType) {
+        return failure();
+      }
+
+      if (isTied) {
+        initTypes.push_back(resultTypes[i]);
+      } else if (succeeded(parser.parseOptionalLBrace())) {
+        // Only parse dynamic dims for non-tied operands.
+        SmallVector<OpAsmParser::UnresolvedOperand> dims;
+        if (failed(parser.parseOperandList(dims))) {
+          return failure();
+        }
+        size_t numDynamicDims = shapedType.getNumDynamicDims();
+        if (dims.size() != numDynamicDims) {
+          return failure();
+        }
+        if (failed(parser.parseRBrace())) {
+          return failure();
+        }
+        dynamicSizes.append(dims);
+      }
+
+      if (i < numResults - 1 && failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  // The stored argument order is:
+  // (initialized vals) (result tied refs) (num threads).
+  SmallVector<OpAsmParser::Argument> args;
+  args.append(regionLeadingArgs);
+  args.append(regionRefArgs);
+  args.append(indexArgs);
+  return parser.parseRegion(body, args, /*enableNameShadowing=*/false);
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body) {
+  int64_t numLeadingArgs = 0;
+  return parseParallelExecutionBody(parser, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, numLeadingArgs,
+                                    false);
+}
+
+static void printParallelExecutionBody(
+    OpAsmPrinter &p, Operation *op, OperandRange inits, TypeRange initTypes,
+    OperandRange dynamicSizes, TypeRange resultTypes, ArrayRef<bool> isTied,
+    Region &body, int64_t numLeadingArgs, bool printOptionalLeadingArgs) {
+  if (printOptionalLeadingArgs && numLeadingArgs > 0) {
+    p << "-> (";
+    MutableArrayRef<BlockArgument> leadingArgRange =
+        body.getArguments().take_front(numLeadingArgs);
+    llvm::interleaveComma(leadingArgRange, p, [&](BlockArgument arg) {
+      p.printRegionArgument(arg);
+    });
+    p << ")";
+  }
+
+  p.printNewline();
+  p << "  execute";
+
+  int64_t numResults = resultTypes.size();
+  int64_t numIndexArgs = body.getNumArguments() - numResults - numLeadingArgs;
+  MutableArrayRef<BlockArgument> threadCountArgRange =
+      body.getArguments().take_back(numIndexArgs);
+  MutableArrayRef<BlockArgument> refArgRange =
+      body.getArguments().drop_back(numIndexArgs).take_back(numResults);
+
+  if (numResults != 0) {
+    p << "(";
+    int64_t currInitIndex = 0;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      p << refArgRange[i];
+      if (isTied[i]) {
+        p << " = ";
+        p << inits[currInitIndex];
+        ++currInitIndex;
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ")";
+  }
+  p << "[";
+  llvm::interleaveComma(threadCountArgRange, p,
+                        [&](BlockArgument arg) { p.printRegionArgument(arg); });
+  p << "]";
+
+  // Now print the function type.
+  if (numResults != 0) {
+    p.printNewline();
+    // Whitespace to line up parentheses.
+    //   |--execute(
+    //   |--_____: (
+    p << "       : (";
+    llvm::interleaveComma(refArgRange, p,
+                          [&](BlockArgument arg) { p << arg.getType(); });
+    p << ")";
+    p.printNewline();
+    //   |--execute(
+    //   |--____-> (
+    p << "      -> (";
+    OperandRange currSizes = dynamicSizes;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      ShapedType resultType = cast<ShapedType>(resultTypes[i]);
+      bool isResultTied = isTied[i];
+      p << resultType;
+      if (!isResultTied && !resultType.hasStaticShape()) {
+        int64_t numDynamicDims = resultType.getNumDynamicDims();
+        p << "{";
+        llvm::interleaveComma(currSizes.take_front(numDynamicDims), p,
+                              [&](Value dim) { p << dim; });
+        currSizes = currSizes.drop_front(numDynamicDims);
+        p << "}";
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ") ";
+  } else {
+    // Print a space before the region brace if there are no loop results.
+    p << " ";
+  }
+  p.printRegion(body, /*printEntryBlockArgs=*/false,
+                /*printBlockTerminators=*/true);
+}
+
+static void printParallelExecutionBody(OpAsmPrinter &p, Operation *op,
+                                       OperandRange inits, TypeRange initTypes,
+                                       OperandRange dynamicSizes,
+                                       TypeRange resultTypes,
+                                       ArrayRef<bool> isTied, Region &body) {
+  return printParallelExecutionBody(p, op, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, 0, false);
+}
+
+//===----------------------------------------------------------------------===//
+// GenericOp
+//===----------------------------------------------------------------------===//
+
+static ParseResult parseInferNumIndexArgs(OpAsmParser &parser, Region &body,
+                                          int64_t &numLeadingArgs,
+                                          int64_t &numIndexArgs) {
+  numIndexArgs = 0;
+  for (BlockArgument bbArg :
+       llvm::reverse(body.getArguments().drop_front(numLeadingArgs))) {
+    if (!bbArg.getType().isIndex()) {
+      return success();
+    }
+    ++numIndexArgs;
+  }
+  return success();
+}
+
+static void printInferNumIndexArgs(OpAsmPrinter &, Operation *, Region &,
+                                   int64_t &, int64_t) {
+  // Nothing to do. The number of count args gets parsed solely from the region.
+}
+
+void GenericOp::getAsmBlockArgumentNames(Region &region,
+                                         OpAsmSetValueNameFn setNameFn) {
+  if (&region == &getInitializer()) {
+    return;
+  }
+
+  assert(&region == &getRegion() && "Unexpected region");
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getCountArgs()) {
+    setNameFn(v, "count");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult GenericOp::verify() {
+  if (getNumIndexArgs() % 2 != 0) {
+    return emitOpError("expected even number of id + count args");
+  }
+  if (getRegion().front().getNumArguments() < getNumIndexArgs()) {
+    return emitOpError(
+        "fewer body arguments than specified number of counts/ids.");
+  }
+  return verifyParallelBodyOp(*this, getNumLeadingArgs(), getNumIndexArgs(),
+                              getIdAndCountArgs());
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, int64_t numIterators,
+                      bool syncOnReturn) {
+  GenericOp::build(b, result, TypeRange(), scope, ArrayRef<Value>{},
+                   ArrayRef<Value>{}, ArrayRef<bool>{}, numIterators,
+                   syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, ValueRange inits, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  GenericOp::build(b, result, resultTypes, scope, inits, ArrayRef<Value>{},
+                   isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope,
+                      ValueRange dynamicSizes, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  GenericOp::build(b, result, resultTypes, scope, ArrayRef<Value>{},
+                   dynamicSizes, isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope, ValueRange inits,
+                      ValueRange dynamicSizes, ArrayRef<bool> isTied,
+                      int64_t numIterators, bool syncOnReturn) {
+
+  result.addAttribute(GenericOp::getScopeAttrName(result.name), scope);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+  inherentAttrs.setNumIndexArgs(2 * numIterators);
+
+  // Add the initializer region.
+  result.addRegion();
+
+  // Add the main region.
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count/id args.
+  Type indexType = b.getIndexType();
+  for (int64_t i = 0; i < 2 * numIterators; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// LoopOp
+//===----------------------------------------------------------------------===//
+
+void LoopOp::getAsmBlockArgumentNames(Region &region,
+                                      OpAsmSetValueNameFn setNameFn) {
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult LoopOp::verify() {
+  if (getCount().empty()) {
+    return emitOpError("expected at least one iteration count argument");
+  }
+  if (getBody()->getNumArguments() < getNumIdArgs()) {
+    return emitOpError("fewer body arguments than specified number of ids.");
+  }
+  return verifyParallelBodyOp(*this, /*numLeadingArgs=*/0, getNumIdArgs(),
+                              getIdArgs());
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, bool syncOnReturn) {
+  LoopOp::build(b, result, TypeRange(), scope, count, ArrayRef<Value>{},
+                ArrayRef<Value>{}, ArrayRef<bool>{}, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, ValueRange inits,
+                   bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  LoopOp::build(b, result, resultTypes, scope, count, inits, ArrayRef<Value>{},
+                isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange dynamicSizes, bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  LoopOp::build(b, result, resultTypes, scope, count, ArrayRef<Value>{},
+                dynamicSizes, isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange inits, ValueRange dynamicSizes,
+                   ArrayRef<bool> isTied, bool syncOnReturn) {
+
+  result.addAttribute(LoopOp::getScopeAttrName(result.name), scope);
+  result.addOperands(count);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(count.size()),
+                              static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count args.
+  Type indexType = b.getIndexType();
+  int64_t numCountArgs = count.size() == 0 ? 1 : count.size();
+  for (int64_t i = 0; i < numCountArgs; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+void LoopOp::getSuccessorRegions(RegionBranchPoint point,
+                                 SmallVectorImpl<RegionSuccessor> &regions) {
+  // If the predecessor is the GenericOp, branch into the body.
+  if (point.isParent()) {
+    regions.push_back(RegionSuccessor(&getRegion()));
+    return;
+  }
+
+  // Otherwise, the region branches back to the parent operation.
+  regions.push_back(RegionSuccessor(getOperation(), getResults()));
+}
+
+//===----------------------------------------------------------------------===//
+// Control Flow Ops
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// BranchCondReturnOp
+//===----------------------------------------------------------------------===//
+
+void BranchCondReturnOp::setDest(Block *block) { return setSuccessor(block); }
+
+void BranchCondReturnOp::eraseOperand(unsigned index) {
+  (*this)->eraseOperand(index);
+}
+
+SuccessorOperands BranchCondReturnOp::getSuccessorOperands(unsigned index) {
+  assert(index == 0 && "invalid successor index");
+  // Single index operand produced by this op.
+  return SuccessorOperands(getDestOperandsMutable());
+}
+
+Block *
+BranchCondReturnOp::getSuccessorForOperands(ArrayRef<Attribute> operands) {
+  if (IntegerAttr condAttr =
+          llvm::dyn_cast_or_null<IntegerAttr>(operands.front())) {
+    return condAttr.getValue().isOne() ? nullptr : getDest();
+  }
+  return nullptr;
+}
+
+//===----------------------------------------------------------------------===//
+// WriteOps
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// ParallelInsertSliceOp
+//===----------------------------------------------------------------------===//
+
+// Build a WriteSliceOp with mixed static and dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<OpFoldResult> offsets,
+                         ArrayRef<OpFoldResult> sizes,
+                         ArrayRef<OpFoldResult> strides,
+                         ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, {}, source, dest, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+/// Build an WriteSliceOp with mixed static and dynamic entries
+/// packed into a Range vector.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<Range> ranges,
+                         ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, source, dest, offsets, sizes, strides, attrs);
+}
+
+// Build a WriteSliceOp with dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ValueRange offsets, ValueRange sizes,
+                         ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, source, dest, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// ReadSliceOp
+//===----------------------------------------------------------------------===//
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// GetMemrefOp
+//===----------------------------------------------------------------------===//
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// Folders
+//===----------------------------------------------------------------------===//
+
+LogicalResult WriteSliceOp::fold(FoldAdaptor adaptor,
+                                 SmallVectorImpl<OpFoldResult> &results) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        stride = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  if (!changed) {
+    return failure();
+  }
+
+  // Dispatch back to static/dynamic.
+  SmallVector<int64_t> staticOffsets, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicStrides;
+  dispatchIndexOpFoldResults(mixedOffsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(mixedStrides, dynamicStrides, staticStrides);
+
+  // Update the op's attributes in-place.
+  setStaticOffsetsAttr(builder.getDenseI64ArrayAttr(staticOffsets));
+  setStaticStridesAttr(builder.getDenseI64ArrayAttr(staticStrides));
+  getOffsetsMutable().assign(dynamicOffsets);
+  getStridesMutable().assign(dynamicStrides);
+
+  return success();
+}
+
+OpFoldResult ReadSliceOp::fold(FoldAdaptor adaptor) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.cpp:860`

```diff
@@ -0,0 +1,952 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include <numeric>
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/Dialect/Utils/StaticValueUtils.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/TypeUtilities.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// AllocOps
+//===----------------------------------------------------------------------===//
+
+LogicalResult AllocOp::verify() {
+  if (getDynamicSizes().size() != getResultType().getNumDynamicDims()) {
+    return emitOpError(
+        "dimension operand count does not equal sref dynamic dimension count");
+  }
+
+  // TODO: Restrict legal parents for this op.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// StructuralOps
+//===----------------------------------------------------------------------===//
+
+template <typename OpTy>
+static LogicalResult verifyParallelBodyOp(OpTy op, int64_t numLeadingArgs,
+                                          int64_t numIndexBodyArgs,
+                                          ArrayRef<BlockArgument> indexArgs) {
+  // Verify tied/token array lengths.
+  ArrayRef<bool> isTied = op.getIsTied();
+  int64_t numResults = op.getNumResults();
+  if (isTied.size() != numResults) {
+    return op.emitOpError(
+               "`is_tied` mask length expected to match number of results ")
+           << numResults;
+  }
+
+  int64_t numInits =
+      std::accumulate(isTied.begin(), isTied.end(), (int64_t)(0));
+  if (op.getInits().size() != numInits) {
+    return op.emitOpError("number of inits ")
+           << op.getInits().size()
+           << " does not match the number of results marked as tied "
+           << numInits;
+  }
+
+  if (op.getRegion().getArguments().size() !=
+      numLeadingArgs + numResults + numIndexBodyArgs) {
+    return op.emitOpError("expected region to have |numLeadingArgs| + "
+                          "|numIndexArgs| + |numResults| "
+                          "total arguments");
+  }
+
+  for (BlockArgument countArg : indexArgs) {
+    if (!countArg.getType().isIndex()) {
+      return op.emitOpError(
+          "expected index type for thread count/id region arguments");
+    }
+  }
+
+  PCF::ScopeAttr scope = op.getScope();
+  int64_t currIsTiedIndex = 0;
+  int64_t currResultIndex = 0;
+  for (auto [resultType, refArg, isTied] : llvm::zip_equal(
+           op.getResultTypes(), op.getRegionRefArgs(), op.getIsTied())) {
+    auto srefType = dyn_cast<PCF::ShapedRefType>(refArg.getType());
+    if (!srefType || srefType.getScope() != scope) {
+      return op.emitOpError(
+                 "expected region ref argument to be of type !pcf.sref "
+                 "with scope ")
+             << scope;
+    }
+    if (!srefType.isParentScopeOnlySync() && srefType.getSyncScope()) {
+      return op.emitOpError(
+          "expected region ref argument to have none or parent sync scope");
+    }
+
+    // Traits guarantee this cast to be valid.
+    auto shapedResultType = cast<ShapedType>(resultType);
+    if (shapedResultType.getShape() != srefType.getShape()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " with type " << srefType
+             << " shape mismatch with tied result of type " << resultType;
+    }
+
+    if (shapedResultType.getElementType() != srefType.getElementType()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " element type mismatch of "
+             << srefType.getElementType() << " vs "
+             << shapedResultType.getElementType();
+    }
+
+    if (isTied) {
+      Value init = op.getInits()[currIsTiedIndex];
+      if (init.getType() != resultType) {
+        return op.emitOpError("tied init at index ")
+               << currIsTiedIndex << " does not match the type " << resultType
+               << " at result index " << currResultIndex;
+      }
+      ++currIsTiedIndex;
+    }
+    ++currResultIndex;
+  }
+  return success();
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body, int64_t &numLeadingArgs, bool parseOptionalLeadingArgs) {
+  SmallVector<OpAsmParser::Argument> regionLeadingArgs;
+  if (parseOptionalLeadingArgs) {
+    if (succeeded(parser.parseOptionalArrow())) {
+      if (failed(parser.parseArgumentList(regionLeadingArgs,
+                                          OpAsmParser::Delimiter::Paren,
+                                          /*allowType=*/true))) {
+        return failure();
+      }
+    }
+    numLeadingArgs = regionLeadingArgs.size();
+  }
+  if (failed(parser.parseKeyword("execute")))
+    return failure();
+  SmallVector<OpAsmParser::Argument> regionRefArgs;
+  if (succeeded(parser.parseOptionalLParen())) {
+    do {
+      // Reserve entries in the lists.
+      regionRefArgs.emplace_back();
+      if (failed(parser.parseArgument(regionRefArgs.back(),
+                                      /*allowType=*/false,
+                                      /*allowAttrs=*/true))) {
+        return failure();
+      }
+
+      // Parse the tied init if present.
+      if (succeeded(parser.parseOptionalEqual())) {
+        inits.emplace_back();
+        if (failed(parser.parseOperand(inits.back()))) {
+          return failure();
+        }
+        isTied.push_back(true);
+      } else {
+        isTied.push_back(false);
+      }
+    } while (succeeded(parser.parseOptionalComma()));
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  SmallVector<OpAsmParser::Argument> indexArgs;
+  if (failed(parser.parseLSquare())) {
+    return failure();
+  }
+
+  if (failed(parser.parseArgumentList(
+          indexArgs, /*delimiter=*/OpAsmParser::Delimiter::None,
+          /*allowType=*/true, /*allowAttrs=*/true))) {
+    return failure();
+  }
+
+  if (failed(parser.parseRSquare())) {
+    return failure();
+  }
+
+  // If there is at least one region arg the arg types and op result types need
+  // to be parsed.
+  if (!regionRefArgs.empty()) {
+    if (failed(parser.parseColon()) || failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    // Parse all types except the last followed by commas.
+    for (OpAsmParser::Argument &arg :
+         MutableArrayRef<OpAsmParser::Argument>(regionRefArgs.begin(),
+                                                regionRefArgs.end())
+             .drop_back()) {
+      if (failed(parser.parseType(arg.type)) || failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    // Parse the last type.
+    if (failed(parser.parseType(regionRefArgs.back().type))) {
+      return failure();
+    }
+
+    if (failed(parser.parseRParen()) || failed(parser.parseArrow()) ||
+        failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    int64_t numResults = isTied.size();
+    resultTypes.resize(numResults);
+    for (auto [i, isTied] : llvm::enumerate(isTied)) {
+      if (failed(parser.parseType(resultTypes[i]))) {
+        return failure();
+      }
+
+      auto shapedType = dyn_cast<ShapedType>(resultTypes[i]);
+      if (!shapedType) {
+        return failure();
+      }
+
+      if (isTied) {
+        initTypes.push_back(resultTypes[i]);
+      } else if (succeeded(parser.parseOptionalLBrace())) {
+        // Only parse dynamic dims for non-tied operands.
+        SmallVector<OpAsmParser::UnresolvedOperand> dims;
+        if (failed(parser.parseOperandList(dims))) {
+          return failure();
+        }
+        size_t numDynamicDims = shapedType.getNumDynamicDims();
+        if (dims.size() != numDynamicDims) {
+          return failure();
+        }
+        if (failed(parser.parseRBrace())) {
+          return failure();
+        }
+        dynamicSizes.append(dims);
+      }
+
+      if (i < numResults - 1 && failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  // The stored argument order is:
+  // (initialized vals) (result tied refs) (num threads).
+  SmallVector<OpAsmParser::Argument> args;
+  args.append(regionLeadingArgs);
+  args.append(regionRefArgs);
+  args.append(indexArgs);
+  return parser.parseRegion(body, args, /*enableNameShadowing=*/false);
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body) {
+  int64_t numLeadingArgs = 0;
+  return parseParallelExecutionBody(parser, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, numLeadingArgs,
+                                    false);
+}
+
+static void printParallelExecutionBody(
+    OpAsmPrinter &p, Operation *op, OperandRange inits, TypeRange initTypes,
+    OperandRange dynamicSizes, TypeRange resultTypes, ArrayRef<bool> isTied,
+    Region &body, int64_t numLeadingArgs, bool printOptionalLeadingArgs) {
+  if (printOptionalLeadingArgs && numLeadingArgs > 0) {
+    p << "-> (";
+    MutableArrayRef<BlockArgument> leadingArgRange =
+        body.getArguments().take_front(numLeadingArgs);
+    llvm::interleaveComma(leadingArgRange, p, [&](BlockArgument arg) {
+      p.printRegionArgument(arg);
+    });
+    p << ")";
+  }
+
+  p.printNewline();
+  p << "  execute";
+
+  int64_t numResults = resultTypes.size();
+  int64_t numIndexArgs = body.getNumArguments() - numResults - numLeadingArgs;
+  MutableArrayRef<BlockArgument> threadCountArgRange =
+      body.getArguments().take_back(numIndexArgs);
+  MutableArrayRef<BlockArgument> refArgRange =
+      body.getArguments().drop_back(numIndexArgs).take_back(numResults);
+
+  if (numResults != 0) {
+    p << "(";
+    int64_t currInitIndex = 0;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      p << refArgRange[i];
+      if (isTied[i]) {
+        p << " = ";
+        p << inits[currInitIndex];
+        ++currInitIndex;
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ")";
+  }
+  p << "[";
+  llvm::interleaveComma(threadCountArgRange, p,
+                        [&](BlockArgument arg) { p.printRegionArgument(arg); });
+  p << "]";
+
+  // Now print the function type.
+  if (numResults != 0) {
+    p.printNewline();
+    // Whitespace to line up parentheses.
+    //   |--execute(
+    //   |--_____: (
+    p << "       : (";
+    llvm::interleaveComma(refArgRange, p,
+                          [&](BlockArgument arg) { p << arg.getType(); });
+    p << ")";
+    p.printNewline();
+    //   |--execute(
+    //   |--____-> (
+    p << "      -> (";
+    OperandRange currSizes = dynamicSizes;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      ShapedType resultType = cast<ShapedType>(resultTypes[i]);
+      bool isResultTied = isTied[i];
+      p << resultType;
+      if (!isResultTied && !resultType.hasStaticShape()) {
+        int64_t numDynamicDims = resultType.getNumDynamicDims();
+        p << "{";
+        llvm::interleaveComma(currSizes.take_front(numDynamicDims), p,
+                              [&](Value dim) { p << dim; });
+        currSizes = currSizes.drop_front(numDynamicDims);
+        p << "}";
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ") ";
+  } else {
+    // Print a space before the region brace if there are no loop results.
+    p << " ";
+  }
+  p.printRegion(body, /*printEntryBlockArgs=*/false,
+                /*printBlockTerminators=*/true);
+}
+
+static void printParallelExecutionBody(OpAsmPrinter &p, Operation *op,
+                                       OperandRange inits, TypeRange initTypes,
+                                       OperandRange dynamicSizes,
+                                       TypeRange resultTypes,
+                                       ArrayRef<bool> isTied, Region &body) {
+  return printParallelExecutionBody(p, op, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, 0, false);
+}
+
+//===----------------------------------------------------------------------===//
+// GenericOp
+//===----------------------------------------------------------------------===//
+
+static ParseResult parseInferNumIndexArgs(OpAsmParser &parser, Region &body,
+                                          int64_t &numLeadingArgs,
+                                          int64_t &numIndexArgs) {
+  numIndexArgs = 0;
+  for (BlockArgument bbArg :
+       llvm::reverse(body.getArguments().drop_front(numLeadingArgs))) {
+    if (!bbArg.getType().isIndex()) {
+      return success();
+    }
+    ++numIndexArgs;
+  }
+  return success();
+}
+
+static void printInferNumIndexArgs(OpAsmPrinter &, Operation *, Region &,
+                                   int64_t &, int64_t) {
+  // Nothing to do. The number of count args gets parsed solely from the region.
+}
+
+void GenericOp::getAsmBlockArgumentNames(Region &region,
+                                         OpAsmSetValueNameFn setNameFn) {
+  if (&region == &getInitializer()) {
+    return;
+  }
+
+  assert(&region == &getRegion() && "Unexpected region");
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getCountArgs()) {
+    setNameFn(v, "count");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult GenericOp::verify() {
+  if (getNumIndexArgs() % 2 != 0) {
+    return emitOpError("expected even number of id + count args");
+  }
+  if (getRegion().front().getNumArguments() < getNumIndexArgs()) {
+    return emitOpError(
+        "fewer body arguments than specified number of counts/ids.");
+  }
+  return verifyParallelBodyOp(*this, getNumLeadingArgs(), getNumIndexArgs(),
+                              getIdAndCountArgs());
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, int64_t numIterators,
+                      bool syncOnReturn) {
+  GenericOp::build(b, result, TypeRange(), scope, ArrayRef<Value>{},
+                   ArrayRef<Value>{}, ArrayRef<bool>{}, numIterators,
+                   syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, ValueRange inits, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  GenericOp::build(b, result, resultTypes, scope, inits, ArrayRef<Value>{},
+                   isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope,
+                      ValueRange dynamicSizes, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  GenericOp::build(b, result, resultTypes, scope, ArrayRef<Value>{},
+                   dynamicSizes, isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope, ValueRange inits,
+                      ValueRange dynamicSizes, ArrayRef<bool> isTied,
+                      int64_t numIterators, bool syncOnReturn) {
+
+  result.addAttribute(GenericOp::getScopeAttrName(result.name), scope);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+  inherentAttrs.setNumIndexArgs(2 * numIterators);
+
+  // Add the initializer region.
+  result.addRegion();
+
+  // Add the main region.
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count/id args.
+  Type indexType = b.getIndexType();
+  for (int64_t i = 0; i < 2 * numIterators; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// LoopOp
+//===----------------------------------------------------------------------===//
+
+void LoopOp::getAsmBlockArgumentNames(Region &region,
+                                      OpAsmSetValueNameFn setNameFn) {
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult LoopOp::verify() {
+  if (getCount().empty()) {
+    return emitOpError("expected at least one iteration count argument");
+  }
+  if (getBody()->getNumArguments() < getNumIdArgs()) {
+    return emitOpError("fewer body arguments than specified number of ids.");
+  }
+  return verifyParallelBodyOp(*this, /*numLeadingArgs=*/0, getNumIdArgs(),
+                              getIdArgs());
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, bool syncOnReturn) {
+  LoopOp::build(b, result, TypeRange(), scope, count, ArrayRef<Value>{},
+                ArrayRef<Value>{}, ArrayRef<bool>{}, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, ValueRange inits,
+                   bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  LoopOp::build(b, result, resultTypes, scope, count, inits, ArrayRef<Value>{},
+                isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange dynamicSizes, bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  LoopOp::build(b, result, resultTypes, scope, count, ArrayRef<Value>{},
+                dynamicSizes, isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange inits, ValueRange dynamicSizes,
+                   ArrayRef<bool> isTied, bool syncOnReturn) {
+
+  result.addAttribute(LoopOp::getScopeAttrName(result.name), scope);
+  result.addOperands(count);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(count.size()),
+                              static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count args.
+  Type indexType = b.getIndexType();
+  int64_t numCountArgs = count.size() == 0 ? 1 : count.size();
+  for (int64_t i = 0; i < numCountArgs; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+void LoopOp::getSuccessorRegions(RegionBranchPoint point,
+                                 SmallVectorImpl<RegionSuccessor> &regions) {
+  // If the predecessor is the GenericOp, branch into the body.
+  if (point.isParent()) {
+    regions.push_back(RegionSuccessor(&getRegion()));
+    return;
+  }
+
+  // Otherwise, the region branches back to the parent operation.
+  regions.push_back(RegionSuccessor(getOperation(), getResults()));
+}
+
+//===----------------------------------------------------------------------===//
+// Control Flow Ops
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// BranchCondReturnOp
+//===----------------------------------------------------------------------===//
+
+void BranchCondReturnOp::setDest(Block *block) { return setSuccessor(block); }
+
+void BranchCondReturnOp::eraseOperand(unsigned index) {
+  (*this)->eraseOperand(index);
+}
+
+SuccessorOperands BranchCondReturnOp::getSuccessorOperands(unsigned index) {
+  assert(index == 0 && "invalid successor index");
+  // Single index operand produced by this op.
+  return SuccessorOperands(getDestOperandsMutable());
+}
+
+Block *
+BranchCondReturnOp::getSuccessorForOperands(ArrayRef<Attribute> operands) {
+  if (IntegerAttr condAttr =
+          llvm::dyn_cast_or_null<IntegerAttr>(operands.front())) {
+    return condAttr.getValue().isOne() ? nullptr : getDest();
+  }
+  return nullptr;
+}
+
+//===----------------------------------------------------------------------===//
+// WriteOps
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// ParallelInsertSliceOp
+//===----------------------------------------------------------------------===//
+
+// Build a WriteSliceOp with mixed static and dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<OpFoldResult> offsets,
+                         ArrayRef<OpFoldResult> sizes,
+                         ArrayRef<OpFoldResult> strides,
+                         ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, {}, source, dest, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+/// Build an WriteSliceOp with mixed static and dynamic entries
+/// packed into a Range vector.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<Range> ranges,
+                         ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, source, dest, offsets, sizes, strides, attrs);
+}
+
+// Build a WriteSliceOp with dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ValueRange offsets, ValueRange sizes,
+                         ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, source, dest, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// ReadSliceOp
+//===----------------------------------------------------------------------===//
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// GetMemrefOp
+//===----------------------------------------------------------------------===//
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// Folders
+//===----------------------------------------------------------------------===//
+
+LogicalResult WriteSliceOp::fold(FoldAdaptor adaptor,
+                                 SmallVectorImpl<OpFoldResult> &results) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        stride = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  if (!changed) {
+    return failure();
+  }
+
+  // Dispatch back to static/dynamic.
+  SmallVector<int64_t> staticOffsets, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicStrides;
+  dispatchIndexOpFoldResults(mixedOffsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(mixedStrides, dynamicStrides, staticStrides);
+
+  // Update the op's attributes in-place.
+  setStaticOffsetsAttr(builder.getDenseI64ArrayAttr(staticOffsets));
+  setStaticStridesAttr(builder.getDenseI64ArrayAttr(staticStrides));
+  getOffsetsMutable().assign(dynamicOffsets);
+  getStridesMutable().assign(dynamicStrides);
+
+  return success();
+}
+
+OpFoldResult ReadSliceOp::fold(FoldAdaptor adaptor) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        stride = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  if (!changed) {
+    return {};
+  }
+
+  // Dispatch back to static/dynamic.
+  SmallVector<int64_t> staticOffsets, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicStrides;
+  dispatchIndexOpFoldResults(mixedOffsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(mixedStrides, dynamicStrides, staticStrides);
+
+  // Update the op's attributes in-place.
+  setStaticOffsetsAttr(builder.getDenseI64ArrayAttr(staticOffsets));
+  setStaticStridesAttr(builder.getDenseI64ArrayAttr(staticStrides));
+  getOffsetsMutable().assign(dynamicOffsets);
+  getStridesMutable().assign(dynamicStrides);
+
+  return {};
+}
+
+OpFoldResult GetMemrefOp::fold(FoldAdaptor adaptor) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
```

**Comment:**
type

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.cpp:863`

```diff
@@ -0,0 +1,952 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include <numeric>
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/Dialect/Utils/StaticValueUtils.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/TypeUtilities.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// AllocOps
+//===----------------------------------------------------------------------===//
+
+LogicalResult AllocOp::verify() {
+  if (getDynamicSizes().size() != getResultType().getNumDynamicDims()) {
+    return emitOpError(
+        "dimension operand count does not equal sref dynamic dimension count");
+  }
+
+  // TODO: Restrict legal parents for this op.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// StructuralOps
+//===----------------------------------------------------------------------===//
+
+template <typename OpTy>
+static LogicalResult verifyParallelBodyOp(OpTy op, int64_t numLeadingArgs,
+                                          int64_t numIndexBodyArgs,
+                                          ArrayRef<BlockArgument> indexArgs) {
+  // Verify tied/token array lengths.
+  ArrayRef<bool> isTied = op.getIsTied();
+  int64_t numResults = op.getNumResults();
+  if (isTied.size() != numResults) {
+    return op.emitOpError(
+               "`is_tied` mask length expected to match number of results ")
+           << numResults;
+  }
+
+  int64_t numInits =
+      std::accumulate(isTied.begin(), isTied.end(), (int64_t)(0));
+  if (op.getInits().size() != numInits) {
+    return op.emitOpError("number of inits ")
+           << op.getInits().size()
+           << " does not match the number of results marked as tied "
+           << numInits;
+  }
+
+  if (op.getRegion().getArguments().size() !=
+      numLeadingArgs + numResults + numIndexBodyArgs) {
+    return op.emitOpError("expected region to have |numLeadingArgs| + "
+                          "|numIndexArgs| + |numResults| "
+                          "total arguments");
+  }
+
+  for (BlockArgument countArg : indexArgs) {
+    if (!countArg.getType().isIndex()) {
+      return op.emitOpError(
+          "expected index type for thread count/id region arguments");
+    }
+  }
+
+  PCF::ScopeAttr scope = op.getScope();
+  int64_t currIsTiedIndex = 0;
+  int64_t currResultIndex = 0;
+  for (auto [resultType, refArg, isTied] : llvm::zip_equal(
+           op.getResultTypes(), op.getRegionRefArgs(), op.getIsTied())) {
+    auto srefType = dyn_cast<PCF::ShapedRefType>(refArg.getType());
+    if (!srefType || srefType.getScope() != scope) {
+      return op.emitOpError(
+                 "expected region ref argument to be of type !pcf.sref "
+                 "with scope ")
+             << scope;
+    }
+    if (!srefType.isParentScopeOnlySync() && srefType.getSyncScope()) {
+      return op.emitOpError(
+          "expected region ref argument to have none or parent sync scope");
+    }
+
+    // Traits guarantee this cast to be valid.
+    auto shapedResultType = cast<ShapedType>(resultType);
+    if (shapedResultType.getShape() != srefType.getShape()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " with type " << srefType
+             << " shape mismatch with tied result of type " << resultType;
+    }
+
+    if (shapedResultType.getElementType() != srefType.getElementType()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " element type mismatch of "
+             << srefType.getElementType() << " vs "
+             << shapedResultType.getElementType();
+    }
+
+    if (isTied) {
+      Value init = op.getInits()[currIsTiedIndex];
+      if (init.getType() != resultType) {
+        return op.emitOpError("tied init at index ")
+               << currIsTiedIndex << " does not match the type " << resultType
+               << " at result index " << currResultIndex;
+      }
+      ++currIsTiedIndex;
+    }
+    ++currResultIndex;
+  }
+  return success();
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body, int64_t &numLeadingArgs, bool parseOptionalLeadingArgs) {
+  SmallVector<OpAsmParser::Argument> regionLeadingArgs;
+  if (parseOptionalLeadingArgs) {
+    if (succeeded(parser.parseOptionalArrow())) {
+      if (failed(parser.parseArgumentList(regionLeadingArgs,
+                                          OpAsmParser::Delimiter::Paren,
+                                          /*allowType=*/true))) {
+        return failure();
+      }
+    }
+    numLeadingArgs = regionLeadingArgs.size();
+  }
+  if (failed(parser.parseKeyword("execute")))
+    return failure();
+  SmallVector<OpAsmParser::Argument> regionRefArgs;
+  if (succeeded(parser.parseOptionalLParen())) {
+    do {
+      // Reserve entries in the lists.
+      regionRefArgs.emplace_back();
+      if (failed(parser.parseArgument(regionRefArgs.back(),
+                                      /*allowType=*/false,
+                                      /*allowAttrs=*/true))) {
+        return failure();
+      }
+
+      // Parse the tied init if present.
+      if (succeeded(parser.parseOptionalEqual())) {
+        inits.emplace_back();
+        if (failed(parser.parseOperand(inits.back()))) {
+          return failure();
+        }
+        isTied.push_back(true);
+      } else {
+        isTied.push_back(false);
+      }
+    } while (succeeded(parser.parseOptionalComma()));
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  SmallVector<OpAsmParser::Argument> indexArgs;
+  if (failed(parser.parseLSquare())) {
+    return failure();
+  }
+
+  if (failed(parser.parseArgumentList(
+          indexArgs, /*delimiter=*/OpAsmParser::Delimiter::None,
+          /*allowType=*/true, /*allowAttrs=*/true))) {
+    return failure();
+  }
+
+  if (failed(parser.parseRSquare())) {
+    return failure();
+  }
+
+  // If there is at least one region arg the arg types and op result types need
+  // to be parsed.
+  if (!regionRefArgs.empty()) {
+    if (failed(parser.parseColon()) || failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    // Parse all types except the last followed by commas.
+    for (OpAsmParser::Argument &arg :
+         MutableArrayRef<OpAsmParser::Argument>(regionRefArgs.begin(),
+                                                regionRefArgs.end())
+             .drop_back()) {
+      if (failed(parser.parseType(arg.type)) || failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    // Parse the last type.
+    if (failed(parser.parseType(regionRefArgs.back().type))) {
+      return failure();
+    }
+
+    if (failed(parser.parseRParen()) || failed(parser.parseArrow()) ||
+        failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    int64_t numResults = isTied.size();
+    resultTypes.resize(numResults);
+    for (auto [i, isTied] : llvm::enumerate(isTied)) {
+      if (failed(parser.parseType(resultTypes[i]))) {
+        return failure();
+      }
+
+      auto shapedType = dyn_cast<ShapedType>(resultTypes[i]);
+      if (!shapedType) {
+        return failure();
+      }
+
+      if (isTied) {
+        initTypes.push_back(resultTypes[i]);
+      } else if (succeeded(parser.parseOptionalLBrace())) {
+        // Only parse dynamic dims for non-tied operands.
+        SmallVector<OpAsmParser::UnresolvedOperand> dims;
+        if (failed(parser.parseOperandList(dims))) {
+          return failure();
+        }
+        size_t numDynamicDims = shapedType.getNumDynamicDims();
+        if (dims.size() != numDynamicDims) {
+          return failure();
+        }
+        if (failed(parser.parseRBrace())) {
+          return failure();
+        }
+        dynamicSizes.append(dims);
+      }
+
+      if (i < numResults - 1 && failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  // The stored argument order is:
+  // (initialized vals) (result tied refs) (num threads).
+  SmallVector<OpAsmParser::Argument> args;
+  args.append(regionLeadingArgs);
+  args.append(regionRefArgs);
+  args.append(indexArgs);
+  return parser.parseRegion(body, args, /*enableNameShadowing=*/false);
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body) {
+  int64_t numLeadingArgs = 0;
+  return parseParallelExecutionBody(parser, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, numLeadingArgs,
+                                    false);
+}
+
+static void printParallelExecutionBody(
+    OpAsmPrinter &p, Operation *op, OperandRange inits, TypeRange initTypes,
+    OperandRange dynamicSizes, TypeRange resultTypes, ArrayRef<bool> isTied,
+    Region &body, int64_t numLeadingArgs, bool printOptionalLeadingArgs) {
+  if (printOptionalLeadingArgs && numLeadingArgs > 0) {
+    p << "-> (";
+    MutableArrayRef<BlockArgument> leadingArgRange =
+        body.getArguments().take_front(numLeadingArgs);
+    llvm::interleaveComma(leadingArgRange, p, [&](BlockArgument arg) {
+      p.printRegionArgument(arg);
+    });
+    p << ")";
+  }
+
+  p.printNewline();
+  p << "  execute";
+
+  int64_t numResults = resultTypes.size();
+  int64_t numIndexArgs = body.getNumArguments() - numResults - numLeadingArgs;
+  MutableArrayRef<BlockArgument> threadCountArgRange =
+      body.getArguments().take_back(numIndexArgs);
+  MutableArrayRef<BlockArgument> refArgRange =
+      body.getArguments().drop_back(numIndexArgs).take_back(numResults);
+
+  if (numResults != 0) {
+    p << "(";
+    int64_t currInitIndex = 0;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      p << refArgRange[i];
+      if (isTied[i]) {
+        p << " = ";
+        p << inits[currInitIndex];
+        ++currInitIndex;
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ")";
+  }
+  p << "[";
+  llvm::interleaveComma(threadCountArgRange, p,
+                        [&](BlockArgument arg) { p.printRegionArgument(arg); });
+  p << "]";
+
+  // Now print the function type.
+  if (numResults != 0) {
+    p.printNewline();
+    // Whitespace to line up parentheses.
+    //   |--execute(
+    //   |--_____: (
+    p << "       : (";
+    llvm::interleaveComma(refArgRange, p,
+                          [&](BlockArgument arg) { p << arg.getType(); });
+    p << ")";
+    p.printNewline();
+    //   |--execute(
+    //   |--____-> (
+    p << "      -> (";
+    OperandRange currSizes = dynamicSizes;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      ShapedType resultType = cast<ShapedType>(resultTypes[i]);
+      bool isResultTied = isTied[i];
+      p << resultType;
+      if (!isResultTied && !resultType.hasStaticShape()) {
+        int64_t numDynamicDims = resultType.getNumDynamicDims();
+        p << "{";
+        llvm::interleaveComma(currSizes.take_front(numDynamicDims), p,
+                              [&](Value dim) { p << dim; });
+        currSizes = currSizes.drop_front(numDynamicDims);
+        p << "}";
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ") ";
+  } else {
+    // Print a space before the region brace if there are no loop results.
+    p << " ";
+  }
+  p.printRegion(body, /*printEntryBlockArgs=*/false,
+                /*printBlockTerminators=*/true);
+}
+
+static void printParallelExecutionBody(OpAsmPrinter &p, Operation *op,
+                                       OperandRange inits, TypeRange initTypes,
+                                       OperandRange dynamicSizes,
+                                       TypeRange resultTypes,
+                                       ArrayRef<bool> isTied, Region &body) {
+  return printParallelExecutionBody(p, op, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, 0, false);
+}
+
+//===----------------------------------------------------------------------===//
+// GenericOp
+//===----------------------------------------------------------------------===//
+
+static ParseResult parseInferNumIndexArgs(OpAsmParser &parser, Region &body,
+                                          int64_t &numLeadingArgs,
+                                          int64_t &numIndexArgs) {
+  numIndexArgs = 0;
+  for (BlockArgument bbArg :
+       llvm::reverse(body.getArguments().drop_front(numLeadingArgs))) {
+    if (!bbArg.getType().isIndex()) {
+      return success();
+    }
+    ++numIndexArgs;
+  }
+  return success();
+}
+
+static void printInferNumIndexArgs(OpAsmPrinter &, Operation *, Region &,
+                                   int64_t &, int64_t) {
+  // Nothing to do. The number of count args gets parsed solely from the region.
+}
+
+void GenericOp::getAsmBlockArgumentNames(Region &region,
+                                         OpAsmSetValueNameFn setNameFn) {
+  if (&region == &getInitializer()) {
+    return;
+  }
+
+  assert(&region == &getRegion() && "Unexpected region");
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getCountArgs()) {
+    setNameFn(v, "count");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult GenericOp::verify() {
+  if (getNumIndexArgs() % 2 != 0) {
+    return emitOpError("expected even number of id + count args");
+  }
+  if (getRegion().front().getNumArguments() < getNumIndexArgs()) {
+    return emitOpError(
+        "fewer body arguments than specified number of counts/ids.");
+  }
+  return verifyParallelBodyOp(*this, getNumLeadingArgs(), getNumIndexArgs(),
+                              getIdAndCountArgs());
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, int64_t numIterators,
+                      bool syncOnReturn) {
+  GenericOp::build(b, result, TypeRange(), scope, ArrayRef<Value>{},
+                   ArrayRef<Value>{}, ArrayRef<bool>{}, numIterators,
+                   syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, ValueRange inits, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  GenericOp::build(b, result, resultTypes, scope, inits, ArrayRef<Value>{},
+                   isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope,
+                      ValueRange dynamicSizes, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  GenericOp::build(b, result, resultTypes, scope, ArrayRef<Value>{},
+                   dynamicSizes, isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope, ValueRange inits,
+                      ValueRange dynamicSizes, ArrayRef<bool> isTied,
+                      int64_t numIterators, bool syncOnReturn) {
+
+  result.addAttribute(GenericOp::getScopeAttrName(result.name), scope);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+  inherentAttrs.setNumIndexArgs(2 * numIterators);
+
+  // Add the initializer region.
+  result.addRegion();
+
+  // Add the main region.
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count/id args.
+  Type indexType = b.getIndexType();
+  for (int64_t i = 0; i < 2 * numIterators; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// LoopOp
+//===----------------------------------------------------------------------===//
+
+void LoopOp::getAsmBlockArgumentNames(Region &region,
+                                      OpAsmSetValueNameFn setNameFn) {
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult LoopOp::verify() {
+  if (getCount().empty()) {
+    return emitOpError("expected at least one iteration count argument");
+  }
+  if (getBody()->getNumArguments() < getNumIdArgs()) {
+    return emitOpError("fewer body arguments than specified number of ids.");
+  }
+  return verifyParallelBodyOp(*this, /*numLeadingArgs=*/0, getNumIdArgs(),
+                              getIdArgs());
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, bool syncOnReturn) {
+  LoopOp::build(b, result, TypeRange(), scope, count, ArrayRef<Value>{},
+                ArrayRef<Value>{}, ArrayRef<bool>{}, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, ValueRange inits,
+                   bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  LoopOp::build(b, result, resultTypes, scope, count, inits, ArrayRef<Value>{},
+                isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange dynamicSizes, bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  LoopOp::build(b, result, resultTypes, scope, count, ArrayRef<Value>{},
+                dynamicSizes, isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange inits, ValueRange dynamicSizes,
+                   ArrayRef<bool> isTied, bool syncOnReturn) {
+
+  result.addAttribute(LoopOp::getScopeAttrName(result.name), scope);
+  result.addOperands(count);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(count.size()),
+                              static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count args.
+  Type indexType = b.getIndexType();
+  int64_t numCountArgs = count.size() == 0 ? 1 : count.size();
+  for (int64_t i = 0; i < numCountArgs; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+void LoopOp::getSuccessorRegions(RegionBranchPoint point,
+                                 SmallVectorImpl<RegionSuccessor> &regions) {
+  // If the predecessor is the GenericOp, branch into the body.
+  if (point.isParent()) {
+    regions.push_back(RegionSuccessor(&getRegion()));
+    return;
+  }
+
+  // Otherwise, the region branches back to the parent operation.
+  regions.push_back(RegionSuccessor(getOperation(), getResults()));
+}
+
+//===----------------------------------------------------------------------===//
+// Control Flow Ops
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// BranchCondReturnOp
+//===----------------------------------------------------------------------===//
+
+void BranchCondReturnOp::setDest(Block *block) { return setSuccessor(block); }
+
+void BranchCondReturnOp::eraseOperand(unsigned index) {
+  (*this)->eraseOperand(index);
+}
+
+SuccessorOperands BranchCondReturnOp::getSuccessorOperands(unsigned index) {
+  assert(index == 0 && "invalid successor index");
+  // Single index operand produced by this op.
+  return SuccessorOperands(getDestOperandsMutable());
+}
+
+Block *
+BranchCondReturnOp::getSuccessorForOperands(ArrayRef<Attribute> operands) {
+  if (IntegerAttr condAttr =
+          llvm::dyn_cast_or_null<IntegerAttr>(operands.front())) {
+    return condAttr.getValue().isOne() ? nullptr : getDest();
+  }
+  return nullptr;
+}
+
+//===----------------------------------------------------------------------===//
+// WriteOps
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// ParallelInsertSliceOp
+//===----------------------------------------------------------------------===//
+
+// Build a WriteSliceOp with mixed static and dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<OpFoldResult> offsets,
+                         ArrayRef<OpFoldResult> sizes,
+                         ArrayRef<OpFoldResult> strides,
+                         ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, {}, source, dest, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+/// Build an WriteSliceOp with mixed static and dynamic entries
+/// packed into a Range vector.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<Range> ranges,
+                         ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, source, dest, offsets, sizes, strides, attrs);
+}
+
+// Build a WriteSliceOp with dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ValueRange offsets, ValueRange sizes,
+                         ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, source, dest, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// ReadSliceOp
+//===----------------------------------------------------------------------===//
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// GetMemrefOp
+//===----------------------------------------------------------------------===//
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// Folders
+//===----------------------------------------------------------------------===//
+
+LogicalResult WriteSliceOp::fold(FoldAdaptor adaptor,
+                                 SmallVectorImpl<OpFoldResult> &results) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        stride = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  if (!changed) {
+    return failure();
+  }
+
+  // Dispatch back to static/dynamic.
+  SmallVector<int64_t> staticOffsets, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicStrides;
+  dispatchIndexOpFoldResults(mixedOffsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(mixedStrides, dynamicStrides, staticStrides);
+
+  // Update the op's attributes in-place.
+  setStaticOffsetsAttr(builder.getDenseI64ArrayAttr(staticOffsets));
+  setStaticStridesAttr(builder.getDenseI64ArrayAttr(staticStrides));
+  getOffsetsMutable().assign(dynamicOffsets);
+  getStridesMutable().assign(dynamicStrides);
+
+  return success();
+}
+
+OpFoldResult ReadSliceOp::fold(FoldAdaptor adaptor) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        stride = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  if (!changed) {
+    return {};
+  }
+
+  // Dispatch back to static/dynamic.
+  SmallVector<int64_t> staticOffsets, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicStrides;
+  dispatchIndexOpFoldResults(mixedOffsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(mixedStrides, dynamicStrides, staticStrides);
+
+  // Update the op's attributes in-place.
+  setStaticOffsetsAttr(builder.getDenseI64ArrayAttr(staticOffsets));
+  setStaticStridesAttr(builder.getDenseI64ArrayAttr(staticStrides));
+  getOffsetsMutable().assign(dynamicOffsets);
+  getStridesMutable().assign(dynamicStrides);
+
+  return {};
+}
+
+OpFoldResult GetMemrefOp::fold(FoldAdaptor adaptor) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.cpp:874`

```diff
@@ -0,0 +1,952 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.h"
+#include <numeric>
+#include "iree/compiler/Codegen/Dialect/PCF/IR/PCFTypes.h"
+#include "llvm/ADT/SmallVectorExtras.h"
+#include "mlir/Dialect/Utils/IndexingUtils.h"
+#include "mlir/Dialect/Utils/StaticValueUtils.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/IR/TypeUtilities.h"
+
+namespace mlir::iree_compiler::IREE::PCF {
+
+//===----------------------------------------------------------------------===//
+// AllocOps
+//===----------------------------------------------------------------------===//
+
+LogicalResult AllocOp::verify() {
+  if (getDynamicSizes().size() != getResultType().getNumDynamicDims()) {
+    return emitOpError(
+        "dimension operand count does not equal sref dynamic dimension count");
+  }
+
+  // TODO: Restrict legal parents for this op.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// StructuralOps
+//===----------------------------------------------------------------------===//
+
+template <typename OpTy>
+static LogicalResult verifyParallelBodyOp(OpTy op, int64_t numLeadingArgs,
+                                          int64_t numIndexBodyArgs,
+                                          ArrayRef<BlockArgument> indexArgs) {
+  // Verify tied/token array lengths.
+  ArrayRef<bool> isTied = op.getIsTied();
+  int64_t numResults = op.getNumResults();
+  if (isTied.size() != numResults) {
+    return op.emitOpError(
+               "`is_tied` mask length expected to match number of results ")
+           << numResults;
+  }
+
+  int64_t numInits =
+      std::accumulate(isTied.begin(), isTied.end(), (int64_t)(0));
+  if (op.getInits().size() != numInits) {
+    return op.emitOpError("number of inits ")
+           << op.getInits().size()
+           << " does not match the number of results marked as tied "
+           << numInits;
+  }
+
+  if (op.getRegion().getArguments().size() !=
+      numLeadingArgs + numResults + numIndexBodyArgs) {
+    return op.emitOpError("expected region to have |numLeadingArgs| + "
+                          "|numIndexArgs| + |numResults| "
+                          "total arguments");
+  }
+
+  for (BlockArgument countArg : indexArgs) {
+    if (!countArg.getType().isIndex()) {
+      return op.emitOpError(
+          "expected index type for thread count/id region arguments");
+    }
+  }
+
+  PCF::ScopeAttr scope = op.getScope();
+  int64_t currIsTiedIndex = 0;
+  int64_t currResultIndex = 0;
+  for (auto [resultType, refArg, isTied] : llvm::zip_equal(
+           op.getResultTypes(), op.getRegionRefArgs(), op.getIsTied())) {
+    auto srefType = dyn_cast<PCF::ShapedRefType>(refArg.getType());
+    if (!srefType || srefType.getScope() != scope) {
+      return op.emitOpError(
+                 "expected region ref argument to be of type !pcf.sref "
+                 "with scope ")
+             << scope;
+    }
+    if (!srefType.isParentScopeOnlySync() && srefType.getSyncScope()) {
+      return op.emitOpError(
+          "expected region ref argument to have none or parent sync scope");
+    }
+
+    // Traits guarantee this cast to be valid.
+    auto shapedResultType = cast<ShapedType>(resultType);
+    if (shapedResultType.getShape() != srefType.getShape()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " with type " << srefType
+             << " shape mismatch with tied result of type " << resultType;
+    }
+
+    if (shapedResultType.getElementType() != srefType.getElementType()) {
+      return op.emitOpError("region arg at index ")
+             << currResultIndex << " element type mismatch of "
+             << srefType.getElementType() << " vs "
+             << shapedResultType.getElementType();
+    }
+
+    if (isTied) {
+      Value init = op.getInits()[currIsTiedIndex];
+      if (init.getType() != resultType) {
+        return op.emitOpError("tied init at index ")
+               << currIsTiedIndex << " does not match the type " << resultType
+               << " at result index " << currResultIndex;
+      }
+      ++currIsTiedIndex;
+    }
+    ++currResultIndex;
+  }
+  return success();
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body, int64_t &numLeadingArgs, bool parseOptionalLeadingArgs) {
+  SmallVector<OpAsmParser::Argument> regionLeadingArgs;
+  if (parseOptionalLeadingArgs) {
+    if (succeeded(parser.parseOptionalArrow())) {
+      if (failed(parser.parseArgumentList(regionLeadingArgs,
+                                          OpAsmParser::Delimiter::Paren,
+                                          /*allowType=*/true))) {
+        return failure();
+      }
+    }
+    numLeadingArgs = regionLeadingArgs.size();
+  }
+  if (failed(parser.parseKeyword("execute")))
+    return failure();
+  SmallVector<OpAsmParser::Argument> regionRefArgs;
+  if (succeeded(parser.parseOptionalLParen())) {
+    do {
+      // Reserve entries in the lists.
+      regionRefArgs.emplace_back();
+      if (failed(parser.parseArgument(regionRefArgs.back(),
+                                      /*allowType=*/false,
+                                      /*allowAttrs=*/true))) {
+        return failure();
+      }
+
+      // Parse the tied init if present.
+      if (succeeded(parser.parseOptionalEqual())) {
+        inits.emplace_back();
+        if (failed(parser.parseOperand(inits.back()))) {
+          return failure();
+        }
+        isTied.push_back(true);
+      } else {
+        isTied.push_back(false);
+      }
+    } while (succeeded(parser.parseOptionalComma()));
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  SmallVector<OpAsmParser::Argument> indexArgs;
+  if (failed(parser.parseLSquare())) {
+    return failure();
+  }
+
+  if (failed(parser.parseArgumentList(
+          indexArgs, /*delimiter=*/OpAsmParser::Delimiter::None,
+          /*allowType=*/true, /*allowAttrs=*/true))) {
+    return failure();
+  }
+
+  if (failed(parser.parseRSquare())) {
+    return failure();
+  }
+
+  // If there is at least one region arg the arg types and op result types need
+  // to be parsed.
+  if (!regionRefArgs.empty()) {
+    if (failed(parser.parseColon()) || failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    // Parse all types except the last followed by commas.
+    for (OpAsmParser::Argument &arg :
+         MutableArrayRef<OpAsmParser::Argument>(regionRefArgs.begin(),
+                                                regionRefArgs.end())
+             .drop_back()) {
+      if (failed(parser.parseType(arg.type)) || failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    // Parse the last type.
+    if (failed(parser.parseType(regionRefArgs.back().type))) {
+      return failure();
+    }
+
+    if (failed(parser.parseRParen()) || failed(parser.parseArrow()) ||
+        failed(parser.parseLParen())) {
+      return failure();
+    }
+
+    int64_t numResults = isTied.size();
+    resultTypes.resize(numResults);
+    for (auto [i, isTied] : llvm::enumerate(isTied)) {
+      if (failed(parser.parseType(resultTypes[i]))) {
+        return failure();
+      }
+
+      auto shapedType = dyn_cast<ShapedType>(resultTypes[i]);
+      if (!shapedType) {
+        return failure();
+      }
+
+      if (isTied) {
+        initTypes.push_back(resultTypes[i]);
+      } else if (succeeded(parser.parseOptionalLBrace())) {
+        // Only parse dynamic dims for non-tied operands.
+        SmallVector<OpAsmParser::UnresolvedOperand> dims;
+        if (failed(parser.parseOperandList(dims))) {
+          return failure();
+        }
+        size_t numDynamicDims = shapedType.getNumDynamicDims();
+        if (dims.size() != numDynamicDims) {
+          return failure();
+        }
+        if (failed(parser.parseRBrace())) {
+          return failure();
+        }
+        dynamicSizes.append(dims);
+      }
+
+      if (i < numResults - 1 && failed(parser.parseComma())) {
+        return failure();
+      }
+    }
+
+    if (failed(parser.parseRParen())) {
+      return failure();
+    }
+  }
+
+  // The stored argument order is:
+  // (initialized vals) (result tied refs) (num threads).
+  SmallVector<OpAsmParser::Argument> args;
+  args.append(regionLeadingArgs);
+  args.append(regionRefArgs);
+  args.append(indexArgs);
+  return parser.parseRegion(body, args, /*enableNameShadowing=*/false);
+}
+
+static ParseResult parseParallelExecutionBody(
+    OpAsmParser &parser, SmallVectorImpl<OpAsmParser::UnresolvedOperand> &inits,
+    SmallVectorImpl<Type> &initTypes,
+    SmallVectorImpl<OpAsmParser::UnresolvedOperand> &dynamicSizes,
+    SmallVectorImpl<Type> &resultTypes, SmallVectorImpl<bool> &isTied,
+    Region &body) {
+  int64_t numLeadingArgs = 0;
+  return parseParallelExecutionBody(parser, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, numLeadingArgs,
+                                    false);
+}
+
+static void printParallelExecutionBody(
+    OpAsmPrinter &p, Operation *op, OperandRange inits, TypeRange initTypes,
+    OperandRange dynamicSizes, TypeRange resultTypes, ArrayRef<bool> isTied,
+    Region &body, int64_t numLeadingArgs, bool printOptionalLeadingArgs) {
+  if (printOptionalLeadingArgs && numLeadingArgs > 0) {
+    p << "-> (";
+    MutableArrayRef<BlockArgument> leadingArgRange =
+        body.getArguments().take_front(numLeadingArgs);
+    llvm::interleaveComma(leadingArgRange, p, [&](BlockArgument arg) {
+      p.printRegionArgument(arg);
+    });
+    p << ")";
+  }
+
+  p.printNewline();
+  p << "  execute";
+
+  int64_t numResults = resultTypes.size();
+  int64_t numIndexArgs = body.getNumArguments() - numResults - numLeadingArgs;
+  MutableArrayRef<BlockArgument> threadCountArgRange =
+      body.getArguments().take_back(numIndexArgs);
+  MutableArrayRef<BlockArgument> refArgRange =
+      body.getArguments().drop_back(numIndexArgs).take_back(numResults);
+
+  if (numResults != 0) {
+    p << "(";
+    int64_t currInitIndex = 0;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      p << refArgRange[i];
+      if (isTied[i]) {
+        p << " = ";
+        p << inits[currInitIndex];
+        ++currInitIndex;
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ")";
+  }
+  p << "[";
+  llvm::interleaveComma(threadCountArgRange, p,
+                        [&](BlockArgument arg) { p.printRegionArgument(arg); });
+  p << "]";
+
+  // Now print the function type.
+  if (numResults != 0) {
+    p.printNewline();
+    // Whitespace to line up parentheses.
+    //   |--execute(
+    //   |--_____: (
+    p << "       : (";
+    llvm::interleaveComma(refArgRange, p,
+                          [&](BlockArgument arg) { p << arg.getType(); });
+    p << ")";
+    p.printNewline();
+    //   |--execute(
+    //   |--____-> (
+    p << "      -> (";
+    OperandRange currSizes = dynamicSizes;
+    for (int64_t i = 0, e = numResults; i < e; ++i) {
+      ShapedType resultType = cast<ShapedType>(resultTypes[i]);
+      bool isResultTied = isTied[i];
+      p << resultType;
+      if (!isResultTied && !resultType.hasStaticShape()) {
+        int64_t numDynamicDims = resultType.getNumDynamicDims();
+        p << "{";
+        llvm::interleaveComma(currSizes.take_front(numDynamicDims), p,
+                              [&](Value dim) { p << dim; });
+        currSizes = currSizes.drop_front(numDynamicDims);
+        p << "}";
+      }
+      if (i < numResults - 1) {
+        p << ", ";
+      }
+    }
+    p << ") ";
+  } else {
+    // Print a space before the region brace if there are no loop results.
+    p << " ";
+  }
+  p.printRegion(body, /*printEntryBlockArgs=*/false,
+                /*printBlockTerminators=*/true);
+}
+
+static void printParallelExecutionBody(OpAsmPrinter &p, Operation *op,
+                                       OperandRange inits, TypeRange initTypes,
+                                       OperandRange dynamicSizes,
+                                       TypeRange resultTypes,
+                                       ArrayRef<bool> isTied, Region &body) {
+  return printParallelExecutionBody(p, op, inits, initTypes, dynamicSizes,
+                                    resultTypes, isTied, body, 0, false);
+}
+
+//===----------------------------------------------------------------------===//
+// GenericOp
+//===----------------------------------------------------------------------===//
+
+static ParseResult parseInferNumIndexArgs(OpAsmParser &parser, Region &body,
+                                          int64_t &numLeadingArgs,
+                                          int64_t &numIndexArgs) {
+  numIndexArgs = 0;
+  for (BlockArgument bbArg :
+       llvm::reverse(body.getArguments().drop_front(numLeadingArgs))) {
+    if (!bbArg.getType().isIndex()) {
+      return success();
+    }
+    ++numIndexArgs;
+  }
+  return success();
+}
+
+static void printInferNumIndexArgs(OpAsmPrinter &, Operation *, Region &,
+                                   int64_t &, int64_t) {
+  // Nothing to do. The number of count args gets parsed solely from the region.
+}
+
+void GenericOp::getAsmBlockArgumentNames(Region &region,
+                                         OpAsmSetValueNameFn setNameFn) {
+  if (&region == &getInitializer()) {
+    return;
+  }
+
+  assert(&region == &getRegion() && "Unexpected region");
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getCountArgs()) {
+    setNameFn(v, "count");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult GenericOp::verify() {
+  if (getNumIndexArgs() % 2 != 0) {
+    return emitOpError("expected even number of id + count args");
+  }
+  if (getRegion().front().getNumArguments() < getNumIndexArgs()) {
+    return emitOpError(
+        "fewer body arguments than specified number of counts/ids.");
+  }
+  return verifyParallelBodyOp(*this, getNumLeadingArgs(), getNumIndexArgs(),
+                              getIdAndCountArgs());
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, int64_t numIterators,
+                      bool syncOnReturn) {
+  GenericOp::build(b, result, TypeRange(), scope, ArrayRef<Value>{},
+                   ArrayRef<Value>{}, ArrayRef<bool>{}, numIterators,
+                   syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      ScopeAttr scope, ValueRange inits, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  GenericOp::build(b, result, resultTypes, scope, inits, ArrayRef<Value>{},
+                   isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope,
+                      ValueRange dynamicSizes, int64_t numIterators,
+                      bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  GenericOp::build(b, result, resultTypes, scope, ArrayRef<Value>{},
+                   dynamicSizes, isTied, numIterators, syncOnReturn);
+}
+
+void GenericOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                      TypeRange resultTypes, ScopeAttr scope, ValueRange inits,
+                      ValueRange dynamicSizes, ArrayRef<bool> isTied,
+                      int64_t numIterators, bool syncOnReturn) {
+
+  result.addAttribute(GenericOp::getScopeAttrName(result.name), scope);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+  inherentAttrs.setNumIndexArgs(2 * numIterators);
+
+  // Add the initializer region.
+  result.addRegion();
+
+  // Add the main region.
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count/id args.
+  Type indexType = b.getIndexType();
+  for (int64_t i = 0; i < 2 * numIterators; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// LoopOp
+//===----------------------------------------------------------------------===//
+
+void LoopOp::getAsmBlockArgumentNames(Region &region,
+                                      OpAsmSetValueNameFn setNameFn) {
+  for (Value v : getIdArgs()) {
+    setNameFn(v, "id");
+  }
+  for (Value v : getRegionRefArgs()) {
+    setNameFn(v, "ref");
+  }
+}
+
+LogicalResult LoopOp::verify() {
+  if (getCount().empty()) {
+    return emitOpError("expected at least one iteration count argument");
+  }
+  if (getBody()->getNumArguments() < getNumIdArgs()) {
+    return emitOpError("fewer body arguments than specified number of ids.");
+  }
+  return verifyParallelBodyOp(*this, /*numLeadingArgs=*/0, getNumIdArgs(),
+                              getIdArgs());
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, bool syncOnReturn) {
+  LoopOp::build(b, result, TypeRange(), scope, count, ArrayRef<Value>{},
+                ArrayRef<Value>{}, ArrayRef<bool>{}, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   ScopeAttr scope, ValueRange count, ValueRange inits,
+                   bool syncOnReturn) {
+  SmallVector<bool> isTied(inits.size(), true);
+  SmallVector<Type> resultTypes =
+      llvm::map_to_vector(inits, [](Value v) -> Type { return v.getType(); });
+  LoopOp::build(b, result, resultTypes, scope, count, inits, ArrayRef<Value>{},
+                isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange dynamicSizes, bool syncOnReturn) {
+  SmallVector<bool> isTied(resultTypes.size(), false);
+  LoopOp::build(b, result, resultTypes, scope, count, ArrayRef<Value>{},
+                dynamicSizes, isTied, syncOnReturn);
+}
+
+void LoopOp::build(mlir::OpBuilder &b, mlir::OperationState &result,
+                   TypeRange resultTypes, ScopeAttr scope, ValueRange count,
+                   ValueRange inits, ValueRange dynamicSizes,
+                   ArrayRef<bool> isTied, bool syncOnReturn) {
+
+  result.addAttribute(LoopOp::getScopeAttrName(result.name), scope);
+  result.addOperands(count);
+  result.addOperands(inits);
+  result.addOperands(dynamicSizes);
+  result.addTypes(resultTypes);
+
+  result.addAttribute(
+      "operandSegmentSizes",
+      b.getDenseI32ArrayAttr({static_cast<int32_t>(count.size()),
+                              static_cast<int32_t>(inits.size()),
+                              static_cast<int32_t>(dynamicSizes.size())}));
+
+  Properties &inherentAttrs = result.getOrAddProperties<Properties>();
+  inherentAttrs.setIsTied(isTied);
+  inherentAttrs.setSyncOnReturn(syncOnReturn);
+
+  Region *region = result.addRegion();
+  OpBuilder::InsertionGuard g(b);
+  b.createBlock(region);
+  Block &entryBlock = region->front();
+
+  // Add block arguments.
+
+  // sref args.
+  for (Type resultType : resultTypes) {
+    auto shapedType = cast<ShapedType>(resultType);
+    entryBlock.addArgument(
+        PCF::ShapedRefType::get(b.getContext(), shapedType.getShape(),
+                                shapedType.getElementType(), scope),
+        result.location);
+  }
+
+  // Thread count args.
+  Type indexType = b.getIndexType();
+  int64_t numCountArgs = count.size() == 0 ? 1 : count.size();
+  for (int64_t i = 0; i < numCountArgs; ++i) {
+    entryBlock.addArgument(indexType, result.location);
+  }
+}
+
+void LoopOp::getSuccessorRegions(RegionBranchPoint point,
+                                 SmallVectorImpl<RegionSuccessor> &regions) {
+  // If the predecessor is the GenericOp, branch into the body.
+  if (point.isParent()) {
+    regions.push_back(RegionSuccessor(&getRegion()));
+    return;
+  }
+
+  // Otherwise, the region branches back to the parent operation.
+  regions.push_back(RegionSuccessor(getOperation(), getResults()));
+}
+
+//===----------------------------------------------------------------------===//
+// Control Flow Ops
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// BranchCondReturnOp
+//===----------------------------------------------------------------------===//
+
+void BranchCondReturnOp::setDest(Block *block) { return setSuccessor(block); }
+
+void BranchCondReturnOp::eraseOperand(unsigned index) {
+  (*this)->eraseOperand(index);
+}
+
+SuccessorOperands BranchCondReturnOp::getSuccessorOperands(unsigned index) {
+  assert(index == 0 && "invalid successor index");
+  // Single index operand produced by this op.
+  return SuccessorOperands(getDestOperandsMutable());
+}
+
+Block *
+BranchCondReturnOp::getSuccessorForOperands(ArrayRef<Attribute> operands) {
+  if (IntegerAttr condAttr =
+          llvm::dyn_cast_or_null<IntegerAttr>(operands.front())) {
+    return condAttr.getValue().isOne() ? nullptr : getDest();
+  }
+  return nullptr;
+}
+
+//===----------------------------------------------------------------------===//
+// WriteOps
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// ParallelInsertSliceOp
+//===----------------------------------------------------------------------===//
+
+// Build a WriteSliceOp with mixed static and dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<OpFoldResult> offsets,
+                         ArrayRef<OpFoldResult> sizes,
+                         ArrayRef<OpFoldResult> strides,
+                         ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, {}, source, dest, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+/// Build an WriteSliceOp with mixed static and dynamic entries
+/// packed into a Range vector.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ArrayRef<Range> ranges,
+                         ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, source, dest, offsets, sizes, strides, attrs);
+}
+
+// Build a WriteSliceOp with dynamic entries.
+void WriteSliceOp::build(OpBuilder &b, OperationState &result, Value source,
+                         Value dest, ValueRange offsets, ValueRange sizes,
+                         ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, source, dest, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// ReadSliceOp
+//===----------------------------------------------------------------------===//
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void ReadSliceOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// GetMemrefOp
+//===----------------------------------------------------------------------===//
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<OpFoldResult> offsets,
+                        ArrayRef<OpFoldResult> sizes,
+                        ArrayRef<OpFoldResult> strides,
+                        ArrayRef<NamedAttribute> attrs) {
+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);
+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);
+  result.addAttributes(attrs);
+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),
+        b.getDenseI64ArrayAttr(staticSizes),
+        b.getDenseI64ArrayAttr(staticStrides));
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ArrayRef<Range> ranges,
+                        ArrayRef<NamedAttribute> attrs) {
+  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
+  build(b, result, resultType, source, offsets, sizes, strides, attrs);
+}
+
+void GetMemrefOp::build(OpBuilder &b, OperationState &result, Type resultType,
+                        Value source, ValueRange offsets, ValueRange sizes,
+                        ValueRange strides, ArrayRef<NamedAttribute> attrs) {
+  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
+      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
+      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
+  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
+      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
+  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
+}
+
+//===----------------------------------------------------------------------===//
+// Folders
+//===----------------------------------------------------------------------===//
+
+LogicalResult WriteSliceOp::fold(FoldAdaptor adaptor,
+                                 SmallVectorImpl<OpFoldResult> &results) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        stride = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  if (!changed) {
+    return failure();
+  }
+
+  // Dispatch back to static/dynamic.
+  SmallVector<int64_t> staticOffsets, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicStrides;
+  dispatchIndexOpFoldResults(mixedOffsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(mixedStrides, dynamicStrides, staticStrides);
+
+  // Update the op's attributes in-place.
+  setStaticOffsetsAttr(builder.getDenseI64ArrayAttr(staticOffsets));
+  setStaticStridesAttr(builder.getDenseI64ArrayAttr(staticStrides));
+  getOffsetsMutable().assign(dynamicOffsets);
+  getStridesMutable().assign(dynamicStrides);
+
+  return success();
+}
+
+OpFoldResult ReadSliceOp::fold(FoldAdaptor adaptor) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        stride = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  if (!changed) {
+    return {};
+  }
+
+  // Dispatch back to static/dynamic.
+  SmallVector<int64_t> staticOffsets, staticStrides;
+  SmallVector<Value> dynamicOffsets, dynamicStrides;
+  dispatchIndexOpFoldResults(mixedOffsets, dynamicOffsets, staticOffsets);
+  dispatchIndexOpFoldResults(mixedStrides, dynamicStrides, staticStrides);
+
+  // Update the op's attributes in-place.
+  setStaticOffsetsAttr(builder.getDenseI64ArrayAttr(staticOffsets));
+  setStaticStridesAttr(builder.getDenseI64ArrayAttr(staticStrides));
+  getOffsetsMutable().assign(dynamicOffsets);
+  getStridesMutable().assign(dynamicStrides);
+
+  return {};
+}
+
+OpFoldResult GetMemrefOp::fold(FoldAdaptor adaptor) {
+  bool changed = false;
+  OpBuilder builder(getContext());
+
+  SmallVector<OpFoldResult> mixedOffsets = getMixedOffsets();
+  SmallVector<OpFoldResult> mixedStrides = getMixedStrides();
+
+  // Try to fold dynamic offsets to static.
+  for (auto &offset : mixedOffsets) {
+    if (auto value = dyn_cast<Value>(offset)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
+        offset = builder.getI64IntegerAttr(*constantValue);
+        changed = true;
+      }
+    }
+  }
+
+  // Try to fold dynamic strides to static.
+  for (auto &stride : mixedStrides) {
+    if (auto value = dyn_cast<Value>(stride)) {
+      std::optional<int64_t> constantValue = getConstantIntValue(value);
+      if (constantValue.has_value()) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/PCF/IR/PCFOps.td:41`

```diff
@@ -0,0 +1,894 @@
+// Copyright 2025 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#ifndef IREE_CODEGEN_DIALECT_PCF_OPS
+#define IREE_CODEGEN_DIALECT_PCF_OPS
+
+include "iree/compiler/Codegen/Dialect/PCF/IR/PCFBase.td"
+include "mlir/IR/BuiltinAttributeInterfaces.td"
+include "mlir/IR/OpAsmInterface.td"
+include "mlir/IR/Properties.td"
+include "mlir/IR/SymbolInterfaces.td"
+include "mlir/Interfaces/ControlFlowInterfaces.td"
+include "mlir/Interfaces/InferTypeOpInterface.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/ViewLikeInterface.td"
+
+//===----------------------------------------------------------------------===//
+// Alloc
+//===----------------------------------------------------------------------===//
+
+def OpGroupAllocOps : OpDocGroup {
+  let summary = "Alloc ops";
+  let description = "";
+}
+
+let opDocGroup = OpGroupAllocOps in {
+
+//===----------------------------------------------------------------------===//
+// AllocOp
+//===----------------------------------------------------------------------===//
+
+def AllocOp : PCF_Op<"alloc", []> {
+  let summary = [{
+    Shaped ref allocation operation
+  }];
+  let description = [{
+    Allocates a `pcf.sref` with the given element type and shape. Dynamic
```

**Comment:**
Is it `pcf.sref` or `pcf.shaped_ref`?

---


---


## [PR #22475](https://github.com/iree-org/iree/pull/22475): [ci] Add CI for MI355x machines

### Review Summary

**COMMENTED** (2025-10-30)

**APPROVED** (2025-10-30)


### Code Comments

**File:** `.github/workflows/pkgci_test_amd_mi35x.yml:21`

```diff
@@ -0,0 +1,69 @@
+# Copyright 2025 The IREE Authors
+#
+# Licensed under the Apache License v2.0 with LLVM Exceptions.
+# See https://llvm.org/LICENSE.txt for license information.
+# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+name: PkgCI Test AMD MI35x
+on:
+  workflow_call:
+    inputs:
+      artifact_run_id:
+        type: string
+        default: ""
+  workflow_dispatch:
+    inputs:
+      artifact_run_id:
+        type: string
+        default: ""
+
+jobs:
+  test_mi325:
```

**Comment:**
Old name?

---

**File:** `.github/workflows/pkgci_test_amd_mi35x.yml:7`

```diff
@@ -0,0 +1,69 @@
+# Copyright 2025 The IREE Authors
+#
+# Licensed under the Apache License v2.0 with LLVM Exceptions.
+# See https://llvm.org/LICENSE.txt for license information.
+# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+name: PkgCI Test AMD MI35x
```

**Comment:**
Can we call this MI355X? I think the runner name is a typo/oversight and we only have mi355x machines 

---


---


## [PR #22342](https://github.com/iree-org/iree/pull/22342): [GPU][Codegen] Expand iteration space based on new `expand_dims` attribute

### Review Summary

**COMMENTED** (2025-11-21)

**COMMENTED** (2025-12-04)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUExpandDimensions.cpp:52`

```diff
@@ -35,51 +37,45 @@ struct GPUExpandDimensionsPass final
 };
 } // namespace
 
-// Map from tensor dimension index to thread level tile size.
 using DimensionExpansionInfo = llvm::SmallDenseMap<unsigned, int64_t>;
 
 static DimensionExpansionInfo
-getExpansionInfo(IREE::GPU::LoweringConfigAttr config) {
-  IREE::GPU::DimensionExpansion expansionFactors =
-      IREE::GPU::getDimensionExpansion(config).value();
-
+getExpansionInfo(IREE::GPU::DimensionExpansionAttr config) {
   DimensionExpansionInfo expansionInfo;
 
-  for (auto [origDimIdx, factors] : llvm::enumerate(expansionFactors)) {
-    if (!factors.empty()) {
-      int64_t expansionFactor = 1;
-      for (int64_t factor : factors) {
-        if (factor > 1) {
-          expansionFactor *= factor;
-        }
-      }
-      if (expansionFactor > 1) {
-        expansionInfo[origDimIdx] = expansionFactor;
-      }
-    }
+  auto reassociationIndices = config.getReassociationIndices();
+  auto outputShape = config.getOutputShape();
+
+  auto computeExpansion = [&](ArrayRef<int64_t> indices) {
+    return llvm::product_of(llvm::make_filter_range(
+        llvm::map_range(indices, [&](int64_t i) { return outputShape[i]; }),
+        [](int64_t size) { return !ShapedType::isDynamic(size); }));
```

**Comment:**
```suggestion
        &ShapedType::isStatic));
```

---


---


## [PR #22317](https://github.com/iree-org/iree/pull/22317): [PJRT] Update rocm pjrt

### Review Summary

**COMMENTED** (2025-11-04)

**COMMENTED** (2025-11-04)


### Code Comments

**File:** `.github/workflows/pkgci_test_pjrt.yml:35`

```diff
@@ -32,9 +32,10 @@ jobs:
             pjrt_platform: cpu
           - runner: ubuntu-24.04
             pjrt_platform: cuda
-          # TODO: enable these AMD runners
-          # - runner: nodai-amdgpu-w7900-x86-64
-          #   pjrt_platform: rocm
+          - runner: nodai-amdgpu-w7900-x86-64
```

**Comment:**
This should be `[Linux, X64, gfx1100]` if it doesn't have to run on w7900 specifically

---

**File:** `integrations/pjrt/src/iree_pjrt/common/api_impl.cc:466`

```diff
@@ -463,6 +463,12 @@ iree_status_t BufferInstance::Delete() {
 
 iree_status_t BufferInstance::CopyToHost(void* dst, iree_host_size_t dst_size,
                                          EventInstance** out_done_event) {
+  // Return immediately if the buffer has zero size
```

**Comment:**
```suggestion
  // Return immediately if the buffer has zero size.
```

---

**File:** `.github/workflows/pkgci_test_pjrt.yml:39`

```diff
@@ -32,10 +32,11 @@ jobs:
             pjrt_platform: cpu
           - runner: ubuntu-24.04
             pjrt_platform: cuda
-          # TODO: enable these AMD runners
-          # - runner: iree-w7900x2
-          #   pjrt_platform: rocm
-          # - runner: iree-w7900x2
+          - runner: [Linux, X64, gfx1100]
+            rocm-chip: gfx1100
+            pjrt_platform: rocm
+          # TODO: enable this AMD runner
+          # - runner: nodai-amdgpu-w7900-x86-64
```

**Comment:**
this comment is outdated

---


---


## [PR #22864](https://github.com/iree-org/iree/pull/22864): Integrates/llvm 20251209

### Review Summary

**APPROVED** (2025-12-09)



---


## [PR #22727](https://github.com/iree-org/iree/pull/22727): [DispatchCreation][NFC] Refactor split reduction helper methods to static functions

### Review Summary

**APPROVED** (2025-11-21)

Thanks. Can you mark your PR title with 'NFC' (non-functional change)?



---


## [PR #22692](https://github.com/iree-org/iree/pull/22692): [tuner][docs] update the example td spec in sharktuner readme

### Review Summary

**APPROVED** (2025-11-18)



---


## [PR #22683](https://github.com/iree-org/iree/pull/22683): [tuner][docs] update sharktuner readme

### Review Summary

**APPROVED** (2025-11-18)


### Code Comments

**File:** `docs/website/docs/reference/tuning.md:308`

```diff
@@ -306,6 +305,12 @@ that conform to the following format:
   the tuning spec includes a named sequence op with name `__kernel_config`,
   which must contain exactly one `foreach_match` op. That `foreach_match` op
   must have exactly one argument and one result of type any_op.
+* IREE provides transform matcher operations (e.g.,
```

**Comment:**
```suggestion
* IREE provides transform match operations (e.g.,
```

---


---


## [PR #22598](https://github.com/iree-org/iree/pull/22598): [Codegen][Tuner] expose python binding for getIGEMMGenericConvDetails

### Review Summary

**CHANGES_REQUESTED** (2025-11-08)


**COMMENTED** (2025-11-08)


**CHANGES_REQUESTED** (2025-11-08)


**COMMENTED** (2025-11-08)


**COMMENTED** (2025-11-10)


**COMMENTED** (2025-11-10)


**COMMENTED** (2025-11-10)


**COMMENTED** (2025-11-13)


**APPROVED** (2025-11-13)


### Code Comments

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_codegen.h:103`

```diff
@@ -91,6 +91,21 @@ ireeCodegenGetAttentionOpDetail(MlirAffineMap qMap, MlirAffineMap kMap,
 MLIR_CAPI_EXPORTED bool
 ireeCodegenMlirOperationIsACodegenAttentionOp(MlirOperation op);
 
+struct ireeCodegenIGEMMGenericConvDetails {
+  MlirAttribute igemmLoopBounds;
+  MlirAttribute convDimsBatch;
+  MlirAttribute convDimsOutputImage;
+  MlirAttribute convDimsOutputChannel;
+  MlirAttribute convDimsFilterLoop;
+  MlirAttribute convDimsInputChannel;
+  MlirAttribute convDimsDepth;
+  bool isOutputChannelFirst;
+  bool isValid;
```

**Comment:**
What does this mean? Would it make sense to have a separate helper function for checking if IGEMM details can be queried for a given op in the first place?

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:256`

```diff
@@ -244,3 +246,71 @@ bool ireeCodegenMlirOperationIsACodegenAttentionOp(MlirOperation op) {
   return llvm::isa<mlir::iree_compiler::IREE::LinalgExt::AttentionOp>(
       unwrap(op));
 }
+
+ireeCodegenIGEMMGenericConvDetails
+ireeCodegenGetIGEMMGenericConvDetails(MlirOperation op) {
+  mlir::Operation *operation = unwrap(op);
+  auto linalgOp = llvm::dyn_cast<mlir::linalg::LinalgOp>(operation);
+
+  if (!linalgOp) {
+    return ireeCodegenIGEMMGenericConvDetails{
```

**Comment:**
Yeah, I think this would be better handled inside python bindings, and to assert here instead.

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_codegen.h:103`

```diff
@@ -91,6 +91,21 @@ ireeCodegenGetAttentionOpDetail(MlirAffineMap qMap, MlirAffineMap kMap,
 MLIR_CAPI_EXPORTED bool
 ireeCodegenMlirOperationIsACodegenAttentionOp(MlirOperation op);
 
+struct ireeCodegenIGEMMGenericConvDetails {
+  MlirAttribute igemmLoopBounds;
+  MlirAttribute convDimsBatch;
+  MlirAttribute convDimsOutputImage;
+  MlirAttribute convDimsOutputChannel;
+  MlirAttribute convDimsFilterLoop;
+  MlirAttribute convDimsInputChannel;
+  MlirAttribute convDimsDepth;
+  bool isOutputChannelFirst;
+  bool isValid;
```

**Comment:**
The rationale is that we generally prefer to invalid states not to be representable: https://geeklaunch.io/blog/make-invalid-states-unrepresentable/

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:33`

```diff
@@ -29,6 +29,9 @@ namespace py = nanobind;
 using namespace nanobind::literals;
 using namespace mlir::python::nanobind_adaptors;
 
+using ireeCodegenIGEMMGenericConvDetails =
+    struct ireeCodegenIGEMMGenericConvDetails;
```

**Comment:**
What does this do?

---

**File:** `compiler/bindings/python/test/api/tuner_api_test.py:305`

```diff
@@ -149,3 +149,182 @@ def test_isa_attention_op():
     assert len(root_op_list) == 1
     assert root_op_list[0].name == "iree_linalg_ext.attention"
     assert iree_codegen.isa_attention_op(root_op_list[0])
+
+
+@run
+def test_igemm_conv_details():
+    # Test 1: conv_2d_nhwc_hwcf.
+    module_str = """
+        module {
+            func.func @conv_2d_nhwc_hwcf(%arg0: tensor<1x16x16x4xf32>, %arg1: tensor<3x3x4x16xf32>, %arg2: tensor<1x14x14x16xf32>) -> tensor<1x14x14x16xf32> {
+                %0 = linalg.conv_2d_nhwc_hwcf { root_op, dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64> }
+                    ins(%arg0, %arg1 : tensor<1x16x16x4xf32>, tensor<3x3x4x16xf32>)
+                    outs(%arg2 : tensor<1x14x14x16xf32>) -> tensor<1x14x14x16xf32>
+                return %0 : tensor<1x14x14x16xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
+    root_op_list = iree_codegen.get_tuner_root_ops(input_module)
+
+    details = iree_codegen.get_igemm_generic_conv_details(root_op_list[0])
+    assert details is not None, "IGEMM details should be valid for NHWC_HWCF conv"
+    assert details.igemm_loop_bounds == [
+        1,
+        14,
+        14,
+        16,
+        36,
+    ], f"Expected [1,14,14,16,36], got {details.igemm_loop_bounds}"
+    assert details.conv_dims_batch == [
+        0
+    ], f"Expected batch=[0], got {details.conv_dims_batch}"
+    assert details.conv_dims_output_image == [
+        1,
+        2,
+    ], f"Expected output_image=[1,2], got {details.conv_dims_output_image}"
+    assert details.conv_dims_output_channel == [
+        3
+    ], f"Expected output_channel=[3], got {details.conv_dims_output_channel}"
+    assert details.conv_dims_filter_loop == [
+        4,
+        5,
+    ], f"Expected filter_loop=[4,5], got {details.conv_dims_filter_loop}"
+    assert details.conv_dims_input_channel == [
+        6
+    ], f"Expected input_channel=[6], got {details.conv_dims_input_channel}"
+    assert (
+        details.is_output_channel_first == False
+    ), f"Expected is_output_channel_first=False, got {details.is_output_channel_first}"
+
+    # Test 2: conv_2d_nhwc_fhwc.
+    module_str = """
+        module {
+            func.func @conv_2d_nhwc_fhwc(%arg0: tensor<1x16x16x4xf32>, %arg1: tensor<16x3x3x4xf32>, %arg2: tensor<1x14x14x16xf32>) -> tensor<1x14x14x16xf32> {
+                %0 = linalg.conv_2d_nhwc_fhwc { root_op, dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64> }
+                    ins(%arg0, %arg1 : tensor<1x16x16x4xf32>, tensor<16x3x3x4xf32>)
+                    outs(%arg2 : tensor<1x14x14x16xf32>) -> tensor<1x14x14x16xf32>
+                return %0 : tensor<1x14x14x16xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
+    root_op_list = iree_codegen.get_tuner_root_ops(input_module)
+
+    details = iree_codegen.get_igemm_generic_conv_details(root_op_list[0])
+    assert details is not None, "IGEMM details should be valid for NHWC_FHWC conv"
+    assert details.igemm_loop_bounds == [
+        1,
+        14,
+        14,
+        16,
+        36,
+    ], f"Expected [1,14,14,16,36], got {details.igemm_loop_bounds}"
+    assert details.conv_dims_batch == [
+        0
+    ], f"Expected batch=[0], got {details.conv_dims_batch}"
+    assert details.conv_dims_output_image == [
+        1,
+        2,
+    ], f"Expected output_image=[1,2], got {details.conv_dims_output_image}"
+    assert details.conv_dims_output_channel == [
+        3
+    ], f"Expected output_channel=[3], got {details.conv_dims_output_channel}"
+    assert isinstance(
+        details.is_output_channel_first, bool
+    ), "Should have is_output_channel_first flag"
+
+    # Test 3: conv_2d_nchw_fchw.
+    module_str = """
+        module {
+            func.func @conv_2d_nchw_fchw(%arg0: tensor<1x4x16x16xf32>, %arg1: tensor<16x4x3x3xf32>, %arg2: tensor<1x16x14x14xf32>) -> tensor<1x16x14x14xf32> {
+                %0 = linalg.conv_2d_nchw_fchw { root_op, dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64> }
+                    ins(%arg0, %arg1 : tensor<1x4x16x16xf32>, tensor<16x4x3x3xf32>)
+                    outs(%arg2 : tensor<1x16x14x14xf32>) -> tensor<1x16x14x14xf32>
+                return %0 : tensor<1x16x14x14xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
+    root_op_list = iree_codegen.get_tuner_root_ops(input_module)
+
+    details = iree_codegen.get_igemm_generic_conv_details(root_op_list[0])
+    assert details is not None, "IGEMM details should be valid for NCHW conv"
+    assert details.igemm_loop_bounds == [
+        1,
+        16,
+        14,
+        14,
+        36,
+    ], f"Expected [1,16,14,14,36], got {details.igemm_loop_bounds}"
+    assert details.conv_dims_batch == [
+        0
+    ], f"Expected batch=[0], got {details.conv_dims_batch}"
+    assert details.conv_dims_output_image == [
+        2,
+        3,
+    ], f"Expected output_image=[2,3], got {details.conv_dims_output_image}"
+    assert details.conv_dims_filter_loop == [
+        5,
+        6,
+    ], f"Expected filter_loop=[5,6], got {details.conv_dims_filter_loop}"
+
+    # Test 4: linalg.generic with convolution pattern (weight backward).
+    module_str = """
+        module {
+            func.func @conv_generic_weight_backward(%arg0: tensor<16x98x64x96xf32>, %arg1: tensor<16x96x64x96xf32>, %arg2: tensor<96x3x96xf32>) -> tensor<96x3x96xf32> {
+                %0 = linalg.generic {
+                    indexing_maps = [
+                        affine_map<(d0, d1, d2, d3, d4, d5) -> (d3, d1 + d4, d5, d2)>,
+                        affine_map<(d0, d1, d2, d3, d4, d5) -> (d3, d4, d5, d0)>,
+                        affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2)>
+                    ],
+                    iterator_types = ["parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]
+                } ins(%arg0, %arg1 : tensor<16x98x64x96xf32>, tensor<16x96x64x96xf32>) outs(%arg2 : tensor<96x3x96xf32>) attrs = {root_op} {
+                ^bb0(%in: f32, %in_1: f32, %out: f32):
+                    %mul = arith.mulf %in, %in_1 : f32
+                    %add = arith.addf %out, %mul : f32
+                    linalg.yield %add : f32
+                } -> tensor<96x3x96xf32>
+                return %0 : tensor<96x3x96xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
+    root_op_list = iree_codegen.get_tuner_root_ops(input_module)
+
+    details = iree_codegen.get_igemm_generic_conv_details(root_op_list[0])
+    assert (
+        details is not None
+    ), "IGEMM details should be valid for generic 1D conv weight backward"
+    assert details.igemm_loop_bounds == [
+        96,
+        3,
+        96,
+        98304,
+    ], f"Expected [96,3,96,98304], got {details.igemm_loop_bounds}"
```

**Comment:**
I think we should either switch these to pytest and get these prints for free or repeating the expected values in error messages

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:264`

```diff
@@ -244,3 +246,49 @@ bool ireeCodegenMlirOperationIsACodegenAttentionOp(MlirOperation op) {
   return llvm::isa<mlir::iree_compiler::IREE::LinalgExt::AttentionOp>(
       unwrap(op));
 }
+
+ireeCodegenIGEMMGenericConvDetails
+ireeCodegenGetIGEMMGenericConvDetails(MlirOperation op) {
+  ireeCodegenIGEMMGenericConvDetails result{};
+  auto linalgOp = llvm::dyn_cast<mlir::linalg::LinalgOp>(unwrap(op));
+  if (!linalgOp) {
+    return result;
+  }
+
+  llvm::FailureOr<mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails>
+      maybeDetails =
+          mlir::iree_compiler::IREE::LinalgExt::getIGEMMGenericConvDetails(
+              linalgOp);
+  if (failed(maybeDetails)) {
+    return result;
+  }
```

**Comment:**
This doesn't remove the invalid state, it just changes the representation.

My suggestion is to have a new public C API function that checks if something has igemm details, and separately a get function that asserts that the igemm details can be queried.

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:33`

```diff
@@ -29,6 +29,9 @@ namespace py = nanobind;
 using namespace nanobind::literals;
 using namespace mlir::python::nanobind_adaptors;
 
+using ireeCodegenIGEMMGenericConvDetails =
+    struct ireeCodegenIGEMMGenericConvDetails;
```

**Comment:**
I don't understand what it does

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:260`

```diff
@@ -244,3 +246,59 @@ bool ireeCodegenMlirOperationIsACodegenAttentionOp(MlirOperation op) {
   return llvm::isa<mlir::iree_compiler::IREE::LinalgExt::AttentionOp>(
       unwrap(op));
 }
+
+bool ireeCodegenHasIGEMMGenericConvDetails(MlirOperation op) {
+  auto linalgOp = llvm::dyn_cast<mlir::linalg::LinalgOp>(unwrap(op));
+  if (!linalgOp) {
+    return false;
+  }
+
+  llvm::FailureOr<mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails>
+      maybeDetails =
+          mlir::iree_compiler::IREE::LinalgExt::getIGEMMGenericConvDetails(
+              linalgOp);
+  return succeeded(maybeDetails);
```

**Comment:**
```suggestion
  return succeeded(mlir::iree_compiler::IREE::LinalgExt::getIGEMMGenericConvDetails(
              linalgOp));
```

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:283`

```diff
@@ -244,3 +246,59 @@ bool ireeCodegenMlirOperationIsACodegenAttentionOp(MlirOperation op) {
   return llvm::isa<mlir::iree_compiler::IREE::LinalgExt::AttentionOp>(
       unwrap(op));
 }
+
+bool ireeCodegenHasIGEMMGenericConvDetails(MlirOperation op) {
+  auto linalgOp = llvm::dyn_cast<mlir::linalg::LinalgOp>(unwrap(op));
+  if (!linalgOp) {
+    return false;
+  }
+
+  llvm::FailureOr<mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails>
+      maybeDetails =
+          mlir::iree_compiler::IREE::LinalgExt::getIGEMMGenericConvDetails(
+              linalgOp);
+  return succeeded(maybeDetails);
+}
+
+ireeCodegenIGEMMGenericConvDetails
+ireeCodegenGetIGEMMGenericConvDetails(MlirOperation op) {
+  auto linalgOp = llvm::cast<mlir::linalg::LinalgOp>(unwrap(op));
+
+  llvm::FailureOr<mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails>
+      maybeDetails =
+          mlir::iree_compiler::IREE::LinalgExt::getIGEMMGenericConvDetails(
+              linalgOp);
+  assert(succeeded(maybeDetails) &&
+         "Failed to get IGEMM details; must check with "
+         "ireeCodegenCanGetIGEMMGenericConvDetails first");
+
+  const mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails &details =
+      *maybeDetails;
+
+  mlir::Builder builder(linalgOp.getContext());
+
+  // Helper to convert unsigned to int64_t.
+  auto toInt64 = [](const llvm::SmallVector<unsigned, 2> &vec) {
+    return llvm::map_to_vector(
+        vec, [](unsigned val) { return static_cast<int64_t>(val); });
```

**Comment:**
```suggestion
    return llvm::map_to_vector(
        vec, llvm::StaticCastTo<int64_t>);
```

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:283`

```diff
@@ -244,3 +246,59 @@ bool ireeCodegenMlirOperationIsACodegenAttentionOp(MlirOperation op) {
   return llvm::isa<mlir::iree_compiler::IREE::LinalgExt::AttentionOp>(
       unwrap(op));
 }
+
+bool ireeCodegenHasIGEMMGenericConvDetails(MlirOperation op) {
+  auto linalgOp = llvm::dyn_cast<mlir::linalg::LinalgOp>(unwrap(op));
+  if (!linalgOp) {
+    return false;
+  }
+
+  llvm::FailureOr<mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails>
+      maybeDetails =
+          mlir::iree_compiler::IREE::LinalgExt::getIGEMMGenericConvDetails(
+              linalgOp);
+  return succeeded(maybeDetails);
+}
+
+ireeCodegenIGEMMGenericConvDetails
+ireeCodegenGetIGEMMGenericConvDetails(MlirOperation op) {
+  auto linalgOp = llvm::cast<mlir::linalg::LinalgOp>(unwrap(op));
+
+  llvm::FailureOr<mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails>
+      maybeDetails =
+          mlir::iree_compiler::IREE::LinalgExt::getIGEMMGenericConvDetails(
+              linalgOp);
+  assert(succeeded(maybeDetails) &&
+         "Failed to get IGEMM details; must check with "
+         "ireeCodegenCanGetIGEMMGenericConvDetails first");
+
+  const mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails &details =
+      *maybeDetails;
+
+  mlir::Builder builder(linalgOp.getContext());
+
+  // Helper to convert unsigned to int64_t.
+  auto toInt64 = [](const llvm::SmallVector<unsigned, 2> &vec) {
+    return llvm::map_to_vector(
+        vec, [](unsigned val) { return static_cast<int64_t>(val); });
```

**Comment:**
It landed in llvm a week ago

---

**File:** `compiler/bindings/python/test/api/tuner_api_test.py:195`

```diff
@@ -149,3 +149,174 @@ def test_isa_attention_op():
     assert len(root_op_list) == 1
     assert root_op_list[0].name == "iree_linalg_ext.attention"
     assert iree_codegen.isa_attention_op(root_op_list[0])
+
+
+@run
+def test_igemm_conv_details():
+    # Test 1: conv_2d_nhwc_hwcf.
+    module_str = """
+        module {
+            func.func @conv_2d_nhwc_hwcf(%arg0: tensor<1x16x16x4xf32>, %arg1: tensor<3x3x4x16xf32>, %arg2: tensor<1x14x14x16xf32>) -> tensor<1x14x14x16xf32> {
+                %0 = linalg.conv_2d_nhwc_hwcf { root_op, dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64> }
+                    ins(%arg0, %arg1 : tensor<1x16x16x4xf32>, tensor<3x3x4x16xf32>)
+                    outs(%arg2 : tensor<1x14x14x16xf32>) -> tensor<1x14x14x16xf32>
+                return %0 : tensor<1x14x14x16xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
+    root_op_list = iree_codegen.get_tuner_root_ops(input_module)
+
+    details = iree_codegen.get_igemm_generic_conv_details(root_op_list[0])
+    assert details is not None, "IGEMM details should be valid for NHWC_HWCF conv"
+    assert details.igemm_loop_bounds == [
+        1,
+        14,
+        14,
+        16,
+        36,
+    ], f"got {details.igemm_loop_bounds}"
+    assert details.conv_dims_batch == [0], f"got {details.conv_dims_batch}"
+    assert details.conv_dims_output_image == [
+        1,
+        2,
+    ], f"got {details.conv_dims_output_image}"
+    assert details.conv_dims_output_channel == [
+        3
+    ], f"got {details.conv_dims_output_channel}"
+    assert details.conv_dims_filter_loop == [
+        4,
+        5,
+    ], f"got {details.conv_dims_filter_loop}"
+    assert details.conv_dims_input_channel == [
+        6
+    ], f"got {details.conv_dims_input_channel}"
+    assert (
+        details.is_output_channel_first == False
```

**Comment:**
```suggestion
        not details.is_output_channel_first
```

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:279`

```diff
@@ -244,3 +246,56 @@ bool ireeCodegenMlirOperationIsACodegenAttentionOp(MlirOperation op) {
   return llvm::isa<mlir::iree_compiler::IREE::LinalgExt::AttentionOp>(
       unwrap(op));
 }
+
+bool ireeCodegenHasIGEMMGenericConvDetails(MlirOperation op) {
+  auto linalgOp = llvm::dyn_cast<mlir::linalg::LinalgOp>(unwrap(op));
+  if (!linalgOp) {
+    return false;
+  }
+
+  return succeeded(
+      mlir::iree_compiler::IREE::LinalgExt::getIGEMMGenericConvDetails(
+          linalgOp));
+}
+
+ireeCodegenIGEMMGenericConvDetails
+ireeCodegenGetIGEMMGenericConvDetails(MlirOperation op) {
+  auto linalgOp = llvm::cast<mlir::linalg::LinalgOp>(unwrap(op));
+
+  llvm::FailureOr<mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails>
+      maybeDetails =
+          mlir::iree_compiler::IREE::LinalgExt::getIGEMMGenericConvDetails(
+              linalgOp);
+  assert(succeeded(maybeDetails) &&
+         "Failed to get IGEMM details; must check with "
+         "ireeCodegenHasIGEMMGenericConvDetails first");
+
+  const mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails &details =
+      *maybeDetails;
+
+  mlir::Builder builder(linalgOp.getContext());
+
+  // Helper to convert unsigned to int64_t.
+  auto toInt64 = [](const llvm::SmallVector<unsigned, 2> &vec) {
```

**Comment:**
```suggestion
  auto toInt64 = [](ArrayRef<unsigned> vec) {
```
to avoid over-relying on the exact small vector size

---

**File:** `compiler/bindings/python/test/api/tuner_api_test.py:311`

```diff
@@ -149,3 +149,174 @@ def test_isa_attention_op():
     assert len(root_op_list) == 1
     assert root_op_list[0].name == "iree_linalg_ext.attention"
     assert iree_codegen.isa_attention_op(root_op_list[0])
+
+
+@run
+def test_igemm_conv_details():
+    # Test 1: conv_2d_nhwc_hwcf.
+    module_str = """
+        module {
+            func.func @conv_2d_nhwc_hwcf(%arg0: tensor<1x16x16x4xf32>, %arg1: tensor<3x3x4x16xf32>, %arg2: tensor<1x14x14x16xf32>) -> tensor<1x14x14x16xf32> {
+                %0 = linalg.conv_2d_nhwc_hwcf { root_op, dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64> }
+                    ins(%arg0, %arg1 : tensor<1x16x16x4xf32>, tensor<3x3x4x16xf32>)
+                    outs(%arg2 : tensor<1x14x14x16xf32>) -> tensor<1x14x14x16xf32>
+                return %0 : tensor<1x14x14x16xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
+    root_op_list = iree_codegen.get_tuner_root_ops(input_module)
+
+    details = iree_codegen.get_igemm_generic_conv_details(root_op_list[0])
+    assert details is not None, "IGEMM details should be valid for NHWC_HWCF conv"
+    assert details.igemm_loop_bounds == [
+        1,
+        14,
+        14,
+        16,
+        36,
+    ], f"got {details.igemm_loop_bounds}"
+    assert details.conv_dims_batch == [0], f"got {details.conv_dims_batch}"
+    assert details.conv_dims_output_image == [
+        1,
+        2,
+    ], f"got {details.conv_dims_output_image}"
+    assert details.conv_dims_output_channel == [
+        3
+    ], f"got {details.conv_dims_output_channel}"
+    assert details.conv_dims_filter_loop == [
+        4,
+        5,
+    ], f"got {details.conv_dims_filter_loop}"
+    assert details.conv_dims_input_channel == [
+        6
+    ], f"got {details.conv_dims_input_channel}"
+    assert (
+        details.is_output_channel_first == False
+    ), f"got {details.is_output_channel_first}"
+
+    # Test 2: conv_2d_nhwc_fhwc.
+    module_str = """
+        module {
+            func.func @conv_2d_nhwc_fhwc(%arg0: tensor<1x16x16x4xf32>, %arg1: tensor<16x3x3x4xf32>, %arg2: tensor<1x14x14x16xf32>) -> tensor<1x14x14x16xf32> {
+                %0 = linalg.conv_2d_nhwc_fhwc { root_op, dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64> }
+                    ins(%arg0, %arg1 : tensor<1x16x16x4xf32>, tensor<16x3x3x4xf32>)
+                    outs(%arg2 : tensor<1x14x14x16xf32>) -> tensor<1x14x14x16xf32>
+                return %0 : tensor<1x14x14x16xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
+    root_op_list = iree_codegen.get_tuner_root_ops(input_module)
+
+    details = iree_codegen.get_igemm_generic_conv_details(root_op_list[0])
+    assert details is not None, "IGEMM details should be valid for NHWC_FHWC conv"
+    assert details.igemm_loop_bounds == [
+        1,
+        14,
+        14,
+        16,
+        36,
+    ], f"got {details.igemm_loop_bounds}"
+    assert details.conv_dims_batch == [0], f"got {details.conv_dims_batch}"
+    assert details.conv_dims_output_image == [
+        1,
+        2,
+    ], f"got {details.conv_dims_output_image}"
+    assert details.conv_dims_output_channel == [
+        3
+    ], f"got {details.conv_dims_output_channel}"
+    assert isinstance(
+        details.is_output_channel_first, bool
+    ), f"got {type(details.is_output_channel_first)}"
+
+    # Test 3: conv_2d_nchw_fchw.
+    module_str = """
+        module {
+            func.func @conv_2d_nchw_fchw(%arg0: tensor<1x4x16x16xf32>, %arg1: tensor<16x4x3x3xf32>, %arg2: tensor<1x16x14x14xf32>) -> tensor<1x16x14x14xf32> {
+                %0 = linalg.conv_2d_nchw_fchw { root_op, dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64> }
+                    ins(%arg0, %arg1 : tensor<1x4x16x16xf32>, tensor<16x4x3x3xf32>)
+                    outs(%arg2 : tensor<1x16x14x14xf32>) -> tensor<1x16x14x14xf32>
+                return %0 : tensor<1x16x14x14xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
+    root_op_list = iree_codegen.get_tuner_root_ops(input_module)
+
+    details = iree_codegen.get_igemm_generic_conv_details(root_op_list[0])
+    assert details is not None, "IGEMM details should be valid for NCHW conv"
+    assert details.igemm_loop_bounds == [
+        1,
+        16,
+        14,
+        14,
+        36,
+    ], f"got {details.igemm_loop_bounds}"
+    assert details.conv_dims_batch == [0], f"got {details.conv_dims_batch}"
+    assert details.conv_dims_output_image == [
+        2,
+        3,
+    ], f"got {details.conv_dims_output_image}"
+    assert details.conv_dims_filter_loop == [
+        5,
+        6,
+    ], f"got {details.conv_dims_filter_loop}"
+
+    # Test 4: linalg.generic with convolution pattern (weight backward).
+    module_str = """
+        module {
+            func.func @conv_generic_weight_backward(%arg0: tensor<16x98x64x96xf32>, %arg1: tensor<16x96x64x96xf32>, %arg2: tensor<96x3x96xf32>) -> tensor<96x3x96xf32> {
+                %0 = linalg.generic {
+                    indexing_maps = [
+                        affine_map<(d0, d1, d2, d3, d4, d5) -> (d3, d1 + d4, d5, d2)>,
+                        affine_map<(d0, d1, d2, d3, d4, d5) -> (d3, d4, d5, d0)>,
+                        affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2)>
+                    ],
+                    iterator_types = ["parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]
+                } ins(%arg0, %arg1 : tensor<16x98x64x96xf32>, tensor<16x96x64x96xf32>) outs(%arg2 : tensor<96x3x96xf32>) attrs = {root_op} {
+                ^bb0(%in: f32, %in_1: f32, %out: f32):
+                    %mul = arith.mulf %in, %in_1 : f32
+                    %add = arith.addf %out, %mul : f32
+                    linalg.yield %add : f32
+                } -> tensor<96x3x96xf32>
+                return %0 : tensor<96x3x96xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
+    root_op_list = iree_codegen.get_tuner_root_ops(input_module)
+
+    details = iree_codegen.get_igemm_generic_conv_details(root_op_list[0])
+    assert (
+        details is not None
+    ), "IGEMM details should be valid for generic 1D conv weight backward"
+    assert details.igemm_loop_bounds == [
+        96,
+        3,
+        96,
+        98304,
+    ], f"got {details.igemm_loop_bounds}"
+    assert details.conv_dims_output_image == [
+        1
+    ], f"got {details.conv_dims_output_image}"
+    assert details.conv_dims_filter_loop == [4], f"got {details.conv_dims_filter_loop}"
+
+    # Test with a non-conv operation.
+    module_str = """
+        module {
+            func.func @matmul(%arg0: tensor<4x4xf32>, %arg1: tensor<4x4xf32>) -> tensor<4x4xf32> {
+                %cst = arith.constant 0.000000e+00 : f32
+                %0 = tensor.empty() : tensor<4x4xf32>
+                %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<4x4xf32>) -> tensor<4x4xf32>
```

**Comment:**
You can make %1 a third function argument to make this test case more concise

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:728`

```diff
@@ -693,4 +694,80 @@ NB_MODULE(_ireeCompilerDialects, m) {
       "isa_attention_op", &ireeCodegenMlirOperationIsACodegenAttentionOp,
       "Checks if the given operation is an IREE LinalgExt attention op.",
       py::arg("op"));
+
+  //===-------------------------------------------------------------------===//
+  // Binding to utility function ireeCodegenGetIGEMMGenericConvDetails
+  //===-------------------------------------------------------------------===//
+  py::class_<ireeCodegenIGEMMGenericConvDetails>(iree_codegen_module,
+                                                 "IGEMMGenericConvDetails")
+      .def_prop_ro("igemm_contraction_maps",
+                   [](const ireeCodegenIGEMMGenericConvDetails &self) {
+                     return self.igemmContractionMaps;
+                   })
+      .def_prop_ro("igemm_loop_bounds",
+                   [](const ireeCodegenIGEMMGenericConvDetails &self) {
+                     return getIntArrayAttrValues(self.igemmLoopBounds);
+                   })
+      .def_prop_ro("igemm_loop_iterators",
+                   [](const ireeCodegenIGEMMGenericConvDetails &self) {
+                     return self.igemmLoopIterators;
+                   })
+      .def_prop_ro("im2col_output_perm",
+                   [](const ireeCodegenIGEMMGenericConvDetails &self) {
+                     return getIntArrayAttrValues(self.im2colOutputPerm);
+                   })
+      .def_prop_ro(
+          "filter_reassoc_indices",
+          [](const ireeCodegenIGEMMGenericConvDetails &self)
+              -> std::vector<std::vector<int64_t>> {
+            std::vector<std::vector<int64_t>> result;
+            MlirAttribute attr = self.filterReassocIndices;
+            assert(!mlirAttributeIsNull(attr) && mlirAttributeIsAArray(attr) &&
+                   "filterReassocIndices should be a valid ArrayAttr");
+            size_t n = mlirArrayAttrGetNumElements(attr);
+            result.reserve(n);
```

**Comment:**
```suggestion
            MlirAttribute attr = self.filterReassocIndices;
            assert(!mlirAttributeIsNull(attr) && mlirAttributeIsAArray(attr) &&
                   "filterReassocIndices should be a valid ArrayAttr");
            size_t n = mlirArrayAttrGetNumElements(attr);
            std::vector<std::vector<int64_t>> result;
            result.reserve(n);
```
try to keep the definition close to the first use

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:298`

```diff
@@ -244,3 +245,76 @@ bool ireeCodegenMlirOperationIsACodegenAttentionOp(MlirOperation op) {
   return llvm::isa<mlir::iree_compiler::IREE::LinalgExt::AttentionOp>(
       unwrap(op));
 }
+
+bool ireeCodegenHasIGEMMGenericConvDetails(MlirOperation op) {
+  auto linalgOp = llvm::dyn_cast<mlir::linalg::LinalgOp>(unwrap(op));
+  if (!linalgOp) {
+    return false;
+  }
+
+  return succeeded(
+      mlir::iree_compiler::IREE::LinalgExt::getIGEMMGenericConvDetails(
+          linalgOp));
+}
+
+ireeCodegenIGEMMGenericConvDetails
+ireeCodegenGetIGEMMGenericConvDetails(MlirOperation op) {
+  auto linalgOp = llvm::cast<mlir::linalg::LinalgOp>(unwrap(op));
+
+  llvm::FailureOr<mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails>
+      maybeDetails =
+          mlir::iree_compiler::IREE::LinalgExt::getIGEMMGenericConvDetails(
+              linalgOp);
+  assert(succeeded(maybeDetails) &&
+         "Failed to get IGEMM details; must check with "
+         "ireeCodegenHasIGEMMGenericConvDetails first");
+
+  const mlir::iree_compiler::IREE::LinalgExt::IGEMMGenericConvDetails &details =
+      *maybeDetails;
+
+  mlir::Builder builder(linalgOp.getContext());
+
+  ireeCodegenIGEMMGenericConvDetails result;
+
+  result.igemmContractionMaps = wrap(builder.getArrayAttr(llvm::map_to_vector(
+      details.igemmContractionMaps, [](auto map) -> mlir::Attribute {
+        return mlir::AffineMapAttr::get(map);
+      })));
+
+  result.igemmLoopBounds =
+      wrap(builder.getI64ArrayAttr(details.igemmLoopBounds));
+
+  llvm::SmallVector<mlir::Attribute> iteratorAttrs;
+  for (auto iterType : details.igemmLoopIterators) {
+    iteratorAttrs.push_back(
+        builder.getStringAttr(mlir::utils::stringifyIteratorType(iterType)));
+  }
+  result.igemmLoopIterators = wrap(builder.getArrayAttr(iteratorAttrs));
+
+  result.im2colOutputPerm =
+      wrap(builder.getI64ArrayAttr(details.im2colOutputPerm));
+
+  llvm::SmallVector<mlir::Attribute> reassocAttrs;
+  for (const auto &indices : details.filterReassocIndices) {
```

**Comment:**
can you spell out this type? https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---


---


## [PR #22490](https://github.com/iree-org/iree/pull/22490): [Codegen][GPU] Generalize linalg.reduce operations

### Review Summary

**COMMENTED** (2025-10-31)


**APPROVED** (2025-11-20)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/TypePropagationPass.cpp:434`

```diff
@@ -429,6 +429,91 @@ struct IREELinalgExtSortTypePropagation
   }
 };
 
+/// Pattern to legalize `linalg.reduce` operations.
+struct ReduceOpTypePropagation
+    : public TypePropagationPattern<linalg::ReduceOp> {
```

**Comment:**
```suggestion
struct ReduceOpTypePropagation final
    : TypePropagationPattern<linalg::ReduceOp> {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/TypePropagationPass.cpp:435`

```diff
@@ -429,6 +429,91 @@ struct IREELinalgExtSortTypePropagation
   }
 };
 
+/// Pattern to legalize `linalg.reduce` operations.
+struct ReduceOpTypePropagation
+    : public TypePropagationPattern<linalg::ReduceOp> {
+  using TypePropagationPattern<linalg::ReduceOp>::TypePropagationPattern;
```

**Comment:**
```suggestion
  using TypePropagationPattern::TypePropagationPattern;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/TypePropagationPass.cpp:441`

```diff
@@ -429,6 +429,91 @@ struct IREELinalgExtSortTypePropagation
   }
 };
 
+/// Pattern to legalize `linalg.reduce` operations.
+struct ReduceOpTypePropagation
+    : public TypePropagationPattern<linalg::ReduceOp> {
+  using TypePropagationPattern<linalg::ReduceOp>::TypePropagationPattern;
+
+  LogicalResult
+  matchAndRewrite(linalg::ReduceOp reduceOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const final {
+    bool needsConversion = false;
+    for (auto &operand : reduceOp->getOpOperands()) {
```

**Comment:**
spell out types that are not obvious based on the immediate context: https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---

**File:** `compiler/src/iree/compiler/Codegen/Common/TypePropagationPass.cpp:443`

```diff
@@ -429,6 +429,91 @@ struct IREELinalgExtSortTypePropagation
   }
 };
 
+/// Pattern to legalize `linalg.reduce` operations.
+struct ReduceOpTypePropagation
+    : public TypePropagationPattern<linalg::ReduceOp> {
+  using TypePropagationPattern<linalg::ReduceOp>::TypePropagationPattern;
+
+  LogicalResult
+  matchAndRewrite(linalg::ReduceOp reduceOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const final {
+    bool needsConversion = false;
+    for (auto &operand : reduceOp->getOpOperands()) {
+      Type operandType = operand.get().getType();
+      Type legalizedType = this->getTypeConverter()->convertType(operandType);
```

**Comment:**
what if type conversion fails?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/TypePropagationPass.cpp:465`

```diff
@@ -429,6 +429,91 @@ struct IREELinalgExtSortTypePropagation
   }
 };
 
+/// Pattern to legalize `linalg.reduce` operations.
+struct ReduceOpTypePropagation
+    : public TypePropagationPattern<linalg::ReduceOp> {
+  using TypePropagationPattern<linalg::ReduceOp>::TypePropagationPattern;
+
+  LogicalResult
+  matchAndRewrite(linalg::ReduceOp reduceOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const final {
+    bool needsConversion = false;
+    for (auto &operand : reduceOp->getOpOperands()) {
+      Type operandType = operand.get().getType();
+      Type legalizedType = this->getTypeConverter()->convertType(operandType);
+      if (operandType != legalizedType) {
+        needsConversion = true;
+        break;
+      }
+    }
+
+    if (!needsConversion) {
+      return rewriter.notifyMatchFailure(
+          reduceOp, "all types already legal within conversion pattern");
+    }
+
+    SmallVector<Type> resultTypes;
+    for (auto init : reduceOp.getInits()) {
+      Type legalizedType =
+          this->getTypeConverter()->convertType(init.getType());
+      resultTypes.push_back(legalizedType);
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/TypePropagationPass.cpp:497`

```diff
@@ -429,6 +429,91 @@ struct IREELinalgExtSortTypePropagation
   }
 };
 
+/// Pattern to legalize `linalg.reduce` operations.
+struct ReduceOpTypePropagation
+    : public TypePropagationPattern<linalg::ReduceOp> {
+  using TypePropagationPattern<linalg::ReduceOp>::TypePropagationPattern;
+
+  LogicalResult
+  matchAndRewrite(linalg::ReduceOp reduceOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const final {
+    bool needsConversion = false;
+    for (auto &operand : reduceOp->getOpOperands()) {
+      Type operandType = operand.get().getType();
+      Type legalizedType = this->getTypeConverter()->convertType(operandType);
+      if (operandType != legalizedType) {
+        needsConversion = true;
+        break;
+      }
+    }
+
+    if (!needsConversion) {
+      return rewriter.notifyMatchFailure(
+          reduceOp, "all types already legal within conversion pattern");
+    }
+
+    SmallVector<Type> resultTypes;
+    for (auto init : reduceOp.getInits()) {
+      Type legalizedType =
+          this->getTypeConverter()->convertType(init.getType());
+      resultTypes.push_back(legalizedType);
+    }
+
+    auto modifiedOp = cast<linalg::ReduceOp>(mlir::cloneWithoutRegions(
+        rewriter, reduceOp.getOperation(), resultTypes, adaptor.getOperands()));
+
+    rewriter.inlineRegionBefore(reduceOp.getCombiner(),
+                                modifiedOp.getCombiner(),
+                                modifiedOp.getCombiner().begin());
+    Region &modifiedOpRegion = modifiedOp.getCombiner();
+
+    TypeConverter::SignatureConversion signatureConverter(
+        modifiedOpRegion.getNumArguments());
+    for (auto [index, arg] : llvm::enumerate(modifiedOpRegion.getArguments())) {
+      Type argType = arg.getType();
+      std::optional<Type> legalizedArgType =
+          legalizeStorageElementType(argType);
+      if (!legalizedArgType) {
+        return reduceOp.emitOpError("failed to get legalized type for arg ")
+               << index;
+      }
+      signatureConverter.addInputs(index, legalizedArgType.value());
+    }
+    rewriter.applySignatureConversion(&modifiedOpRegion.front(),
+                                      signatureConverter, getTypeConverter());
+
+    OpBuilder::InsertionGuard g(rewriter);
+    Block *entryBlock = &modifiedOpRegion.front();
+    Operation *yieldOp = entryBlock->getTerminator();
+    rewriter.setInsertionPoint(yieldOp);
+
+    SmallVector<Value> convertedYieldValues;
+    bool needsUpdate = false;
+    for (Value yieldValue : yieldOp->getOperands()) {
+      Type yieldType = yieldValue.getType();
+      std::optional<Type> legalizedYieldType =
+          legalizeStorageElementType(yieldType);
+
+      Value valueToYield = yieldValue; // Default to original value
```

**Comment:**
```suggestion
      Value valueToYield = yieldValue; // Default to original value.
```
https://llvm.org/docs/CodingStandards.html#commenting

---


---


## [PR #22466](https://github.com/iree-org/iree/pull/22466): [DispatchCreation] Set split reduction size for ArgCompare

### Review Summary

**COMMENTED** (2025-11-20)


**APPROVED** (2025-11-20)


### Code Comments

**File:** `compiler/src/iree/compiler/DispatchCreation/test/set_split_reduction_sizes_outer_reduction.mlir:188`

```diff
@@ -179,3 +179,24 @@ util.func public @arg_compare_negative_outer_dynamic_reduction(
 
   util.return %res_val, %res_idx : tensor<64xf32>, tensor<64xindex>
 }
+
+// -----
+
+// CHECK-LABEL: @arg_compare_large_inner_reduction
+util.func public @arg_compare_large_inner_reduction(%arg0: tensor<4x1x128256xf16>)
+    -> tensor<4x1xi32> {
+  // CHECK: iree_linalg_ext.split_reduction = [1336 : index]
```

**Comment:**
Maybe it would be also worth adding a test case for when split reduction is not applied because the reduction dimension is below the threshold?

---


---


## [PR #22409](https://github.com/iree-org/iree/pull/22409): [Codegen][Tuner] solve name conflicts for merging td specs

### Review Summary

**CHANGES_REQUESTED** (2025-10-25)

Typo in the PR title: s/conflictions/conflicts/


**COMMENTED** (2025-10-27)


**APPROVED** (2025-10-27)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:189`

```diff
@@ -174,7 +177,17 @@ static void updateNamedSequenceOp(
   }
 
   std::string newSpecName = llvm::formatv("{}_{}", moduleName, specName).str();
-  op.setSymName(newSpecName);
+
+  // Ensure the new name is unique by appending a counter if needed.
+  std::string uniqueNewSpecName = newSpecName;
+  unsigned suffix = 0;
+  while (seenNames.contains(uniqueNewSpecName)) {
+    uniqueNewSpecName = llvm::formatv("{}_{}", newSpecName, suffix).str();
+    ++suffix;
+  }
+
+  seenNames.insert(builder.getStringAttr(uniqueNewSpecName).getValue());
```

**Comment:**
There's no need to build a string attribute here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:153`

```diff
@@ -144,16 +144,19 @@ static TuningSpecsToMerge collectTuningSpecsToMerge(ModuleOp module) {
 
 // Renames a `NamedSequenceOp` to resolve name conflicts caused by merging
 // tuning specs.
-// The name conflict resolution strategy follows below two rules:
+// The name conflict resolution strategy follows below rules:
 //   1. If the `NamedSequenceOp` is inside a module with a valid symbol name,
 //      its new name is prefixed with its containing module's symbol name.
 //   2. If the module has no symbol name, an incrementing counter is used
 //      to generate a unique prefix (e.g., `m0_`, `m1_`, etc.).
+//   3. If the prefixed name still conflicts with an existing name, a numeric
+//      suffix is appended (e.g., `m0_apply_op_config_0`) until a unique name
```

**Comment:**
Could we make it simpler by appending unique suffix only? Not sure how much value there is in having these prefixes since usually the names we append come from tiny modules with a single matcher only.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:159`

```diff
@@ -143,38 +143,24 @@ static TuningSpecsToMerge collectTuningSpecsToMerge(ModuleOp module) {
 }
 
 // Renames a `NamedSequenceOp` to resolve name conflicts caused by merging
-// tuning specs.
-// The name conflict resolution strategy follows below two rules:
-//   1. If the `NamedSequenceOp` is inside a module with a valid symbol name,
-//      its new name is prefixed with its containing module's symbol name.
-//   2. If the module has no symbol name, an incrementing counter is used
-//      to generate a unique prefix (e.g., `m0_`, `m1_`, etc.).
+// tuning specs by appending a numeric suffix until a unique name is found.
 static void updateNamedSequenceOp(
     NamedSequenceOp op, OpBuilder &builder,
     llvm::DenseMap<NamedSequenceOp, ForeachMatchOp> &namedSequenceToUser,
-    llvm::DenseMap<ModuleOp, std::string> &unnamedModuleNames,
-    unsigned &unnamedModuleCounter) {
+    llvm::DenseSet<StringRef> &seenNames) {
   StringRef specName = op.getSymName();
-  ModuleOp parentModule = op->getParentOfType<ModuleOp>();
-  assert(parentModule);
-  StringAttr parentSymbol = parentModule.getSymNameAttr();
-  std::string moduleName;
-  if (parentSymbol) {
-    moduleName = parentSymbol.getValue().str();
-  } else {
-    if (unnamedModuleNames.contains(parentModule)) {
-      moduleName = unnamedModuleNames[parentModule];
-    } else {
-      std::string newModuleName =
-          llvm::formatv("m{}", unnamedModuleCounter).str();
-      ++unnamedModuleCounter;
-      unnamedModuleNames[parentModule] = newModuleName;
-      moduleName = newModuleName;
-    }
+
+  // Ensure the name is unique by appending a numeric suffix if needed.
+  std::string uniqueNewSpecName = specName.str();
+  unsigned suffix = 0;
+  while (seenNames.contains(uniqueNewSpecName)) {
+    uniqueNewSpecName = llvm::formatv("{}_{}", specName, suffix).str();
+    ++suffix;
   }
```

**Comment:**
nit: You can turn this into a for loop since `suffix` is not used anywhere else

```suggestion
  for (unsigned suffix = 0; seenNames.contains(uniqueNewSpecName); ++suffix) {
    uniqueNewSpecName = llvm::formatv("{}_{}", specName, suffix).str();
  }
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:226`

```diff
@@ -237,11 +223,8 @@ static LogicalResult resolveAndMoveNamedSequenceOps(
 
   // Update conflicted named sequence ops.
   if (!nameConflictOps.empty()) {
-    llvm::DenseMap<ModuleOp, std::string> unnamedModuleNames;
-    unsigned unnamedModuleCounter = 0;
     for (NamedSequenceOp op : nameConflictOps) {
```

**Comment:**
We don't need the `if` statement above

---


---


## [PR #22348](https://github.com/iree-org/iree/pull/22348): [Codegen][Tuner] Add root_op for matvec and reduction along VectorDistribute pipeline 

### Review Summary

**COMMENTED** (2025-10-18)

Nice catch. Can we simply the tests though (e.g., use function arguments like the other existing rest)?


**COMMENTED** (2025-10-20)


**APPROVED** (2025-10-20)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/test/config_root_op_attribute.mlir:22`

```diff
@@ -10,3 +10,52 @@ func.func @matmul(%lhs: tensor<4x4xf32>, %rhs: tensor<4x4xf32>) -> tensor<4x4xf3
 }
 
 // CHECK: %2 = linalg.matmul {lowering_config = #{{.*}}, root_op} ins(%arg0, %arg1 : tensor<4x4xf32>, tensor<4x4xf32>) outs(%1 : tensor<4x4xf32>) -> tensor<4x4xf32>
+
+// -----
+
+#map = affine_map<(d0, d1, d2) -> (d0, d2)>
+#map1 = affine_map<(d0, d1, d2) -> (d1, d2)>
+#map2 = affine_map<(d0, d1, d2) -> (d0, d1)>
+
+func.func @matvec_like(%lhs: tensor<1x4096xf16>, %rhs: tensor<32000x4096xf16>, %init: tensor<1x32000xf16>) {
+  %output = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer>]>) binding(0) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x32000xf16>>
+  %result = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction"]}
```

**Comment:**
Can we use linalg.matmul like in the test cases above? This should make the IR more concise.

---


---


## [PR #22311](https://github.com/iree-org/iree/pull/22311): [python] Set up python binding for matcher convolution and attention op

### Review Summary

**APPROVED** (2025-10-15)



---


## [PR #22270](https://github.com/iree-org/iree/pull/22270): [Codegen] Update the assembly formats and corresponding tests for matcher ops

### Review Summary

**APPROVED** (2025-10-10)



---


## [PR #22266](https://github.com/iree-org/iree/pull/22266): [Codegen] Update the td spec using the attention matcher op

### Review Summary

**COMMENTED** (2025-10-10)

Can you un-stack this PR? I don't think it depends on any of the base PRs.


**COMMENTED** (2025-10-10)

Can we layer these upgrades to that we handle contractions / convs / attention separately, in case one of them causes issues? Splitting them by file doesn't really change the blast radius IMO.


**COMMENTED** (2025-10-10)

LGTM but this doesn't need the `(2/2)` in the PR title since there's only one PR for attention


**APPROVED** (2025-10-10)



---


## [PR #22249](https://github.com/iree-org/iree/pull/22249): [Codegen] Update the td spec using the contraction matcher op

### Review Summary

**COMMENTED** (2025-10-10)

Can you rebase it and remove unrelated base commits?

> It seems that using
> // transform.print %attention {name = "Applied attention config"} : !transform.any_op
> causes a segment fault when compiling sdxl mlir (stable_diffusion_xl_base_1_0_punet_bs1_64_1024x1024_i8.mlir) .

You need to run with threading disabled for prints to work. `--mlir-disable-threading`


**COMMENTED** (2025-10-10)

Looks good, just two high level comments:
1. Can you specify it's for contraction ops in the PR description?
2. Have you checked that the new transform applies to the same ops and results in the same compilation info applied across all dispatches?


**COMMENTED** (2025-10-10)

Looks good overall, but I'd like to make sure we can drop the flux spec.

Also, we don't need the '(1/2)' in the PR title anymore, since there's no follow up PR for contraction matrchers


**COMMENTED** (2025-10-10)


**APPROVED** (2025-10-10)


### Code Comments

**File:** `tests/external/iree-test-suites/test_suite_files/attention_and_matmul_spec_punet_mi300.mlir:9`

```diff
@@ -6,7 +6,6 @@ module attributes { transform.with_named_sequence } {
   transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly},
                                             %config: !transform.any_param {transform.readonly}) {
     transform.annotate %op "compilation_info" = %config : !transform.any_op, !transform.any_param
-    // transform.print %op {name = "Applied"} : !transform.any_op
```

**Comment:**
Can you undo this? These prints are useful for local debugging and it's nice to able to just uncomment them.

---

**File:** `tests/external/iree-test-suites/test_suite_files/attention_and_matmul_spec_punet_mi300.mlir:18`

```diff
@@ -15,32 +14,9 @@ module attributes { transform.with_named_sequence } {
                                                  %decomposition_config: !transform.any_param {transform.readonly}) {
     transform.annotate %attention "compilation_info" = %config : !transform.any_op, !transform.any_param
     transform.annotate %attention "decomposition_config" = %decomposition_config : !transform.any_op, !transform.any_param
-    // transform.print %attention {name = "Applied attention config"} : !transform.any_op
```

**Comment:**
also here

---

**File:** `tests/external/iree-test-suites/test_suite_files/attention_and_matmul_spec_flux_mi300.mlir:1`

```diff
@@ -1,186 +0,0 @@
-module attributes {transform.with_named_sequence} {
```

**Comment:**
How do you know this is unused?

---

**File:** `tests/external/iree-test-suites/test_suite_files/attention_and_matmul_spec_flux_mi300.mlir:1`

```diff
@@ -1,186 +0,0 @@
-module attributes {transform.with_named_sequence} {
```

**Comment:**
I also checked iree-test-suites and I don't see it being used there either: https://github.com/search?q=repo%3Airee-org%2Firee-test-suites%20attention_and_matmul&type=code

---


---


## [PR #22227](https://github.com/iree-org/iree/pull/22227): [python] Set up binding for preprocessing transform ops

### Review Summary

**COMMENTED** (2025-10-07)

Looks good overall. Maybe reference the mlir PR you used as a reference in the PR description, so that we can track the overall design/mechanics to that original PR?


**COMMENTED** (2025-10-08)


**APPROVED** (2025-10-10)


### Code Comments

**File:** `compiler/bindings/python/test/ir/dialects_test.py:650`

```diff
@@ -602,3 +611,83 @@ def gpu_target_info_constructor_error_cases():
         assert False, "Expected TypeError for wrong MMA intrinsic object type"
     except TypeError:
         pass
+
+
+# ======================================================================
+# Preprocessing Transform Extensions
+# ======================================================================
+
+
+@run
+def preprocessing_transform_contraction_example():
+    ctx = ir.Context.current
+    module = ir.Module.parse(
+        """
+        #map_lhs = affine_map<(d0, d1, d2) -> (d0, d2)>
+        #map_rhs = affine_map<(d0, d1, d2) -> (d2, d1)>
+        #map_output = affine_map<(d0, d1, d2) -> (d0, d1)>
+        module attributes {transform.with_named_sequence} {
+            transform.named_sequence @match_matmul(
+                %module: !transform.any_op {transform.readonly})
+                -> !transform.any_op {
+                %matmuls = transform.structured.match ops{["linalg.matmul"]}
+                    in %module : (!transform.any_op) -> !transform.any_op
+                %batch, %m, %n, %k =
+                    transform.iree.match.contraction %matmuls,
+                    lhs_type = f32, rhs_type = f32, output_type = f32
+                    {indexing_maps = [#map_lhs, #map_rhs, #map_output]}
+                    : !transform.any_op -> !transform.param<i64>
+                transform.iree.match.dims_equal %batch, [] : !transform.param<i64>
+                transform.iree.match.dims_equal %m, [4096] : !transform.param<i64>
+                transform.iree.match.dims_equal %n, [2048] : !transform.param<i64>
+                transform.iree.match.dims_equal %k, [8192] : !transform.param<i64>
+                transform.yield %matmuls : !transform.any_op
+            }
+        }
+    """,
+        ctx,
+    )
+    assert module is not None
```

**Comment:**
I don't understand why we have this test case -- I think it will parse correctly no matter how we set up python bindings, won't it?

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:693`

```diff
@@ -602,3 +611,83 @@ def gpu_target_info_constructor_error_cases():
         assert False, "Expected TypeError for wrong MMA intrinsic object type"
     except TypeError:
         pass
+
+
+# ======================================================================
+# Preprocessing Transform Extensions
+# ======================================================================
+
+
+@run
+def preprocessing_transform_contraction_example():
+    ctx = ir.Context.current
+    module = ir.Module.parse(
+        """
+        #map_lhs = affine_map<(d0, d1, d2) -> (d0, d2)>
+        #map_rhs = affine_map<(d0, d1, d2) -> (d2, d1)>
+        #map_output = affine_map<(d0, d1, d2) -> (d0, d1)>
+        module attributes {transform.with_named_sequence} {
+            transform.named_sequence @match_matmul(
+                %module: !transform.any_op {transform.readonly})
+                -> !transform.any_op {
+                %matmuls = transform.structured.match ops{["linalg.matmul"]}
+                    in %module : (!transform.any_op) -> !transform.any_op
+                %batch, %m, %n, %k =
+                    transform.iree.match.contraction %matmuls,
+                    lhs_type = f32, rhs_type = f32, output_type = f32
+                    {indexing_maps = [#map_lhs, #map_rhs, #map_output]}
+                    : !transform.any_op -> !transform.param<i64>
+                transform.iree.match.dims_equal %batch, [] : !transform.param<i64>
+                transform.iree.match.dims_equal %m, [4096] : !transform.param<i64>
+                transform.iree.match.dims_equal %n, [2048] : !transform.param<i64>
+                transform.iree.match.dims_equal %k, [8192] : !transform.param<i64>
+                transform.yield %matmuls : !transform.any_op
+            }
+        }
+    """,
+        ctx,
+    )
+    assert module is not None
+
+
+@run
+def preprocessing_transform_match_contraction_in_named_sequence():
+    module_op = ir.Module.create()
+    module_op.operation.attributes["transform.with_named_sequence"] = ir.UnitAttr.get()
+    map_lhs = ir.AffineMap.get(
+        dim_count=3,
+        symbol_count=0,
+        exprs=[ir.AffineExpr.get_dim(0), ir.AffineExpr.get_dim(2)],
+    )
+    map_rhs = ir.AffineMap.get(
+        dim_count=3,
+        symbol_count=0,
+        exprs=[ir.AffineExpr.get_dim(2), ir.AffineExpr.get_dim(1)],
+    )
+    map_output = ir.AffineMap.get(
+        dim_count=3,
+        symbol_count=0,
+        exprs=[ir.AffineExpr.get_dim(0), ir.AffineExpr.get_dim(1)],
+    )
+
+    with ir.InsertionPoint(module_op.body):
+        named_seq = transform.NamedSequenceOp(
+            "match_matmul", [transform.AnyOpType.get()], [transform.AnyOpType.get()]
+        )
+        with ir.InsertionPoint(named_seq.body):
+            batch, m, n, k = preprocessing_transform.MatchContractionOp(
+                operand_handle=named_seq.bodyTarget,
+                lhs_type=ir.F32Type.get(),
+                rhs_type=ir.F32Type.get(),
+                output_type=ir.F32Type.get(),
+                indexing_maps=[map_lhs, map_rhs, map_output],
+            )
+            transform.YieldOp([named_seq.bodyTarget])
+
+    module_str = str(module_op)
+    assert "affine_map<(d0, d1, d2) -> (d0, d2)>" in module_str
+    assert "affine_map<(d0, d1, d2) -> (d2, d1)>" in module_str
+    assert "affine_map<(d0, d1, d2) -> (d0, d1)>" in module_str
+    assert "transform.with_named_sequence" in module_str
+    assert "transform.named_sequence @match_matmul" in module_str
+    assert "transform.iree.match.contraction" in module_str
```

**Comment:**
Can we check that this contraction matcher has the expected indexing maps?

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:239`

```diff
@@ -219,6 +219,7 @@ def MatchContractionOp : Op<Transform_Dialect, "iree.match.contraction",
     `,` `lhs_type` `=` $lhs_type
     `,` `rhs_type` `=` $rhs_type
     `,` `output_type` `=` $output_type
+    (`,` `indexing_maps` `=` $indexing_maps^)?
```

**Comment:**
Can we also use this syntax in all tests and add an op example in `let description = ` above?

---


---


## [PR #22201](https://github.com/iree-org/iree/pull/22201): [Codegen] Follow-up Fix for MatchContractionOp

### Review Summary

**APPROVED** (2025-10-03)



---


## [PR #22199](https://github.com/iree-org/iree/pull/22199): [Codegen] add transform op for matching attention op

### Review Summary

**COMMENTED** (2025-10-03)


**APPROVED** (2025-10-03)


### Code Comments

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:377`

```diff
@@ -225,6 +225,66 @@ def MatchContractionOp : Op<Transform_Dialect, "iree.match.contraction",
   let cppNamespace = "mlir::iree_compiler::IREE::transform_dialect";
 }
 
+def MatchAttentionOp : Op<Transform_Dialect, "iree.match.attention",
+    [MatchOpInterface,
+     SingleOpMatcher,
+     MemoryEffectsOpInterface,
+     AllTypesMatch<["batch_dims", "m_dims", "n_dims",
+                    "k1_dims", "k2_dims"]>
+     ]> {
+  let summary = [{Check whether the op is an attention operation.}];
+  let description = [{
+    Matches operations from the IREELinalgExt dialect that implement
+    attention: iree_linalg_ext.attention.
+
+    #### Return modes
+
+    Succeeds if the operation is an attention operation, and
+    produces a silenceable failure otherwise.
+
+    #### Results
+
+    Returns arrays of dimension sizes extracted from the iteration domain:
+    - batch_dims: Array of batch dimension sizes.
+    - m_dims: Array of query sequence length dimension sizes.
+    - n_dims: Array of number of heads dimension sizes.
+    - k1_dims: Array of key/value sequence length dimension sizes.
+    - k2_dims: Array of key embedding dimension sizes.
+
+    The exact interpretation depends on the indexing maps of the attention op.
```

**Comment:**
Can you also add an example?

---


---


## [PR #22194](https://github.com/iree-org/iree/pull/22194): [Codegen] Add transform op for matching convolution ops

### Review Summary

**COMMENTED** (2025-10-02)


**COMMENTED** (2025-10-03)

Could you move the contraction matcher syntax and error printing changes to a separate PR?


**COMMENTED** (2025-10-03)


**COMMENTED** (2025-10-03)


**COMMENTED** (2025-10-03)


**APPROVED** (2025-10-03)


**COMMENTED** (2025-10-03)


### Code Comments

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:318`

```diff
@@ -303,6 +303,108 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchConvolutionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchConvolutionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  Location loc = current->getLoc();
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(loc)
+           << "Operation " << *current << " is not a LinalgOp.";
```

**Comment:**
Don't print the whole op, this can be very verbose. We should also remove that from the matcher for contractions.

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:323`

```diff
@@ -303,6 +303,108 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchConvolutionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchConvolutionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  Location loc = current->getLoc();
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(loc)
+           << "Operation " << *current << " is not a LinalgOp.";
+  }
+
+  if (!linalg::isaConvolutionOpInterface(linalgOp)) {
+    return emitSilenceableFailure(loc)
+           << "Operation " << *current << " is not a convolution operation.";
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:283`

```diff
@@ -225,6 +225,68 @@ def MatchContractionOp : Op<Transform_Dialect, "iree.match.contraction",
   let cppNamespace = "mlir::iree_compiler::IREE::transform_dialect";
 }
 
+def MatchConvolutionOp : Op<Transform_Dialect, "iree.match.convolution",
+    [MatchOpInterface,
+     SingleOpMatcher,
+     MemoryEffectsOpInterface]> {
+  let summary = [{Check whether the op is a convolution operation.}];
+  let description = [{
+    Matches operations that implement the ConvolutionOpInterface.
+    This includes operations like linalg.conv_2d_nhwc_hwcf,
+    linalg.conv_2d_nchw_fchw, linalg.depthwise_conv_2d_nhwc_hwc, etc.
+
+    Optionally matches specific indexing maps patterns.
+
+    #### Return modes
+
+    Succeeds if the operation is a convolution operation, and
+    produces a silenceable failure otherwise.
+
+    #### Results
+
+    Returns arrays of dimension sizes for each convolution dimension:
+    - batch_dims: Array of batch dimension sizes.
+    - output_image_dims: Array of output spatial dimension sizes.
+    - output_channel_dims: Array of output channel dimension sizes.
+    - filter_dims: Array of filter spatial dimension sizes.
+    - input_channel_dims: Array of input channel dimension sizes.
+    - depth_dims: Array of depth dimension sizes (for depthwise convolutions).
+    - strides: Array of stride values.
+    - dilations: Array of dilation values.
+  }];
+
+  let arguments = (ins
+    TransformHandleTypeInterface:$operand_handle,
+    TypeAttr:$lhs_type,
+    TypeAttr:$rhs_type,
+    TypeAttr:$output_type,
+    OptionalAttr<AffineMapArrayAttr>:$indexing_maps
+  );
+
+  let results = (outs
+    TransformParamTypeInterface:$batch_dims,
+    TransformParamTypeInterface:$output_image_dims,
+    TransformParamTypeInterface:$output_channel_dims,
+    TransformParamTypeInterface:$filter_dims,
+    TransformParamTypeInterface:$input_channel_dims,
+    TransformParamTypeInterface:$depth_dims,
+    TransformParamTypeInterface:$strides,
+    TransformParamTypeInterface:$dilations
+  );
+
+  let assemblyFormat = [{
+    $operand_handle
+    `,` `lhs_type` `=` $lhs_type
+    `,` `rhs_type` `=` $rhs_type
+    `,` `output_type` `=` $output_type
+    (`indexing_maps` $indexing_maps^)?
+    attr-dict `:` functional-type(operands, results)
```

**Comment:**
Can we pick an assembly format that doesn't print `!transform.param<i64>` 8 times? This was still OK with contractions but with convs it got very verbose... Maybe we can do:
`!transform.any_op -> !transform.param<i64>`
?

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:245`

```diff
@@ -233,15 +233,16 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
     Operation *current, transform::TransformResults &results,
     transform::TransformState &state) {
   Location loc = current->getLoc();
+  StringRef opName = current->getName().getStringRef();
   auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
   if (!linalgOp) {
     return emitSilenceableFailure(loc)
-           << "Operation " << *current << " is not a LinalgOp.";
+           << "Operation " << opName << " is not a LinalgOp.";
   }
 
   if (!linalg::isaContractionOpInterface(linalgOp)) {
     return emitSilenceableFailure(loc)
-           << "Operation " << *current << " is not a contraction operation.";
+           << "Operation " << opName << " is not a contraction operation.";
```

**Comment:**
I don't think we need to print the op name either -- we already pass in the location

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:326`

```diff
@@ -303,6 +304,109 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchConvolutionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchConvolutionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  Location loc = current->getLoc();
+  StringRef opName = current->getName().getStringRef();
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(loc)
+           << "Operation " << opName << " is not a LinalgOp.";
+  }
+
+  if (!linalg::isaConvolutionOpInterface(linalgOp)) {
+    return emitSilenceableFailure(loc)
+           << "Operation " << opName << " is not a convolution operation.";
+  }
```

**Comment:**
Also here, we don't need the op name

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:371`

```diff
@@ -303,6 +304,109 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchConvolutionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchConvolutionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  Location loc = current->getLoc();
+  StringRef opName = current->getName().getStringRef();
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(loc)
+           << "Operation " << opName << " is not a LinalgOp.";
+  }
+
+  if (!linalg::isaConvolutionOpInterface(linalgOp)) {
+    return emitSilenceableFailure(loc)
+           << "Operation " << opName << " is not a convolution operation.";
+  }
+
+  Type targetLhsType = getLhsType();
+  Type currentLhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[0]);
+  if (currentLhsType != targetLhsType) {
+    return emitSilenceableFailure(loc)
+           << "LHS type doesn't match: expected " << targetLhsType << ", got "
+           << currentLhsType;
+  }
+
+  Type targetRhsType = getRhsType();
+  Type currentRhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[1]);
+  if (currentRhsType != targetRhsType) {
+    return emitSilenceableFailure(loc)
+           << "RHS type doesn't match: expected " << targetRhsType << ", got "
+           << currentRhsType;
+  }
+
+  Type targetOutputType = getOutputType();
+  Type currentOutputType =
+      getElementTypeOrSelf(linalgOp.getDpsInits()[0].getType());
+  if (currentOutputType != targetOutputType) {
+    return emitSilenceableFailure(loc)
+           << "output type doesn't match: expected " << targetOutputType
+           << ", got " << currentOutputType;
+  }
+
+  ArrayAttr currentIndexingMaps = linalgOp.getIndexingMaps();
+  if (std::optional<ArrayAttr> targetIndexingMaps = getIndexingMaps()) {
+    if (currentIndexingMaps != *targetIndexingMaps) {
+      return emitSilenceableFailure(loc) << "indexing maps don't match";
+    }
+  }
+
+  FailureOr<linalg::ConvolutionDimensions> maybeConvDims =
+      linalg::inferConvolutionDims(linalgOp);
+  if (failed(maybeConvDims)) {
+    return emitSilenceableFailure(loc)
+           << "Failed to infer convolution dimensions.";
+  }
+  linalg::ConvolutionDimensions convDims = *maybeConvDims;
+  SmallVector<int64_t> iterationDomain = linalgOp.getStaticLoopRanges();
+  MLIRContext *ctx = getContext();
+  Builder builder(ctx);
+
+  auto buildI64Attrs = [&](auto values, auto transform) {
```

**Comment:**
We should avoid copying vectors inside convDims: https://llvm.org/docs/CodingStandards.html#beware-unnecessary-copies-with-auto
```suggestion
  auto buildI64Attrs = [&builder](const auto& values, const auto& transform) {
```

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:369`

```diff
@@ -303,6 +304,109 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchConvolutionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchConvolutionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  Location loc = current->getLoc();
+  StringRef opName = current->getName().getStringRef();
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(loc)
+           << "Operation " << opName << " is not a LinalgOp.";
+  }
+
+  if (!linalg::isaConvolutionOpInterface(linalgOp)) {
+    return emitSilenceableFailure(loc)
+           << "Operation " << opName << " is not a convolution operation.";
+  }
+
+  Type targetLhsType = getLhsType();
+  Type currentLhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[0]);
+  if (currentLhsType != targetLhsType) {
+    return emitSilenceableFailure(loc)
+           << "LHS type doesn't match: expected " << targetLhsType << ", got "
+           << currentLhsType;
+  }
+
+  Type targetRhsType = getRhsType();
+  Type currentRhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[1]);
+  if (currentRhsType != targetRhsType) {
+    return emitSilenceableFailure(loc)
+           << "RHS type doesn't match: expected " << targetRhsType << ", got "
+           << currentRhsType;
+  }
+
+  Type targetOutputType = getOutputType();
+  Type currentOutputType =
+      getElementTypeOrSelf(linalgOp.getDpsInits()[0].getType());
+  if (currentOutputType != targetOutputType) {
+    return emitSilenceableFailure(loc)
+           << "output type doesn't match: expected " << targetOutputType
+           << ", got " << currentOutputType;
+  }
+
+  ArrayAttr currentIndexingMaps = linalgOp.getIndexingMaps();
+  if (std::optional<ArrayAttr> targetIndexingMaps = getIndexingMaps()) {
+    if (currentIndexingMaps != *targetIndexingMaps) {
+      return emitSilenceableFailure(loc) << "indexing maps don't match";
+    }
+  }
+
+  FailureOr<linalg::ConvolutionDimensions> maybeConvDims =
+      linalg::inferConvolutionDims(linalgOp);
+  if (failed(maybeConvDims)) {
+    return emitSilenceableFailure(loc)
+           << "Failed to infer convolution dimensions.";
+  }
+  linalg::ConvolutionDimensions convDims = *maybeConvDims;
+  SmallVector<int64_t> iterationDomain = linalgOp.getStaticLoopRanges();
+  MLIRContext *ctx = getContext();
+  Builder builder(ctx);
+
+  auto buildI64Attrs = [&](auto values, auto transform) {
+    return llvm::map_to_vector(values, [&](auto val) -> Attribute {
```

**Comment:**
```suggestion
    return llvm::map_to_vector(values, [&](unsigned val) -> Attribute {
```

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:369`

```diff
@@ -303,6 +304,109 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchConvolutionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchConvolutionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  Location loc = current->getLoc();
+  StringRef opName = current->getName().getStringRef();
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(loc)
+           << "Operation " << opName << " is not a LinalgOp.";
+  }
+
+  if (!linalg::isaConvolutionOpInterface(linalgOp)) {
+    return emitSilenceableFailure(loc)
+           << "Operation " << opName << " is not a convolution operation.";
+  }
+
+  Type targetLhsType = getLhsType();
+  Type currentLhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[0]);
+  if (currentLhsType != targetLhsType) {
+    return emitSilenceableFailure(loc)
+           << "LHS type doesn't match: expected " << targetLhsType << ", got "
+           << currentLhsType;
+  }
+
+  Type targetRhsType = getRhsType();
+  Type currentRhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[1]);
+  if (currentRhsType != targetRhsType) {
+    return emitSilenceableFailure(loc)
+           << "RHS type doesn't match: expected " << targetRhsType << ", got "
+           << currentRhsType;
+  }
+
+  Type targetOutputType = getOutputType();
+  Type currentOutputType =
+      getElementTypeOrSelf(linalgOp.getDpsInits()[0].getType());
+  if (currentOutputType != targetOutputType) {
+    return emitSilenceableFailure(loc)
+           << "output type doesn't match: expected " << targetOutputType
+           << ", got " << currentOutputType;
+  }
+
+  ArrayAttr currentIndexingMaps = linalgOp.getIndexingMaps();
+  if (std::optional<ArrayAttr> targetIndexingMaps = getIndexingMaps()) {
+    if (currentIndexingMaps != *targetIndexingMaps) {
+      return emitSilenceableFailure(loc) << "indexing maps don't match";
+    }
+  }
+
+  FailureOr<linalg::ConvolutionDimensions> maybeConvDims =
+      linalg::inferConvolutionDims(linalgOp);
+  if (failed(maybeConvDims)) {
+    return emitSilenceableFailure(loc)
+           << "Failed to infer convolution dimensions.";
+  }
+  linalg::ConvolutionDimensions convDims = *maybeConvDims;
+  SmallVector<int64_t> iterationDomain = linalgOp.getStaticLoopRanges();
+  MLIRContext *ctx = getContext();
+  Builder builder(ctx);
+
+  auto buildI64Attrs = [&](auto values, auto transform) {
+    return llvm::map_to_vector(values, [&](auto val) -> Attribute {
```

**Comment:**
ah no, this can be int64_t for dilations, disregard

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:377`

```diff
@@ -303,6 +303,107 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchConvolutionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchConvolutionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  Location loc = current->getLoc();
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(loc) << "Operation is not a LinalgOp.";
+  }
+
+  if (!linalg::isaConvolutionOpInterface(linalgOp)) {
+    return emitSilenceableFailure(loc)
+           << "Operation is not a convolution operation.";
+  }
+
+  Type targetLhsType = getLhsType();
+  Type currentLhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[0]);
+  if (currentLhsType != targetLhsType) {
+    return emitSilenceableFailure(loc)
+           << "LHS type doesn't match: expected " << targetLhsType << ", got "
+           << currentLhsType;
+  }
+
+  Type targetRhsType = getRhsType();
+  Type currentRhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[1]);
+  if (currentRhsType != targetRhsType) {
+    return emitSilenceableFailure(loc)
+           << "RHS type doesn't match: expected " << targetRhsType << ", got "
+           << currentRhsType;
+  }
+
+  Type targetOutputType = getOutputType();
+  Type currentOutputType =
+      getElementTypeOrSelf(linalgOp.getDpsInits()[0].getType());
+  if (currentOutputType != targetOutputType) {
+    return emitSilenceableFailure(loc)
+           << "output type doesn't match: expected " << targetOutputType
+           << ", got " << currentOutputType;
+  }
+
+  ArrayAttr currentIndexingMaps = linalgOp.getIndexingMaps();
+  if (std::optional<ArrayAttr> targetIndexingMaps = getIndexingMaps()) {
+    if (currentIndexingMaps != *targetIndexingMaps) {
+      return emitSilenceableFailure(loc) << "indexing maps don't match";
+    }
+  }
+
+  FailureOr<linalg::ConvolutionDimensions> maybeConvDims =
+      linalg::inferConvolutionDims(linalgOp);
+  if (failed(maybeConvDims)) {
+    return emitSilenceableFailure(loc)
+           << "Failed to infer convolution dimensions.";
+  }
+  linalg::ConvolutionDimensions convDims = *maybeConvDims;
+  SmallVector<int64_t> iterationDomain = linalgOp.getStaticLoopRanges();
+  MLIRContext *ctx = getContext();
+  Builder builder(ctx);
+
+  auto buildI64Attrs = [&builder](const auto &values, const auto &transform) {
+    return llvm::map_to_vector(values, [&](auto val) -> Attribute {
+      return builder.getI64IntegerAttr(transform(val));
+    });
+  };
+
+  results.setParams(cast<OpResult>(getBatchDims()),
+                    buildI64Attrs(convDims.batch, [&](unsigned idx) {
+                      return iterationDomain[idx];
+                    }));
```

**Comment:**
Can you hoist this lambda to a local variable? It's used 6 time total

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:366`

```diff
@@ -303,6 +303,107 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchConvolutionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchConvolutionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  Location loc = current->getLoc();
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(loc) << "Operation is not a LinalgOp.";
+  }
+
+  if (!linalg::isaConvolutionOpInterface(linalgOp)) {
+    return emitSilenceableFailure(loc)
+           << "Operation is not a convolution operation.";
+  }
+
+  Type targetLhsType = getLhsType();
+  Type currentLhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[0]);
+  if (currentLhsType != targetLhsType) {
+    return emitSilenceableFailure(loc)
+           << "LHS type doesn't match: expected " << targetLhsType << ", got "
+           << currentLhsType;
+  }
+
+  Type targetRhsType = getRhsType();
+  Type currentRhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[1]);
+  if (currentRhsType != targetRhsType) {
+    return emitSilenceableFailure(loc)
+           << "RHS type doesn't match: expected " << targetRhsType << ", got "
+           << currentRhsType;
+  }
+
+  Type targetOutputType = getOutputType();
+  Type currentOutputType =
+      getElementTypeOrSelf(linalgOp.getDpsInits()[0].getType());
+  if (currentOutputType != targetOutputType) {
+    return emitSilenceableFailure(loc)
+           << "output type doesn't match: expected " << targetOutputType
+           << ", got " << currentOutputType;
+  }
+
+  ArrayAttr currentIndexingMaps = linalgOp.getIndexingMaps();
+  if (std::optional<ArrayAttr> targetIndexingMaps = getIndexingMaps()) {
+    if (currentIndexingMaps != *targetIndexingMaps) {
+      return emitSilenceableFailure(loc) << "indexing maps don't match";
+    }
+  }
+
+  FailureOr<linalg::ConvolutionDimensions> maybeConvDims =
+      linalg::inferConvolutionDims(linalgOp);
+  if (failed(maybeConvDims)) {
+    return emitSilenceableFailure(loc)
+           << "Failed to infer convolution dimensions.";
+  }
+  linalg::ConvolutionDimensions convDims = *maybeConvDims;
+  SmallVector<int64_t> iterationDomain = linalgOp.getStaticLoopRanges();
+  MLIRContext *ctx = getContext();
+  Builder builder(ctx);
```

**Comment:**
```suggestion
  Builder builder(getContext());
```
`ctx` is not used anywhere else AFAICT

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:259`

```diff
@@ -225,6 +225,72 @@ def MatchContractionOp : Op<Transform_Dialect, "iree.match.contraction",
   let cppNamespace = "mlir::iree_compiler::IREE::transform_dialect";
 }
 
+def MatchConvolutionOp : Op<Transform_Dialect, "iree.match.convolution",
+    [MatchOpInterface,
+     SingleOpMatcher,
+     MemoryEffectsOpInterface,
+     AllTypesMatch<["batch_dims", "output_image_dims", "output_channel_dims",
+                    "filter_dims", "input_channel_dims", "depth_dims",
+                    "strides", "dilations"]>
+     ]> {
+  let summary = [{Check whether the op is a convolution operation.}];
+  let description = [{
+    Matches operations that implement the ConvolutionOpInterface.
+    This includes operations like linalg.conv_2d_nhwc_hwcf,
+    linalg.conv_2d_nchw_fchw, linalg.depthwise_conv_2d_nhwc_hwc, etc.
+
+    Optionally matches specific indexing maps patterns.
+
+    #### Return modes
+
+    Succeeds if the operation is a convolution operation, and
+    produces a silenceable failure otherwise.
+
+    #### Results
+
+    Returns arrays of dimension sizes for each convolution dimension:
+    - batch_dims: Array of batch dimension sizes.
+    - output_image_dims: Array of output spatial dimension sizes.
+    - output_channel_dims: Array of output channel dimension sizes.
+    - filter_dims: Array of filter spatial dimension sizes.
+    - input_channel_dims: Array of input channel dimension sizes.
+    - depth_dims: Array of depth dimension sizes (for depthwise convolutions).
+    - strides: Array of stride values.
+    - dilations: Array of dilation values.
```

**Comment:**
Can you also add an example?

---


---


## [PR #22178](https://github.com/iree-org/iree/pull/22178): [Codegen][GPU] Disable MMA Intrinsics Sorting

### Review Summary

**COMMENTED** (2025-10-02)


**COMMENTED** (2025-10-02)


**COMMENTED** (2025-10-02)


**APPROVED** (2025-10-02)

LGTM but please add a brief description of why we need to delete this to the PR description, not just the issue number


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUHeuristics.cpp:641`

```diff
@@ -636,11 +636,9 @@ FailureOr<GPUMMASchedule> deduceMMASchedule(
     int64_t subgroupSize, std::optional<int64_t> wgpCount, bool transposedLhs,
     bool transposedRhs, bool canUpcastAcc, bool mustBeAligned,
     bool doCPromotion) {
-
-  SmallVector<GPUIntrinsicType> sortedIntrinsics =
-      sortMMAIntrinsics(problem, intrinsics);
-
-  for (const GPUIntrinsicType &intrinsic : sortedIntrinsics) {
+  // TODO(#22160): sortMMAIntrinsics call is disabled for now since it causes
+  // performance regression. Re-enable once the issue is addressed.
+  for (const GPUIntrinsicType &intrinsic : intrinsics) {
```

**Comment:**
We should also remove the sorting functions. Just commenting out the code will lead to unused private function warnings.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUHeuristics.cpp:580`

```diff
@@ -571,17 +571,6 @@ static bool compareIntrinsics(const GPUMatmulShapeType &problem,
          ShapedType::getNumElements(rhs.kSizes);
 }
 
-static SmallVector<GPUIntrinsicType>
-sortMMAIntrinsics(GPUMatmulShapeType problem,
-                  ArrayRef<GPUIntrinsicType> intrinsics) {
-  SmallVector<GPUIntrinsicType> sortedIntrinsics(intrinsics);
-  llvm::stable_sort(sortedIntrinsics, [&](const GPUMatmulShapeType &lhs,
-                                          const GPUMatmulShapeType &rhs) {
-    return compareIntrinsics(problem, lhs, rhs);
```

**Comment:**
The comparison function also needs to be removed

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUHeuristics.cpp:580`

```diff
@@ -571,17 +571,6 @@ static bool compareIntrinsics(const GPUMatmulShapeType &problem,
          ShapedType::getNumElements(rhs.kSizes);
 }
 
-static SmallVector<GPUIntrinsicType>
-sortMMAIntrinsics(GPUMatmulShapeType problem,
-                  ArrayRef<GPUIntrinsicType> intrinsics) {
-  SmallVector<GPUIntrinsicType> sortedIntrinsics(intrinsics);
-  llvm::stable_sort(sortedIntrinsics, [&](const GPUMatmulShapeType &lhs,
-                                          const GPUMatmulShapeType &rhs) {
-    return compareIntrinsics(problem, lhs, rhs);
```

**Comment:**
Gotcha. BTW, the code where this is used performs another unstable sorting -- I wonder if that's also asking for trouble

---


---


## [PR #22154](https://github.com/iree-org/iree/pull/22154): [DispatchCreation] infer split-reduction sizes for ArgCompare

### Review Summary

**COMMENTED** (2025-10-01)


**COMMENTED** (2025-10-01)


**COMMENTED** (2025-10-01)


**COMMENTED** (2025-10-01)


**APPROVED** (2025-10-01)

LGTM % one minor issue


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1248`

```diff
@@ -1220,6 +1220,34 @@ LogicalResult ArgCompareOp::reifyResultShapes(
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+SmallVector<AffineMap>
+IREE::LinalgExt::ArgCompareOp::getIndexingMapsForOperands() {
+  Builder b(getContext());
+  const int64_t rank = getInputRank();
+  return SmallVector<AffineMap>{b.getMultiDimIdentityMap(rank)};
+}
+
+SmallVector<AffineMap>
+IREE::LinalgExt::ArgCompareOp::getIndexingMapsForResults() {
+  MLIRContext *ctx = getContext();
+  const int64_t rank = getInputRank();
+  const int64_t redDim = static_cast<int64_t>(getDimension());
+
+  SmallVector<AffineExpr> proj;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == redDim)
+      continue;
+    proj.push_back(getAffineDimExpr(i, ctx));
+  }
+
+  AffineMap resultMap = AffineMap::get(rank, 0, proj, ctx);
+  return SmallVector<AffineMap>{resultMap, resultMap};
+}
+
+SmallVector<int64_t> IREE::LinalgExt::ArgCompareOp::getStaticLoopRanges() {
+  return SmallVector<int64_t>(getInputType().getShape());
```

**Comment:**
nit: you can use brace initialization
```suggestion
  return {getInputType().getShape()};
```

---

**File:** `compiler/src/iree/compiler/DispatchCreation/SetSplitReductionSizes.cpp:45`

```diff
@@ -29,6 +29,28 @@ static SmallVector<int64_t> getStaticReductionDimSizes(linalg::LinalgOp op) {
   return dimSizes;
 }
 
+static std::optional<SmallVector<int64_t>> getReductionDimSizes(Operation *Op) {
+  SmallVector<int64_t> loopRanges;
+  if (auto fusionOp = dyn_cast<IREE::LinalgExt::LinalgFusionOpInterface>(Op)) {
+    loopRanges = fusionOp.getStaticLoopRanges();
+  }
+
+  auto tilingInterfaceOp = dyn_cast<TilingInterface>(Op);
+  if (!tilingInterfaceOp) {
+    LDBG() << "skipping op; not a TilingInterface op";
+    return std::nullopt;
+  }
+
+  SmallVector<utils::IteratorType> iters;
+  iters = tilingInterfaceOp.getLoopIteratorTypes();
```

**Comment:**
```suggestion
  SmallVector<utils::IteratorType> iters = tilingInterfaceOp.getLoopIteratorTypes();
```

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1227`

```diff
@@ -1220,6 +1220,34 @@ LogicalResult ArgCompareOp::reifyResultShapes(
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+SmallVector<AffineMap>
+IREE::LinalgExt::ArgCompareOp::getIndexingMapsForOperands() {
+  Builder b(getContext());
+  const int64_t rank = getInputRank();
+  return SmallVector<AffineMap>{b.getMultiDimIdentityMap(rank)};
```

**Comment:**
nit: this is only used in one place, I'd inline it and use brace initialization
```suggestion
  return {b.getMultiDimIdentityMap(getInputRank())};
```

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1244`

```diff
@@ -1220,6 +1220,34 @@ LogicalResult ArgCompareOp::reifyResultShapes(
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+SmallVector<AffineMap>
+IREE::LinalgExt::ArgCompareOp::getIndexingMapsForOperands() {
+  Builder b(getContext());
+  const int64_t rank = getInputRank();
+  return SmallVector<AffineMap>{b.getMultiDimIdentityMap(rank)};
+}
+
+SmallVector<AffineMap>
+IREE::LinalgExt::ArgCompareOp::getIndexingMapsForResults() {
+  MLIRContext *ctx = getContext();
+  const int64_t rank = getInputRank();
+  const int64_t redDim = static_cast<int64_t>(getDimension());
+
+  SmallVector<AffineExpr> proj;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == redDim)
+      continue;
+    proj.push_back(getAffineDimExpr(i, ctx));
+  }
+
+  AffineMap resultMap = AffineMap::get(rank, 0, proj, ctx);
+  return SmallVector<AffineMap>{resultMap, resultMap};
```

**Comment:**
nit: you can return an initializer list
```suggestion
  AffineMap resultMap = AffineMap::get(rank, 0, proj, ctx);
  return {resultMap, resultMap};
```

---

**File:** `compiler/src/iree/compiler/DispatchCreation/SetSplitReductionSizes.cpp:36`

```diff
@@ -29,6 +29,29 @@ static SmallVector<int64_t> getStaticReductionDimSizes(linalg::LinalgOp op) {
   return dimSizes;
 }
 
+static std::optional<SmallVector<int64_t>> getReductionDimSizes(Operation *Op) {
+  SmallVector<int64_t> loopRanges;
+  if (auto fusionOp = dyn_cast<IREE::LinalgExt::LinalgFusionOpInterface>(Op)) {
+    loopRanges = fusionOp.getStaticLoopRanges();
+  }
```

**Comment:**
Would it be possible to have a test that covers this?

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1248`

```diff
@@ -1220,6 +1220,34 @@ LogicalResult ArgCompareOp::reifyResultShapes(
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+SmallVector<AffineMap>
+IREE::LinalgExt::ArgCompareOp::getIndexingMapsForOperands() {
+  Builder b(getContext());
+  const int64_t rank = getInputRank();
+  return SmallVector<AffineMap>{b.getMultiDimIdentityMap(rank)};
+}
+
+SmallVector<AffineMap>
+IREE::LinalgExt::ArgCompareOp::getIndexingMapsForResults() {
+  MLIRContext *ctx = getContext();
+  const int64_t rank = getInputRank();
+  const int64_t redDim = static_cast<int64_t>(getDimension());
+
+  SmallVector<AffineExpr> proj;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == redDim)
+      continue;
+    proj.push_back(getAffineDimExpr(i, ctx));
+  }
+
+  AffineMap resultMap = AffineMap::get(rank, 0, proj, ctx);
+  return SmallVector<AffineMap>{resultMap, resultMap};
+}
+
+SmallVector<int64_t> IREE::LinalgExt::ArgCompareOp::getStaticLoopRanges() {
+  return SmallVector<int64_t>(getInputType().getShape());
```

**Comment:**
the alternative is to use `llvm::to_vector(someArrayRef)` -- you don't need the exact type. But fine to keep as-is.

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1238`

```diff
@@ -1220,6 +1220,33 @@ LogicalResult ArgCompareOp::reifyResultShapes(
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+SmallVector<AffineMap>
+IREE::LinalgExt::ArgCompareOp::getIndexingMapsForOperands() {
+  Builder b(getContext());
+  return {b.getMultiDimIdentityMap(getInputRank())};
+}
+
+SmallVector<AffineMap>
+IREE::LinalgExt::ArgCompareOp::getIndexingMapsForResults() {
+  MLIRContext *ctx = getContext();
+  const int64_t rank = getInputRank();
+  const int64_t redDim = static_cast<int64_t>(getDimension());
+
+  SmallVector<AffineExpr> proj;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == redDim)
+      continue;
```

**Comment:**
missing braces, see https://iree.dev/developers/general/contributing/#compiler

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:1225`

```diff
@@ -1220,6 +1220,43 @@ LogicalResult ArgCompareOp::reifyResultShapes(
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+SmallVector<AffineMap> IREE::LinalgExt::ArgCompareOp::getIndexingMapsArray() {
+  Builder b(getContext());
+  MLIRContext *ctx = b.getContext();
```

**Comment:**
Move `b` closer to the first use and use the `ctx` variable

---


---


## [PR #22149](https://github.com/iree-org/iree/pull/22149): [Codegen] support matching any values for dims_equal transform op

### Review Summary

**CHANGES_REQUESTED** (2025-09-30)

If you want to replicate what the old tuning spec was doing in https://github.com/nod-ai/sdxl-scripts/blob/4fa7ccbc3de4873c0751a48ef9b4b86a7f24428e/int8-model/specs/attention_and_matmul_spec.mlir#L636-L637 , your testcase should use static tensors only. And in your dims_equal implementation, you can teach it to treat -1 as 'match any value'.

We can still allow for dynamic dims on the input IR side, but the priority is to support any values as the match target. 

You can see how the `match_cast_compatible_types` matcher handled this here: https://github.com/iree-org/iree/blob/e7bd805ccea762c462069d47f4ea5be364636168/compiler/src/iree/compiler/Utils/ShapeUtils.cpp#L41-L65 and
https://github.com/iree-org/iree/blob/e7bd805ccea762c462069d47f4ea5be364636168/compiler/src/iree/compiler/Utils/ShapeUtils.cpp#L54-L57


**COMMENTED** (2025-09-30)


**COMMENTED** (2025-09-30)


**COMMENTED** (2025-10-01)


**COMMENTED** (2025-10-01)


**COMMENTED** (2025-10-01)


**COMMENTED** (2025-10-01)


**COMMENTED** (2025-10-01)


**COMMENTED** (2025-10-01)


**COMMENTED** (2025-10-01)


**COMMENTED** (2025-10-01)


**APPROVED** (2025-10-01)

LGTM % nit


### Code Comments

**File:** `compiler/src/iree/compiler/Preprocessing/Common/test/preprocessing_match_ops.mlir:716`

```diff
@@ -644,3 +644,136 @@ module attributes {transform.with_named_sequence} {
     transform.yield
   }
 }
+
+// -----
+
+// CHECK-LABEL: func.func @op_broadcast_rhs_mmt_d0
+func.func @op_broadcast_rhs_mmt_d0(
+  %lhs: tensor<4x8x512xi8>,
+  %rhs: tensor<1024x512xi8>,
+  %out: tensor<4x8x1024xi32>)
+  -> tensor<4x8x1024xi32> {
+  // CHECK-NEXT: linalg.generic
+  // CHECK-SAME:   match_status = "matched"
+  %res = linalg.generic
+    { indexing_maps = [
+        affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>,
+        affine_map<(d0, d1, d2, d3) -> (d2, d3)>,
+        affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
+      ],
+      iterator_types = ["parallel", "parallel", "parallel", "reduction"], match_status = "unmatched"}
+    ins(%lhs, %rhs : tensor<4x8x512xi8>, tensor<1024x512xi8>)
+    outs(%out : tensor<4x8x1024xi32>) {
+  ^bb0(%in_l: i8, %in_r: i8, %acc: i32):
+    %l = arith.extsi %in_l : i8 to i32
+    %r = arith.extsi %in_r : i8 to i32
+    %m = arith.muli %l, %r : i32
+    %a = arith.addi %acc, %m : i32
+    linalg.yield %a : i32
+  } -> tensor<4x8x1024xi32>
+  return %res : tensor<4x8x1024xi32>
+}
+
+module attributes {transform.with_named_sequence} {
+  transform.named_sequence @match_broadcast_rhs_mmt_i8_i8_i32_d0(
+    %op: !transform.any_op {transform.readonly}) -> !transform.any_op {
+
+    %batch, %m, %n, %k = transform.iree.match.contraction %op,
+      lhs_type = i8, rhs_type = i8, output_type = i32
+      { indexing_maps = [
+          affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>,
+          affine_map<(d0, d1, d2, d3) -> (d2, d3)>,
+          affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
+        ] } :
+      (!transform.any_op)
+      -> (!transform.param<i64>, !transform.param<i64>,
+          !transform.param<i64>, !transform.param<i64>)
+
+    transform.iree.match.dims_equal %batch, []      : !transform.param<i64>
+    transform.iree.match.dims_equal %m, [-1, 8]     : !transform.param<i64>
+    transform.iree.match.dims_equal %n, [1024]      : !transform.param<i64>
+    transform.iree.match.dims_equal %k, [512]       : !transform.param<i64>
+
+    transform.yield %op : !transform.any_op
+  }
+
+  transform.named_sequence @annotate(%op: !transform.any_op {transform.readonly}) {
+    %0 = transform.param.constant "matched" -> !transform.any_param
+    transform.annotate %op "match_status" = %0 : !transform.any_op, !transform.any_param
+    transform.yield
+  }
+
+  transform.named_sequence @__transform_main(%module: !transform.any_op) {
+    transform.foreach_match in %module
+        @match_broadcast_rhs_mmt_i8_i8_i32_d0 -> @annotate
+      : (!transform.any_op) -> (!transform.any_op)
+    transform.yield
+  }
+}
+
+// -----
+
+// CHECK-LABEL: func.func @op_broadcast_lhs_mmt_d0
```

**Comment:**
Instead of having each test cases in a different split, could we have multiple ops to match in the same function? These tests are currently quite verbose.

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:339`

```diff
@@ -314,9 +314,29 @@ DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
       state.getParams(getDimensionSizes());
   ArrayAttr targetDimensionSizes = getExpectedValues();
 
-  if (!llvm::equal(currentDimSizes, targetDimensionSizes)) {
+  if (currentDimSizes.size() != targetDimensionSizes.size()) {
     return emitSilenceableError()
-           << "Dimension sizes do not match expected values";
+           << "dimension sizes and expected values have different lengths";
+  }
+
+  for (auto [currentDimSizeAttr, targetDimSizeAttr] :
+       llvm::zip_equal(currentDimSizes, targetDimensionSizes)) {
+    auto currentDimSizeIntegerAttr = dyn_cast<IntegerAttr>(currentDimSizeAttr);
+    auto targetDimSizeIntegerAttr = dyn_cast<IntegerAttr>(targetDimSizeAttr);
+    if (!currentDimSizeIntegerAttr || !targetDimSizeIntegerAttr) {
+      return emitSilenceableError() << "expected integer attributes";
+    }
+
+    int64_t currentDimSize = currentDimSizeIntegerAttr.getInt();
+    int64_t targetDimSize = targetDimSizeIntegerAttr.getInt();
+
+    if (targetDimSize == -1)
+      continue;
+
+    if (currentDimSize != targetDimSize) {
+      return emitSilenceableError() << "dimension value " << currentDimSize
+                                    << " does not match " << targetDimSize;
+    }
```

**Comment:**
It would be simpler to have a helper lambda/function that takes a range of attrs and returns something like `FailureOr<SmallVector<int64_t>>`. Then you don't have to repeat the same extraction logic for lhs and rhs and can do the comparison with a simple `!=`.

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:339`

```diff
@@ -314,9 +314,29 @@ DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
       state.getParams(getDimensionSizes());
   ArrayAttr targetDimensionSizes = getExpectedValues();
 
-  if (!llvm::equal(currentDimSizes, targetDimensionSizes)) {
+  if (currentDimSizes.size() != targetDimensionSizes.size()) {
     return emitSilenceableError()
-           << "Dimension sizes do not match expected values";
+           << "dimension sizes and expected values have different lengths";
+  }
+
+  for (auto [currentDimSizeAttr, targetDimSizeAttr] :
+       llvm::zip_equal(currentDimSizes, targetDimensionSizes)) {
+    auto currentDimSizeIntegerAttr = dyn_cast<IntegerAttr>(currentDimSizeAttr);
+    auto targetDimSizeIntegerAttr = dyn_cast<IntegerAttr>(targetDimSizeAttr);
+    if (!currentDimSizeIntegerAttr || !targetDimSizeIntegerAttr) {
+      return emitSilenceableError() << "expected integer attributes";
+    }
+
+    int64_t currentDimSize = currentDimSizeIntegerAttr.getInt();
+    int64_t targetDimSize = targetDimSizeIntegerAttr.getInt();
+
+    if (targetDimSize == -1)
+      continue;
+
+    if (currentDimSize != targetDimSize) {
+      return emitSilenceableError() << "dimension value " << currentDimSize
+                                    << " does not match " << targetDimSize;
+    }
```

**Comment:**
Or alternatively map to vector of `std::optional<int64_t>`, I think that's even simpler

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:357`

```diff
@@ -307,16 +307,54 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
 // MatchDimsEqualOp
 //===----------------------------------------------------------------------===//
 
+template <typename Range, typename Mapper>
+static FailureOr<SmallVector<std::optional<int64_t>>>
+extractDimSizes(Range &&attrs, Mapper mapValueToOptional) {
+  SmallVector<std::optional<int64_t>> dimSizes;
+  dimSizes.reserve(std::distance(std::begin(attrs), std::end(attrs)));
+  for (Attribute attr : attrs) {
+    auto valAttr = dyn_cast<IntegerAttr>(attr);
+    if (!valAttr) {
+      return failure();
+    }
+    dimSizes.push_back(mapValueToOptional(valAttr.getInt()));
+  }
+  return dimSizes;
+}
+
 DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
     transform::TransformRewriter &rewriter,
     transform::TransformResults &results, transform::TransformState &state) {
   ArrayRef<transform::Param> currentDimSizes =
       state.getParams(getDimensionSizes());
-  ArrayAttr targetDimensionSizes = getExpectedValues();
+  ArrayAttr targetDimSizes = getExpectedValues();
+
+  if (currentDimSizes.size() != targetDimSizes.size()) {
+    return emitSilenceableError()
+           << "dimension sizes and expected values have different lengths";
+  }
 
-  if (!llvm::equal(currentDimSizes, targetDimensionSizes)) {
+  FailureOr<SmallVector<std::optional<int64_t>>> currentDims = extractDimSizes(
+      currentDimSizes, [](int64_t v) { return std::optional<int64_t>(v); });
+  if (failed(currentDims)) {
     return emitSilenceableError()
-           << "Dimension sizes do not match expected values";
+           << "expected integer attributes for current sizes";
+  }
+
+  FailureOr<SmallVector<std::optional<int64_t>>> targetDims =
+      extractDimSizes(targetDimSizes.getValue(), [](int64_t v) {
+        return v == -1 ? std::nullopt : std::optional<int64_t>(v);
+      });
+  if (failed(targetDims)) {
+    return emitSilenceableError()
+           << "expected integer attributes for expected sizes";
+  }
+
+  for (auto [current, target] : llvm::zip_equal(*currentDims, *targetDims)) {
+    if (target && *current != *target) {
+      return emitSilenceableError()
+             << "dimension size " << *current << " does not match " << *target;
+    }
```

**Comment:**
This whole thing seems needlessly complicated -- I don't think we need two layers of possible failures and emit precise silenceable failures for every failure case. I'd do something like:
```c++
auto extractDimSizes = [](const auto &range) {
  return llvm::map_to_vector(range, [](Attribute attr) -> std::optional<int64_t> {
    if (intAttr = dyn_cast<IntegerAttr>(attr)) {
      return intAttr.getValue();
    }
    return std::nullopt;
  });
};
```

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:357`

```diff
@@ -307,16 +307,54 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
 // MatchDimsEqualOp
 //===----------------------------------------------------------------------===//
 
+template <typename Range, typename Mapper>
+static FailureOr<SmallVector<std::optional<int64_t>>>
+extractDimSizes(Range &&attrs, Mapper mapValueToOptional) {
+  SmallVector<std::optional<int64_t>> dimSizes;
+  dimSizes.reserve(std::distance(std::begin(attrs), std::end(attrs)));
+  for (Attribute attr : attrs) {
+    auto valAttr = dyn_cast<IntegerAttr>(attr);
+    if (!valAttr) {
+      return failure();
+    }
+    dimSizes.push_back(mapValueToOptional(valAttr.getInt()));
+  }
+  return dimSizes;
+}
+
 DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
     transform::TransformRewriter &rewriter,
     transform::TransformResults &results, transform::TransformState &state) {
   ArrayRef<transform::Param> currentDimSizes =
       state.getParams(getDimensionSizes());
-  ArrayAttr targetDimensionSizes = getExpectedValues();
+  ArrayAttr targetDimSizes = getExpectedValues();
+
+  if (currentDimSizes.size() != targetDimSizes.size()) {
+    return emitSilenceableError()
+           << "dimension sizes and expected values have different lengths";
+  }
 
-  if (!llvm::equal(currentDimSizes, targetDimensionSizes)) {
+  FailureOr<SmallVector<std::optional<int64_t>>> currentDims = extractDimSizes(
+      currentDimSizes, [](int64_t v) { return std::optional<int64_t>(v); });
+  if (failed(currentDims)) {
     return emitSilenceableError()
-           << "Dimension sizes do not match expected values";
+           << "expected integer attributes for current sizes";
+  }
+
+  FailureOr<SmallVector<std::optional<int64_t>>> targetDims =
+      extractDimSizes(targetDimSizes.getValue(), [](int64_t v) {
+        return v == -1 ? std::nullopt : std::optional<int64_t>(v);
+      });
+  if (failed(targetDims)) {
+    return emitSilenceableError()
+           << "expected integer attributes for expected sizes";
+  }
+
+  for (auto [current, target] : llvm::zip_equal(*currentDims, *targetDims)) {
+    if (target && *current != *target) {
+      return emitSilenceableError()
+             << "dimension size " << *current << " does not match " << *target;
+    }
```

**Comment:**
And then you can check for equality with https://github.com/llvm/llvm-project/blob/9ce0dae54e7d34ef4e0266069c0d3f1ae5968612/llvm/include/llvm/ADT/STLExtras.h#L2073-L2077:

```c++
if (!llvm::equal(actual, target, [](std::optional<int64_t> lhs, std::optional<int64_t> rhs) {
    return rhs == -1 || lhs == rhs;
  }) {
  return emitSilenceableError() << ...;
}

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:320`

```diff
@@ -310,13 +310,36 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
 DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
     transform::TransformRewriter &rewriter,
     transform::TransformResults &results, transform::TransformState &state) {
-  ArrayRef<transform::Param> currentDimSizes =
+  ArrayRef<transform::Param> currentDimAttrs =
       state.getParams(getDimensionSizes());
-  ArrayAttr targetDimensionSizes = getExpectedValues();
+  ArrayAttr targetDimAttrs = getExpectedValues();
 
-  if (!llvm::equal(currentDimSizes, targetDimensionSizes)) {
+  if (currentDimAttrs.size() != targetDimAttrs.size()) {
     return emitSilenceableError()
-           << "Dimension sizes do not match expected values";
+           << "dimension sizes and expected values have different lengths";
+  }
```

**Comment:**
nit: We could skip the size check as `llvm::equal` is going to do it anyway. I don't anticipate more than a few dims, so this should fit well within the small vector static size and not allocate any memory. But you can also keep as-is if you want.

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:340`

```diff
@@ -310,13 +310,36 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
 DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
     transform::TransformRewriter &rewriter,
     transform::TransformResults &results, transform::TransformState &state) {
-  ArrayRef<transform::Param> currentDimSizes =
+  ArrayRef<transform::Param> currentDimAttrs =
       state.getParams(getDimensionSizes());
-  ArrayAttr targetDimensionSizes = getExpectedValues();
+  ArrayAttr targetDimAttrs = getExpectedValues();
 
-  if (!llvm::equal(currentDimSizes, targetDimensionSizes)) {
+  if (currentDimAttrs.size() != targetDimAttrs.size()) {
     return emitSilenceableError()
-           << "Dimension sizes do not match expected values";
+           << "dimension sizes and expected values have different lengths";
+  }
+
+  auto extractDims = [](const auto &range) {
+    return llvm::map_to_vector(
+        range, [](Attribute attr) -> std::optional<int64_t> {
+          if (auto intAttr = dyn_cast<IntegerAttr>(attr)) {
+            return intAttr.getInt();
+          }
+          return std::nullopt;
+        });
+  };
+
+  SmallVector<std::optional<int64_t>> currentDims =
+      extractDims(currentDimAttrs);
+  SmallVector<std::optional<int64_t>> targetDims =
+      extractDims(targetDimAttrs.getValue());
+
+  if (!llvm::equal(currentDims, targetDims,
+                   [](const std::optional<int64_t> &lhs,
+                      const std::optional<int64_t> &rhs) {
+                     return (rhs && *rhs == -1) || (lhs && rhs && *lhs == *rhs);
```

**Comment:**
Do we need to check for the `nullopt` state explicitly? I thought that `std::optional`'s comparison operators handle it like you would expect: https://en.cppreference.com/w/cpp/utility/optional/operator_cmp.html

I wrote a quick test and it seems to be the case: https://godbolt.org/z/hbfvbjK98

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:326`

```diff
@@ -310,13 +310,36 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
 DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
     transform::TransformRewriter &rewriter,
     transform::TransformResults &results, transform::TransformState &state) {
-  ArrayRef<transform::Param> currentDimSizes =
+  ArrayRef<transform::Param> currentDimAttrs =
       state.getParams(getDimensionSizes());
-  ArrayAttr targetDimensionSizes = getExpectedValues();
+  ArrayAttr targetDimAttrs = getExpectedValues();
 
-  if (!llvm::equal(currentDimSizes, targetDimensionSizes)) {
+  if (currentDimAttrs.size() != targetDimAttrs.size()) {
     return emitSilenceableError()
-           << "Dimension sizes do not match expected values";
+           << "dimension sizes and expected values have different lengths";
+  }
+
+  auto extractDims = [](const auto &range) {
+    return llvm::map_to_vector(
+        range, [](Attribute attr) -> std::optional<int64_t> {
+          if (auto intAttr = dyn_cast<IntegerAttr>(attr)) {
+            return intAttr.getInt();
```

**Comment:**
If you don't want to deal with arbitrary values, you can also tighten the op definition in tablegen / move this to the verifier. That's probably a better way to go about this error mode. (This is a side comment, we don't have to fix it in this PR.)

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:326`

```diff
@@ -310,13 +310,36 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
 DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
     transform::TransformRewriter &rewriter,
     transform::TransformResults &results, transform::TransformState &state) {
-  ArrayRef<transform::Param> currentDimSizes =
+  ArrayRef<transform::Param> currentDimAttrs =
       state.getParams(getDimensionSizes());
-  ArrayAttr targetDimensionSizes = getExpectedValues();
+  ArrayAttr targetDimAttrs = getExpectedValues();
 
-  if (!llvm::equal(currentDimSizes, targetDimensionSizes)) {
+  if (currentDimAttrs.size() != targetDimAttrs.size()) {
     return emitSilenceableError()
-           << "Dimension sizes do not match expected values";
+           << "dimension sizes and expected values have different lengths";
+  }
+
+  auto extractDims = [](const auto &range) {
+    return llvm::map_to_vector(
+        range, [](Attribute attr) -> std::optional<int64_t> {
+          if (auto intAttr = dyn_cast<IntegerAttr>(attr)) {
+            return intAttr.getInt();
```

**Comment:**
For example, you can make the target dims be `DenseI64ArrayAttr` instead of `ArrayAttr`. I don't think we can do anything about `currentDimAttrs` though.

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:340`

```diff
@@ -310,13 +310,36 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
 DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
     transform::TransformRewriter &rewriter,
     transform::TransformResults &results, transform::TransformState &state) {
-  ArrayRef<transform::Param> currentDimSizes =
+  ArrayRef<transform::Param> currentDimAttrs =
       state.getParams(getDimensionSizes());
-  ArrayAttr targetDimensionSizes = getExpectedValues();
+  ArrayAttr targetDimAttrs = getExpectedValues();
 
-  if (!llvm::equal(currentDimSizes, targetDimensionSizes)) {
+  if (currentDimAttrs.size() != targetDimAttrs.size()) {
     return emitSilenceableError()
-           << "Dimension sizes do not match expected values";
+           << "dimension sizes and expected values have different lengths";
+  }
+
+  auto extractDims = [](const auto &range) {
+    return llvm::map_to_vector(
+        range, [](Attribute attr) -> std::optional<int64_t> {
+          if (auto intAttr = dyn_cast<IntegerAttr>(attr)) {
+            return intAttr.getInt();
+          }
+          return std::nullopt;
+        });
+  };
+
+  SmallVector<std::optional<int64_t>> currentDims =
+      extractDims(currentDimAttrs);
+  SmallVector<std::optional<int64_t>> targetDims =
+      extractDims(targetDimAttrs.getValue());
+
+  if (!llvm::equal(currentDims, targetDims,
+                   [](const std::optional<int64_t> &lhs,
+                      const std::optional<int64_t> &rhs) {
+                     return (rhs && *rhs == -1) || (lhs && rhs && *lhs == *rhs);
```

**Comment:**
This doesn't seem to have been addressed

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:315`

```diff
@@ -310,13 +310,23 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
 DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
     transform::TransformRewriter &rewriter,
     transform::TransformResults &results, transform::TransformState &state) {
-  ArrayRef<transform::Param> currentDimSizes =
+  ArrayRef<transform::Param> currentDimAttrs =
       state.getParams(getDimensionSizes());
-  ArrayAttr targetDimensionSizes = getExpectedValues();
+  ArrayRef<int64_t> targetDims = getExpectedValues();
```

**Comment:**
nit: since you this is called in `expected_values` in tablegen, I'd keep similar variables names in the code here. Then you can have `actualDimAttrs` and `expectedDims`, which is a common pairing in any test code that performs pairwise comparisons.

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:340`

```diff
@@ -310,13 +310,36 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
 DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
     transform::TransformRewriter &rewriter,
     transform::TransformResults &results, transform::TransformState &state) {
-  ArrayRef<transform::Param> currentDimSizes =
+  ArrayRef<transform::Param> currentDimAttrs =
       state.getParams(getDimensionSizes());
-  ArrayAttr targetDimensionSizes = getExpectedValues();
+  ArrayAttr targetDimAttrs = getExpectedValues();
 
-  if (!llvm::equal(currentDimSizes, targetDimensionSizes)) {
+  if (currentDimAttrs.size() != targetDimAttrs.size()) {
     return emitSilenceableError()
-           << "Dimension sizes do not match expected values";
+           << "dimension sizes and expected values have different lengths";
+  }
+
+  auto extractDims = [](const auto &range) {
+    return llvm::map_to_vector(
+        range, [](Attribute attr) -> std::optional<int64_t> {
+          if (auto intAttr = dyn_cast<IntegerAttr>(attr)) {
+            return intAttr.getInt();
+          }
+          return std::nullopt;
+        });
+  };
+
+  SmallVector<std::optional<int64_t>> currentDims =
+      extractDims(currentDimAttrs);
+  SmallVector<std::optional<int64_t>> targetDims =
+      extractDims(targetDimAttrs.getValue());
+
+  if (!llvm::equal(currentDims, targetDims,
+                   [](const std::optional<int64_t> &lhs,
+                      const std::optional<int64_t> &rhs) {
+                     return (rhs && *rhs == -1) || (lhs && rhs && *lhs == *rhs);
```

**Comment:**
It's supported in c++17 https://godbolt.org/z/WzMvh6eYP
<img width="820" height="145" alt="image" src="https://github.com/user-attachments/assets/f012f6b2-614e-48db-87b1-dd99b4f3c24c" />

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:327`

```diff
@@ -310,21 +310,21 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
 DiagnosedSilenceableFailure IREE::transform_dialect::MatchDimsEqualOp::apply(
     transform::TransformRewriter &rewriter,
     transform::TransformResults &results, transform::TransformState &state) {
-  ArrayRef<transform::Param> currentDimAttrs =
+  ArrayRef<transform::Param> actualDimAttrs =
       state.getParams(getDimensionSizes());
-  ArrayRef<int64_t> targetDims = getExpectedValues();
+  ArrayRef<int64_t> expectedDims = getExpectedValues();
 
-  SmallVector<std::optional<int64_t>> currentDims = llvm::map_to_vector(
-      currentDimAttrs, [](Attribute attr) -> std::optional<int64_t> {
+  SmallVector<std::optional<int64_t>> actualDims = llvm::map_to_vector(
+      actualDimAttrs, [](Attribute attr) -> std::optional<int64_t> {
         if (auto intAttr = dyn_cast<IntegerAttr>(attr)) {
           return intAttr.getInt();
         }
         return std::nullopt;
       });
 
-  if (!llvm::equal(currentDims, targetDims,
+  if (!llvm::equal(actualDims, expectedDims,
                    [](const std::optional<int64_t> &lhs, int64_t rhs) {
-                     return (rhs == -1) || (lhs && *lhs == rhs);
+                     return (rhs == -1) || (lhs == rhs);
```

**Comment:**
```suggestion
                     return rhs == -1 || lhs == rhs;
```

---


---


## [PR #22137](https://github.com/iree-org/iree/pull/22137): [Codegen][GPU] Perfer MMA over VirtualMMA in Sorting. 

### Review Summary

**CHANGES_REQUESTED** (2025-09-28)

Does it matter that `sortMMAInstrinsics` uses `llvm::sort` instead of `llvm::stable_sort`?


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUHeuristics.cpp:579`

```diff
@@ -566,9 +566,30 @@ bool compareIntrinsics(const GPUMatmulShapeType &problem,
     return lhsArea > rhsArea;
   }
 
-  // Finally if everything else is the same, prefer large K size.
-  return ShapedType::getNumElements(lhs.kSizes) >
-         ShapedType::getNumElements(rhs.kSizes);
+  // Prefer large K size here.
+  int64_t lhsKSize = ShapedType::getNumElements(lhs.kSizes);
+  int64_t rhsKSize = ShapedType::getNumElements(rhs.kSizes);
+  if (lhsKSize != rhsKSize) {
+    return lhsKSize > rhsKSize;
+  }
+
+  const GPUIntrinsicType *lhsIntrinsic =
+      static_cast<const GPUIntrinsicType *>(&lhs);
+  const GPUIntrinsicType *rhsIntrinsic =
+      static_cast<const GPUIntrinsicType *>(&rhs);
```

**Comment:**
See https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable
```suggestion
  const auto *lhsIntrinsic =
      static_cast<const GPUIntrinsicType *>(&lhs);
  const auto *rhsIntrinsic =
      static_cast<const GPUIntrinsicType *>(&rhs);
```

But this cast seems inherently unsafe -- if we need `GPUIntrinsicType`, we should change the function signature of `compareIntrinsics` to take that instead.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUHeuristics.cpp:574`

```diff
@@ -566,9 +566,30 @@ bool compareIntrinsics(const GPUMatmulShapeType &problem,
     return lhsArea > rhsArea;
   }
 
-  // Finally if everything else is the same, prefer large K size.
-  return ShapedType::getNumElements(lhs.kSizes) >
-         ShapedType::getNumElements(rhs.kSizes);
+  // Prefer large K size here.
+  int64_t lhsKSize = ShapedType::getNumElements(lhs.kSizes);
+  int64_t rhsKSize = ShapedType::getNumElements(rhs.kSizes);
+  if (lhsKSize != rhsKSize) {
+    return lhsKSize > rhsKSize;
+  }
```

**Comment:**
An easier way to write this comparison is to come up with a feature tuple for `GPUInstrinsicType` and then delegate the exact comparison logic to the `std::tuple` comparison operator.

---


---


## [PR #22134](https://github.com/iree-org/iree/pull/22134): [Codegen][GPU] update the tuning spec using new transform ops

### Review Summary

**CHANGES_REQUESTED** (2025-10-07)

Nice, the new matcher ops make this much more manageable. Two things:
1. Can you split this up into multiple PRs, one per each new matcher? This way we can tell which one is at fault if anything breaks.
2. Can you confirm that the new specs work? I'd expect that the should apply to the same number of ops and that the resulting .vmfb binaries are identical.



---


## [PR #22122](https://github.com/iree-org/iree/pull/22122): [Codegen][GPU][NFC] Fix mma sort follow up

### Review Summary

**APPROVED** (2025-09-26)

Thanks. Can you also add `NFC` to the PR subject line?



---


## [PR #22090](https://github.com/iree-org/iree/pull/22090): [Codegen][GPU] Fix MMA Intrinsics Sorting

### Review Summary

**APPROVED** (2025-09-24)

Nice catch


**COMMENTED** (2025-09-26)


**COMMENTED** (2025-09-26)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUHeuristics.h:47`

```diff
@@ -30,6 +36,18 @@ struct GPUMatmulShapeType {
                      Type b, Type c)
       : mSizes(m), nSizes(n), kSizes(k), batchSizes(batch), aType(a), bType(b),
         cType(c) {}
+
+  // Constructor with the num_rhs parameter.
+  GPUMatmulShapeType(int64_t m, int64_t n, int64_t k, Type a, Type b, Type c,
+                     int64_t numHorizontallyFusedOps)
+      : mSizes({m}), nSizes({n}), kSizes({k}), batchSizes({}), aType(a),
+        bType(b), cType(c), numHorizontallyFusedOps(numHorizontallyFusedOps) {}
+
+  GPUMatmulShapeType(ArrayRef<int64_t> m, ArrayRef<int64_t> n,
+                     ArrayRef<int64_t> k, ArrayRef<int64_t> batch, Type a,
```

**Comment:**
Why not make `numHorizontallyFusedOps` a default constructor parameter to the existing consturctors?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUHeuristics.h:40`

```diff
@@ -30,6 +36,18 @@ struct GPUMatmulShapeType {
                      Type b, Type c)
       : mSizes(m), nSizes(n), kSizes(k), batchSizes(batch), aType(a), bType(b),
         cType(c) {}
+
+  // Constructor with the num_rhs parameter.
```

**Comment:**
This is not super helpful IMO and the variable name in the comment doesn't match the code.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUHeuristics.h:47`

```diff
@@ -30,6 +36,18 @@ struct GPUMatmulShapeType {
                      Type b, Type c)
       : mSizes(m), nSizes(n), kSizes(k), batchSizes(batch), aType(a), bType(b),
         cType(c) {}
+
+  // Constructor with the num_rhs parameter.
+  GPUMatmulShapeType(int64_t m, int64_t n, int64_t k, Type a, Type b, Type c,
+                     int64_t numHorizontallyFusedOps)
+      : mSizes({m}), nSizes({n}), kSizes({k}), batchSizes({}), aType(a),
+        bType(b), cType(c), numHorizontallyFusedOps(numHorizontallyFusedOps) {}
+
+  GPUMatmulShapeType(ArrayRef<int64_t> m, ArrayRef<int64_t> n,
+                     ArrayRef<int64_t> k, ArrayRef<int64_t> batch, Type a,
```

**Comment:**
I'm not super familiar with this code, does a default argument make it incompatible with the existing usage?

---


---


## [PR #22040](https://github.com/iree-org/iree/pull/22040): [Codegen] Add transform op for matching dimension sizes.

### Review Summary

**COMMENTED** (2025-09-22)


**COMMENTED** (2025-09-22)


**COMMENTED** (2025-09-22)


**COMMENTED** (2025-09-22)


**COMMENTED** (2025-09-22)


**COMMENTED** (2025-09-24)


**COMMENTED** (2025-09-24)


**COMMENTED** (2025-09-25)


**APPROVED** (2025-09-25)

LGTM but let's wait for @qedawkins or @Max191 to review too


### Code Comments

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:331`

```diff
@@ -303,6 +303,41 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchSizeEqualsOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchSizeEqualsOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  Location loc = current->getLoc();
+  ArrayRef<transform::Param> actualDimSizes =
+      state.getParams(getDimensionSizes());
+  ArrayAttr targetDimensionSizes = getExpectedValues();
+
+  if (actualDimSizes.size() != targetDimensionSizes.size()) {
+    return emitSilenceableFailure(loc)
+           << "Dimension sizes array and target sizes array have different "
+              "lengths";
+  }
+
+  for (auto [dimParam, targetValuesAttr] :
+       llvm::zip_equal(actualDimSizes, targetDimensionSizes)) {
+    int64_t currentDimSize = cast<IntegerAttr>(dimParam).getInt();
+    ArrayAttr allowedSizes = cast<ArrayAttr>(targetValuesAttr);
+    bool foundMatch = llvm::any_of(allowedSizes, [&](Attribute targetAttr) {
+      return cast<IntegerAttr>(targetAttr).getInt() == currentDimSize;
+    });
```

**Comment:**
How do you know you know each dim gets matched? For example, I think this would match:
expected: `[64, 64, 64]`
actual: `[64, 128, 256]`

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:243`

```diff
@@ -225,4 +225,42 @@ def MatchContractionOp : Op<Transform_Dialect, "iree.match.contraction",
   let cppNamespace = "mlir::iree_compiler::IREE::transform_dialect";
 }
 
+def MatchSizeEqualsOp : Op<Transform_Dialect, "iree.match.size_equals",
+    [MatchOpInterface,
+     SingleOpMatcher,
+     MemoryEffectsOpInterface]> {
+  let summary = [{Check whether a single transform parameter matches expected size values.}];
+  let description = [{
+    Matches dimension sizes against expected values.
+    The dimension sizes is an array of transform parameters, and the expected values
+    are structured as nested arrays where each sub-array corresponds to a position
+    in the size array.
+
+    Example: %batch_dims = [[2, 4], [8, 16], [32, 64]]
+    This means:
+    - %batch_dims[0] should be either 2 or 4.
+    - %batch_dims[1] should be either 8 or 16.
+    - %batch_dims[2] should be either 32 or 64.
```

**Comment:**
I don't understand why we need to support alternatives. What's the usecase?

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:331`

```diff
@@ -303,6 +303,41 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchSizeEqualsOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchSizeEqualsOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  Location loc = current->getLoc();
+  ArrayRef<transform::Param> actualDimSizes =
+      state.getParams(getDimensionSizes());
+  ArrayAttr targetDimensionSizes = getExpectedValues();
+
+  if (actualDimSizes.size() != targetDimensionSizes.size()) {
+    return emitSilenceableFailure(loc)
+           << "Dimension sizes array and target sizes array have different "
+              "lengths";
+  }
+
+  for (auto [dimParam, targetValuesAttr] :
+       llvm::zip_equal(actualDimSizes, targetDimensionSizes)) {
+    int64_t currentDimSize = cast<IntegerAttr>(dimParam).getInt();
+    ArrayAttr allowedSizes = cast<ArrayAttr>(targetValuesAttr);
+    bool foundMatch = llvm::any_of(allowedSizes, [&](Attribute targetAttr) {
+      return cast<IntegerAttr>(targetAttr).getInt() == currentDimSize;
+    });
```

**Comment:**
OK, I see what's going on: https://github.com/iree-org/iree/pull/22040/files#r2370334264

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:243`

```diff
@@ -225,4 +225,42 @@ def MatchContractionOp : Op<Transform_Dialect, "iree.match.contraction",
   let cppNamespace = "mlir::iree_compiler::IREE::transform_dialect";
 }
 
+def MatchSizeEqualsOp : Op<Transform_Dialect, "iree.match.size_equals",
+    [MatchOpInterface,
+     SingleOpMatcher,
+     MemoryEffectsOpInterface]> {
+  let summary = [{Check whether a single transform parameter matches expected size values.}];
+  let description = [{
+    Matches dimension sizes against expected values.
+    The dimension sizes is an array of transform parameters, and the expected values
+    are structured as nested arrays where each sub-array corresponds to a position
+    in the size array.
+
+    Example: %batch_dims = [[2, 4], [8, 16], [32, 64]]
+    This means:
+    - %batch_dims[0] should be either 2 or 4.
+    - %batch_dims[1] should be either 8 or 16.
+    - %batch_dims[2] should be either 32 or 64.
```

**Comment:**
but why do we need this?

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:243`

```diff
@@ -225,4 +225,42 @@ def MatchContractionOp : Op<Transform_Dialect, "iree.match.contraction",
   let cppNamespace = "mlir::iree_compiler::IREE::transform_dialect";
 }
 
+def MatchSizeEqualsOp : Op<Transform_Dialect, "iree.match.size_equals",
+    [MatchOpInterface,
+     SingleOpMatcher,
+     MemoryEffectsOpInterface]> {
+  let summary = [{Check whether a single transform parameter matches expected size values.}];
+  let description = [{
+    Matches dimension sizes against expected values.
+    The dimension sizes is an array of transform parameters, and the expected values
+    are structured as nested arrays where each sub-array corresponds to a position
+    in the size array.
+
+    Example: %batch_dims = [[2, 4], [8, 16], [32, 64]]
+    This means:
+    - %batch_dims[0] should be either 2 or 4.
+    - %batch_dims[1] should be either 8 or 16.
+    - %batch_dims[2] should be either 32 or 64.
```

**Comment:**
Yeah I don't think we will have to emit this from the tuner. Later on we could potentially allow for merging of matchers, but I don't think we will need it any time soon. For the v0, I'd like to be able to replace DAG matching only.

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:332`

```diff
@@ -303,6 +303,42 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchSizeEqualsOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure IREE::transform_dialect::MatchSizeEqualsOp::apply(
+    transform::TransformRewriter &rewriter,
+    transform::TransformResults &results, transform::TransformState &state) {
+  ArrayRef<transform::Param> currentDimSizes =
+      state.getParams(getDimensionSizes());
+  ArrayAttr targetDimensionSizes = getExpectedValues();
+
+  if (currentDimSizes.size() != targetDimensionSizes.size()) {
+    return emitSilenceableError() << "dimension sizes have different lengths ("
+                                  << currentDimSizes.size() << " vs "
+                                  << targetDimensionSizes.size() << ")";
+  }
+
+  for (auto [currentDimSizeAttr, targetDimSizeAttr] :
+       llvm::zip_equal(currentDimSizes, targetDimensionSizes)) {
+    int64_t currentDimSize = cast<IntegerAttr>(currentDimSizeAttr).getInt();
+    int64_t targetDimSize = cast<IntegerAttr>(targetDimSizeAttr).getInt();
+    if (currentDimSize != targetDimSize) {
+      return emitSilenceableError()
+             << "Dimension size " << currentDimSize
+             << " does not match expected size " << targetDimSize;
+    }
+  }
```

**Comment:**
Could we replace this with `if(!llvm::equal(currentDimSizes, targetDimSizes))`? I don't think we need a precise diagnostics beyond that the two arrays don't match

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:241`

```diff
@@ -225,4 +225,40 @@ def MatchContractionOp : Op<Transform_Dialect, "iree.match.contraction",
   let cppNamespace = "mlir::iree_compiler::IREE::transform_dialect";
 }
 
+def MatchSizeEqualsOp : Op<Transform_Dialect, "iree.match.size_equals",
+    [DeclareOpInterfaceMethods<TransformOpInterface>,
+     MatchOpInterface,
+     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
+  let summary = [{Check whether transform parameters match expected size values exactly.}];
+  let description = [{
+    Matches dimension sizes against expected values.
+    Each position in the dimension sizes array must match the corresponding
+    expected value exactly.
+
+    Example: %batch_dims contains [2, 4] and expected_values is [2, 4].
+    This means:
+    - %batch_dims[0] must equal 2.
+    - %batch_dims[1] must equal 4.
```

**Comment:**
Can we use an actual op to show the syntax?

Something like:
```
 `transform.iree.match.size_equals %m, [512, 256]` matches when `%m` has two dims, 512 and 256.
```

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:322`

```diff
@@ -303,6 +304,36 @@ IREE::transform_dialect::MatchContractionOp::matchOperation(
   return DiagnosedSilenceableFailure::success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchSizeEqualsOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure IREE::transform_dialect::MatchSizeEqualsOp::apply(
+    transform::TransformRewriter &rewriter,
+    transform::TransformResults &results, transform::TransformState &state) {
+  ArrayRef<transform::Param> currentDimSizes =
+      state.getParams(getDimensionSizes());
+  ArrayAttr targetDimensionSizes = getExpectedValues();
+
+  if (currentDimSizes.size() != targetDimensionSizes.size()) {
+    return emitSilenceableError() << "dimension sizes have different lengths ("
+                                  << currentDimSizes.size() << " vs "
+                                  << targetDimensionSizes.size() << ")";
+  }
```

**Comment:**
`llvm::equal` below already checks the sizes for you: https://en.cppreference.com/w/cpp/algorithm/equal.html#:~:text=of%20BinaryPredicate.-,Return%20value,-1%2D4)

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:241`

```diff
@@ -225,4 +225,40 @@ def MatchContractionOp : Op<Transform_Dialect, "iree.match.contraction",
   let cppNamespace = "mlir::iree_compiler::IREE::transform_dialect";
 }
 
+def MatchSizeEqualsOp : Op<Transform_Dialect, "iree.match.size_equals",
+    [DeclareOpInterfaceMethods<TransformOpInterface>,
+     MatchOpInterface,
+     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
+  let summary = [{Check whether transform parameters match expected size values exactly.}];
+  let description = [{
+    Matches dimension sizes against expected values.
+    Each position in the dimension sizes array must match the corresponding
+    expected value exactly.
+
+    ### Example
+
+    `transform.iree.match.size_equals %m, [512, 256] : !transform.param<i64>`
+    succeeds when `%m` has exactly two dimensions, 512 and 256.
```

**Comment:**
Code blocks render nicely on the website
```suggestion
    ```milr
    transform.iree.match.size_equals %m, [512, 256] : !transform.param<i64>
    ```

    This succeeds when `%m` has exactly two dimensions, 512 and 256.
```

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:228`

```diff
@@ -225,4 +225,40 @@ def MatchContractionOp : Op<Transform_Dialect, "iree.match.contraction",
   let cppNamespace = "mlir::iree_compiler::IREE::transform_dialect";
 }
 
+def MatchSizeEqualsOp : Op<Transform_Dialect, "iree.match.size_equals",
```

**Comment:**
I wonder if we should call it `iree.match.dims_equal` or something like that -- we are no longer taking a handle to the op as an argument, we are only looking at some of its dimension. I think sizes would imply we are looking at the overall size.

We already have two custom dim matching ops: https://github.com/iree-org/iree/blob/2c5b07daa588cd81c41e1ff19aa4130c7e41e6ce/compiler/src/iree/compiler/Preprocessing/Common/test/preprocessing_match_ops.mlir#L180-L181

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:228`

```diff
@@ -225,4 +225,40 @@ def MatchContractionOp : Op<Transform_Dialect, "iree.match.contraction",
   let cppNamespace = "mlir::iree_compiler::IREE::transform_dialect";
 }
 
+def MatchSizeEqualsOp : Op<Transform_Dialect, "iree.match.size_equals",
```

**Comment:**
Another example: https://mlir.llvm.org/docs/Dialects/Transform/#transformmatchparamcmpi-transformmatchparamcmpiop

https://github.com/search?q=repo%3Allvm%2Fllvm-project%20transform.match.param.cmpi&type=code

---


---


## [PR #22027](https://github.com/iree-org/iree/pull/22027): [Codegen][Tuner] update lowering config binding for subgroup basis

### Review Summary

**APPROVED** (2025-09-18)


**COMMENTED** (2025-09-18)


### Code Comments

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:378`

```diff
@@ -356,6 +356,33 @@ ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment) {
   return result;
 }
 
+ireeGPUSubgroupBasisInfo
+ireeGPULoweringConfigAttrGetSubgroupBasis(MlirAttribute attr) {
+  auto loweringConfigAttr =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr));
+
+  mlir::FailureOr<mlir::iree_compiler::IREE::GPU::Basis> basisResult =
+      mlir::iree_compiler::IREE::GPU::getBasis(
+          loweringConfigAttr,
+          mlir::iree_compiler::IREE::GPU::TilingLevel::Subgroup);
+
+  ireeGPUSubgroupBasisInfo info = {};
+  if (failed(basisResult)) {
+    return info;
+  }
+
+  mlir::iree_compiler::IREE::GPU::Basis basis = *basisResult;
+  mlir::Builder builder(loweringConfigAttr.getContext());
+  mlir::ArrayAttr countsAttr = builder.getI64ArrayAttr(basis.counts);
+  mlir::ArrayAttr mappingAttr = builder.getI64ArrayAttr(basis.mapping);
```

**Comment:**
Do we know what's the cost of creating the builder just to create some attributes? I know we've been doing that for a while in this file, but this stood out for me just now. No need to change this here, just something to check separately.

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:378`

```diff
@@ -356,6 +356,33 @@ ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment) {
   return result;
 }
 
+ireeGPUSubgroupBasisInfo
+ireeGPULoweringConfigAttrGetSubgroupBasis(MlirAttribute attr) {
+  auto loweringConfigAttr =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr));
+
+  mlir::FailureOr<mlir::iree_compiler::IREE::GPU::Basis> basisResult =
+      mlir::iree_compiler::IREE::GPU::getBasis(
+          loweringConfigAttr,
+          mlir::iree_compiler::IREE::GPU::TilingLevel::Subgroup);
+
+  ireeGPUSubgroupBasisInfo info = {};
+  if (failed(basisResult)) {
+    return info;
+  }
+
+  mlir::iree_compiler::IREE::GPU::Basis basis = *basisResult;
+  mlir::Builder builder(loweringConfigAttr.getContext());
+  mlir::ArrayAttr countsAttr = builder.getI64ArrayAttr(basis.counts);
+  mlir::ArrayAttr mappingAttr = builder.getI64ArrayAttr(basis.mapping);
```

**Comment:**
I've just checked and it's cheap: https://github.com/llvm/llvm-project/blob/1a172b9924948f10f1bd3db07a83fe5e884f7b64/mlir/include/mlir/IR/Builders.h#L51-L55

---


---


## [PR #21981](https://github.com/iree-org/iree/pull/21981): [Codegen] Add transform ops for matching contraction ops

### Review Summary

**COMMENTED** (2025-09-15)

How do you expect these to be used within a tuning spec? Right now, these don't have a way of specifying things like indexing maps or operand/result types, so I think we would still have to perform dag-based matching, no?


**COMMENTED** (2025-09-21)


**COMMENTED** (2025-09-22)


**COMMENTED** (2025-09-22)


**COMMENTED** (2025-09-22)


**APPROVED** (2025-09-22)

I think calling it `transform.iree.match.contraction` would be more consistent with the other match extension ops. Otherwise looks good.


### Code Comments

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:205`

```diff
@@ -172,4 +172,57 @@ def MatchRegionsOp : Op<Transform_Dialect, "iree.match.regions",
   let hasVerifier = 1;
 }
 
+def MatchContractionOp : Op<Transform_Dialect, "iree.match.is_contraction",
+    [MatchOpInterface,
+     SingleOpMatcher,
+     MemoryEffectsOpInterface]> {
+  let summary = [{Check whether the op is a contraction operation.}];
+  let description = [{
+    Matches operations that implement the ContractionOpInterface.
+    This includes operations like linalg.matmul, linalg.batch_matmul, etc.
+
+    Optionally matches specific indexing maps patterns.
+
+    #### Return modes
+
+    Succeeds if the operation is a contraction operation, and
+    produces a silenceable failure otherwise.
+
+    #### Results
+
+    Returns arrays of dimension sizes for each contraction dimension:
+    - batch_dims: Array of batch dimension sizes.
+    - m_dims: Array of M dimension sizes.
+    - n_dims: Array of N dimension sizes.
+    - k_dims: Array of K dimension sizes.
+  }];
+
+  let arguments = (ins
+    TransformHandleTypeInterface:$operand_handle,
+    OptionalAttr<AffineMapArrayAttr>:$indexing_maps,
+    OptionalAttr<TypeAttr>:$lhs_type,                 // LHS input type.
+    OptionalAttr<TypeAttr>:$rhs_type,                 // RHS input type.
+    OptionalAttr<TypeAttr>:$output_type               // Output type.
```

**Comment:**
Should we make these non-optional? I don't think we have any use for matching contraction ops with arbitrary types

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:202`

```diff
@@ -172,4 +172,57 @@ def MatchRegionsOp : Op<Transform_Dialect, "iree.match.regions",
   let hasVerifier = 1;
 }
 
+def MatchContractionOp : Op<Transform_Dialect, "iree.match.is_contraction",
+    [MatchOpInterface,
+     SingleOpMatcher,
+     MemoryEffectsOpInterface]> {
+  let summary = [{Check whether the op is a contraction operation.}];
+  let description = [{
+    Matches operations that implement the ContractionOpInterface.
+    This includes operations like linalg.matmul, linalg.batch_matmul, etc.
+
+    Optionally matches specific indexing maps patterns.
+
+    #### Return modes
+
+    Succeeds if the operation is a contraction operation, and
+    produces a silenceable failure otherwise.
+
+    #### Results
+
+    Returns arrays of dimension sizes for each contraction dimension:
+    - batch_dims: Array of batch dimension sizes.
+    - m_dims: Array of M dimension sizes.
+    - n_dims: Array of N dimension sizes.
+    - k_dims: Array of K dimension sizes.
+  }];
+
+  let arguments = (ins
+    TransformHandleTypeInterface:$operand_handle,
+    OptionalAttr<AffineMapArrayAttr>:$indexing_maps,
```

**Comment:**
Why not define a few variants like `NN`, `NT`, etc., like we discussed before? I'm not saying indexing maps are a bad option, but these are quite verbose and I'd like to understand what motivates this extra complexity.

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:256`

```diff
@@ -223,6 +223,114 @@ IREE::transform_dialect::MatchCastCompatibleDagFromRootOp::verify() {
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchContractionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchContractionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(current->getLoc())
+           << "Operation " << *current << " is not a LinalgOp.";
+  }
+
+  if (!linalg::isaContractionOpInterface(linalgOp)) {
+    return emitSilenceableFailure(current->getLoc())
+           << "Operation " << *current << " is not a contraction operation.";
+  }
+
+  if (std::optional<ArrayAttr> indexingMaps = getIndexingMaps()) {
+    ArrayAttr currentIndexingMaps = linalgOp.getIndexingMaps();
+    ArrayAttr targetIndexingMaps = *indexingMaps;
+    if (currentIndexingMaps.size() != targetIndexingMaps.size()) {
+      return emitSilenceableFailure(current->getLoc())
+             << "indexing maps count mismatch: expected "
+             << targetIndexingMaps.size() << ", got "
+             << currentIndexingMaps.size();
+    }
+
+    for (auto [currentMapAttr, targetMapAttr] :
+         llvm::zip(currentIndexingMaps, targetIndexingMaps)) {
```

**Comment:**
Use `zip_equal` for ranges of equal lengths. See https://llvm.org/docs/ProgrammersManual.html#iterating-over-ranges

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:248`

```diff
@@ -223,6 +223,114 @@ IREE::transform_dialect::MatchCastCompatibleDagFromRootOp::verify() {
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchContractionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchContractionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(current->getLoc())
+           << "Operation " << *current << " is not a LinalgOp.";
+  }
+
+  if (!linalg::isaContractionOpInterface(linalgOp)) {
+    return emitSilenceableFailure(current->getLoc())
+           << "Operation " << *current << " is not a contraction operation.";
+  }
+
+  if (std::optional<ArrayAttr> indexingMaps = getIndexingMaps()) {
+    ArrayAttr currentIndexingMaps = linalgOp.getIndexingMaps();
+    ArrayAttr targetIndexingMaps = *indexingMaps;
+    if (currentIndexingMaps.size() != targetIndexingMaps.size()) {
```

**Comment:**
Why not compare two array attributes? I don't think there's value in producing verbose silenceable failures -- nobody looks at them

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:236`

```diff
@@ -223,6 +223,114 @@ IREE::transform_dialect::MatchCastCompatibleDagFromRootOp::verify() {
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchContractionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchContractionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(current->getLoc())
```

**Comment:**
Since the `current->getLoc()` expression appears often in this function, I'd hoist it to a local variable

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:317`

```diff
@@ -223,6 +223,114 @@ IREE::transform_dialect::MatchCastCompatibleDagFromRootOp::verify() {
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchContractionOp
+//===----------------------------------------------------------------------===//
+
+DiagnosedSilenceableFailure
+IREE::transform_dialect::MatchContractionOp::matchOperation(
+    Operation *current, transform::TransformResults &results,
+    transform::TransformState &state) {
+  auto linalgOp = dyn_cast<linalg::LinalgOp>(current);
+  if (!linalgOp) {
+    return emitSilenceableFailure(current->getLoc())
+           << "Operation " << *current << " is not a LinalgOp.";
+  }
+
+  if (!linalg::isaContractionOpInterface(linalgOp)) {
+    return emitSilenceableFailure(current->getLoc())
+           << "Operation " << *current << " is not a contraction operation.";
+  }
+
+  if (std::optional<ArrayAttr> indexingMaps = getIndexingMaps()) {
+    ArrayAttr currentIndexingMaps = linalgOp.getIndexingMaps();
+    ArrayAttr targetIndexingMaps = *indexingMaps;
+    if (currentIndexingMaps.size() != targetIndexingMaps.size()) {
+      return emitSilenceableFailure(current->getLoc())
+             << "indexing maps count mismatch: expected "
+             << targetIndexingMaps.size() << ", got "
+             << currentIndexingMaps.size();
+    }
+
+    for (auto [currentMapAttr, targetMapAttr] :
+         llvm::zip(currentIndexingMaps, targetIndexingMaps)) {
+      AffineMapAttr currentMap = cast<AffineMapAttr>(currentMapAttr);
+      AffineMapAttr targetMap = cast<AffineMapAttr>(targetMapAttr);
+      if (currentMap.getValue() != targetMap.getValue()) {
+        return emitSilenceableFailure(current->getLoc())
+               << "indexing maps don't match: expected " << targetMap
+               << ", got " << currentMap;
+      }
+    }
+  }
+
+  if (std::optional<Type> lhsType = getLhsType()) {
+    Type targetLhsType = *lhsType;
+    Type currentLhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[0]);
+    if (currentLhsType != targetLhsType) {
+      return emitSilenceableFailure(current->getLoc())
+             << "LHS type doesn't match: expected " << targetLhsType << ", got "
+             << currentLhsType;
+    }
+  }
+
+  if (std::optional<Type> rhsType = getRhsType()) {
+    Type targetRhsType = *rhsType;
+    Type currentRhsType = getElementTypeOrSelf(linalgOp.getDpsInputs()[1]);
+    if (currentRhsType != targetRhsType) {
+      return emitSilenceableFailure(current->getLoc())
+             << "RHS type doesn't match: expected " << targetRhsType << ", got "
+             << currentRhsType;
+    }
+  }
+
+  if (std::optional<Type> outputType = getOutputType()) {
+    const Type targetOutputType = *outputType;
+    SmallVector<Type> currentOutputTypes;
+    for (Value output : linalgOp.getDpsInits()) {
+      currentOutputTypes.push_back(getElementTypeOrSelf(output.getType()));
+    }
+
+    if (currentOutputTypes.size() != 1) {
+      return emitSilenceableFailure(current->getLoc())
+             << "expected single output, got " << currentOutputTypes.size();
+    }
+
+    Type currentOutputType = currentOutputTypes[0];
+    if (currentOutputType != targetOutputType) {
+      return emitSilenceableFailure(current->getLoc())
+             << "output type doesn't match: expected " << targetOutputType
+             << ", got " << currentOutputType;
+    }
+  }
+
+  // Get the actual size values for batch/m/n/k dimensions after verifying it's
+  // a contraction operation.
+  linalg::ContractionDimensions contractionDims =
+      linalg::inferContractionDims(linalgOp).value();
+  SmallVector<int64_t> iterationDomain = linalgOp.getStaticLoopRanges();
+  MLIRContext *context = current->getContext();
+  Builder builder(context);
+
+  auto iterationSizes = [&](ArrayRef<unsigned> dimIndices) {
+    return llvm::to_vector(
+        llvm::map_range(dimIndices, [&](unsigned dimIdx) -> Attribute {
```

**Comment:**
Use `map_to_vector`

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:202`

```diff
@@ -172,4 +172,57 @@ def MatchRegionsOp : Op<Transform_Dialect, "iree.match.regions",
   let hasVerifier = 1;
 }
 
+def MatchContractionOp : Op<Transform_Dialect, "iree.match.is_contraction",
+    [MatchOpInterface,
+     SingleOpMatcher,
+     MemoryEffectsOpInterface]> {
+  let summary = [{Check whether the op is a contraction operation.}];
+  let description = [{
+    Matches operations that implement the ContractionOpInterface.
+    This includes operations like linalg.matmul, linalg.batch_matmul, etc.
+
+    Optionally matches specific indexing maps patterns.
+
+    #### Return modes
+
+    Succeeds if the operation is a contraction operation, and
+    produces a silenceable failure otherwise.
+
+    #### Results
+
+    Returns arrays of dimension sizes for each contraction dimension:
+    - batch_dims: Array of batch dimension sizes.
+    - m_dims: Array of M dimension sizes.
+    - n_dims: Array of N dimension sizes.
+    - k_dims: Array of K dimension sizes.
+  }];
+
+  let arguments = (ins
+    TransformHandleTypeInterface:$operand_handle,
+    OptionalAttr<AffineMapArrayAttr>:$indexing_maps,
```

**Comment:**
You can also see how we defined these in named matmul ops in linalg: https://github.com/llvm/llvm-project/blob/105fc90b6b96e0edb7529062fcba513a3a347820/mlir/lib/Dialect/Linalg/IR/LinalgOps.cpp#L4053-L4076

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensionsOps.td:202`

```diff
@@ -172,4 +172,57 @@ def MatchRegionsOp : Op<Transform_Dialect, "iree.match.regions",
   let hasVerifier = 1;
 }
 
+def MatchContractionOp : Op<Transform_Dialect, "iree.match.is_contraction",
+    [MatchOpInterface,
+     SingleOpMatcher,
+     MemoryEffectsOpInterface]> {
+  let summary = [{Check whether the op is a contraction operation.}];
+  let description = [{
+    Matches operations that implement the ContractionOpInterface.
+    This includes operations like linalg.matmul, linalg.batch_matmul, etc.
+
+    Optionally matches specific indexing maps patterns.
+
+    #### Return modes
+
+    Succeeds if the operation is a contraction operation, and
+    produces a silenceable failure otherwise.
+
+    #### Results
+
+    Returns arrays of dimension sizes for each contraction dimension:
+    - batch_dims: Array of batch dimension sizes.
+    - m_dims: Array of M dimension sizes.
+    - n_dims: Array of N dimension sizes.
+    - k_dims: Array of K dimension sizes.
+  }];
+
+  let arguments = (ins
+    TransformHandleTypeInterface:$operand_handle,
+    OptionalAttr<AffineMapArrayAttr>:$indexing_maps,
```

**Comment:**
I think the main criteria has to be that we should be able to easily emit these matchers inside the tuner, given a linalg.generic. In this setting, getting indexing maps is easier than deciding the matmul variant. Let's leave this as-is then -- I think we don't need these `NN`/`NT`/etc. variants after all.

---

**File:** `compiler/src/iree/compiler/Preprocessing/TransformExtensions/PreprocessingExtensions.cpp:232`

```diff
@@ -223,6 +224,112 @@ IREE::transform_dialect::MatchCastCompatibleDagFromRootOp::verify() {
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// MatchContractionOp
+//===----------------------------------------------------------------------===//
+
+// Helper function to get indexing maps for matmul variants.
+static ArrayAttr getMatmulIndexingMaps(StringAttr variant,
```

**Comment:**
This function is unused now

---

**File:** `compiler/src/iree/compiler/Preprocessing/Common/test/preprocessing_match_ops.mlir:447`

```diff
@@ -271,3 +271,177 @@ module attributes {transform.with_named_sequence} {
     transform.yield
   }
 }
+
+// -----
+
+// Verify that the basic contraction matcher works and can extract dimension sizes.
+
+#map0 = affine_map<(d0, d1, d2) -> (d0, d2)>
+#map1 = affine_map<(d0, d1, d2) -> (d1, d2)>
+#map2 = affine_map<(d0, d1, d2) -> (d0, d1)>
+#map_batch0 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
+#map_batch1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
+#map_batch2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
+
+// CHECK-LABEL: func.func @op_matmul
+func.func @op_matmul(%input0: tensor<32x64xi8>, %input1: tensor<32x64xi8>, %dest: tensor<32x32xi32>) -> tensor<32x32xi32> {
+  // CHECK-NEXT: linalg.matmul
+  // CHECK-SAME:   match_status = "matched"
+  %res = linalg.matmul
+        indexing_maps = [#map0, #map1, #map2]
+        ins(%input0, %input1 : tensor<32x64xi8>, tensor<32x64xi8>)
+        outs(%dest : tensor<32x32xi32>) {match_status = "unmatched"} -> tensor<32x32xi32>
+  return %res : tensor<32x32xi32>
+}
+
+// CHECK-LABEL: func.func @op_batch_matmul
+func.func @op_batch_matmul(%input0: tensor<2x32x64xi8>, %input1: tensor<2x32x64xi8>, %dest: tensor<2x32x32xi32>) -> tensor<2x32x32xi32> {
+  // CHECK-NEXT: linalg.batch_matmul
+  // CHECK-SAME:   match_status = "matched"
+  %res = linalg.batch_matmul
+        indexing_maps = [#map_batch0, #map_batch1, #map_batch2]
+        ins(%input0, %input1 : tensor<2x32x64xi8>, tensor<2x32x64xi8>)
+        outs(%dest : tensor<2x32x32xi32>) {match_status = "unmatched"} -> tensor<2x32x32xi32>
+  return %res : tensor<2x32x32xi32>
+}
+
+// CHECK-LABEL: func.func @op_fill
+func.func @op_fill(%dest: tensor<32x64xf32>, %value: f32) -> tensor<32x64xf32> {
+  // CHECK-NEXT: linalg.fill
+  // CHECK-SAME:   match_status = "unmatched"
+  %res = linalg.fill ins(%value : f32) outs(%dest : tensor<32x64xf32>) {match_status = "unmatched"} -> tensor<32x64xf32>
+  return %res : tensor<32x64xf32>
+}
+
+module attributes {transform.with_named_sequence} {
+  transform.named_sequence @match_matmul(%op: !transform.any_op {transform.readonly}) -> !transform.any_op {
+    %batch_dims, %m_dims, %n_dims, %k_dims = transform.iree.match.is_contraction %op,
+      lhs_type = i8, rhs_type = i8, output_type = i32 :
+      (!transform.any_op) -> (!transform.param<i64>, !transform.param<i64>, !transform.param<i64>, !transform.param<i64>)
+    %c32 = transform.param.constant 32 : i64 -> !transform.param<i64>
+    transform.match.param.cmpi eq %m_dims, %c32 : !transform.param<i64>
+    transform.match.param.cmpi eq %n_dims, %c32 : !transform.param<i64>
+    transform.yield %op : !transform.any_op
+  }
+
+  transform.named_sequence @match_batch_matmul(%op: !transform.any_op {transform.readonly}) -> !transform.any_op {
+    %batch_dims, %m_dims, %n_dims, %k_dims = transform.iree.match.is_contraction %op,
+      lhs_type = i8, rhs_type = i8, output_type = i32 :
+      (!transform.any_op) -> (!transform.param<i64>, !transform.param<i64>, !transform.param<i64>, !transform.param<i64>)
+    %c2 = transform.param.constant 2 : i64 -> !transform.param<i64>
+    %c32 = transform.param.constant 32 : i64 -> !transform.param<i64>
+    transform.match.param.cmpi eq %batch_dims, %c2 : !transform.param<i64>
+    transform.match.param.cmpi eq %m_dims, %c32 : !transform.param<i64>
+    transform.match.param.cmpi eq %n_dims, %c32 : !transform.param<i64>
+    transform.yield %op : !transform.any_op
+  }
+
+  transform.named_sequence @annotate(%op: !transform.any_op {transform.readonly}) {
+    %0 = transform.param.constant "matched" -> !transform.any_param
+    transform.annotate %op "match_status" = %0 : !transform.any_op, !transform.any_param
+    transform.yield
+  }
+
+  transform.named_sequence @__transform_main(%module: !transform.any_op) {
+    transform.foreach_match in %module
+        @match_matmul -> @annotate,
+        @match_batch_matmul -> @annotate
+      : (!transform.any_op) -> (!transform.any_op)
+    transform.yield
+  }
+}
+
+// -----
+
+// Verify that operations with exact same matching indexing maps are matched correctly,
+// and operations with different indexing map patterns are not matched.
+
+#map_matmul0 = affine_map<(d0, d1, d2) -> (d0, d2)>
+#map_matmul1 = affine_map<(d0, d1, d2) -> (d1, d2)>
+#map_matmul2 = affine_map<(d0, d1, d2) -> (d0, d1)>
+#map_transpose_b = affine_map<(d0, d1, d2) -> (d2, d1)>  // Transpose_b RHS map.
+
+// CHECK-LABEL: func.func @op_matmul
+func.func @op_matmul(%input0: tensor<32x64xi8>, %input1: tensor<32x64xi8>, %input1_transposed: tensor<64x32xi8>, %dest: tensor<32x32xi32>) -> tensor<32x32xi32> {
+  // CHECK-NEXT: linalg.matmul
+  // CHECK-SAME:   indexing_maps_match = "matched"
+  %res1 = linalg.matmul
+        indexing_maps = [#map_matmul0, #map_matmul1, #map_matmul2]
+        ins(%input0, %input1 : tensor<32x64xi8>, tensor<32x64xi8>)
+        outs(%dest : tensor<32x32xi32>) {indexing_maps_match = "unmatched"} -> tensor<32x32xi32>
+
+  // Transpose_b matmul - should NOT match (different RHS indexing map).
+  // CHECK-NEXT: linalg.matmul
+  // CHECK-SAME:   indexing_maps_match = "unmatched"
+  %res2 = linalg.matmul
+        indexing_maps = [#map_matmul0, #map_transpose_b, #map_matmul2]
+        ins(%input0, %input1_transposed : tensor<32x64xi8>, tensor<64x32xi8>)
+        outs(%dest : tensor<32x32xi32>) {indexing_maps_match = "unmatched"} -> tensor<32x32xi32>
+
+  return %res1 : tensor<32x32xi32>
+}
+
+module attributes {transform.with_named_sequence} {
+  transform.named_sequence @match_correct_maps(%op: !transform.any_op {transform.readonly}) -> !transform.any_op {
+   %batch, %m, %n, %k = transform.iree.match.is_contraction %op,
+    lhs_type = i8, rhs_type = i8, output_type = i32 {indexing_maps = [#map_matmul0, #map_matmul1, #map_matmul2]} :
+    (!transform.any_op) ->
+    (!transform.param<i64>, !transform.param<i64>, !transform.param<i64>, !transform.param<i64>)
+    transform.yield %op : !transform.any_op
+  }
+
+  transform.named_sequence @annotate_matched(%op: !transform.any_op {transform.readonly}) {
+    %0 = transform.param.constant "matched" -> !transform.any_param
+    transform.annotate %op "indexing_maps_match" = %0 : !transform.any_op, !transform.any_param
+    transform.yield
+  }
+
+  transform.named_sequence @__transform_main(%module: !transform.any_op) {
+    transform.foreach_match in %module
+        @match_correct_maps -> @annotate_matched
+      : (!transform.any_op) -> (!transform.any_op)
+    transform.yield
+  }
+}
+
+// -----
+
+// Verify that operations with different number of indexing maps are correctly not matched.
+
+#map_matmul0 = affine_map<(d0, d1, d2) -> (d0, d2)>
+#map_matmul1 = affine_map<(d0, d1, d2) -> (d1, d2)>
+#map_matmul2 = affine_map<(d0, d1, d2) -> (d0, d1)>
+
+// CHECK-LABEL: func.func @op_matmul
+func.func @op_matmul(%input0: tensor<32x64xi8>, %input1: tensor<32x64xi8>, %dest: tensor<32x32xi32>) -> tensor<32x32xi32> {
+  // CHECK-NEXT: linalg.matmul
+  // CHECK-SAME:   indexing_maps_match = "unmatched"
+  %res = linalg.matmul
+        indexing_maps = [#map_matmul0, #map_matmul1, #map_matmul2]
+        ins(%input0, %input1 : tensor<32x64xi8>, tensor<32x64xi8>)
+        outs(%dest : tensor<32x32xi32>) {indexing_maps_match = "unmatched"} -> tensor<32x32xi32>
+  return %res : tensor<32x32xi32>
+}
+
+module attributes {transform.with_named_sequence} {
+  transform.named_sequence @match_different_count(%op: !transform.any_op {transform.readonly}) -> !transform.any_op {
+    %batch, %m, %n, %k = transform.iree.match.is_contraction %op,
+      lhs_type = i8, rhs_type = i8, output_type = i32 {indexing_maps = [#map_matmul0, #map_matmul1, #map_matmul2, #map_matmul0]} :
+      (!transform.any_op) -> (!transform.param<i64>, !transform.param<i64>, !transform.param<i64>, !transform.param<i64>)
+    transform.yield %op : !transform.any_op
+  }
+
+  transform.named_sequence @annotate_matched(%op: !transform.any_op {transform.readonly}) {
+    %0 = transform.param.constant "matched" -> !transform.any_param
+    transform.annotate %op "indexing_maps_match" = %0 : !transform.any_op, !transform.any_param
+    transform.yield
+  }
+
+  transform.named_sequence @__transform_main(%module: !transform.any_op) {
+     // Should NOT match: operation has 3 indexing maps but matcher expects 4.
+    transform.foreach_match in %module
+        @match_different_count -> @annotate_matched
+      : (!transform.any_op) -> (!transform.any_op)
+    transform.yield
+  }
+}
```

**Comment:**
Can you add a testcase that shows if we match contractions that already have some attribute?

---


---


## [PR #21903](https://github.com/iree-org/iree/pull/21903): [DispatchCreation]: Add FormSplitReductionDispatchesPass support for ArgCompare op

### Review Summary

**COMMENTED** (2025-09-09)


**COMMENTED** (2025-09-09)


**COMMENTED** (2025-09-09)


**COMMENTED** (2025-09-09)


**COMMENTED** (2025-09-09)


**COMMENTED** (2025-09-10)

I don't have any other comments. Because I don't maintain this part of the codebase, I'd leave the final approval to Ian.


**COMMENTED** (2025-09-11)


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1458`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
```

**Comment:**
These are deprecated, use free create functions instead: https://discourse.llvm.org/t/psa-opty-create-now-with-100-more-tab-complete/87339

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1468`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims;
+  for (unsigned dim : reductionDims) {
+    broadcastDims.push_back(static_cast<int64_t>(dim));
+  }
```

**Comment:**
use `llvm::to_vector_of<int64_t>`

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1532`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims;
+  for (unsigned dim : reductionDims) {
+    broadcastDims.push_back(static_cast<int64_t>(dim));
+  }
+  Operation *valueBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results).
+  // For split-reduction, we need to slice the init values along
+  // the reduction dimension to get the appropriate chunk.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim)
+      continue;
```

**Comment:**
missing braces

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1536`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims;
+  for (unsigned dim : reductionDims) {
+    broadcastDims.push_back(static_cast<int64_t>(dim));
+  }
+  Operation *valueBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results).
+  // For split-reduction, we need to slice the init values along
+  // the reduction dimension to get the appropriate chunk.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim)
+      continue;
+
+    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
```

**Comment:**
```suggestion
    if (auto sizeAttr = dyn_cast<IntegerAttr>(initSizes[i])) {
      int64_t size = sizeAttr.getInt();
```

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1543`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims;
+  for (unsigned dim : reductionDims) {
+    broadcastDims.push_back(static_cast<int64_t>(dim));
+  }
+  Operation *valueBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results).
+  // For split-reduction, we need to slice the init values along
+  // the reduction dimension to get the appropriate chunk.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim)
+      continue;
+
+    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
+      resultValShape.push_back(size);
+      resultIdxShape.push_back(size);
+      continue;
+    }
+
+    resultValShape.push_back(ShapedType::kDynamic);
+    resultIdxShape.push_back(ShapedType::kDynamic);
+  }
+
+  RankedTensorType sliceValResultType =
+      RankedTensorType::get(resultValShape, initValType.getElementType(),
+                            cast<RankedTensorType>(initValType).getEncoding());
+  RankedTensorType sliceIdxResultType =
+      RankedTensorType::get(resultIdxShape, initIdxType.getElementType(),
```

**Comment:**
```suggestion
  auto sliceValResultType =
      RankedTensorType::get(resultValShape, initValType.getElementType(),
                            cast<RankedTensorType>(initValType).getEncoding());
  auto sliceIdxResultType =
      RankedTensorType::get(resultIdxShape, initIdxType.getElementType(),
```
See https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1566`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims;
+  for (unsigned dim : reductionDims) {
+    broadcastDims.push_back(static_cast<int64_t>(dim));
+  }
+  Operation *valueBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results).
+  // For split-reduction, we need to slice the init values along
+  // the reduction dimension to get the appropriate chunk.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim)
+      continue;
+
+    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
+      resultValShape.push_back(size);
+      resultIdxShape.push_back(size);
+      continue;
+    }
+
+    resultValShape.push_back(ShapedType::kDynamic);
+    resultIdxShape.push_back(ShapedType::kDynamic);
+  }
+
+  RankedTensorType sliceValResultType =
+      RankedTensorType::get(resultValShape, initValType.getElementType(),
+                            cast<RankedTensorType>(initValType).getEncoding());
+  RankedTensorType sliceIdxResultType =
+      RankedTensorType::get(resultIdxShape, initIdxType.getElementType(),
+                            cast<RankedTensorType>(initIdxType).getEncoding());
+
+  auto initValSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceValResultType, init[0], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initValSlice.getResult());
+  slices.push_back(initValSlice);
+
+  auto initIdxSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceIdxResultType, init[1], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initIdxSlice.getResult());
+  slices.push_back(initIdxSlice);
+
+  // Create index_base for this chunk.
+  Value tileIndex =
+      getValueOrCreateConstantIndexOp(b, loc, splitReductionIvs[0]);
+  Value tileSize = getValueOrCreateConstantIndexOp(b, loc, sizes[reductionDim]);
+  Value tileStartIndex = b.create<arith::MulIOp>(loc, tileIndex, tileSize);
+
+  Value newIndexBase;
+  if (Value globalIndexBase = getIndexBase()) {
+    // Add chunk start to existing index_base.
+    newIndexBase =
+        b.create<arith::AddIOp>(loc, globalIndexBase, tileStartIndex);
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1572`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims;
+  for (unsigned dim : reductionDims) {
+    broadcastDims.push_back(static_cast<int64_t>(dim));
+  }
+  Operation *valueBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results).
+  // For split-reduction, we need to slice the init values along
+  // the reduction dimension to get the appropriate chunk.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim)
+      continue;
+
+    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
+      resultValShape.push_back(size);
+      resultIdxShape.push_back(size);
+      continue;
+    }
+
+    resultValShape.push_back(ShapedType::kDynamic);
+    resultIdxShape.push_back(ShapedType::kDynamic);
+  }
+
+  RankedTensorType sliceValResultType =
+      RankedTensorType::get(resultValShape, initValType.getElementType(),
+                            cast<RankedTensorType>(initValType).getEncoding());
+  RankedTensorType sliceIdxResultType =
+      RankedTensorType::get(resultIdxShape, initIdxType.getElementType(),
+                            cast<RankedTensorType>(initIdxType).getEncoding());
+
+  auto initValSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceValResultType, init[0], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initValSlice.getResult());
+  slices.push_back(initValSlice);
+
+  auto initIdxSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceIdxResultType, init[1], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initIdxSlice.getResult());
+  slices.push_back(initIdxSlice);
+
+  // Create index_base for this chunk.
+  Value tileIndex =
+      getValueOrCreateConstantIndexOp(b, loc, splitReductionIvs[0]);
+  Value tileSize = getValueOrCreateConstantIndexOp(b, loc, sizes[reductionDim]);
+  Value tileStartIndex = b.create<arith::MulIOp>(loc, tileIndex, tileSize);
+
+  Value newIndexBase;
+  if (Value globalIndexBase = getIndexBase()) {
+    // Add chunk start to existing index_base.
+    newIndexBase =
+        b.create<arith::AddIOp>(loc, globalIndexBase, tileStartIndex);
+  } else {
+    // Use chunk start as index_base.
+    newIndexBase = tileStartIndex;
+  }
+
+  SmallVector<Type> resultTypes;
```

**Comment:**
This can be an array of exactly two elements

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1581`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims;
+  for (unsigned dim : reductionDims) {
+    broadcastDims.push_back(static_cast<int64_t>(dim));
+  }
+  Operation *valueBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results).
+  // For split-reduction, we need to slice the init values along
+  // the reduction dimension to get the appropriate chunk.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim)
+      continue;
+
+    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
+      resultValShape.push_back(size);
+      resultIdxShape.push_back(size);
+      continue;
+    }
+
+    resultValShape.push_back(ShapedType::kDynamic);
+    resultIdxShape.push_back(ShapedType::kDynamic);
+  }
+
+  RankedTensorType sliceValResultType =
+      RankedTensorType::get(resultValShape, initValType.getElementType(),
+                            cast<RankedTensorType>(initValType).getEncoding());
+  RankedTensorType sliceIdxResultType =
+      RankedTensorType::get(resultIdxShape, initIdxType.getElementType(),
+                            cast<RankedTensorType>(initIdxType).getEncoding());
+
+  auto initValSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceValResultType, init[0], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initValSlice.getResult());
+  slices.push_back(initValSlice);
+
+  auto initIdxSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceIdxResultType, init[1], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initIdxSlice.getResult());
+  slices.push_back(initIdxSlice);
+
+  // Create index_base for this chunk.
+  Value tileIndex =
+      getValueOrCreateConstantIndexOp(b, loc, splitReductionIvs[0]);
+  Value tileSize = getValueOrCreateConstantIndexOp(b, loc, sizes[reductionDim]);
+  Value tileStartIndex = b.create<arith::MulIOp>(loc, tileIndex, tileSize);
+
+  Value newIndexBase;
+  if (Value globalIndexBase = getIndexBase()) {
+    // Add chunk start to existing index_base.
+    newIndexBase =
+        b.create<arith::AddIOp>(loc, globalIndexBase, tileStartIndex);
+  } else {
+    // Use chunk start as index_base.
+    newIndexBase = tileStartIndex;
+  }
+
+  SmallVector<Type> resultTypes;
+  if (hasPureTensorSemantics()) {
+    resultTypes.push_back(initValSlice->getResult(0).getType());
+    resultTypes.push_back(initIdxSlice->getResult(0).getType());
+  }
+
+  // Create the tiled operation with adjusted index_base.
+  SmallVector<Value> operands = std::move(tiledOperands);
+  operands.push_back(newIndexBase);
+  Operation *tiledArgmaxOp = b.create<ArgCompareOp>(
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1603`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims;
+  for (unsigned dim : reductionDims) {
+    broadcastDims.push_back(static_cast<int64_t>(dim));
+  }
+  Operation *valueBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results).
+  // For split-reduction, we need to slice the init values along
+  // the reduction dimension to get the appropriate chunk.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim)
+      continue;
+
+    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
+      resultValShape.push_back(size);
+      resultIdxShape.push_back(size);
+      continue;
+    }
+
+    resultValShape.push_back(ShapedType::kDynamic);
+    resultIdxShape.push_back(ShapedType::kDynamic);
+  }
+
+  RankedTensorType sliceValResultType =
+      RankedTensorType::get(resultValShape, initValType.getElementType(),
+                            cast<RankedTensorType>(initValType).getEncoding());
+  RankedTensorType sliceIdxResultType =
+      RankedTensorType::get(resultIdxShape, initIdxType.getElementType(),
+                            cast<RankedTensorType>(initIdxType).getEncoding());
+
+  auto initValSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceValResultType, init[0], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initValSlice.getResult());
+  slices.push_back(initValSlice);
+
+  auto initIdxSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceIdxResultType, init[1], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initIdxSlice.getResult());
+  slices.push_back(initIdxSlice);
+
+  // Create index_base for this chunk.
+  Value tileIndex =
+      getValueOrCreateConstantIndexOp(b, loc, splitReductionIvs[0]);
+  Value tileSize = getValueOrCreateConstantIndexOp(b, loc, sizes[reductionDim]);
+  Value tileStartIndex = b.create<arith::MulIOp>(loc, tileIndex, tileSize);
+
+  Value newIndexBase;
+  if (Value globalIndexBase = getIndexBase()) {
+    // Add chunk start to existing index_base.
+    newIndexBase =
+        b.create<arith::AddIOp>(loc, globalIndexBase, tileStartIndex);
+  } else {
+    // Use chunk start as index_base.
+    newIndexBase = tileStartIndex;
+  }
+
+  SmallVector<Type> resultTypes;
+  if (hasPureTensorSemantics()) {
+    resultTypes.push_back(initValSlice->getResult(0).getType());
+    resultTypes.push_back(initIdxSlice->getResult(0).getType());
+  }
+
+  // Create the tiled operation with adjusted index_base.
+  SmallVector<Value> operands = std::move(tiledOperands);
+  operands.push_back(newIndexBase);
+  Operation *tiledArgmaxOp = b.create<ArgCompareOp>(
+      loc, resultTypes,
+      operands[0],                          // input slice.
+      ValueRange{operands[1], operands[2]}, // init slices.
+      operands[3],                          // index_base.
+      reductionDim                          // dimension.
+  );
+
+  // Copy the region.
+  Region &targetRegion = tiledArgmaxOp->getRegion(0);
+  Region &sourceRegion = getRegion();
+  targetRegion.takeBody(sourceRegion);
+
+  return TilingResult{
+      {tiledArgmaxOp}, SmallVector<Value>(tiledArgmaxOp->getResults()), slices};
+}
+
+FailureOr<MergeResult>
+ArgCompareOp::mergeReductions(OpBuilder &b, Location loc,
+                              ValueRange partialReduce,
+                              const llvm::SetVector<unsigned> &reductionDims) {
+  SmallVector<int64_t> mergeReductionDims(reductionDims.begin(),
+                                          reductionDims.end());
```

**Comment:**
use `llvm::to_vector`

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1560`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims;
+  for (unsigned dim : reductionDims) {
+    broadcastDims.push_back(static_cast<int64_t>(dim));
+  }
+  Operation *valueBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results).
+  // For split-reduction, we need to slice the init values along
+  // the reduction dimension to get the appropriate chunk.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim)
+      continue;
+
+    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
+      resultValShape.push_back(size);
+      resultIdxShape.push_back(size);
+      continue;
+    }
+
+    resultValShape.push_back(ShapedType::kDynamic);
+    resultIdxShape.push_back(ShapedType::kDynamic);
+  }
+
+  RankedTensorType sliceValResultType =
+      RankedTensorType::get(resultValShape, initValType.getElementType(),
+                            cast<RankedTensorType>(initValType).getEncoding());
+  RankedTensorType sliceIdxResultType =
+      RankedTensorType::get(resultIdxShape, initIdxType.getElementType(),
+                            cast<RankedTensorType>(initIdxType).getEncoding());
+
+  auto initValSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceValResultType, init[0], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initValSlice.getResult());
+  slices.push_back(initValSlice);
+
+  auto initIdxSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceIdxResultType, init[1], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initIdxSlice.getResult());
+  slices.push_back(initIdxSlice);
+
+  // Create index_base for this chunk.
+  Value tileIndex =
+      getValueOrCreateConstantIndexOp(b, loc, splitReductionIvs[0]);
+  Value tileSize = getValueOrCreateConstantIndexOp(b, loc, sizes[reductionDim]);
+  Value tileStartIndex = b.create<arith::MulIOp>(loc, tileIndex, tileSize);
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1586`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims;
+  for (unsigned dim : reductionDims) {
+    broadcastDims.push_back(static_cast<int64_t>(dim));
+  }
+  Operation *valueBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results).
+  // For split-reduction, we need to slice the init values along
+  // the reduction dimension to get the appropriate chunk.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim)
+      continue;
+
+    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
+      resultValShape.push_back(size);
+      resultIdxShape.push_back(size);
+      continue;
+    }
+
+    resultValShape.push_back(ShapedType::kDynamic);
+    resultIdxShape.push_back(ShapedType::kDynamic);
+  }
+
+  RankedTensorType sliceValResultType =
+      RankedTensorType::get(resultValShape, initValType.getElementType(),
+                            cast<RankedTensorType>(initValType).getEncoding());
+  RankedTensorType sliceIdxResultType =
+      RankedTensorType::get(resultIdxShape, initIdxType.getElementType(),
+                            cast<RankedTensorType>(initIdxType).getEncoding());
+
+  auto initValSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceValResultType, init[0], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initValSlice.getResult());
+  slices.push_back(initValSlice);
+
+  auto initIdxSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceIdxResultType, init[1], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initIdxSlice.getResult());
+  slices.push_back(initIdxSlice);
+
+  // Create index_base for this chunk.
+  Value tileIndex =
+      getValueOrCreateConstantIndexOp(b, loc, splitReductionIvs[0]);
+  Value tileSize = getValueOrCreateConstantIndexOp(b, loc, sizes[reductionDim]);
+  Value tileStartIndex = b.create<arith::MulIOp>(loc, tileIndex, tileSize);
+
+  Value newIndexBase;
+  if (Value globalIndexBase = getIndexBase()) {
+    // Add chunk start to existing index_base.
+    newIndexBase =
+        b.create<arith::AddIOp>(loc, globalIndexBase, tileStartIndex);
+  } else {
+    // Use chunk start as index_base.
+    newIndexBase = tileStartIndex;
+  }
+
+  SmallVector<Type> resultTypes;
+  if (hasPureTensorSemantics()) {
+    resultTypes.push_back(initValSlice->getResult(0).getType());
+    resultTypes.push_back(initIdxSlice->getResult(0).getType());
+  }
+
+  // Create the tiled operation with adjusted index_base.
+  SmallVector<Value> operands = std::move(tiledOperands);
+  operands.push_back(newIndexBase);
+  Operation *tiledArgmaxOp = b.create<ArgCompareOp>(
+      loc, resultTypes,
+      operands[0],                          // input slice.
+      ValueRange{operands[1], operands[2]}, // init slices.
+      operands[3],                          // index_base.
+      reductionDim                          // dimension.
```

**Comment:**
Use the `/*argName=*/ paramName` format. Clang tidy and other linters can check that the names are up to date

---

**File:** `compiler/src/iree/compiler/DispatchCreation/test/form_split_reduction_dispatches.mlir:161`

```diff
@@ -137,3 +137,116 @@ util.func public @split_reduction_multiple_dims(%arg0: tensor<?x?x?xf32>) -> ten
 //  CHECK-SAME:        ins(%[[DISPATCH]] :
 //  CHECK-SAME:        dimensions = [1, 2]
 //       CHECK:    util.return %[[REDUCE]]
+
+// -----
+
+util.func public @arg_compare_split_reduction_dynamic(%arg0: tensor<?x?xf32>) -> (tensor<?xf32>, tensor<?xi32>) {
+  %c0 = arith.constant 0 : index
+  %cst = arith.constant 0.000000e+00 : f32
+  %c0_i32 = arith.constant 0 : i32
+
+  %dim = tensor.dim %arg0, %c0 : tensor<?x?xf32>
+  %0 = tensor.empty(%dim) : tensor<?xf32>
+  %1 = tensor.empty(%dim) : tensor<?xi32>
+
+  %2 = linalg.fill ins(%cst : f32) outs(%0 : tensor<?xf32>) -> tensor<?xf32>
+  %3 = linalg.fill ins(%c0_i32 : i32) outs(%1 : tensor<?xi32>) -> tensor<?xi32>
+
+  %4:2 = iree_linalg_ext.arg_compare {
+      indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0)>],
+      iterator_types = ["parallel", "reduction"],
+      iree_linalg_ext.split_reduction = [128]}
+      dimension(1)
+      ins(%arg0 : tensor<?x?xf32>) outs(%2, %3 : tensor<?xf32>, tensor<?xi32>) {
+    ^bb0(%in: f32, %out_val: f32):  // Only 2 arguments: input and output value
```

**Comment:**
```suggestion
    ^bb0(%in: f32, %out_val: f32):  // Only 2 arguments: input and output value.
```

See https://llvm.org/docs/CodingStandards.html#commenting

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1507`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
+  Value emptyValueTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      b.create<tensor::EmptyOp>(loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims;
+  for (unsigned dim : reductionDims) {
+    broadcastDims.push_back(static_cast<int64_t>(dim));
+  }
+  Operation *valueBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = b.create<linalg::BroadcastOp>(
+      loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results).
+  // For split-reduction, we need to slice the init values along
+  // the reduction dimension to get the appropriate chunk.
```

**Comment:**
nit reflow this comment

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1454`

```diff
@@ -1440,6 +1440,225 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes.begin(), sizes.end());
```

**Comment:**
```suggestion
  SmallVector<OpFoldResult> partialResultShape(sizes);
```

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1466`

```diff
@@ -1440,6 +1440,222 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes);
+  Value emptyValueTensor =
+      tensor::EmptyOp::create(b, loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      tensor::EmptyOp::create(b, loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims =
+      llvm::to_vector_of<int64_t>(reductionDims);
```

**Comment:**
```suggestion
  auto broadcastDims =
      llvm::to_vector_of<int64_t>(reductionDims);
```
See https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1519`

```diff
@@ -1440,6 +1440,222 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes);
+  Value emptyValueTensor =
+      tensor::EmptyOp::create(b, loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      tensor::EmptyOp::create(b, loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims =
+      llvm::to_vector_of<int64_t>(reductionDims);
+  Operation *valueBroadcastOp = linalg::BroadcastOp::create(
+      b, loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = linalg::BroadcastOp::create(
+      b, loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results). For split-reduction,
+  // slice along the reduction dimension to get extra parallelism.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
```

**Comment:**
Do these have to be const references?

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1600`

```diff
@@ -1440,6 +1440,222 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes);
+  Value emptyValueTensor =
+      tensor::EmptyOp::create(b, loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      tensor::EmptyOp::create(b, loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims =
+      llvm::to_vector_of<int64_t>(reductionDims);
+  Operation *valueBroadcastOp = linalg::BroadcastOp::create(
+      b, loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = linalg::BroadcastOp::create(
+      b, loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results). For split-reduction,
+  // slice along the reduction dimension to get extra parallelism.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      continue;
+    }
+
+    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
+      resultValShape.push_back(size);
+      resultIdxShape.push_back(size);
+      continue;
+    }
+
+    resultValShape.push_back(ShapedType::kDynamic);
+    resultIdxShape.push_back(ShapedType::kDynamic);
+  }
+
+  auto sliceValResultType =
+      RankedTensorType::get(resultValShape, initValType.getElementType(),
+                            cast<RankedTensorType>(initValType).getEncoding());
+  auto sliceIdxResultType =
+      RankedTensorType::get(resultIdxShape, initIdxType.getElementType(),
+                            cast<RankedTensorType>(initIdxType).getEncoding());
+
+  auto initValSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceValResultType, init[0], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initValSlice.getResult());
+  slices.push_back(initValSlice);
+
+  auto initIdxSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceIdxResultType, init[1], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initIdxSlice.getResult());
+  slices.push_back(initIdxSlice);
+
+  // Create index_base for this chunk.
+  Value tileIndex =
+      getValueOrCreateConstantIndexOp(b, loc, splitReductionIvs[0]);
+  Value tileSize = getValueOrCreateConstantIndexOp(b, loc, sizes[reductionDim]);
+  Value tileStartIndex = arith::MulIOp::create(b, loc, tileIndex, tileSize);
+
+  Value newIndexBase;
+  if (Value globalIndexBase = getIndexBase()) {
+    // Add chunk start to existing index_base.
+    newIndexBase =
+        arith::AddIOp::create(b, loc, globalIndexBase, tileStartIndex);
+  } else {
+    // Use chunk start as index_base.
+    newIndexBase = tileStartIndex;
+  }
+
+  SmallVector<Type, 2> resultTypes;
+  if (hasPureTensorSemantics()) {
+    resultTypes = {initValSlice->getResult(0).getType(),
+                   initIdxSlice->getResult(0).getType()};
+  }
+
+  // Create the tiled operation with adjusted index_base.
+  SmallVector<Value> operands = std::move(tiledOperands);
+  operands.push_back(newIndexBase);
+  Operation *tiledArgmaxOp =
+      b.create<ArgCompareOp>(loc, /*results=*/resultTypes,
+                             /*inputs=*/operands[0],
+                             /*outputs=*/ValueRange{operands[1], operands[2]},
+                             /*indexBase=*/operands[3],
+                             /*dimension=*/reductionDim);
+
+  // Copy the region.
+  Region &targetRegion = tiledArgmaxOp->getRegion(0);
+  Region &sourceRegion = getRegion();
+  targetRegion.takeBody(sourceRegion);
+
+  return TilingResult{
+      {tiledArgmaxOp}, SmallVector<Value>(tiledArgmaxOp->getResults()), slices};
+}
+
+FailureOr<MergeResult>
+ArgCompareOp::mergeReductions(OpBuilder &b, Location loc,
+                              ValueRange partialReduce,
+                              const llvm::SetVector<unsigned> &reductionDims) {
+  SmallVector<int64_t> mergeReductionDims =
+      llvm::to_vector_of<int64_t>(reductionDims);
```

**Comment:**
```suggestion
  auto mergeReductionDims =
      llvm::to_vector_of<int64_t>(reductionDims);
```
See https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1580`

```diff
@@ -1440,6 +1440,222 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes);
+  Value emptyValueTensor =
+      tensor::EmptyOp::create(b, loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      tensor::EmptyOp::create(b, loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  SmallVector<int64_t> broadcastDims =
+      llvm::to_vector_of<int64_t>(reductionDims);
+  Operation *valueBroadcastOp = linalg::BroadcastOp::create(
+      b, loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = linalg::BroadcastOp::create(
+      b, loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  assert(strategy == ReductionTilingStrategy::PartialReductionOuterParallel &&
+         "Requires PartialReductionOuterParallel strategy");
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results). For split-reduction,
+  // slice along the reduction dimension to get extra parallelism.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  const ShapedType &initValType = getOutputValueType();
+  const ShapedType &initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      continue;
+    }
+
+    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
+      resultValShape.push_back(size);
+      resultIdxShape.push_back(size);
+      continue;
+    }
+
+    resultValShape.push_back(ShapedType::kDynamic);
+    resultIdxShape.push_back(ShapedType::kDynamic);
+  }
+
+  auto sliceValResultType =
+      RankedTensorType::get(resultValShape, initValType.getElementType(),
+                            cast<RankedTensorType>(initValType).getEncoding());
+  auto sliceIdxResultType =
+      RankedTensorType::get(resultIdxShape, initIdxType.getElementType(),
+                            cast<RankedTensorType>(initIdxType).getEncoding());
+
+  auto initValSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceValResultType, init[0], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initValSlice.getResult());
+  slices.push_back(initValSlice);
+
+  auto initIdxSlice = tensor::ExtractSliceOp::create(
+      b, loc, sliceIdxResultType, init[1], initOffsets, initSizes, initStrides);
+  tiledOperands.push_back(initIdxSlice.getResult());
+  slices.push_back(initIdxSlice);
+
+  // Create index_base for this chunk.
+  Value tileIndex =
+      getValueOrCreateConstantIndexOp(b, loc, splitReductionIvs[0]);
+  Value tileSize = getValueOrCreateConstantIndexOp(b, loc, sizes[reductionDim]);
+  Value tileStartIndex = arith::MulIOp::create(b, loc, tileIndex, tileSize);
+
+  Value newIndexBase;
+  if (Value globalIndexBase = getIndexBase()) {
+    // Add chunk start to existing index_base.
+    newIndexBase =
+        arith::AddIOp::create(b, loc, globalIndexBase, tileStartIndex);
+  } else {
+    // Use chunk start as index_base.
+    newIndexBase = tileStartIndex;
+  }
+
+  SmallVector<Type, 2> resultTypes;
+  if (hasPureTensorSemantics()) {
+    resultTypes = {initValSlice->getResult(0).getType(),
+                   initIdxSlice->getResult(0).getType()};
+  }
+
+  // Create the tiled operation with adjusted index_base.
+  SmallVector<Value> operands = std::move(tiledOperands);
+  operands.push_back(newIndexBase);
+  Operation *tiledArgmaxOp =
+      b.create<ArgCompareOp>(loc, /*results=*/resultTypes,
```

**Comment:**
This is deprecated

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1540`

```diff
@@ -1440,6 +1441,221 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
   return success();
 }
 
+FailureOr<SmallVector<Value>>
+ArgCompareOp::generateInitialTensorForPartialReduction(
+    OpBuilder &b, Location loc, ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims) {
+  // Get the original init tensors.
+  Value valueInit = outputValue();
+  Value indexInit = outputIndex();
+
+  // Create tensors with the partial result shape.
+  Type valueElTy = getElementTypeOrSelf(valueInit.getType());
+  Type indexElTy = getElementTypeOrSelf(indexInit.getType());
+  SmallVector<OpFoldResult> partialResultShape(sizes);
+  Value emptyValueTensor =
+      tensor::EmptyOp::create(b, loc, partialResultShape, valueElTy);
+  Value emptyIndexTensor =
+      tensor::EmptyOp::create(b, loc, partialResultShape, indexElTy);
+
+  // Broadcast init values to partial result shape for slicing inside
+  // scf.forall. Each tile in the parallel loop will extract slices from
+  // these broadcasted tensors to get initialized values for the ArgCompareOp.
+  // Example: tensor<64xf32> -> tensor<64x32xf32> for 32 reduction tiles along
+  // dim 1.
+  auto broadcastDims = llvm::to_vector_of<int64_t>(reductionDims);
+  Operation *valueBroadcastOp = linalg::BroadcastOp::create(
+      b, loc, valueInit, emptyValueTensor, broadcastDims);
+  Operation *indexBroadcastOp = linalg::BroadcastOp::create(
+      b, loc, indexInit, emptyIndexTensor, broadcastDims);
+
+  return SmallVector<Value>{valueBroadcastOp->getResult(0),
+                            indexBroadcastOp->getResult(0)};
+}
+
+FailureOr<TilingResult> ArgCompareOp::tileToPartialReduction(
+    OpBuilder &b, Location loc, ReductionTilingStrategy strategy,
+    ValueRange init, ArrayRef<OpFoldResult> offsets,
+    ArrayRef<OpFoldResult> sizes,
+    const llvm::SetVector<unsigned> &reductionDims,
+    ArrayRef<OpFoldResult> splitReductionIvs) {
+  if (strategy != ReductionTilingStrategy::PartialReductionOuterParallel) {
+    return failure();
+  }
+  OpBuilder::InsertionGuard guard(b);
+
+  int64_t rank = getInputRank();
+  int64_t reductionDim = getDimension();
+
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         "Unexpected offsets size");
+  assert(sizes.size() == static_cast<size_t>(rank) && "Unexpected sizes size");
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  // Extract a slice of the input operand.
+  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(b, loc, getInputValue(), offsets, sizes, strides);
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  // Extract slices of the init operands (partial results). For split-reduction,
+  // slice along the reduction dimension to get extra parallelism.
+  SmallVector<OpFoldResult> initOffsets, initSizes, initStrides;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      initOffsets.push_back(splitReductionIvs[0]);
+      initSizes.push_back(b.getIndexAttr(1));
+    } else {
+      // For non-reduction dimensions, use the same offsets/sizes as input.
+      initOffsets.push_back(offsets[i]);
+      initSizes.push_back(sizes[i]);
+    }
+    initStrides.push_back(b.getIndexAttr(1));
+  }
+
+  ShapedType initValType = getOutputValueType();
+  ShapedType initIdxType = getOutputIndexType();
+  SmallVector<int64_t> resultValShape, resultIdxShape;
+  for (int64_t i = 0; i < rank; ++i) {
+    if (i == reductionDim) {
+      continue;
+    }
+
+    if (auto sizeAttr = dyn_cast<Attribute>(initSizes[i])) {
+      int64_t size = (cast<IntegerAttr>(sizeAttr)).getInt();
+      resultValShape.push_back(size);
+      resultIdxShape.push_back(size);
+      continue;
+    }
+
+    resultValShape.push_back(ShapedType::kDynamic);
+    resultIdxShape.push_back(ShapedType::kDynamic);
+  }
+
+  auto sliceValResultType =
+      RankedTensorType::get(resultValShape, initValType.getElementType(),
+                            cast<RankedTensorType>(initValType).getEncoding());
```

**Comment:**
Since this is the only use of these two types, you can query them here where they are needed

---


---


## [PR #21816](https://github.com/iree-org/iree/pull/21816): [Codegen][Tuner] retire the C/Python binding for querying mma intrinsic. NFC. 

### Review Summary

**APPROVED** (2025-09-04)



---


## [PR #21812](https://github.com/iree-org/iree/pull/21812): [Codegen][Tuner]: improve python binding to query target info

### Review Summary

**CHANGES_REQUESTED** (2025-09-02)

I think this is a good idea in general but I'm confused by the implementation and so much work is being done on the python bindings code instead of the C API.

Also, make sure you follow the LLVM coding standards throughout the PR: https://llvm.org/docs/CodingStandards.html


**COMMENTED** (2025-09-02)


**COMMENTED** (2025-09-02)


**COMMENTED** (2025-09-03)


**CHANGES_REQUESTED** (2025-09-03)


**COMMENTED** (2025-09-03)


**COMMENTED** (2025-09-03)


**CHANGES_REQUESTED** (2025-09-03)


**COMMENTED** (2025-09-03)


**COMMENTED** (2025-09-04)


**COMMENTED** (2025-09-04)


**COMMENTED** (2025-09-04)


**COMMENTED** (2025-09-04)


**COMMENTED** (2025-09-04)


**COMMENTED** (2025-09-04)


**COMMENTED** (2025-09-04)


**COMMENTED** (2025-09-04)

The static assert is still missing


**COMMENTED** (2025-09-04)


### Code Comments

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:86`

```diff
@@ -76,6 +77,56 @@ static std::vector<int64_t> getIntArrayAttrValues(MlirAttribute attr) {
   return result;
 }
 
+static ireeGPUTargetInfo
+createGPUTargetInfo(MlirContext context, const std::string &arch,
+                    const std::vector<int64_t> &subgroupChoices,
+                    const std::vector<int64_t> &workgroupSizes,
+                    int64_t threadCount, int64_t memoryBytes,
+                    const py::list &mmaIntrinsicObjects) {
+  ireeGPUTargetInfo gpuTargetInfo;
```

**Comment:**
Let's initialize this since it's a C struct and we don't want to accidentally end up with uninitialized fields.

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:81`

```diff
@@ -76,6 +77,56 @@ static std::vector<int64_t> getIntArrayAttrValues(MlirAttribute attr) {
   return result;
 }
 
+static ireeGPUTargetInfo
+createGPUTargetInfo(MlirContext context, const std::string &arch,
```

**Comment:**
Can you explain why we need this function? I'd expect this to be queried by C bindings which return a complete `ireeGPUTargetInfo` struct

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:600`

```diff
@@ -529,7 +587,37 @@ NB_MODULE(_ireeCompilerDialects, m) {
       .def_prop_ro("max_workgroup_memory_bytes",
                    [](const ireeGPUTargetInfo &self) -> int64_t {
                      return self.maxWorkgroupMemoryBytes;
-                   });
+                   })
+      .def_prop_ro(
+          "mma_intrinsics", [](const ireeGPUTargetInfo &self) -> py::list {
+            std::vector<int64_t> rawValues =
+                getIntArrayAttrValues(self.mmaIntrinsics);
+
+            py::list result;
+            py::module_ gpuModule = py::module_::import_(kGpuModuleImportPath);
+
+            for (uint32_t rawValue : rawValues) {
+              try {
```

**Comment:**
Could we do this without throwing any exceptions?

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:81`

```diff
@@ -76,6 +77,56 @@ static std::vector<int64_t> getIntArrayAttrValues(MlirAttribute attr) {
   return result;
 }
 
+static ireeGPUTargetInfo
+createGPUTargetInfo(MlirContext context, const std::string &arch,
```

**Comment:**
This is understand, I just don't think we should have so much work to do on the python binding side. We should only do minimal work to take data from C structs and put them in a format friendly to python.

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:81`

```diff
@@ -76,6 +77,56 @@ static std::vector<int64_t> getIntArrayAttrValues(MlirAttribute attr) {
   return result;
 }
 
+static ireeGPUTargetInfo
+createGPUTargetInfo(MlirContext context, const std::string &arch,
```

**Comment:**
you can have two constructors (or one constructor and one static method): one to allow construction in python, and one to query target details and return you `TargetInfo`: https://nanobind.readthedocs.io/en/latest/classes.html . Not every python class has to be backed by a struct in the C api

> But I can look into how to make it work that way.

+1

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:469`

```diff
@@ -456,3 +456,53 @@ def gpu_target_info_attribute_parsing():
         512,
         1024,
     ], f"Expected max_workgroup_sizes [256, 512, 1024], got {max_workgroup_sizes}"
+
+    mma_intrinsics = gpu_target_info.mma_intrinsics
+    assert mma_intrinsics == [
+        iree_gpu.MMAIntrinsic.MFMA_F32_16x16x4_F32,
+        iree_gpu.MMAIntrinsic.MFMA_F32_16x16x16_F16,
+        iree_gpu.VirtualMMAIntrinsic.VMFMA_F32_16x16x32_F16,
+    ], f"Expected mma_intrinsics [MFMA_F32_16x16x4_F32, MFMA_F32_16x16x16_F16, VMFMA_F32_16x16x32_F16], got {mma_intrinsics}"
+
+
+@run
+def gpu_target_info_constructor():
```

**Comment:**
We also need a test for the cases when the constructor is given values of wrong type

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:7`

```diff
@@ -4,6 +4,7 @@
 # See https://llvm.org/LICENSE.txt for license information.
 # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 
+import pytest
```

**Comment:**
I don't think we use pytest anywhere else in IREE -- would be worth checking with other folks if we want to add it and then clean up other pieces of the infra.

If you grep python files, you will notice that some use `untitest` instead -- can we switch to that?

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:164`

```diff
@@ -150,12 +150,19 @@ struct ireeGPUTargetInfo {
   MlirAttribute maxWorkgroupSizes;    // Max threads per X/Y/Z dimension.
   int64_t maxThreadCountPerWorkgroup; // Max threads per workgroup.
   int64_t maxWorkgroupMemoryBytes;    // Max workgroup memory.
+  MlirAttribute mmaIntrinsics;        // MMA Intrinsics.
 };
 
 // Queries GPU target info from the given `ExecutableTargetAttr` attribute.
 MLIR_CAPI_EXPORTED ireeGPUTargetInfo
 ireeHALExecutableTargetAttrGetGPUTargetInfo(MlirAttribute attr);
 
+MLIR_CAPI_EXPORTED ireeGPUTargetInfo ireeGPUTargetInfoGet(
+    MlirContext mlirCtx, const char *arch, const int32_t *subgroupChoices,
+    size_t numSubgroupChoices, const int32_t *workgroupSizes,
+    size_t numWorkgroupSizes, int64_t threadCount, int64_t memoryBytes,
+    const int32_t *mmaIntrinsics, size_t numMmaIntrinsics);
```

**Comment:**
How do you know `mmaIntrinsics` map to `int32_t`?

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:522`

```diff
@@ -509,6 +510,37 @@ NB_MODULE(_ireeCompilerDialects, m) {
   //===-------------------------------------------------------------------===//
 
   py::class_<ireeGPUTargetInfo>(iree_gpu_module, "TargetInfo")
+      .def(
+          "__init__",
+          [](ireeGPUTargetInfo *self, MlirContext context,
+             const std::string &arch,
+             const std::vector<int32_t> &subgroupChoices,
+             const std::vector<int32_t> &workgroupSizes, int64_t threadCount,
+             int64_t memoryBytes, const py::list &mmaIntrinsicObjs) {
+            std::vector<int32_t> mmaIntrinsicVals;
+            for (auto item : mmaIntrinsicObjs) {
+              int32_t enumValue = py::cast<int32_t>(item.attr("value"));
```

**Comment:**
1. How do we know this is the underlying enum type?
2. How do we know that this is an mma intrinsic type and not some other enum?

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:541`

```diff
@@ -509,6 +510,37 @@ NB_MODULE(_ireeCompilerDialects, m) {
   //===-------------------------------------------------------------------===//
 
   py::class_<ireeGPUTargetInfo>(iree_gpu_module, "TargetInfo")
+      .def(
+          "__init__",
+          [](ireeGPUTargetInfo *self, MlirContext context,
+             const std::string &arch,
+             const std::vector<int32_t> &subgroupChoices,
+             const std::vector<int32_t> &workgroupSizes, int64_t threadCount,
+             int64_t memoryBytes, const py::list &mmaIntrinsicObjs) {
+            std::vector<int32_t> mmaIntrinsicVals;
+            for (auto item : mmaIntrinsicObjs) {
+              int32_t enumValue = py::cast<int32_t>(item.attr("value"));
+              mmaIntrinsicVals.push_back(enumValue);
+            }
+
+            *self = ireeGPUTargetInfoGet(
+                context, arch.c_str(), subgroupChoices.data(),
+                subgroupChoices.size(), workgroupSizes.data(),
+                workgroupSizes.size(), threadCount, memoryBytes,
+                mmaIntrinsicVals.data(), mmaIntrinsicVals.size());
+          },
+          "context"_a, "arch"_a, "subgroup_size_choices"_a,
+          "max_workgroup_sizes"_a, "max_thread_count_per_workgroup"_a,
+          "max_workgroup_memory_bytes"_a, "mma_intrinsics"_a = py::list{},
+          "Create a GPUTargetInfo with the given parameters")
+      .def_static(
+          "get_gpu_target_info",
+          [](MlirAttribute executable_target_attr) -> ireeGPUTargetInfo {
+            return ireeHALExecutableTargetAttrGetGPUTargetInfo(
+                executable_target_attr);
+          },
```

**Comment:**
Do we need the lambda?

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:566`

```diff
@@ -529,12 +561,48 @@ NB_MODULE(_ireeCompilerDialects, m) {
       .def_prop_ro("max_workgroup_memory_bytes",
                    [](const ireeGPUTargetInfo &self) -> int64_t {
                      return self.maxWorkgroupMemoryBytes;
-                   });
+                   })
+      .def_prop_ro(
+          "mma_intrinsics", [](const ireeGPUTargetInfo &self) -> py::list {
```

**Comment:**
Can we move this to the C API instead?

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:478`

```diff
@@ -420,5 +420,95 @@ ireeHALExecutableTargetAttrGetGPUTargetInfo(MlirAttribute attr) {
       wgpAttr.getMaxThreadCountPerWorkgroup();
   targetInfo.maxWorkgroupMemoryBytes = wgpAttr.getMaxWorkgroupMemoryBytes();
 
+  targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr({}));
+  mlir::iree_compiler::IREE::GPU::MMAOpsArrayAttr mmaOpsArray =
+      wgpAttr.getMma();
+  if (mmaOpsArray) {
+    std::vector<mlir::Attribute> mmaIntrinsicAttrs;
+    for (mlir::iree_compiler::IREE::GPU::MMAAttr mmaAttr : mmaOpsArray) {
+      mlir::iree_compiler::IREE::GPU::MMAIntrinsic intrinsic =
+          mmaAttr.getIntrinsic();
+      auto mmaIntrinsicAttr =
+          mlir::iree_compiler::IREE::GPU::MMAIntrinsicAttr::get(context,
+                                                                intrinsic);
+      mmaIntrinsicAttrs.push_back(mmaIntrinsicAttr);
+
+      for (mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsic
+               virtualIntrinsic : mmaAttr.getVirtualIntrinsics()) {
+        auto virtualMmaIntrinsicAttr =
+            mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsicAttr::get(
+                context, virtualIntrinsic);
+        mmaIntrinsicAttrs.push_back(virtualMmaIntrinsicAttr);
+      }
+    }
+    targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr(mmaIntrinsicAttrs));
+  }
+  return targetInfo;
+}
+
+ireeGPUTargetInfo
+ireeGPUTargetInfoGet(MlirContext mlirCtx, const char *arch,
+                     const int32_t *subgroupChoices, size_t numSubgroupChoices,
+                     const int32_t *workgroupSizes, size_t numWorkgroupSizes,
+                     int64_t threadCount, int64_t memoryBytes,
+                     const int32_t *mmaIntrinsics, size_t numMmaIntrinsics) {
+  assert(!mlirContextIsNull(mlirCtx) && "mlirCtx cannot be null");
+  assert(arch && "arch cannot be null");
+
+  mlir::MLIRContext *context = unwrap(mlirCtx);
+  mlir::Builder builder(context);
+
+  ireeGPUTargetInfo targetInfo = {};
+
+  targetInfo.arch = wrap(mlir::StringAttr::get(context, arch));
+  std::vector<int32_t> subgroupChoicesVec(subgroupChoices,
+                                          subgroupChoices + numSubgroupChoices);
+  targetInfo.subgroupSizeChoices =
+      wrap(builder.getI32ArrayAttr(subgroupChoicesVec));
+  std::vector<int32_t> workgroupSizesVec(workgroupSizes,
+                                         workgroupSizes + numWorkgroupSizes);
+  targetInfo.maxWorkgroupSizes =
+      wrap(builder.getI32ArrayAttr(workgroupSizesVec));
+
+  targetInfo.maxThreadCountPerWorkgroup = threadCount;
+  targetInfo.maxWorkgroupMemoryBytes = memoryBytes;
+
+  if (numMmaIntrinsics == 0) {
+    targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr({}));
+  } else {
```

**Comment:**
Do we need to handle the empty case in a separate code path?

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:164`

```diff
@@ -150,12 +150,19 @@ struct ireeGPUTargetInfo {
   MlirAttribute maxWorkgroupSizes;    // Max threads per X/Y/Z dimension.
   int64_t maxThreadCountPerWorkgroup; // Max threads per workgroup.
   int64_t maxWorkgroupMemoryBytes;    // Max workgroup memory.
+  MlirAttribute mmaIntrinsics;        // MMA Intrinsics.
 };
 
 // Queries GPU target info from the given `ExecutableTargetAttr` attribute.
 MLIR_CAPI_EXPORTED ireeGPUTargetInfo
 ireeHALExecutableTargetAttrGetGPUTargetInfo(MlirAttribute attr);
 
+MLIR_CAPI_EXPORTED ireeGPUTargetInfo ireeGPUTargetInfoGet(
+    MlirContext mlirCtx, const char *arch, const int32_t *subgroupChoices,
+    size_t numSubgroupChoices, const int32_t *workgroupSizes,
+    size_t numWorkgroupSizes, int64_t threadCount, int64_t memoryBytes,
+    const int32_t *mmaIntrinsics, size_t numMmaIntrinsics);
```

**Comment:**
How can I tell it's i32 instead of u32? Also, what happens if this changes in the future? I think we need to have a typedef on the C api side, static_assert in the C implementation, and then use this typedef on the python side.

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:20`

```diff
@@ -14,6 +14,11 @@
 extern "C" {
 #endif
 
+// This typedef ensures consistency between the C API, C++ implementation, and
+// Python bindings. Update both this typedef and the static assertions if the
+// enum underlying types change.
+typedef uint32_t mma_intrinsic_t;
```

**Comment:**
```suggestion
typedef uint32_t mma_intrinsic_enum_t;
```

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:175`

```diff
@@ -160,8 +167,20 @@ ireeHALExecutableTargetAttrGetGPUTargetInfo(MlirAttribute attr);
 MLIR_CAPI_EXPORTED ireeGPUTargetInfo ireeGPUTargetInfoGet(
     MlirContext mlirCtx, const char *arch, const int32_t *subgroupChoices,
     size_t numSubgroupChoices, const int32_t *workgroupSizes,
-    size_t numWorkgroupSizes, int64_t threadCount, int64_t memoryBytes,
-    const int32_t *mmaIntrinsics, size_t numMmaIntrinsics);
+    size_t numWorkgroupSizes, int32_t threadCount, int32_t memoryBytes,
+    const mma_intrinsic_t *mmaIntrinsics, size_t numMmaIntrinsics);
+
+struct ireeGPUMMAIntrinsicResult {
+  mma_intrinsic_t *mmaIntrinsicVals;
+  bool *isVirtual; // true if VirtualMMAIntrinsic, false if MMAIntrinsic.
```

**Comment:**
Could you make this a plural so that it's clear there is more than one value behind this pointer?

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:528`

```diff
@@ -515,11 +515,22 @@ NB_MODULE(_ireeCompilerDialects, m) {
           [](ireeGPUTargetInfo *self, MlirContext context,
              const std::string &arch,
              const std::vector<int32_t> &subgroupChoices,
-             const std::vector<int32_t> &workgroupSizes, int64_t threadCount,
-             int64_t memoryBytes, const py::list &mmaIntrinsicObjs) {
-            std::vector<int32_t> mmaIntrinsicVals;
+             const std::vector<int32_t> &workgroupSizes, int32_t threadCount,
+             int32_t memoryBytes, const py::list &mmaIntrinsicObjs) {
+            std::vector<mma_intrinsic_t> mmaIntrinsicVals;
+            py::module_ gpuModule = py::module_::import_(kGpuModuleImportPath);
+            py::object mmaIntrinsicClass = gpuModule.attr("MMAIntrinsic");
+            py::object virtualMmaIntrinsicClass =
+                gpuModule.attr("VirtualMMAIntrinsic");
+
             for (auto item : mmaIntrinsicObjs) {
-              int32_t enumValue = py::cast<int32_t>(item.attr("value"));
+              if (!py::isinstance(item, mmaIntrinsicClass) &&
+                  !py::isinstance(item, virtualMmaIntrinsicClass)) {
```

**Comment:**
Could we make these share the same base class?

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:528`

```diff
@@ -473,42 +474,85 @@ ireeGPUTargetInfoGet(MlirContext mlirCtx, const char *arch,
   targetInfo.maxThreadCountPerWorkgroup = threadCount;
   targetInfo.maxWorkgroupMemoryBytes = memoryBytes;
 
-  if (numMmaIntrinsics == 0) {
-    targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr({}));
-  } else {
-    std::vector<mlir::Attribute> mmaIntrinsicAttrs;
-    mmaIntrinsicAttrs.reserve(numMmaIntrinsics);
-    for (size_t i = 0; i < numMmaIntrinsics; i++) {
-      int32_t enumValue = mmaIntrinsics[i];
-
-      std::optional<mlir::iree_compiler::IREE::GPU::MMAIntrinsic> mmaIntrinsic =
-          mlir::iree_compiler::IREE::GPU::symbolizeMMAIntrinsic(enumValue);
-      if (mmaIntrinsic) {
-        auto mmaIntrinsicAttr =
-            mlir::iree_compiler::IREE::GPU::MMAIntrinsicAttr::get(
-                context, *mmaIntrinsic);
-        mmaIntrinsicAttrs.push_back(mmaIntrinsicAttr);
-        continue;
-      }
+  std::vector<mlir::Attribute> mmaIntrinsicAttrs;
+  mmaIntrinsicAttrs.reserve(numMmaIntrinsics);
+  for (size_t i = 0; i < numMmaIntrinsics; i++) {
+    mma_intrinsic_t enumValue = mmaIntrinsics[i];
 
-      std::optional<mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsic>
-          virtualMmaIntrinsic =
-              mlir::iree_compiler::IREE::GPU::symbolizeVirtualMMAIntrinsic(
-                  enumValue);
-      if (virtualMmaIntrinsic) {
-        auto virtualMmaIntrinsicAttr =
-            mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsicAttr::get(
-                context, *virtualMmaIntrinsic);
-        mmaIntrinsicAttrs.push_back(virtualMmaIntrinsicAttr);
-        continue;
-      }
+    std::optional<mlir::iree_compiler::IREE::GPU::MMAIntrinsic> mmaIntrinsic =
+        mlir::iree_compiler::IREE::GPU::symbolizeMMAIntrinsic(enumValue);
+    if (mmaIntrinsic) {
+      auto mmaIntrinsicAttr =
+          mlir::iree_compiler::IREE::GPU::MMAIntrinsicAttr::get(context,
+                                                                *mmaIntrinsic);
+      mmaIntrinsicAttrs.push_back(mmaIntrinsicAttr);
+      continue;
+    }
 
-      assert(false &&
-             ("Invalid MMA intrinsic value: " + std::to_string(enumValue))
-                 .c_str());
+    std::optional<mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsic>
+        virtualMmaIntrinsic =
+            mlir::iree_compiler::IREE::GPU::symbolizeVirtualMMAIntrinsic(
+                enumValue);
+    if (virtualMmaIntrinsic) {
+      auto virtualMmaIntrinsicAttr =
+          mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsicAttr::get(
+              context, *virtualMmaIntrinsic);
+      mmaIntrinsicAttrs.push_back(virtualMmaIntrinsicAttr);
+      continue;
     }
-    targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr(mmaIntrinsicAttrs));
+
+    assert(
+        false &&
+        ("Invalid MMA intrinsic value: " + std::to_string(enumValue)).c_str());
   }
+  targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr(mmaIntrinsicAttrs));
 
   return targetInfo;
 }
+
+ireeGPUMMAIntrinsicResult
+ireeGPUTargetInfoGetMMAIntrinsics(MlirAttribute mmaIntrinsics) {
+  ireeGPUMMAIntrinsicResult result = {NULL, NULL, 0};
+  if (mlirAttributeIsNull(mmaIntrinsics) ||
+      !mlirAttributeIsAArray(mmaIntrinsics)) {
+    return result;
+  }
+
+  size_t numElements = mlirArrayAttrGetNumElements(mmaIntrinsics);
+  if (numElements == 0) {
+    return result;
+  }
+
+  result.mmaIntrinsicVals =
+      (mma_intrinsic_t *)malloc(numElements * sizeof(mma_intrinsic_t));
+  result.isVirtual = (bool *)malloc(numElements * sizeof(bool));
```

**Comment:**
llvm prefers C++ casts: https://llvm.org/docs/CodingStandards.html#prefer-c-style-casts

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:537`

```diff
@@ -473,42 +474,85 @@ ireeGPUTargetInfoGet(MlirContext mlirCtx, const char *arch,
   targetInfo.maxThreadCountPerWorkgroup = threadCount;
   targetInfo.maxWorkgroupMemoryBytes = memoryBytes;
 
-  if (numMmaIntrinsics == 0) {
-    targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr({}));
-  } else {
-    std::vector<mlir::Attribute> mmaIntrinsicAttrs;
-    mmaIntrinsicAttrs.reserve(numMmaIntrinsics);
-    for (size_t i = 0; i < numMmaIntrinsics; i++) {
-      int32_t enumValue = mmaIntrinsics[i];
-
-      std::optional<mlir::iree_compiler::IREE::GPU::MMAIntrinsic> mmaIntrinsic =
-          mlir::iree_compiler::IREE::GPU::symbolizeMMAIntrinsic(enumValue);
-      if (mmaIntrinsic) {
-        auto mmaIntrinsicAttr =
-            mlir::iree_compiler::IREE::GPU::MMAIntrinsicAttr::get(
-                context, *mmaIntrinsic);
-        mmaIntrinsicAttrs.push_back(mmaIntrinsicAttr);
-        continue;
-      }
+  std::vector<mlir::Attribute> mmaIntrinsicAttrs;
+  mmaIntrinsicAttrs.reserve(numMmaIntrinsics);
+  for (size_t i = 0; i < numMmaIntrinsics; i++) {
+    mma_intrinsic_t enumValue = mmaIntrinsics[i];
 
-      std::optional<mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsic>
-          virtualMmaIntrinsic =
-              mlir::iree_compiler::IREE::GPU::symbolizeVirtualMMAIntrinsic(
-                  enumValue);
-      if (virtualMmaIntrinsic) {
-        auto virtualMmaIntrinsicAttr =
-            mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsicAttr::get(
-                context, *virtualMmaIntrinsic);
-        mmaIntrinsicAttrs.push_back(virtualMmaIntrinsicAttr);
-        continue;
-      }
+    std::optional<mlir::iree_compiler::IREE::GPU::MMAIntrinsic> mmaIntrinsic =
+        mlir::iree_compiler::IREE::GPU::symbolizeMMAIntrinsic(enumValue);
+    if (mmaIntrinsic) {
+      auto mmaIntrinsicAttr =
+          mlir::iree_compiler::IREE::GPU::MMAIntrinsicAttr::get(context,
+                                                                *mmaIntrinsic);
+      mmaIntrinsicAttrs.push_back(mmaIntrinsicAttr);
+      continue;
+    }
 
-      assert(false &&
-             ("Invalid MMA intrinsic value: " + std::to_string(enumValue))
-                 .c_str());
+    std::optional<mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsic>
+        virtualMmaIntrinsic =
+            mlir::iree_compiler::IREE::GPU::symbolizeVirtualMMAIntrinsic(
+                enumValue);
+    if (virtualMmaIntrinsic) {
+      auto virtualMmaIntrinsicAttr =
+          mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsicAttr::get(
+              context, *virtualMmaIntrinsic);
+      mmaIntrinsicAttrs.push_back(virtualMmaIntrinsicAttr);
+      continue;
     }
-    targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr(mmaIntrinsicAttrs));
+
+    assert(
+        false &&
+        ("Invalid MMA intrinsic value: " + std::to_string(enumValue)).c_str());
   }
+  targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr(mmaIntrinsicAttrs));
 
   return targetInfo;
 }
+
+ireeGPUMMAIntrinsicResult
+ireeGPUTargetInfoGetMMAIntrinsics(MlirAttribute mmaIntrinsics) {
+  ireeGPUMMAIntrinsicResult result = {NULL, NULL, 0};
+  if (mlirAttributeIsNull(mmaIntrinsics) ||
+      !mlirAttributeIsAArray(mmaIntrinsics)) {
+    return result;
+  }
+
+  size_t numElements = mlirArrayAttrGetNumElements(mmaIntrinsics);
+  if (numElements == 0) {
+    return result;
+  }
+
+  result.mmaIntrinsicVals =
+      (mma_intrinsic_t *)malloc(numElements * sizeof(mma_intrinsic_t));
+  result.isVirtual = (bool *)malloc(numElements * sizeof(bool));
+  result.numMmaIntrinsics = numElements;
+
+  for (size_t i = 0; i < numElements; i++) {
+    MlirAttribute element = mlirArrayAttrGetElement(mmaIntrinsics, i);
+    if (ireeAttributeIsAGPUMMAIntrinsicAttr(element)) {
+      result.mmaIntrinsicVals[i] = ireeGPUMMAIntrinsicAttrGetValue(element);
+      result.isVirtual[i] = false;
+    } else if (ireeAttributeIsAGPUVirtualMMAIntrinsicAttr(element)) {
+      result.mmaIntrinsicVals[i] =
```

**Comment:**
Use early exits: https://llvm.org/docs/CodingStandards.html#use-early-exits-and-continue-to-simplify-code

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:527`

```diff
@@ -473,42 +474,85 @@ ireeGPUTargetInfoGet(MlirContext mlirCtx, const char *arch,
   targetInfo.maxThreadCountPerWorkgroup = threadCount;
   targetInfo.maxWorkgroupMemoryBytes = memoryBytes;
 
-  if (numMmaIntrinsics == 0) {
-    targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr({}));
-  } else {
-    std::vector<mlir::Attribute> mmaIntrinsicAttrs;
-    mmaIntrinsicAttrs.reserve(numMmaIntrinsics);
-    for (size_t i = 0; i < numMmaIntrinsics; i++) {
-      int32_t enumValue = mmaIntrinsics[i];
-
-      std::optional<mlir::iree_compiler::IREE::GPU::MMAIntrinsic> mmaIntrinsic =
-          mlir::iree_compiler::IREE::GPU::symbolizeMMAIntrinsic(enumValue);
-      if (mmaIntrinsic) {
-        auto mmaIntrinsicAttr =
-            mlir::iree_compiler::IREE::GPU::MMAIntrinsicAttr::get(
-                context, *mmaIntrinsic);
-        mmaIntrinsicAttrs.push_back(mmaIntrinsicAttr);
-        continue;
-      }
+  std::vector<mlir::Attribute> mmaIntrinsicAttrs;
+  mmaIntrinsicAttrs.reserve(numMmaIntrinsics);
+  for (size_t i = 0; i < numMmaIntrinsics; i++) {
+    mma_intrinsic_t enumValue = mmaIntrinsics[i];
 
-      std::optional<mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsic>
-          virtualMmaIntrinsic =
-              mlir::iree_compiler::IREE::GPU::symbolizeVirtualMMAIntrinsic(
-                  enumValue);
-      if (virtualMmaIntrinsic) {
-        auto virtualMmaIntrinsicAttr =
-            mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsicAttr::get(
-                context, *virtualMmaIntrinsic);
-        mmaIntrinsicAttrs.push_back(virtualMmaIntrinsicAttr);
-        continue;
-      }
+    std::optional<mlir::iree_compiler::IREE::GPU::MMAIntrinsic> mmaIntrinsic =
+        mlir::iree_compiler::IREE::GPU::symbolizeMMAIntrinsic(enumValue);
+    if (mmaIntrinsic) {
+      auto mmaIntrinsicAttr =
+          mlir::iree_compiler::IREE::GPU::MMAIntrinsicAttr::get(context,
+                                                                *mmaIntrinsic);
+      mmaIntrinsicAttrs.push_back(mmaIntrinsicAttr);
+      continue;
+    }
 
-      assert(false &&
-             ("Invalid MMA intrinsic value: " + std::to_string(enumValue))
-                 .c_str());
+    std::optional<mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsic>
+        virtualMmaIntrinsic =
+            mlir::iree_compiler::IREE::GPU::symbolizeVirtualMMAIntrinsic(
+                enumValue);
+    if (virtualMmaIntrinsic) {
+      auto virtualMmaIntrinsicAttr =
+          mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsicAttr::get(
+              context, *virtualMmaIntrinsic);
+      mmaIntrinsicAttrs.push_back(virtualMmaIntrinsicAttr);
+      continue;
     }
-    targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr(mmaIntrinsicAttrs));
+
+    assert(
+        false &&
+        ("Invalid MMA intrinsic value: " + std::to_string(enumValue)).c_str());
   }
+  targetInfo.mmaIntrinsics = wrap(builder.getArrayAttr(mmaIntrinsicAttrs));
 
   return targetInfo;
 }
+
+ireeGPUMMAIntrinsicResult
+ireeGPUTargetInfoGetMMAIntrinsics(MlirAttribute mmaIntrinsics) {
+  ireeGPUMMAIntrinsicResult result = {NULL, NULL, 0};
+  if (mlirAttributeIsNull(mmaIntrinsics) ||
+      !mlirAttributeIsAArray(mmaIntrinsics)) {
+    return result;
+  }
+
+  size_t numElements = mlirArrayAttrGetNumElements(mmaIntrinsics);
+  if (numElements == 0) {
+    return result;
+  }
+
+  result.mmaIntrinsicVals =
+      (mma_intrinsic_t *)malloc(numElements * sizeof(mma_intrinsic_t));
```

**Comment:**
The memory should be allocated and freed on the python side. We have a few examples of this already.

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:528`

```diff
@@ -515,11 +515,22 @@ NB_MODULE(_ireeCompilerDialects, m) {
           [](ireeGPUTargetInfo *self, MlirContext context,
              const std::string &arch,
              const std::vector<int32_t> &subgroupChoices,
-             const std::vector<int32_t> &workgroupSizes, int64_t threadCount,
-             int64_t memoryBytes, const py::list &mmaIntrinsicObjs) {
-            std::vector<int32_t> mmaIntrinsicVals;
+             const std::vector<int32_t> &workgroupSizes, int32_t threadCount,
+             int32_t memoryBytes, const py::list &mmaIntrinsicObjs) {
+            std::vector<mma_intrinsic_t> mmaIntrinsicVals;
+            py::module_ gpuModule = py::module_::import_(kGpuModuleImportPath);
+            py::object mmaIntrinsicClass = gpuModule.attr("MMAIntrinsic");
+            py::object virtualMmaIntrinsicClass =
+                gpuModule.attr("VirtualMMAIntrinsic");
+
             for (auto item : mmaIntrinsicObjs) {
-              int32_t enumValue = py::cast<int32_t>(item.attr("value"));
+              if (!py::isinstance(item, mmaIntrinsicClass) &&
+                  !py::isinstance(item, virtualMmaIntrinsicClass)) {
```

**Comment:**
There are defined on the Python side though: https://github.com/iree-org/iree/blob/933f798046a817dcff48d84df8fd987c5cb9e72b/compiler/bindings/python/IREECompilerDialectsModule.cpp#L312 so I'd think we can add a python base clase that doesn't exist in tablegen/c++

Fine to leave as is though for now

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:526`

```diff
@@ -509,6 +510,44 @@ NB_MODULE(_ireeCompilerDialects, m) {
   //===-------------------------------------------------------------------===//
 
   py::class_<ireeGPUTargetInfo>(iree_gpu_module, "TargetInfo")
+      .def(
+          "__init__",
+          [](ireeGPUTargetInfo *self, MlirContext context,
+             const std::string &arch,
+             const std::vector<int32_t> &subgroupChoices,
+             const std::vector<int32_t> &workgroupSizes, int32_t threadCount,
+             int32_t memoryBytes, const py::list &mmaIntrinsicObjs) {
+            std::vector<mma_intrinsic_enum_t> mmaIntrinsicVals;
+            py::module_ gpuModule = py::module_::import_(kGpuModuleImportPath);
+            py::object mmaIntrinsicClass = gpuModule.attr("MMAIntrinsic");
+            py::object virtualMmaIntrinsicClass =
+                gpuModule.attr("VirtualMMAIntrinsic");
+
+            for (auto item : mmaIntrinsicObjs) {
```

**Comment:**
https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:176`

```diff
@@ -148,14 +155,26 @@ struct ireeGPUTargetInfo {
   MlirIdentifier arch;                // E.g., "gfx942".
   MlirAttribute subgroupSizeChoices;  // Subgroup size choices.
   MlirAttribute maxWorkgroupSizes;    // Max threads per X/Y/Z dimension.
-  int64_t maxThreadCountPerWorkgroup; // Max threads per workgroup.
-  int64_t maxWorkgroupMemoryBytes;    // Max workgroup memory.
+  int32_t maxThreadCountPerWorkgroup; // Max threads per workgroup.
+  int32_t maxWorkgroupMemoryBytes;    // Max workgroup memory.
+  MlirAttribute mmaIntrinsics;        // MMA Intrinsics.
 };
 
 // Queries GPU target info from the given `ExecutableTargetAttr` attribute.
 MLIR_CAPI_EXPORTED ireeGPUTargetInfo
 ireeHALExecutableTargetAttrGetGPUTargetInfo(MlirAttribute attr);
 
+MLIR_CAPI_EXPORTED ireeGPUTargetInfo ireeGPUTargetInfoGet(
+    MlirContext mlirCtx, const char *arch, const int32_t *subgroupChoices,
+    size_t numSubgroupChoices, const int32_t *workgroupSizes,
+    size_t numWorkgroupSizes, int32_t threadCount, int32_t memoryBytes,
+    const mma_intrinsic_enum_t *mmaIntrinsics, size_t numMmaIntrinsics);
+
+MLIR_CAPI_EXPORTED void
+ireeGPUTargetInfoGetMMAIntrinsics(MlirAttribute mmaIntrinsics,
+                                  mma_intrinsic_enum_t *mmaIntrinsicVals,
+                                  uint8_t *isVirtuals, size_t numElements);
```

**Comment:**
This name doesn't make sense to me, I'd call it something like `virtualMmaIntrinsicTags` and add a comment explaining that it's to distinguish virtual from non-virtual intrinsics

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:20`

```diff
@@ -14,6 +14,11 @@
 extern "C" {
 #endif
 
+// This typedef ensures consistency between the C API, C++ implementation, and
+// Python bindings. Update both this typedef and the static assertions if the
+// enum underlying types change.
+typedef uint32_t mma_intrinsic_enum_t;
```

**Comment:**
This needs a static assert on the C bindings implementation side

---


---


## [PR #21782](https://github.com/iree-org/iree/pull/21782): [Codegen][Tuner] expose python binding to query target info

### Review Summary

**COMMENTED** (2025-08-27)


**COMMENTED** (2025-08-27)


**COMMENTED** (2025-08-27)


**APPROVED** (2025-08-28)

LGTM % one remaining issue


### Code Comments

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:148`

```diff
@@ -144,6 +144,18 @@ struct ireeGPUMMASingleSubgroupLayout {
 MLIR_CAPI_EXPORTED ireeGPUMMASingleSubgroupLayout
 ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment);
 
+struct ireeGPUTargetInfo {
+  MlirIdentifier arch;                 // "gfx942".
```

**Comment:**
```suggestion
  MlirIdentifier arch;                 // E.g., "gfx942".
```

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:152`

```diff
@@ -144,6 +144,18 @@ struct ireeGPUMMASingleSubgroupLayout {
 MLIR_CAPI_EXPORTED ireeGPUMMASingleSubgroupLayout
 ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment);
 
+struct ireeGPUTargetInfo {
+  MlirIdentifier arch;                 // "gfx942".
+  MlirAttribute subgroup_size_choices; // Subgroup size choices.
+  MlirAttribute max_workgroup_sizes;   // Max threads per X/Y/Z dimension.
+  MlirAttribute max_thread_count_per_workgroup; // Max threads per workgroup.
+  MlirAttribute max_workgroup_memory_bytes;     // Max workgroup memory.
```

**Comment:**
Can this be int64_t?

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:151`

```diff
@@ -144,6 +144,18 @@ struct ireeGPUMMASingleSubgroupLayout {
 MLIR_CAPI_EXPORTED ireeGPUMMASingleSubgroupLayout
 ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment);
 
+struct ireeGPUTargetInfo {
+  MlirIdentifier arch;                 // "gfx942".
+  MlirAttribute subgroup_size_choices; // Subgroup size choices.
+  MlirAttribute max_workgroup_sizes;   // Max threads per X/Y/Z dimension.
+  MlirAttribute max_thread_count_per_workgroup; // Max threads per workgroup.
```

**Comment:**
Can we make this int64_t?

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:155`

```diff
@@ -144,6 +144,18 @@ struct ireeGPUMMASingleSubgroupLayout {
 MLIR_CAPI_EXPORTED ireeGPUMMASingleSubgroupLayout
 ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment);
 
+struct ireeGPUTargetInfo {
+  MlirIdentifier arch;                 // "gfx942".
+  MlirAttribute subgroup_size_choices; // Subgroup size choices.
+  MlirAttribute max_workgroup_sizes;   // Max threads per X/Y/Z dimension.
+  MlirAttribute max_thread_count_per_workgroup; // Max threads per workgroup.
+  MlirAttribute max_workgroup_memory_bytes;     // Max workgroup memory.
+};
+
+// Add function to query GPU target info from ExecutableTargetAttr.
```

**Comment:**
This doesn't add any functions, this is a function to query something
```suggestion
// Queries GPU target info from the given `ExecutableTargetAttr` attribute |attr|.
```

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:542`

```diff
@@ -504,6 +504,45 @@ NB_MODULE(_ireeCompilerDialects, m) {
             return std::nullopt;
           });
 
+  //===-------------------------------------------------------------------===//
+  // Binding to query target info
+  //===-------------------------------------------------------------------===//
+
+  py::class_<ireeGPUTargetInfo>(iree_gpu_module, "TargetInfo")
+      .def_prop_ro("arch",
+                   [](const ireeGPUTargetInfo &self) -> std::string {
+                     MlirStringRef strRef = mlirIdentifierStr(self.arch);
+                     return std::string(strRef.data, strRef.length);
+                   })
+      .def_prop_ro("subgroup_size_choices",
+                   [](const ireeGPUTargetInfo &self) -> std::vector<int64_t> {
+                     return getIntArrayAttrValues(self.subgroup_size_choices);
+                   })
+      .def_prop_ro("max_thread_count_per_workgroup",
+                   [](const ireeGPUTargetInfo &self) -> int64_t {
+                     return mlirIntegerAttrGetValueInt(
+                         self.max_thread_count_per_workgroup);
+                   })
+      .def_prop_ro("max_workgroup_sizes",
+                   [](const ireeGPUTargetInfo &self) -> std::vector<int64_t> {
+                     return getIntArrayAttrValues(self.max_workgroup_sizes);
+                   })
+      .def_prop_ro("max_workgroup_memory_bytes",
+                   [](const ireeGPUTargetInfo &self) -> int64_t {
+                     return mlirIntegerAttrGetValueInt(
+                         self.max_workgroup_memory_bytes);
+                   });
+
+  iree_gpu_module.def(
+      "get_gpu_target_info",
+      [](MlirAttribute executableTargetAttr) {
+        ireeGPUTargetInfo result =
+            ireeHALExecutableTargetAttrGetGPUTargetInfo(executableTargetAttr);
+        return result;
+      },
```

**Comment:**
Can we use `ireeHALExecutableTargetAttrGetGPUTargetInfo` directly instead of wrapping it in a lambda?

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:410`

```diff
@@ -391,3 +391,70 @@ def compilation_info():
     assert compilation_info is not None
     assert compilation_info.lowering_config == lowering_config
     assert compilation_info.translation_info == translation_info
+
+
+@run
+def gpu_target_info_attribute_parsing():
+    mlir_string = """
+    hal.executable private @main_dispatch_0 {
+        hal.executable.variant public @rocm_hsaco_fb
+            target(<"rocm", "rocm-hsaco-fb",
+                {
+                abi = "hip",
+                iree_codegen.target_info = #iree_gpu.target<
+                    arch = "gfx942",
+                    features = "",
+                    wgp = <
+                    compute = fp64|fp32|fp16,
+                    storage = b64|b32|b16|b8,
+                    subgroup = shuffle|arithmetic,
```

**Comment:**
Could we set the attributes we don't care about to the empty value, e.g., `none`, to keep this minimal?

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:413`

```diff
@@ -391,3 +391,70 @@ def compilation_info():
     assert compilation_info is not None
     assert compilation_info.lowering_config == lowering_config
     assert compilation_info.translation_info == translation_info
+
+
+@run
+def gpu_target_info_attribute_parsing():
+    mlir_string = """
+    hal.executable private @main_dispatch_0 {
+        hal.executable.variant public @rocm_hsaco_fb
+            target(<"rocm", "rocm-hsaco-fb",
+                {
+                abi = "hip",
+                iree_codegen.target_info = #iree_gpu.target<
+                    arch = "gfx942",
+                    features = "",
+                    wgp = <
+                    compute = fp64|fp32|fp16,
+                    storage = b64|b32|b16|b8,
+                    subgroup = shuffle|arithmetic,
+                    dot = dp4xi8toi32,
+                    mma = [<MFMA_F32_16x16x4_F32>],
+                    subgroup_size_choices = [64],
```

**Comment:**
Can you add a testcase that has more than one value for subgroup size coices?

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:417`

```diff
@@ -391,3 +391,70 @@ def compilation_info():
     assert compilation_info is not None
     assert compilation_info.lowering_config == lowering_config
     assert compilation_info.translation_info == translation_info
+
+
+@run
+def gpu_target_info_attribute_parsing():
+    mlir_string = """
+    hal.executable private @main_dispatch_0 {
+        hal.executable.variant public @rocm_hsaco_fb
+            target(<"rocm", "rocm-hsaco-fb",
+                {
+                abi = "hip",
+                iree_codegen.target_info = #iree_gpu.target<
+                    arch = "gfx942",
+                    features = "",
+                    wgp = <
+                    compute = fp64|fp32|fp16,
+                    storage = b64|b32|b16|b8,
+                    subgroup = shuffle|arithmetic,
+                    dot = dp4xi8toi32,
+                    mma = [<MFMA_F32_16x16x4_F32>],
+                    subgroup_size_choices = [64],
+                    max_workgroup_sizes = [1024, 1024, 1024],
+                    max_thread_count_per_workgroup = 1024,
+                    max_workgroup_memory_bytes = 65536,
+                    max_workgroup_counts = [2147483647, 2147483647, 2147483647],
```

**Comment:**
For max wokrgroup sizes and counts, I would pick 3 distinct values so that we can test they appear in the correct order on the python side

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:438`

```diff
@@ -391,3 +391,70 @@ def compilation_info():
     assert compilation_info is not None
     assert compilation_info.lowering_config == lowering_config
     assert compilation_info.translation_info == translation_info
+
+
+@run
+def gpu_target_info_attribute_parsing():
+    mlir_string = """
+    hal.executable private @main_dispatch_0 {
+        hal.executable.variant public @rocm_hsaco_fb
+            target(<"rocm", "rocm-hsaco-fb",
+                {
+                abi = "hip",
+                iree_codegen.target_info = #iree_gpu.target<
+                    arch = "gfx942",
+                    features = "",
+                    wgp = <
+                    compute = fp64|fp32|fp16,
+                    storage = b64|b32|b16|b8,
+                    subgroup = shuffle|arithmetic,
+                    dot = dp4xi8toi32,
+                    mma = [<MFMA_F32_16x16x4_F32>],
+                    subgroup_size_choices = [64],
+                    max_workgroup_sizes = [1024, 1024, 1024],
+                    max_thread_count_per_workgroup = 1024,
+                    max_workgroup_memory_bytes = 65536,
+                    max_workgroup_counts = [2147483647, 2147483647, 2147483647],
+                    max_load_instruction_bits = 128,
+                    simds_per_wgp = 4,
+                    vgpr_space_bits = 16384
+                    >
+                >
+                }>
+            ) {
+        }
+    }
+    """
+
+    module = ir.Module.parse(mlir_string)
+    variant_op_list = iree_codegen.get_executable_variant_ops(module)
+    assert len(variant_op_list) == 1, "Expect one executable variant op"
+    variant_op = variant_op_list[0]
+    executable_variant_op = variant_op.opview
+    target = executable_variant_op.target
+    gpu_target_info = iree_gpu.get_gpu_target_info(target)
+
+    arch = gpu_target_info.arch
+    assert arch == "gfx942", f"Expected arch 'gfx942', got '{arch}'"
```

**Comment:**
I think failed assertions are going to print the actual and expected values -- could you check if this is the case, and remove those messages if it works like I described? Also below.

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:403`

```diff
@@ -387,3 +388,41 @@ ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment) {
   result.element = wrap(builder.getI64ArrayAttr(layout.element));
   return result;
 }
+
+ireeGPUTargetInfo
+ireeHALExecutableTargetAttrGetGPUTargetInfo(MlirAttribute attr) {
+  assert(!mlirAttributeIsNull(attr) && "attr cannot be null");
+  auto executableTargetAttr =
+      llvm::cast<mlir::iree_compiler::IREE::HAL::ExecutableTargetAttr>(
+          unwrap(attr));
+
+  assert(executableTargetAttr && "attr is not a HAL::ExecutableTargetAttr");
+
+  ireeGPUTargetInfo targetInfo = {};
+  auto context = executableTargetAttr.getContext();
+  auto gpuTargetAttr =
```

**Comment:**
Please spell out the types here and below, where the type is not obvious based on the RHS only. See https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:149`

```diff
@@ -144,6 +144,18 @@ struct ireeGPUMMASingleSubgroupLayout {
 MLIR_CAPI_EXPORTED ireeGPUMMASingleSubgroupLayout
 ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment);
 
+struct ireeGPUTargetInfo {
+  MlirIdentifier arch;                 // "gfx942".
+  MlirAttribute subgroup_size_choices; // Subgroup size choices.
```

**Comment:**
Also use the mlir naming standard for all fields in this class

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:413`

```diff
@@ -391,3 +391,109 @@ def compilation_info():
     assert compilation_info is not None
     assert compilation_info.lowering_config == lowering_config
     assert compilation_info.translation_info == translation_info
+
+
+@run
+def gpu_target_info_attribute_parsing():
+    mlir_string = """
+    hal.executable private @main_dispatch_0 {
+        hal.executable.variant public @rocm_hsaco_fb
+            target(<"rocm", "rocm-hsaco-fb",
+                {
+                abi = "hip",
+                iree_codegen.target_info = #iree_gpu.target<
+                    arch = "gfx942",
+                    features = "",
+                    wgp = <
+                    compute = fp64,
+                    storage = b64,
+                    subgroup = none,
+                    dot = none,
+                    mma = [<MFMA_F32_16x16x4_F32>],
+                    subgroup_size_choices = [64],
```

**Comment:**
Can we populate this with at least two values and make sure they are in the correct order?

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:417`

```diff
@@ -391,3 +391,109 @@ def compilation_info():
     assert compilation_info is not None
     assert compilation_info.lowering_config == lowering_config
     assert compilation_info.translation_info == translation_info
+
+
+@run
+def gpu_target_info_attribute_parsing():
+    mlir_string = """
+    hal.executable private @main_dispatch_0 {
+        hal.executable.variant public @rocm_hsaco_fb
+            target(<"rocm", "rocm-hsaco-fb",
+                {
+                abi = "hip",
+                iree_codegen.target_info = #iree_gpu.target<
+                    arch = "gfx942",
+                    features = "",
+                    wgp = <
+                    compute = fp64,
+                    storage = b64,
+                    subgroup = none,
+                    dot = none,
+                    mma = [<MFMA_F32_16x16x4_F32>],
+                    subgroup_size_choices = [64],
+                    max_workgroup_sizes = [256, 512, 1024],
+                    max_thread_count_per_workgroup = 1024,
+                    max_workgroup_memory_bytes = 65536,
+                    max_workgroup_counts = [2147483647, 2147483647, 2147483647]
```

**Comment:**
Can we make these distinct?

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:478`

```diff
@@ -391,3 +391,109 @@ def compilation_info():
     assert compilation_info is not None
     assert compilation_info.lowering_config == lowering_config
     assert compilation_info.translation_info == translation_info
+
+
+@run
+def gpu_target_info_attribute_parsing():
+    mlir_string = """
+    hal.executable private @main_dispatch_0 {
+        hal.executable.variant public @rocm_hsaco_fb
+            target(<"rocm", "rocm-hsaco-fb",
+                {
+                abi = "hip",
+                iree_codegen.target_info = #iree_gpu.target<
+                    arch = "gfx942",
+                    features = "",
+                    wgp = <
+                    compute = fp64,
+                    storage = b64,
+                    subgroup = none,
+                    dot = none,
+                    mma = [<MFMA_F32_16x16x4_F32>],
+                    subgroup_size_choices = [64],
+                    max_workgroup_sizes = [256, 512, 1024],
+                    max_thread_count_per_workgroup = 1024,
+                    max_workgroup_memory_bytes = 65536,
+                    max_workgroup_counts = [2147483647, 2147483647, 2147483647]
+                    >
+                >
+                }>
+            ) {
+        }
+    }
+    """
+
+    module = ir.Module.parse(mlir_string)
+    variant_op_list = iree_codegen.get_executable_variant_ops(module)
+    assert len(variant_op_list) == 1, "Expect one executable variant op"
+    variant_op = variant_op_list[0]
+    executable_variant_op = variant_op.opview
+    target = executable_variant_op.target
+    gpu_target_info = iree_gpu.get_gpu_target_info(target)
+
+    arch = gpu_target_info.arch
+    assert arch == "gfx942", f"Expected arch 'gfx942', got '{arch}'"
+
+    subgroup_size_choices = gpu_target_info.subgroup_size_choices
+    assert subgroup_size_choices == [
+        64
+    ], f"Expected subgroup_size_choice [64], got {subgroup_size_choices}"
+
+    max_thread_count = gpu_target_info.max_thread_count_per_workgroup
+    assert (
+        max_thread_count == 1024
+    ), f"Expected max_thread_count_per_workgroup 1024, got {max_thread_count}"
+
+    max_memory_bytes = gpu_target_info.max_workgroup_memory_bytes
+    assert (
+        max_memory_bytes == 65536
+    ), f"Expected max_workgroup_memory_bytes 65536, got {max_memory_bytes}"
+
+    max_workgroup_sizes = gpu_target_info.max_workgroup_sizes
+    assert max_workgroup_sizes == [
+        256,
+        512,
+        1024,
+    ], f"Expected max_workgroup_sizes [256, 512, 1024], got {max_workgroup_sizes}"
+
+    mlir_string = """
+    hal.executable private @main_dispatch_1 {
+        hal.executable.variant public @rocm_hsaco_fb
+            target(<"rocm", "rocm-hsaco-fb",
+                {
+                abi = "hip",
+                iree_codegen.target_info = #iree_gpu.target<
+                    arch = "gfx942",
+                    features = "",
+                    wgp = <
+                    compute = fp16,
+                    storage = b16,
+                    subgroup = none,
+                    dot = none,
+                    mma = [],
+                    subgroup_size_choices = [32, 64],
+                    max_workgroup_sizes = [1024, 1024, 1024],
+                    max_thread_count_per_workgroup = 1024,
+                    max_workgroup_memory_bytes = 65536,
+                    max_workgroup_counts = [1024]
```

**Comment:**
IIRC, this should always match the x / y / z dimensions, so this config seems invalid

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:460`

```diff
@@ -391,3 +391,109 @@ def compilation_info():
     assert compilation_info is not None
     assert compilation_info.lowering_config == lowering_config
     assert compilation_info.translation_info == translation_info
+
+
+@run
+def gpu_target_info_attribute_parsing():
+    mlir_string = """
+    hal.executable private @main_dispatch_0 {
+        hal.executable.variant public @rocm_hsaco_fb
+            target(<"rocm", "rocm-hsaco-fb",
+                {
+                abi = "hip",
+                iree_codegen.target_info = #iree_gpu.target<
+                    arch = "gfx942",
+                    features = "",
+                    wgp = <
+                    compute = fp64,
+                    storage = b64,
+                    subgroup = none,
+                    dot = none,
+                    mma = [<MFMA_F32_16x16x4_F32>],
+                    subgroup_size_choices = [64],
+                    max_workgroup_sizes = [256, 512, 1024],
+                    max_thread_count_per_workgroup = 1024,
+                    max_workgroup_memory_bytes = 65536,
+                    max_workgroup_counts = [2147483647, 2147483647, 2147483647]
+                    >
+                >
+                }>
+            ) {
+        }
+    }
+    """
+
+    module = ir.Module.parse(mlir_string)
+    variant_op_list = iree_codegen.get_executable_variant_ops(module)
+    assert len(variant_op_list) == 1, "Expect one executable variant op"
+    variant_op = variant_op_list[0]
+    executable_variant_op = variant_op.opview
+    target = executable_variant_op.target
+    gpu_target_info = iree_gpu.get_gpu_target_info(target)
+
+    arch = gpu_target_info.arch
+    assert arch == "gfx942", f"Expected arch 'gfx942', got '{arch}'"
+
+    subgroup_size_choices = gpu_target_info.subgroup_size_choices
+    assert subgroup_size_choices == [
+        64
+    ], f"Expected subgroup_size_choice [64], got {subgroup_size_choices}"
+
+    max_thread_count = gpu_target_info.max_thread_count_per_workgroup
+    assert (
+        max_thread_count == 1024
+    ), f"Expected max_thread_count_per_workgroup 1024, got {max_thread_count}"
+
+    max_memory_bytes = gpu_target_info.max_workgroup_memory_bytes
+    assert (
+        max_memory_bytes == 65536
+    ), f"Expected max_workgroup_memory_bytes 65536, got {max_memory_bytes}"
+
+    max_workgroup_sizes = gpu_target_info.max_workgroup_sizes
+    assert max_workgroup_sizes == [
+        256,
+        512,
+        1024,
+    ], f"Expected max_workgroup_sizes [256, 512, 1024], got {max_workgroup_sizes}"
+
+    mlir_string = """
+    hal.executable private @main_dispatch_1 {
```

**Comment:**
Why do we need a second test input? I think one should be enough to exercise everything we need?

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:406`

```diff
@@ -387,3 +388,37 @@ ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment) {
   result.element = wrap(builder.getI64ArrayAttr(layout.element));
   return result;
 }
+
+ireeGPUTargetInfo
+ireeHALExecutableTargetAttrGetGPUTargetInfo(MlirAttribute attr) {
+  assert(!mlirAttributeIsNull(attr) && "attr cannot be null");
+  auto executableTargetAttr =
+      llvm::cast<mlir::iree_compiler::IREE::HAL::ExecutableTargetAttr>(
+          unwrap(attr));
+
+  assert(executableTargetAttr && "attr is not a HAL::ExecutableTargetAttr");
+
+  ireeGPUTargetInfo targetInfo = {};
+  mlir::MLIRContext *context = executableTargetAttr.getContext();
+  mlir::iree_compiler::IREE::GPU::TargetAttr gpuTargetAttr =
+      mlir::iree_compiler::getGPUTargetAttr(context, executableTargetAttr);
+
+  if (gpuTargetAttr) {
```

**Comment:**
Use an early return instead: https://llvm.org/docs/CodingStandards.html#use-early-exits-and-continue-to-simplify-code

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:399`

```diff
@@ -387,3 +388,39 @@ ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment) {
   result.element = wrap(builder.getI64ArrayAttr(layout.element));
   return result;
 }
+
+ireeGPUTargetInfo
+ireeHALExecutableTargetAttrGetGPUTargetInfo(MlirAttribute attr) {
+  assert(!mlirAttributeIsNull(attr) && "attr cannot be null");
+  auto executableTargetAttr =
+      llvm::cast<mlir::iree_compiler::IREE::HAL::ExecutableTargetAttr>(
+          unwrap(attr));
+
+  assert(executableTargetAttr && "attr is not a HAL::ExecutableTargetAttr");
```

**Comment:**
This assert will never trigger because `llvm::cast` checks that the types match and asserts internally (unlike `llvm::dyn_cast`). See https://llvm.org/docs/ProgrammersManual.html#the-isa-cast-and-dyn-cast-templates

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:399`

```diff
@@ -387,3 +388,39 @@ ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment) {
   result.element = wrap(builder.getI64ArrayAttr(layout.element));
   return result;
 }
+
+ireeGPUTargetInfo
+ireeHALExecutableTargetAttrGetGPUTargetInfo(MlirAttribute attr) {
+  assert(!mlirAttributeIsNull(attr) && "attr cannot be null");
+  auto executableTargetAttr =
+      llvm::cast<mlir::iree_compiler::IREE::HAL::ExecutableTargetAttr>(
+          unwrap(attr));
+
+  assert(executableTargetAttr && "attr is not a HAL::ExecutableTargetAttr");
```

**Comment:**
We can delete this assert

---


---


## [PR #21537](https://github.com/iree-org/iree/pull/21537): [GPU] Add col_major optional attribution to VirtualMMAAttr

### Review Summary

**APPROVED** (2025-07-31)



---


## [PR #21454](https://github.com/iree-org/iree/pull/21454): [Codegen][Tuner]: expose python binding for mma single subgroup layout

### Review Summary

**COMMENTED** (2025-07-22)


**COMMENTED** (2025-07-22)


**APPROVED** (2025-07-22)

LGTM and thanks for the cleanup! The old code looks much cleaner now


### Code Comments

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:548`

```diff
@@ -518,6 +518,36 @@ NB_MODULE(_ireeCompilerDialects, m) {
             return std::nullopt;
           });
 
+  //===-------------------------------------------------------------------===//
+  // Binding to utility function getSingleSubgroupLayout
+  //===-------------------------------------------------------------------===//
+  py::class_<ireeGPUMMASingleSubgroupLayout>(iree_gpu_module,
+                                             "GPUMMASingleSubgroupLayout")
+      .def_prop_ro(
+          "outer",
+          [](const ireeGPUMMASingleSubgroupLayout &self) { return self.outer; })
+      .def_prop_ro("thread",
+                   [](const ireeGPUMMASingleSubgroupLayout &self) {
+                     return self.thread;
+                   })
+      .def_prop_ro("tstrides",
+                   [](const ireeGPUMMASingleSubgroupLayout &self) {
+                     return self.tstrides;
+                   })
+      .def_prop_ro("element", [](const ireeGPUMMASingleSubgroupLayout &self) {
+        return self.element;
+      });
+
+  iree_gpu_module.def(
+      "get_single_subgroup_layout",
+      [](MlirAttribute attr, int fragment) {
+        return ireeGPUGetSingleSubgroupLayout(attr, fragment);
+      },
+      "Returns the single subgroup layout (element, thread, outer, "
+      "tstrides) "
+      "for a given MMA or VirtualMMA intrinsic and fragment.",
```

**Comment:**
nit: this string is broken in a weird way -- could you reflow this? I'd expect something like
```suggestion
      "tstrides) for a given MMA or VirtualMMA intrinsic and "
      "fragment.",
```

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:390`

```diff
@@ -350,3 +350,42 @@ MlirAttribute ireeGPULoweringConfigAttrGetMmaKind(MlirAttribute attr) {
 
   return wrap(mma_attr);
 }
+
+ireeGPUMMASingleSubgroupLayout
+ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment) {
+  assert(ireeAttributeIsAGPUMMAIntrinsicAttr(attr) ||
+         ireeAttributeIsAGPUVirtualMMAIntrinsicAttr(attr) &&
+             "Expected MMA or VirtualMMA Intrinsic");
+
+  mlir::Attribute baseAttr = unwrap(attr);
+  mlir::iree_compiler::IREE::GPU::MMASingleSubgroupLayout layout;
+  mlir::iree_compiler::IREE::GPU::MMAFragment frag =
+      static_cast<mlir::iree_compiler::IREE::GPU::MMAFragment>(fragment);
+
+  if (auto intrinsicAttr =
+          llvm::dyn_cast<mlir::iree_compiler::IREE::GPU::MMAIntrinsicAttr>(
+              baseAttr)) {
+    layout = mlir::iree_compiler::IREE::GPU::getSingleSubgroupLayout(
+        intrinsicAttr.getValue(), frag);
+  } else if (auto virtualIntrinsicAttr = llvm::dyn_cast<
+                 mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsicAttr>(
+                 baseAttr)) {
+    layout = mlir::iree_compiler::IREE::GPU::getSingleSubgroupLayout(
+        virtualIntrinsicAttr.getValue(), frag);
+  } else {
+    assert(false &&
+           "Unreachable: attribute must be MMA or VirtualMMA intrinsic");
+  }
+
+  mlir::MLIRContext *context = baseAttr.getContext();
+  mlir::Builder builder(context);
+
+  ireeGPUMMASingleSubgroupLayout result;
+
+  result.outer = wrap(builder.getI64ArrayAttr(layout.outer));
+  result.thread = wrap(builder.getI64ArrayAttr(layout.thread));
+  result.tstrides = wrap(builder.getI64ArrayAttr(layout.tstrides));
+  result.element = wrap(builder.getI64ArrayAttr(layout.element));
+
+  return result;
```

**Comment:**
```suggestion
  ireeGPUMMASingleSubgroupLayout result;
  result.outer = wrap(builder.getI64ArrayAttr(layout.outer));
  result.thread = wrap(builder.getI64ArrayAttr(layout.thread));
  result.tstrides = wrap(builder.getI64ArrayAttr(layout.tstrides));
  result.element = wrap(builder.getI64ArrayAttr(layout.element));
  return result;
```

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:383`

```diff
@@ -350,3 +350,42 @@ MlirAttribute ireeGPULoweringConfigAttrGetMmaKind(MlirAttribute attr) {
 
   return wrap(mma_attr);
 }
+
+ireeGPUMMASingleSubgroupLayout
+ireeGPUGetSingleSubgroupLayout(MlirAttribute attr, uint32_t fragment) {
+  assert(ireeAttributeIsAGPUMMAIntrinsicAttr(attr) ||
+         ireeAttributeIsAGPUVirtualMMAIntrinsicAttr(attr) &&
+             "Expected MMA or VirtualMMA Intrinsic");
+
+  mlir::Attribute baseAttr = unwrap(attr);
+  mlir::iree_compiler::IREE::GPU::MMASingleSubgroupLayout layout;
+  mlir::iree_compiler::IREE::GPU::MMAFragment frag =
+      static_cast<mlir::iree_compiler::IREE::GPU::MMAFragment>(fragment);
+
+  if (auto intrinsicAttr =
+          llvm::dyn_cast<mlir::iree_compiler::IREE::GPU::MMAIntrinsicAttr>(
+              baseAttr)) {
+    layout = mlir::iree_compiler::IREE::GPU::getSingleSubgroupLayout(
+        intrinsicAttr.getValue(), frag);
+  } else if (auto virtualIntrinsicAttr = llvm::dyn_cast<
+                 mlir::iree_compiler::IREE::GPU::VirtualMMAIntrinsicAttr>(
+                 baseAttr)) {
+    layout = mlir::iree_compiler::IREE::GPU::getSingleSubgroupLayout(
+        virtualIntrinsicAttr.getValue(), frag);
+  } else {
+    assert(false &&
+           "Unreachable: attribute must be MMA or VirtualMMA intrinsic");
+  }
+
+  mlir::MLIRContext *context = baseAttr.getContext();
+  mlir::Builder builder(context);
+
+  ireeGPUMMASingleSubgroupLayout result;
```

**Comment:**
We can start by zero-initializing the whole struct in case we forgot to initialize some field later on. Zero values are usually easier to track down than uninitialized garbage.
```suggestion
  ireeGPUMMASingleSubgroupLayout result = {};
```

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:139`

```diff
@@ -132,6 +132,16 @@ ireeGPULoweringConfigAttrGetSubgroupCount(MlirAttribute attr);
 MLIR_CAPI_EXPORTED MlirAttribute
 ireeGPULoweringConfigAttrGetMmaKind(MlirAttribute attr);
 
+struct ireeGPUMMASingleSubgroupLayout {
+  MlirAttribute outer;
+  MlirAttribute thread;
+  MlirAttribute tstrides;
+  MlirAttribute element;
```

**Comment:**
Can you comment on the underlying data type? I think this will be an arrayattr of integers/index?

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:528`

```diff
@@ -518,6 +518,36 @@ NB_MODULE(_ireeCompilerDialects, m) {
             return std::nullopt;
           });
 
+  //===-------------------------------------------------------------------===//
+  // Binding to utility function getSingleSubgroupLayout
+  //===-------------------------------------------------------------------===//
+  py::class_<ireeGPUMMASingleSubgroupLayout>(iree_gpu_module,
+                                             "GPUMMASingleSubgroupLayout")
+      .def_prop_ro(
+          "outer",
+          [](const ireeGPUMMASingleSubgroupLayout &self) { return self.outer; })
```

**Comment:**
Should we return an arrayattr or a python list of integers? I think the latter will be easier to work with, especially since all of these are read only

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:66`

```diff
@@ -61,6 +61,23 @@ ireeCodegenGetTunerRootOpsBinding(MlirModule module) {
   return ops;
 }
 
+static std::vector<int64_t> getIntArrayAttrValues(MlirAttribute attr) {
+  mlir::Attribute Attr = unwrap(attr);
+  auto arrayAttr = mlir::dyn_cast_or_null<mlir::ArrayAttr>(Attr);
```

**Comment:**
Use the C API for the type check instead -- notice how the whole file uses C APIs only.

---


---


## [PR #21408](https://github.com/iree-org/iree/pull/21408): Integrate LLVM to llvm/llvm-project@5f53182

### Review Summary

**APPROVED** (2025-07-19)



---


## [PR #21403](https://github.com/iree-org/iree/pull/21403): [Codegen][Tuner] add python binding for VirtualMMAIntrinsic

### Review Summary

**COMMENTED** (2025-07-18)


**COMMENTED** (2025-07-18)


**COMMENTED** (2025-07-19)


**APPROVED** (2025-07-19)

LGTM % nit


### Code Comments

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:106`

```diff
@@ -82,6 +99,12 @@ struct ireeGPUMMAInfo {
 
 MLIR_CAPI_EXPORTED ireeGPUMMAInfo ireeGPUMMAAttrGetInfo(MlirAttribute attr);
 
+MLIR_CAPI_EXPORTED ireeGPUMMAInfo
+ireeGPUVirtualMMAAttrGetInfo(MlirAttribute attr);
+
+MLIR_CAPI_EXPORTED MlirAttribute
+ireeGPUMMAAttrGetVirtualMMAIntrinsic(MlirAttribute attr);
```

**Comment:**
Could we use the same functions to handle virtual and non-virtual mma intrinsics? I mean one function that would subsume both `ireeGPUVirtualMMAAttrGetInfo` and  `ireeGPUMMAAttrGetInfo`.

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:249`

```diff
@@ -220,21 +220,34 @@ MlirAttribute ireeGPUVirtualMMAAttrGet(MlirContext mlirCtx, uint32_t value) {
 }
 
 ireeGPUMMAInfo ireeGPUMMAAttrGetInfo(MlirAttribute attr) {
-  assert(ireeAttributeIsAGPUMMAAttr(attr) && "attr is not a MMAAttr");
-  auto mma = llvm::cast<mlir::iree_compiler::IREE::GPU::MMAAttr>(unwrap(attr));
-
+  assert(ireeAttributeIsAGPUMMAAttr(attr) ||
+         ireeAttributeIsAGPUVirtualMMAAttr(attr) &&
+             "Expected MMAAttr or VirtualMMAAttr");
   ireeGPUMMAInfo info = {};
-  auto [aType, bType, cType] = mma.getABCElementTypes();
-  info.aElementType = wrap(aType);
-  info.bElementType = wrap(bType);
-  info.cElementType = wrap(cType);
 
-  auto [aVecType, bVecType, cVecType] = mma.getABCVectorTypes();
-  info.aVectorType = wrap(aVecType);
-  info.bVectorType = wrap(bVecType);
-  info.cVectorType = wrap(cVecType);
+  auto setMMAInfo = [&](auto mma) {
+    auto [aType, bType, cType] = mma.getABCElementTypes();
+    info.aElementType = wrap(aType);
+    info.bElementType = wrap(bType);
+    info.cElementType = wrap(cType);
+
+    auto [aVecType, bVecType, cVecType] = mma.getABCVectorTypes();
+    info.aVectorType = wrap(aVecType);
+    info.bVectorType = wrap(bVecType);
+    info.cVectorType = wrap(cVecType);
+
+    std::tie(info.mElements, info.nElements, info.kElements) =
+        mma.getMNKShape();
+  };
+
+  if (ireeAttributeIsAGPUMMAAttr(attr)) {
+    setMMAInfo(
+        llvm::cast<mlir::iree_compiler::IREE::GPU::MMAAttr>(unwrap(attr)));
+    return info;
+  }
 
-  std::tie(info.mElements, info.nElements, info.kElements) = mma.getMNKShape();
+  setMMAInfo(
```

**Comment:**
you can use `llvm::TypeSwitch`

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:249`

```diff
@@ -220,21 +220,34 @@ MlirAttribute ireeGPUVirtualMMAAttrGet(MlirContext mlirCtx, uint32_t value) {
 }
 
 ireeGPUMMAInfo ireeGPUMMAAttrGetInfo(MlirAttribute attr) {
-  assert(ireeAttributeIsAGPUMMAAttr(attr) && "attr is not a MMAAttr");
-  auto mma = llvm::cast<mlir::iree_compiler::IREE::GPU::MMAAttr>(unwrap(attr));
-
+  assert(ireeAttributeIsAGPUMMAAttr(attr) ||
+         ireeAttributeIsAGPUVirtualMMAAttr(attr) &&
+             "Expected MMAAttr or VirtualMMAAttr");
   ireeGPUMMAInfo info = {};
-  auto [aType, bType, cType] = mma.getABCElementTypes();
-  info.aElementType = wrap(aType);
-  info.bElementType = wrap(bType);
-  info.cElementType = wrap(cType);
 
-  auto [aVecType, bVecType, cVecType] = mma.getABCVectorTypes();
-  info.aVectorType = wrap(aVecType);
-  info.bVectorType = wrap(bVecType);
-  info.cVectorType = wrap(cVecType);
+  auto setMMAInfo = [&](auto mma) {
+    auto [aType, bType, cType] = mma.getABCElementTypes();
+    info.aElementType = wrap(aType);
+    info.bElementType = wrap(bType);
+    info.cElementType = wrap(cType);
+
+    auto [aVecType, bVecType, cVecType] = mma.getABCVectorTypes();
+    info.aVectorType = wrap(aVecType);
+    info.bVectorType = wrap(bVecType);
+    info.cVectorType = wrap(cVecType);
+
+    std::tie(info.mElements, info.nElements, info.kElements) =
+        mma.getMNKShape();
+  };
+
+  if (ireeAttributeIsAGPUMMAAttr(attr)) {
+    setMMAInfo(
+        llvm::cast<mlir::iree_compiler::IREE::GPU::MMAAttr>(unwrap(attr)));
+    return info;
+  }
 
-  std::tie(info.mElements, info.nElements, info.kElements) = mma.getMNKShape();
+  setMMAInfo(
```

**Comment:**
My apologies, I should have been more explicit here. Type switch is a standard pattern to deal with the case you ran into here: passing type-erased type to same generic code that needs to know the derived type. You can drop the whole `setMMAInfo` lambda and do something like this:
```c++
  return TypeSwitch<Attribute, ireeGPUMMAInfo>(unwrap(attr))
    .Case<AttrTypeA, AttrTypeB>([](auto mmaAttr){
      // The logic from setMMAInfo...
    }).Default([](Attribute) { assert(false && "..."); return ireeGPUMMAInfo{}; });
```

This way you don't have type switch do all the `dyn_cast`s for you and handle the wrong type case. You can also return the mma info directly without having to declare it outside of the lambda and capture it for modification.

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:223`

```diff
@@ -220,40 +220,34 @@ MlirAttribute ireeGPUVirtualMMAAttrGet(MlirContext mlirCtx, uint32_t value) {
 }
 
 ireeGPUMMAInfo ireeGPUMMAAttrGetInfo(MlirAttribute attr) {
-  assert(ireeAttributeIsAGPUMMAAttr(attr) ||
-         ireeAttributeIsAGPUVirtualMMAAttr(attr) &&
-             "Expected MMAAttr or VirtualMMAAttr");
-  ireeGPUMMAInfo info = {};
-
-  auto setMMAInfo = [&](auto mma) {
-    auto [aType, bType, cType] = mma.getABCElementTypes();
-    info.aElementType = wrap(aType);
-    info.bElementType = wrap(bType);
-    info.cElementType = wrap(cType);
-
-    auto [aVecType, bVecType, cVecType] = mma.getABCVectorTypes();
-    info.aVectorType = wrap(aVecType);
-    info.bVectorType = wrap(bVecType);
-    info.cVectorType = wrap(cVecType);
-
-    std::tie(info.mElements, info.nElements, info.kElements) =
-        mma.getMNKShape();
-  };
-
-  if (ireeAttributeIsAGPUMMAAttr(attr)) {
-    setMMAInfo(
-        llvm::cast<mlir::iree_compiler::IREE::GPU::MMAAttr>(unwrap(attr)));
-    return info;
-  }
+  assert((ireeAttributeIsAGPUMMAAttr(attr) ||
```

**Comment:**
This assertion os redundant to the one in the default case

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:230`

```diff
@@ -220,40 +220,34 @@ MlirAttribute ireeGPUVirtualMMAAttrGet(MlirContext mlirCtx, uint32_t value) {
 }
 
 ireeGPUMMAInfo ireeGPUMMAAttrGetInfo(MlirAttribute attr) {
-  assert(ireeAttributeIsAGPUMMAAttr(attr) ||
-         ireeAttributeIsAGPUVirtualMMAAttr(attr) &&
-             "Expected MMAAttr or VirtualMMAAttr");
-  ireeGPUMMAInfo info = {};
-
-  auto setMMAInfo = [&](auto mma) {
-    auto [aType, bType, cType] = mma.getABCElementTypes();
-    info.aElementType = wrap(aType);
-    info.bElementType = wrap(bType);
-    info.cElementType = wrap(cType);
-
-    auto [aVecType, bVecType, cVecType] = mma.getABCVectorTypes();
-    info.aVectorType = wrap(aVecType);
-    info.bVectorType = wrap(bVecType);
-    info.cVectorType = wrap(cVecType);
-
-    std::tie(info.mElements, info.nElements, info.kElements) =
-        mma.getMNKShape();
-  };
-
-  if (ireeAttributeIsAGPUMMAAttr(attr)) {
-    setMMAInfo(
-        llvm::cast<mlir::iree_compiler::IREE::GPU::MMAAttr>(unwrap(attr)));
-    return info;
-  }
+  assert((ireeAttributeIsAGPUMMAAttr(attr) ||
+          ireeAttributeIsAGPUVirtualMMAAttr(attr)) &&
+         "Expected MMAAttr or VirtualMMAAttr");
 
-  llvm::TypeSwitch<mlir::Attribute, void>(unwrap(attr))
+  return llvm::TypeSwitch<mlir::Attribute, ireeGPUMMAInfo>(unwrap(attr))
       .Case<mlir::iree_compiler::IREE::GPU::MMAAttr,
             mlir::iree_compiler::IREE::GPU::VirtualMMAAttr>(
-          [&](auto mma) { setMMAInfo(mma); })
-      .Default([&](mlir::Attribute) {
-        assert(false && "Expected MMAAttr or VirtualMMAAttr");
+          [](auto mma) -> ireeGPUMMAInfo {
```

**Comment:**
```suggestion
          [](auto mma) {
```

The return type appears in the typeswitch definition and in side the lamdba body already

---


---


## [PR #21398](https://github.com/iree-org/iree/pull/21398): Integrate LLVM to llvm/llvm-project@e0cce5c

### Review Summary

**APPROVED** (2025-07-17)



---


## [PR #21377](https://github.com/iree-org/iree/pull/21377): Integrate LLVM to llvm/llvm-project@bda5602

### Review Summary

**APPROVED** (2025-07-15)



---


## [PR #21364](https://github.com/iree-org/iree/pull/21364): Integrate LLVM to llvm/llvm-project@3ed3a33

### Review Summary

**COMMENTED** (2025-07-14)

Can you also mention how stablehlo and torch-mlir were updated (SHAs)?


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.td:1662`

```diff
@@ -1666,11 +1655,11 @@ def IREELinalgExt_WinogradFilterTransformOp : IREELinalgExt_Op<"winograd.filter_
                        DenseI64ArrayAttr:$kernel_dimensions
   );
 
-  let builders = [
-    OpBuilder<(ins "ValueRange":$inputs, "ValueRange":$outputs,
-      CArg<"int64_t", "8">:$output_tile_size, CArg<"int64_t", "3">:$kernel_size,
-      CArg<"ArrayRef<int64_t>", "{0, 1}">:$kernel_dimensions)>
-  ];
+  // let builders = [
+  //   OpBuilder<(ins "ValueRange":$inputs, "ValueRange":$outputs,
+  //     CArg<"int64_t", "8">:$output_tile_size, CArg<"int64_t", "3">:$kernel_size,
+  //     CArg<"ArrayRef<int64_t>", "{0, 1}">:$kernel_dimensions)>
+  // ];
```

**Comment:**
Delete this instead of commenting out?

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.td:1782`

```diff
@@ -1786,11 +1775,11 @@ def IREELinalgExt_WinogradOutputTransformOp : IREELinalgExt_Op<"winograd.output_
                        DenseI64ArrayAttr:$image_dimensions
   );
 
-  let builders = [
-    OpBuilder<(ins "ValueRange":$inputs, "ValueRange":$outputs,
-      CArg<"int64_t", "8">:$output_tile_size, CArg<"int64_t", "3">:$kernel_size,
-      CArg<"ArrayRef<int64_t>", "{1, 2}">:$image_dimensions)>
-  ];
+  // let builders = [
+  //   OpBuilder<(ins "ValueRange":$inputs, "ValueRange":$outputs,
+  //     CArg<"int64_t", "8">:$output_tile_size, CArg<"int64_t", "3">:$kernel_size,
+  //     CArg<"ArrayRef<int64_t>", "{1, 2}">:$image_dimensions)>
+  // ];
```

**Comment:**
Also here

---


---


## [PR #21345](https://github.com/iree-org/iree/pull/21345): [Codegen][Tuner] remove decomposition attr for attention op

### Review Summary

**APPROVED** (2025-07-11)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:86`

```diff
@@ -79,7 +79,11 @@ struct StripAttentionOpCompilationInfo final
                    attr.getName() !=
                        IREE::LinalgExt::AttentionOp::getPVAttrStr();
           }));
-      attentionOp.setDecompositionConfigAttr(newConfig);
+      if (!newConfig.empty()) {
+        attentionOp.setDecompositionConfigAttr(newConfig);
+      } else {
+        attentionOp.removeDecompositionConfigAttr();
+      }
```

**Comment:**
nit: flip this condition to avoid negation

---


---


## [PR #21216](https://github.com/iree-org/iree/pull/21216): [Codegen][Tuner] expose python binding isa_attention_op

### Review Summary

**COMMENTED** (2025-06-30)


**APPROVED** (2025-06-30)


### Code Comments

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:509`

```diff
@@ -501,4 +501,12 @@ NB_MODULE(_ireeCompilerDialects, m) {
       "Infers the structure of an attention operation from affine indexing "
       "maps.",
       py::arg("q"), py::arg("k"), py::arg("v"), py::arg("o"));
+
+  iree_codegen_module.def(
+      "isa_attention_op",
+      [](MlirOperation op) -> bool {
+        return ireeCodegenMlirOperationIsACodegenAttentionOp(op);
+      },
```

**Comment:**
Do we need the lambda here? If the types match, you shouldn't need a wrapper around `ireeCodegenMlirOperationIsACodegenAttentionOp`

---


---


## [PR #21170](https://github.com/iree-org/iree/pull/21170): [Codegen][Tuner] expose python binding for attention op details

### Review Summary

**COMMENTED** (2025-06-24)


**APPROVED** (2025-06-25)


### Code Comments

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_codegen.h:94`

```diff
@@ -78,6 +79,19 @@ MLIR_CAPI_EXPORTED void ireeCodegenQueryMMAIntrinsics(MlirOperation op,
                                                       size_t *numIntrinsics,
                                                       uint32_t *mmaIntrinsics);
 
+struct ireeCodegenAttentionOpDetail {
+  MlirAttribute batch;
+  MlirAttribute m;
+  MlirAttribute k1;
+  MlirAttribute k2;
+  MlirAttribute n;
+  int64_t rank;
+};
+
+MLIR_CAPI_EXPORTED ireeCodegenAttentionOpDetail
+ireeCodegenGetAttentionOpDetail(MlirAffineMap qMap, MlirAffineMap kMap,
+                                MlirAffineMap vMap, MlirAffineMap oMap);
+
```

**Comment:**
What about the rank?

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:25`

```diff
@@ -13,13 +13,16 @@
 #include "mlir-c/IR.h"
 #include "mlir/Bindings/Python/Nanobind.h"
 #include "mlir/Bindings/Python/NanobindAdaptors.h"
+#include "mlir/CAPI/IR.h"
+#include "mlir/IR/BuiltinAttributes.h"
 
 static const char *kCodegenModuleImportPath =
     MAKE_MLIR_PYTHON_QUALNAME("dialects.iree_codegen");
 static const char *kGpuModuleImportPath =
     MAKE_MLIR_PYTHON_QUALNAME("dialects.iree_gpu");
 
 namespace py = nanobind;
+using namespace mlir;
```

**Comment:**
Let's keep the use of mlir C++ apis explicit here

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:16`

```diff
@@ -13,6 +13,8 @@
 #include "mlir-c/IR.h"
 #include "mlir/Bindings/Python/Nanobind.h"
 #include "mlir/Bindings/Python/NanobindAdaptors.h"
+#include "mlir/CAPI/IR.h"
```

**Comment:**
What do we use this for?

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:250`

```diff
@@ -220,3 +224,40 @@ void ireeCodegenGetTunerRootOps(MlirModule module, size_t *numOps,
     rootOps[i] = wrap(tunerRootOps[i]);
   }
 }
+
+ireeCodegenAttentionOpDetail
+ireeCodegenGetAttentionOpDetail(MlirAffineMap qMap, MlirAffineMap kMap,
+                                MlirAffineMap vMap, MlirAffineMap oMap) {
+  mlir::AffineMap QMap = unwrap(qMap);
+  mlir::AffineMap KMap = unwrap(kMap);
+  mlir::AffineMap VMap = unwrap(vMap);
+  mlir::AffineMap OMap = unwrap(oMap);
+
+  llvm::FailureOr<mlir::iree_compiler::IREE::LinalgExt::AttentionOpDetail>
+      maybeDetail =
+          mlir::iree_compiler::IREE::LinalgExt::AttentionOpDetail::get(
+              QMap, KMap, VMap, OMap);
+
+  if (failed(maybeDetail)) {
+    return ireeCodegenAttentionOpDetail{/*batch=*/wrap(mlir::Attribute()),
+                                        /*m=*/wrap(mlir::Attribute()),
+                                        /*k1=*/wrap(mlir::Attribute()),
+                                        /*k2=*/wrap(mlir::Attribute()),
+                                        /*n=*/wrap(mlir::Attribute()),
+                                        /*domainRank=*/-1};
+  }
+
+  const auto &opInfo = *maybeDetail;
```

**Comment:**
Don't use auto here, the type is not obvious without IDE

---


---


## [PR #21138](https://github.com/iree-org/iree/pull/21138): [LinalgExt] support converting argcompare to loops.

### Review Summary

**COMMENTED** (2025-06-20)


**COMMENTED** (2025-06-20)


**COMMENTED** (2025-06-20)


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1365`

```diff
@@ -1340,6 +1340,88 @@ LogicalResult TopkOp::getResultTilePosition(
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// ArgCompareOp
+//===----------------------------------------------------------------------===//
+
+SmallVector<Range> ArgCompareOp::getIterationDomain(OpBuilder &builder) {
+  Location loc = getLoc();
+  OpFoldResult zero = builder.getIndexAttr(0);
+  OpFoldResult one = builder.getIndexAttr(1);
+  int64_t rank = getInputRank();
+  SmallVector<Range> ranges;
+  for (int64_t dim = 0; dim < rank; ++dim) {
+    OpFoldResult ub = getDim(builder, loc, getInputValue(), dim);
+    ranges.push_back(Range{zero, ub, one});
+  }
+  return ranges;
+}
+
+LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
+                                                         Location loc,
+                                                         ValueRange ivs) {
+  uint64_t reductionDim = getDimension();
+  SmallVector<Value> parallelIndices;
+  for (size_t i = 0; i < ivs.size(); ++i) {
```

**Comment:**
nit: do not recalculate the end index at each loop iteration

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1377`

```diff
@@ -1340,6 +1340,88 @@ LogicalResult TopkOp::getResultTilePosition(
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// ArgCompareOp
+//===----------------------------------------------------------------------===//
+
+SmallVector<Range> ArgCompareOp::getIterationDomain(OpBuilder &builder) {
+  Location loc = getLoc();
+  OpFoldResult zero = builder.getIndexAttr(0);
+  OpFoldResult one = builder.getIndexAttr(1);
+  int64_t rank = getInputRank();
+  SmallVector<Range> ranges;
+  for (int64_t dim = 0; dim < rank; ++dim) {
+    OpFoldResult ub = getDim(builder, loc, getInputValue(), dim);
+    ranges.push_back(Range{zero, ub, one});
+  }
+  return ranges;
+}
+
+LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
+                                                         Location loc,
+                                                         ValueRange ivs) {
+  uint64_t reductionDim = getDimension();
+  SmallVector<Value> parallelIndices;
+  for (size_t i = 0; i < ivs.size(); ++i) {
+    if (i == reductionDim)
+      continue;
+    parallelIndices.push_back(ivs[i]);
+  }
+
+  Value candidateValue = b.create<memref::LoadOp>(loc, getInputValue(), ivs);
+  Value indexValue = ivs[reductionDim];
+  if (getIndexBase()) {
+    indexValue = b.create<arith::AddIOp>(loc, getIndexBase(), indexValue);
+  }
+  Value castedIndex = indexValue;
+  auto indexType = getOutputIndexType().getElementType();
```

**Comment:**
```suggestion
  Type indexType = getOutputIndexType().getElementType();
```

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1406`

```diff
@@ -1340,6 +1340,88 @@ LogicalResult TopkOp::getResultTilePosition(
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// ArgCompareOp
+//===----------------------------------------------------------------------===//
+
+SmallVector<Range> ArgCompareOp::getIterationDomain(OpBuilder &builder) {
+  Location loc = getLoc();
+  OpFoldResult zero = builder.getIndexAttr(0);
+  OpFoldResult one = builder.getIndexAttr(1);
+  int64_t rank = getInputRank();
+  SmallVector<Range> ranges;
+  for (int64_t dim = 0; dim < rank; ++dim) {
+    OpFoldResult ub = getDim(builder, loc, getInputValue(), dim);
+    ranges.push_back(Range{zero, ub, one});
+  }
+  return ranges;
+}
+
+LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
+                                                         Location loc,
+                                                         ValueRange ivs) {
+  uint64_t reductionDim = getDimension();
+  SmallVector<Value> parallelIndices;
+  for (size_t i = 0; i < ivs.size(); ++i) {
+    if (i == reductionDim)
+      continue;
+    parallelIndices.push_back(ivs[i]);
+  }
+
+  Value candidateValue = b.create<memref::LoadOp>(loc, getInputValue(), ivs);
+  Value indexValue = ivs[reductionDim];
+  if (getIndexBase()) {
+    indexValue = b.create<arith::AddIOp>(loc, getIndexBase(), indexValue);
+  }
+  Value castedIndex = indexValue;
+  auto indexType = getOutputIndexType().getElementType();
+  if (castedIndex.getType() != indexType) {
+    castedIndex = b.create<arith::IndexCastOp>(loc, indexType, castedIndex);
+  }
+
+  Value isFirst =
+      b.create<arith::CmpIOp>(loc, arith::CmpIPredicate::eq, ivs[reductionDim],
+                              b.create<arith::ConstantIndexOp>(loc, 0));
+  auto ifOp = b.create<scf::IfOp>(loc, isFirst, /*withElseRegion=*/true);
+  {
+    OpBuilder thenBuilder = ifOp.getThenBodyBuilder();
+    thenBuilder.create<memref::StoreOp>(loc, candidateValue, outputValue(),
+                                        parallelIndices);
+    thenBuilder.create<memref::StoreOp>(loc, castedIndex, outputIndex(),
+                                        parallelIndices);
+  }
+
+  {
+    OpBuilder elseBuilder = ifOp.getElseBodyBuilder();
+
+    Value bestValueSoFar =
+        elseBuilder.create<memref::LoadOp>(loc, outputValue(), parallelIndices);
+    Value bestIndexSoFar =
+        elseBuilder.create<memref::LoadOp>(loc, outputIndex(), parallelIndices);
+
+    auto &srcBlock = getRegion().front();
+    IRMapping bvm;
+    bvm.map(srcBlock.getArgument(0), candidateValue);
+    bvm.map(srcBlock.getArgument(1), bestValueSoFar);
+    for (auto &op : srcBlock.without_terminator()) {
```

**Comment:**
nit: The types are not obvious in these two places without using an IDE, we shouldn't use `auto` here

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1403`

```diff
@@ -1340,6 +1340,88 @@ LogicalResult TopkOp::getResultTilePosition(
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// ArgCompareOp
+//===----------------------------------------------------------------------===//
+
+SmallVector<Range> ArgCompareOp::getIterationDomain(OpBuilder &builder) {
+  Location loc = getLoc();
+  OpFoldResult zero = builder.getIndexAttr(0);
+  OpFoldResult one = builder.getIndexAttr(1);
+  int64_t rank = getInputRank();
+  SmallVector<Range> ranges;
+  for (int64_t dim = 0; dim < rank; ++dim) {
+    OpFoldResult ub = getDim(builder, loc, getInputValue(), dim);
+    ranges.push_back(Range{zero, ub, one});
+  }
+  return ranges;
+}
+
+LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
+                                                         Location loc,
+                                                         ValueRange ivs) {
+  uint64_t reductionDim = getDimension();
+  SmallVector<Value> parallelIndices;
+  for (size_t i = 0; i < ivs.size(); ++i) {
+    if (i == reductionDim)
+      continue;
+    parallelIndices.push_back(ivs[i]);
+  }
+
+  Value candidateValue = b.create<memref::LoadOp>(loc, getInputValue(), ivs);
+  Value indexValue = ivs[reductionDim];
+  if (getIndexBase()) {
+    indexValue = b.create<arith::AddIOp>(loc, getIndexBase(), indexValue);
+  }
+  Value castedIndex = indexValue;
+  auto indexType = getOutputIndexType().getElementType();
+  if (castedIndex.getType() != indexType) {
+    castedIndex = b.create<arith::IndexCastOp>(loc, indexType, castedIndex);
+  }
+
+  Value isFirst =
+      b.create<arith::CmpIOp>(loc, arith::CmpIPredicate::eq, ivs[reductionDim],
+                              b.create<arith::ConstantIndexOp>(loc, 0));
+  auto ifOp = b.create<scf::IfOp>(loc, isFirst, /*withElseRegion=*/true);
+  {
+    OpBuilder thenBuilder = ifOp.getThenBodyBuilder();
+    thenBuilder.create<memref::StoreOp>(loc, candidateValue, outputValue(),
+                                        parallelIndices);
+    thenBuilder.create<memref::StoreOp>(loc, castedIndex, outputIndex(),
+                                        parallelIndices);
+  }
+
+  {
+    OpBuilder elseBuilder = ifOp.getElseBodyBuilder();
+
+    Value bestValueSoFar =
+        elseBuilder.create<memref::LoadOp>(loc, outputValue(), parallelIndices);
+    Value bestIndexSoFar =
+        elseBuilder.create<memref::LoadOp>(loc, outputIndex(), parallelIndices);
+
+    auto &srcBlock = getRegion().front();
+    IRMapping bvm;
```

**Comment:**
What is `bvm`? I don't know this TLA.

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1366`

```diff
@@ -1362,7 +1362,8 @@ LogicalResult ArgCompareOp::generateScalarImplementation(OpBuilder &b,
                                                          ValueRange ivs) {
   uint64_t reductionDim = getDimension();
   SmallVector<Value> parallelIndices;
-  for (size_t i = 0; i < ivs.size(); ++i) {
+  size_t rank = ivs.size();
+  for (size_t i = 0; i < rank; ++i) {
```

**Comment:**
FYI, you can also do:
```suggestion
  for (size_t i = 0, rank = ivs.size(); i < rank; ++i) {
```
if you don't use `rank` outside of the loop

---


---


## [PR #21106](https://github.com/iree-org/iree/pull/21106): [LinalgExt] fix arg_compare op with region and start index

### Review Summary

**COMMENTED** (2025-06-18)


**COMMENTED** (2025-06-19)


**APPROVED** (2025-06-19)


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.td:687`

```diff
@@ -668,30 +668,42 @@ def IREELinalgExt_TopkOp : IREELinalgExt_Op<"topk",[
   }];
 }
 
-def IREELinalgExt_ArgmaxOp : IREELinalgExt_Op<"argmax", [
+def IREELinalgExt_ArgCompareOp : IREELinalgExt_Op<"arg_compare", [
+  SingleBlockImplicitTerminator<"::mlir::iree_compiler::IREE::LinalgExt::YieldOp">,
   DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>
 ]> {
-  let summary = "Argmax reduction op.";
+  let summary = "Performs an arg-reduction using a user-defined comparator.";
   let description = [{
-    An argmax op that reduces along a given dimension, returning the max value and its index.
+    The `arg_compare` op performs a reduction over a given dimension of a tensor,
+    returning both the selected value and its corresponding index. The selection
+    logic is defined by a user-specified comparator region.
+
+    The comparator region receives two candidate values and returns a single `i1`
+    result indicating whether the first argument should be preferred over the second.
+
+    This region defines the sorting rule, e.g., "greater than" for argmax or
+    "less than" for argmin. It allows for generalization beyond simple argmax-style
+    behavior.
   }];
```

**Comment:**
It would be nice to add an example of the op here, I rely on these heavily when writing IR by hand. I think argmax specifically would make a nice sample.

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.td:700`

```diff
@@ -684,6 +684,39 @@ def IREELinalgExt_ArgCompareOp : IREELinalgExt_Op<"arg_compare", [
     This region defines the sorting rule, e.g., "greater than" for argmax or
     "less than" for argmin. It allows for generalization beyond simple argmax-style
     behavior.
+
+    Example (argmax over dim 1):
+
+    %input = memref<2x10xf32>
+    %out_val = memref<2xf32>
+    %out_idx = memref<2xi32>
+    iree_linalg_ext.arg_compare
+      dimension(1)
+      ins(%input : memref<2x10xf32>)
+      outs(%out_val, %out_idx : memref<2xf32>, memref<2xi32>) {
+    ^bb0(%a: f32, %b: f32):
+      %cmp = arith.cmpf ogt, %a, %b : f32
+      iree_linalg_ext.yield %cmp : i1
+    }
```

**Comment:**
Please put this in code blocks so that it renders nicely on the website

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.td:716`

```diff
@@ -684,6 +684,39 @@ def IREELinalgExt_ArgCompareOp : IREELinalgExt_Op<"arg_compare", [
     This region defines the sorting rule, e.g., "greater than" for argmax or
     "less than" for argmin. It allows for generalization beyond simple argmax-style
     behavior.
+
+    Example (argmax over dim 1):
+
+    %input = memref<2x10xf32>
+    %out_val = memref<2xf32>
+    %out_idx = memref<2xi32>
+    iree_linalg_ext.arg_compare
+      dimension(1)
+      ins(%input : memref<2x10xf32>)
+      outs(%out_val, %out_idx : memref<2xf32>, memref<2xi32>) {
+    ^bb0(%a: f32, %b: f32):
+      %cmp = arith.cmpf ogt, %a, %b : f32
+      iree_linalg_ext.yield %cmp : i1
+    }
+
+  Example with index_base = 5 (i.e., indices start counting from 5):
+
+    %input = memref<2x10xf32>
+    %out_val = memref<2xf32>
+    %out_idx = memref<2xi32>
+    %base = arith.constant 5 : index
+    iree_linalg_ext.arg_compare
+      dimension(1)
+      ins(%input : memref<2x10xf32>)
+      outs(%out_val, %out_idx : memref<2xf32>, memref<2xi32>)
+      index_base(%base : index) {
+    ^bb0(%a: f32, %b: f32):
+      %cmp = arith.cmpf ogt, %a, %b : f32
+      iree_linalg_ext.yield %cmp : i1
+    }
```

**Comment:**
Also here

---


---


## [PR #21077](https://github.com/iree-org/iree/pull/21077): [LinalgExt] add TilingInterface support for ArgCompareOp

### Review Summary

**COMMENTED** (2025-06-12)


**COMMENTED** (2025-06-13)


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1352`

```diff
@@ -1340,6 +1340,116 @@ LogicalResult TopkOp::getResultTilePosition(
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+SmallVector<Range> ArgmaxOp::getIterationDomain(OpBuilder &builder) {
+  Location loc = getLoc();
+  OpFoldResult zero = builder.getIndexAttr(0);
+  OpFoldResult one = builder.getIndexAttr(1);
+  SmallVector<Range> ranges;
+  for (auto dim : llvm::seq<int64_t>(0, getInputRank())) {
```

**Comment:**
nit: I think a plain loop would be fine as well. If you keep as-is, I think there's an unary version of `seq` that would be more concise.

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1392`

```diff
@@ -1340,6 +1340,116 @@ LogicalResult TopkOp::getResultTilePosition(
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+SmallVector<Range> ArgmaxOp::getIterationDomain(OpBuilder &builder) {
+  Location loc = getLoc();
+  OpFoldResult zero = builder.getIndexAttr(0);
+  OpFoldResult one = builder.getIndexAttr(1);
+  SmallVector<Range> ranges;
+  for (auto dim : llvm::seq<int64_t>(0, getInputRank())) {
+    OpFoldResult ub = getDim(builder, loc, getInputValue(), dim);
+    ranges.push_back(Range{zero, ub, one});
+  }
+  return ranges;
+}
+
+SmallVector<utils::IteratorType> ArgmaxOp::getLoopIteratorTypes() {
+  SmallVector<utils::IteratorType> iteratorTypes(getInputRank(),
+                                                 utils::IteratorType::parallel);
+  iteratorTypes[getDimension()] = utils::IteratorType::reduction;
+  return iteratorTypes;
+}
+
+FailureOr<TilingResult>
+ArgmaxOp::getTiledImplementation(OpBuilder &builder,
+                                 ArrayRef<OpFoldResult> offsets,
+                                 ArrayRef<OpFoldResult> sizes) {
+  Location loc = getLoc();
+  int64_t rank = getInputRank();
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         sizes.size() == static_cast<size_t>(rank));
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  SmallVector<OpFoldResult> strides(rank, builder.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(builder, loc, getInputValue(), offsets, sizes, strides);
+
+  if (!inputSlice)
+    return emitOpError("failed to slice input");
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  SmallVector<OpFoldResult> outputOffsets, outputSizes;
+  if (failed(getResultTilePosition(builder, 0, offsets, sizes, outputOffsets,
+                                   outputSizes))) {
+    return emitOpError("failed to compute output tile position");
```

**Comment:**
IREE requires braces around bodies of single-statements `if`s/loops

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1373`

```diff
@@ -1340,6 +1340,116 @@ LogicalResult TopkOp::getResultTilePosition(
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+SmallVector<Range> ArgmaxOp::getIterationDomain(OpBuilder &builder) {
+  Location loc = getLoc();
+  OpFoldResult zero = builder.getIndexAttr(0);
+  OpFoldResult one = builder.getIndexAttr(1);
+  SmallVector<Range> ranges;
+  for (auto dim : llvm::seq<int64_t>(0, getInputRank())) {
+    OpFoldResult ub = getDim(builder, loc, getInputValue(), dim);
+    ranges.push_back(Range{zero, ub, one});
+  }
+  return ranges;
+}
+
+SmallVector<utils::IteratorType> ArgmaxOp::getLoopIteratorTypes() {
+  SmallVector<utils::IteratorType> iteratorTypes(getInputRank(),
+                                                 utils::IteratorType::parallel);
+  iteratorTypes[getDimension()] = utils::IteratorType::reduction;
+  return iteratorTypes;
+}
+
+FailureOr<TilingResult>
+ArgmaxOp::getTiledImplementation(OpBuilder &builder,
+                                 ArrayRef<OpFoldResult> offsets,
+                                 ArrayRef<OpFoldResult> sizes) {
+  Location loc = getLoc();
+  int64_t rank = getInputRank();
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         sizes.size() == static_cast<size_t>(rank));
```

**Comment:**
Can you break this down into two separate assertions? This way we will know which one failed without having to start a debugger or recompile the source.

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1392`

```diff
@@ -1340,6 +1340,116 @@ LogicalResult TopkOp::getResultTilePosition(
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+SmallVector<Range> ArgmaxOp::getIterationDomain(OpBuilder &builder) {
+  Location loc = getLoc();
+  OpFoldResult zero = builder.getIndexAttr(0);
+  OpFoldResult one = builder.getIndexAttr(1);
+  SmallVector<Range> ranges;
+  for (auto dim : llvm::seq<int64_t>(0, getInputRank())) {
+    OpFoldResult ub = getDim(builder, loc, getInputValue(), dim);
+    ranges.push_back(Range{zero, ub, one});
+  }
+  return ranges;
+}
+
+SmallVector<utils::IteratorType> ArgmaxOp::getLoopIteratorTypes() {
+  SmallVector<utils::IteratorType> iteratorTypes(getInputRank(),
+                                                 utils::IteratorType::parallel);
+  iteratorTypes[getDimension()] = utils::IteratorType::reduction;
+  return iteratorTypes;
+}
+
+FailureOr<TilingResult>
+ArgmaxOp::getTiledImplementation(OpBuilder &builder,
+                                 ArrayRef<OpFoldResult> offsets,
+                                 ArrayRef<OpFoldResult> sizes) {
+  Location loc = getLoc();
+  int64_t rank = getInputRank();
+  assert(offsets.size() == static_cast<size_t>(rank) &&
+         sizes.size() == static_cast<size_t>(rank));
+
+  SmallVector<Operation *> slices;
+  SmallVector<Value> tiledOperands;
+
+  SmallVector<OpFoldResult> strides(rank, builder.getIndexAttr(1));
+  Operation *inputSlice =
+      getSlice(builder, loc, getInputValue(), offsets, sizes, strides);
+
+  if (!inputSlice)
+    return emitOpError("failed to slice input");
+  tiledOperands.push_back(inputSlice->getResult(0));
+  slices.push_back(inputSlice);
+
+  SmallVector<OpFoldResult> outputOffsets, outputSizes;
+  if (failed(getResultTilePosition(builder, 0, offsets, sizes, outputOffsets,
+                                   outputSizes))) {
+    return emitOpError("failed to compute output tile position");
```

**Comment:**
Also elsewhere below

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/TilingInterfaceImpl.cpp:1352`

```diff
@@ -1340,6 +1340,118 @@ LogicalResult TopkOp::getResultTilePosition(
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+SmallVector<Range> ArgmaxOp::getIterationDomain(OpBuilder &builder) {
+  Location loc = getLoc();
+  OpFoldResult zero = builder.getIndexAttr(0);
+  OpFoldResult one = builder.getIndexAttr(1);
+  SmallVector<Range> ranges;
+  for (int64_t dim = 0; dim < getInputRank(); ++dim) {
```

**Comment:**
Do not recalculate the trip count: https://llvm.org/docs/CodingStandards.html#don-t-evaluate-end-every-time-through-a-loop

---


---


## [PR #21021](https://github.com/iree-org/iree/pull/21021): [LinalgExt] Add argmax op with rountrip and invalid mlir test

### Review Summary

**COMMENTED** (2025-06-06)


**COMMENTED** (2025-06-06)


**COMMENTED** (2025-06-06)


**APPROVED** (2025-06-06)

Looks OK % nits.

I guess the next step is to implement the interfaces mentioned by @hanhanW?


### Code Comments

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:924`

```diff
@@ -908,6 +908,62 @@ TopkOp::reifyResultShapes(OpBuilder &b,
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult ArgmaxOp::verify() {
+  Operation *op = getOperation();
+
+  // Check number of inputs and outputs.
+  if (getNumDpsInputs() != 1) {
+    return op->emitOpError("expected exactly one input operand (values)");
+  }
+
+  if (getNumDpsInits() != 2) {
+    return op->emitOpError("expected two output operands (value and index)");
```

**Comment:**
It would be nice to also print the actual number (in addition to the expected one)

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:929`

```diff
@@ -908,6 +908,62 @@ TopkOp::reifyResultShapes(OpBuilder &b,
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult ArgmaxOp::verify() {
+  Operation *op = getOperation();
+
+  // Check number of inputs and outputs.
+  if (getNumDpsInputs() != 1) {
+    return op->emitOpError("expected exactly one input operand (values)");
+  }
+
+  if (getNumDpsInits() != 2) {
+    return op->emitOpError("expected two output operands (value and index)");
+  }
+
+  uint64_t dim = getDimension();
+  if (dim >= getInputRank()) {
+    return op->emitOpError("reduction dimension exceeds input rank");
```

**Comment:**
Also here: print what the wrong value is

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:934`

```diff
@@ -908,6 +908,62 @@ TopkOp::reifyResultShapes(OpBuilder &b,
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult ArgmaxOp::verify() {
+  Operation *op = getOperation();
+
+  // Check number of inputs and outputs.
+  if (getNumDpsInputs() != 1) {
+    return op->emitOpError("expected exactly one input operand (values)");
+  }
+
+  if (getNumDpsInits() != 2) {
+    return op->emitOpError("expected two output operands (value and index)");
+  }
+
+  uint64_t dim = getDimension();
+  if (dim >= getInputRank()) {
+    return op->emitOpError("reduction dimension exceeds input rank");
+  }
+
+  ShapedType inputType = getInputType();
+  ShapedType outputValueType = cast<ShapedType>(outputValue().getType());
+  ShapedType outputIndexType = cast<ShapedType>(outputIndex().getType());
```

**Comment:**
```suggestion
  auto outputValueType = cast<ShapedType>(outputValue().getType());
  auto outputIndexType = cast<ShapedType>(outputIndex().getType());
```
See https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:936`

```diff
@@ -908,6 +908,62 @@ TopkOp::reifyResultShapes(OpBuilder &b,
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult ArgmaxOp::verify() {
+  Operation *op = getOperation();
+
+  // Check number of inputs and outputs.
+  if (getNumDpsInputs() != 1) {
+    return op->emitOpError("expected exactly one input operand (values)");
+  }
+
+  if (getNumDpsInits() != 2) {
+    return op->emitOpError("expected two output operands (value and index)");
+  }
+
+  uint64_t dim = getDimension();
+  if (dim >= getInputRank()) {
+    return op->emitOpError("reduction dimension exceeds input rank");
+  }
+
+  ShapedType inputType = getInputType();
+  ShapedType outputValueType = cast<ShapedType>(outputValue().getType());
+  ShapedType outputIndexType = cast<ShapedType>(outputIndex().getType());
+
+  // Element type compatibility.
```

**Comment:**
I don't think this comment adds much value -- it's pretty clear what is being check. In general, focus on the *why* when writing the comments, not on what the code does.

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:941`

```diff
@@ -908,6 +908,62 @@ TopkOp::reifyResultShapes(OpBuilder &b,
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult ArgmaxOp::verify() {
+  Operation *op = getOperation();
+
+  // Check number of inputs and outputs.
+  if (getNumDpsInputs() != 1) {
+    return op->emitOpError("expected exactly one input operand (values)");
+  }
+
+  if (getNumDpsInits() != 2) {
+    return op->emitOpError("expected two output operands (value and index)");
+  }
+
+  uint64_t dim = getDimension();
+  if (dim >= getInputRank()) {
+    return op->emitOpError("reduction dimension exceeds input rank");
+  }
+
+  ShapedType inputType = getInputType();
+  ShapedType outputValueType = cast<ShapedType>(outputValue().getType());
+  ShapedType outputIndexType = cast<ShapedType>(outputIndex().getType());
+
+  // Element type compatibility.
+  if (inputType.getElementType() != outputValueType.getElementType()) {
+    return op->emitOpError("input and output value element types must match");
+  }
+
+  // Output indicies and values must have the same shape.
```

**Comment:**
Same here...

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:952`

```diff
@@ -908,6 +908,62 @@ TopkOp::reifyResultShapes(OpBuilder &b,
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult ArgmaxOp::verify() {
+  Operation *op = getOperation();
+
+  // Check number of inputs and outputs.
+  if (getNumDpsInputs() != 1) {
+    return op->emitOpError("expected exactly one input operand (values)");
+  }
+
+  if (getNumDpsInits() != 2) {
+    return op->emitOpError("expected two output operands (value and index)");
+  }
+
+  uint64_t dim = getDimension();
+  if (dim >= getInputRank()) {
+    return op->emitOpError("reduction dimension exceeds input rank");
+  }
+
+  ShapedType inputType = getInputType();
+  ShapedType outputValueType = cast<ShapedType>(outputValue().getType());
+  ShapedType outputIndexType = cast<ShapedType>(outputIndex().getType());
+
+  // Element type compatibility.
+  if (inputType.getElementType() != outputValueType.getElementType()) {
+    return op->emitOpError("input and output value element types must match");
+  }
+
+  // Output indicies and values must have the same shape.
+  if (failed(verifyCompatibleShape(outputValueType, outputIndexType))) {
+    return op->emitOpError("output indices/values shape must match");
+  }
+
+  // Expected output shape = input shape with `dim` removed.
+  SmallVector<int64_t> expectedShape;
+  for (int64_t i = 0; i < getInputRank(); ++i) {
+    if (static_cast<uint64_t>(i) != dim)
+      expectedShape.push_back(inputType.getDimSize(i));
+  }
+  if (!llvm::equal(expectedShape, outputValueType.getShape())) {
```

**Comment:**
nit: I think `==` would work here?

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:954`

```diff
@@ -908,6 +908,62 @@ TopkOp::reifyResultShapes(OpBuilder &b,
       .reifyResultShapes(b, reifiedReturnShapes);
 }
 
+//===----------------------------------------------------------------------===//
+// ArgmaxOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult ArgmaxOp::verify() {
+  Operation *op = getOperation();
+
+  // Check number of inputs and outputs.
+  if (getNumDpsInputs() != 1) {
+    return op->emitOpError("expected exactly one input operand (values)");
+  }
+
+  if (getNumDpsInits() != 2) {
+    return op->emitOpError("expected two output operands (value and index)");
+  }
+
+  uint64_t dim = getDimension();
+  if (dim >= getInputRank()) {
+    return op->emitOpError("reduction dimension exceeds input rank");
+  }
+
+  ShapedType inputType = getInputType();
+  ShapedType outputValueType = cast<ShapedType>(outputValue().getType());
+  ShapedType outputIndexType = cast<ShapedType>(outputIndex().getType());
+
+  // Element type compatibility.
+  if (inputType.getElementType() != outputValueType.getElementType()) {
+    return op->emitOpError("input and output value element types must match");
+  }
+
+  // Output indicies and values must have the same shape.
+  if (failed(verifyCompatibleShape(outputValueType, outputIndexType))) {
+    return op->emitOpError("output indices/values shape must match");
+  }
+
+  // Expected output shape = input shape with `dim` removed.
+  SmallVector<int64_t> expectedShape;
+  for (int64_t i = 0; i < getInputRank(); ++i) {
+    if (static_cast<uint64_t>(i) != dim)
+      expectedShape.push_back(inputType.getDimSize(i));
+  }
+  if (!llvm::equal(expectedShape, outputValueType.getShape())) {
+    return op->emitOpError(
+        "output shape must match input shape with reduction dimension removed");
```

**Comment:**
I think it would help to print the expected and the actual shape

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.td:710`

```diff
@@ -668,6 +668,60 @@ def IREELinalgExt_TopkOp : IREELinalgExt_Op<"topk",[
   }];
 }
 
+def IREELinalgExt_ArgmaxOp : IREELinalgExt_Op<"argmax", [
+  DeclareOpInterfaceMethods<ReifyRankedShapedTypeOpInterface>
+]> {
+  let summary = "Argmax reduction op.";
+  let description = [{
+    An argmax op that reduces along a given dimension, returning the max value and its index.
+  }];
+
+  let arguments = (ins
+    Variadic<AnyShaped>:$inputs,
+    Variadic<AnyShaped>:$outputs,
+    I64Attr:$dimension
+  );
+
+  let results = (outs
+    Variadic<AnyRankedTensor>:$results
+  );
+
+let assemblyFormat = [{
+    attr-dict
+    `dimension` `(` $dimension `)`
+    (`ins` `(` $inputs^ `:` type($inputs) `)`)?
+    `outs` `(` $outputs `:` type($outputs) `)`
+    (`:` type($results)^)?
+  }];
+
+  let extraClassDeclaration = extraLinalgExtOpClassDeclaration # [{
+    Value getInputValue() {
+      return getDpsInputOperand(0)->get();
+    }
+
+    Value outputValue() {
+      return getDpsInitOperand(0)->get();
+    }
+
+    Value outputIndex() {
+      return getDpsInitOperand(1)->get();
+    }
+
+    ShapedType getInputType() {
```

**Comment:**
Do we also want `getOutput*Type` for index and value?

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:951`

```diff
@@ -915,43 +915,65 @@ TopkOp::reifyResultShapes(OpBuilder &b,
 LogicalResult ArgmaxOp::verify() {
   Operation *op = getOperation();
 
-  // Check number of inputs and outputs.
   if (getNumDpsInputs() != 1) {
-    return op->emitOpError("expected exactly one input operand (values)");
+    return op->emitOpError(
+               "expected exactly one input operand (values), but got ")
+           << getNumDpsInputs();
   }
 
   if (getNumDpsInits() != 2) {
-    return op->emitOpError("expected two output operands (value and index)");
+    return op->emitOpError(
+               "expected two output operands (value and index), but got ")
+           << getNumDpsInits();
   }
 
   uint64_t dim = getDimension();
-  if (dim >= getInputRank()) {
-    return op->emitOpError("reduction dimension exceeds input rank");
+  int64_t rank = getInputRank();
+  if (dim >= rank) {
+    return op->emitOpError("reduction dimension exceeds or equals input rank. ")
+           << "got dimension: " << dim << ", but input rank is: " << rank;
   }
 
   ShapedType inputType = getInputType();
-  ShapedType outputValueType = cast<ShapedType>(outputValue().getType());
-  ShapedType outputIndexType = cast<ShapedType>(outputIndex().getType());
+  auto outputValueType = getOutputValueType();
+  auto outputIndexType = getOutputIndexType();
 
-  // Element type compatibility.
   if (inputType.getElementType() != outputValueType.getElementType()) {
-    return op->emitOpError("input and output value element types must match");
+    return op->emitOpError("input and output value element types must match. ")
+           << "Input type: " << inputType.getElementType()
+           << ", output value type: " << outputValueType.getElementType();
   }
 
-  // Output indicies and values must have the same shape.
   if (failed(verifyCompatibleShape(outputValueType, outputIndexType))) {
-    return op->emitOpError("output indices/values shape must match");
+    return op->emitOpError("output indices/values shape must match. ")
+           << "Output value shape: ["
+           << llvm::join(map_range(outputValueType.getShape(),
+                                   [](int64_t d) { return std::to_string(d); }),
```

**Comment:**
use `llvm::interleaved_array`

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:955`

```diff
@@ -915,43 +915,65 @@ TopkOp::reifyResultShapes(OpBuilder &b,
 LogicalResult ArgmaxOp::verify() {
   Operation *op = getOperation();
 
-  // Check number of inputs and outputs.
   if (getNumDpsInputs() != 1) {
-    return op->emitOpError("expected exactly one input operand (values)");
+    return op->emitOpError(
+               "expected exactly one input operand (values), but got ")
+           << getNumDpsInputs();
   }
 
   if (getNumDpsInits() != 2) {
-    return op->emitOpError("expected two output operands (value and index)");
+    return op->emitOpError(
+               "expected two output operands (value and index), but got ")
+           << getNumDpsInits();
   }
 
   uint64_t dim = getDimension();
-  if (dim >= getInputRank()) {
-    return op->emitOpError("reduction dimension exceeds input rank");
+  int64_t rank = getInputRank();
+  if (dim >= rank) {
+    return op->emitOpError("reduction dimension exceeds or equals input rank. ")
+           << "got dimension: " << dim << ", but input rank is: " << rank;
   }
 
   ShapedType inputType = getInputType();
-  ShapedType outputValueType = cast<ShapedType>(outputValue().getType());
-  ShapedType outputIndexType = cast<ShapedType>(outputIndex().getType());
+  auto outputValueType = getOutputValueType();
+  auto outputIndexType = getOutputIndexType();
 
-  // Element type compatibility.
   if (inputType.getElementType() != outputValueType.getElementType()) {
-    return op->emitOpError("input and output value element types must match");
+    return op->emitOpError("input and output value element types must match. ")
+           << "Input type: " << inputType.getElementType()
+           << ", output value type: " << outputValueType.getElementType();
   }
 
-  // Output indicies and values must have the same shape.
   if (failed(verifyCompatibleShape(outputValueType, outputIndexType))) {
-    return op->emitOpError("output indices/values shape must match");
+    return op->emitOpError("output indices/values shape must match. ")
+           << "Output value shape: ["
+           << llvm::join(map_range(outputValueType.getShape(),
+                                   [](int64_t d) { return std::to_string(d); }),
+                         ", ")
+           << "]" << ", output index shape: ["
+           << llvm::join(map_range(outputIndexType.getShape(),
+                                   [](int64_t d) { return std::to_string(d); }),
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:974`

```diff
@@ -915,43 +915,65 @@ TopkOp::reifyResultShapes(OpBuilder &b,
 LogicalResult ArgmaxOp::verify() {
   Operation *op = getOperation();
 
-  // Check number of inputs and outputs.
   if (getNumDpsInputs() != 1) {
-    return op->emitOpError("expected exactly one input operand (values)");
+    return op->emitOpError(
+               "expected exactly one input operand (values), but got ")
+           << getNumDpsInputs();
   }
 
   if (getNumDpsInits() != 2) {
-    return op->emitOpError("expected two output operands (value and index)");
+    return op->emitOpError(
+               "expected two output operands (value and index), but got ")
+           << getNumDpsInits();
   }
 
   uint64_t dim = getDimension();
-  if (dim >= getInputRank()) {
-    return op->emitOpError("reduction dimension exceeds input rank");
+  int64_t rank = getInputRank();
+  if (dim >= rank) {
+    return op->emitOpError("reduction dimension exceeds or equals input rank. ")
+           << "got dimension: " << dim << ", but input rank is: " << rank;
   }
 
   ShapedType inputType = getInputType();
-  ShapedType outputValueType = cast<ShapedType>(outputValue().getType());
-  ShapedType outputIndexType = cast<ShapedType>(outputIndex().getType());
+  auto outputValueType = getOutputValueType();
+  auto outputIndexType = getOutputIndexType();
 
-  // Element type compatibility.
   if (inputType.getElementType() != outputValueType.getElementType()) {
-    return op->emitOpError("input and output value element types must match");
+    return op->emitOpError("input and output value element types must match. ")
+           << "Input type: " << inputType.getElementType()
+           << ", output value type: " << outputValueType.getElementType();
   }
 
-  // Output indicies and values must have the same shape.
   if (failed(verifyCompatibleShape(outputValueType, outputIndexType))) {
-    return op->emitOpError("output indices/values shape must match");
+    return op->emitOpError("output indices/values shape must match. ")
+           << "Output value shape: ["
+           << llvm::join(map_range(outputValueType.getShape(),
+                                   [](int64_t d) { return std::to_string(d); }),
+                         ", ")
+           << "]" << ", output index shape: ["
+           << llvm::join(map_range(outputIndexType.getShape(),
+                                   [](int64_t d) { return std::to_string(d); }),
+                         ", ")
+           << "]";
   }
 
-  // Expected output shape = input shape with `dim` removed.
   SmallVector<int64_t> expectedShape;
   for (int64_t i = 0; i < getInputRank(); ++i) {
-    if (static_cast<uint64_t>(i) != dim)
+    if (i != dim)
       expectedShape.push_back(inputType.getDimSize(i));
   }
   if (!llvm::equal(expectedShape, outputValueType.getShape())) {
-    return op->emitOpError(
-        "output shape must match input shape with reduction dimension removed");
+    return op->emitOpError("output shape must match input shape with reduction "
+                           "dimension removed. ")
+           << "Expected: ["
+           << llvm::join(map_range(expectedShape,
+                                   [](int64_t d) { return std::to_string(d); }),
+                         ", ")
+           << "]" << ", but got: ["
+           << llvm::join(map_range(outputValueType.getShape(),
+                                   [](int64_t d) { return std::to_string(d); }),
```

**Comment:**
Also here

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:953`

```diff
@@ -946,15 +947,10 @@ LogicalResult ArgmaxOp::verify() {
 
   if (failed(verifyCompatibleShape(outputValueType, outputIndexType))) {
     return op->emitOpError("output indices/values shape must match. ")
-           << "Output value shape: ["
-           << llvm::join(map_range(outputValueType.getShape(),
-                                   [](int64_t d) { return std::to_string(d); }),
-                         ", ")
-           << "]" << ", output index shape: ["
-           << llvm::join(map_range(outputIndexType.getShape(),
-                                   [](int64_t d) { return std::to_string(d); }),
-                         ", ")
-           << "]";
+           << "Output value shape: "
+           << llvm::interleaved_array(outputValueType.getShape(), ", ")
+           << ", output index shape: "
+           << llvm::interleaved_array(outputIndexType.getShape(), ", ");
```

**Comment:**
```suggestion
           << llvm::interleaved_array(outputValueType.getShape())
           << ", output index shape: "
           << llvm::interleaved_array(outputIndexType.getShape());
```

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/IR/LinalgExtOps.cpp:966`

```diff
@@ -965,15 +961,9 @@ LogicalResult ArgmaxOp::verify() {
   if (!llvm::equal(expectedShape, outputValueType.getShape())) {
     return op->emitOpError("output shape must match input shape with reduction "
                            "dimension removed. ")
-           << "Expected: ["
-           << llvm::join(map_range(expectedShape,
-                                   [](int64_t d) { return std::to_string(d); }),
-                         ", ")
-           << "]" << ", but got: ["
-           << llvm::join(map_range(outputValueType.getShape(),
-                                   [](int64_t d) { return std::to_string(d); }),
-                         ", ")
-           << "]";
+           << "Expected: " << llvm::interleaved_array(expectedShape, ", ")
+           << ", but got: "
+           << llvm::interleaved_array(outputValueType.getShape(), ", ");
```

**Comment:**
```suggestion
           << "Expected: " << llvm::interleaved_array(expectedShape)
           << ", but got: "
           << llvm::interleaved_array(outputValueType.getShape());
```

---


---


## [PR #20906](https://github.com/iree-org/iree/pull/20906): [Codegen] split-k on argmax to ensure ukernel support

### Review Summary

**COMMENTED** (2025-06-04)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/test/gpu_lower_to_ukernels.mlir:147`

```diff
@@ -119,6 +119,48 @@ func.func @argmax_f32i64_without_selected_ukernel(%arg0 : tensor<1x?xf32>) -> te
 
 // -----
 
+func.func @argmax_invalid_index_without_selected_ukernel(
+    %input: tensor<131072xf32>,
+    %init_val_arg: tensor<f32>,
+    %init_idx_arg: tensor<i64>
+) -> tensor<i64> {
+  %c0_i64 = arith.constant 0 : i64
+  %cst_min = arith.constant 0xFF800000 : f32  // -inf
+  %init_val = linalg.fill ins(%cst_min : f32)
+              outs(%init_val_arg : tensor<f32>) -> tensor<f32>
+  %init_idx = linalg.fill ins(%c0_i64 : i64)
+              outs(%init_idx_arg : tensor<i64>) -> tensor<i64>
+
+  // Argmax-style reduction with a matcher-breaking intermediate op (`addi`).
+  %result:2 = linalg.generic {
+      indexing_maps = [
+        affine_map<(d0) -> (d0)>,
+        affine_map<(d0) -> ()>,
+        affine_map<(d0) -> ()>
+      ],
+      iterator_types = ["reduction"]
+    } ins(%input : tensor<131072xf32>)
+      outs(%init_val, %init_idx : tensor<f32>, tensor<i64>) {
+    ^bb0(%in: f32, %val: f32, %idx: i64):
+      %i = linalg.index 0 : index
+      %cast = arith.index_cast %i : index to i64
+      // Breaks isArgmaxOp matching
```

**Comment:**
```suggestion
      // Breaks isArgmaxOp matching.
```

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/Transforms/SplitReduction.cpp:644`

```diff
@@ -641,39 +641,30 @@ splitArgmaxReduction(RewriterBase &rewriter, linalg::GenericOp genericOp,
     }
   }
 
-  // Create partial linalg.generic op with global index computation.
-  Value tileSize = rewriter.create<arith::ConstantIndexOp>(loc, ratio);
-  auto partialOp = rewriter.create<linalg::GenericOp>(
+  // Step 1: Create a pure argmax to partially reduce the split dimension. The
```

**Comment:**
What do you mean by 'pure argmax'?

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/Transforms/SplitReduction.cpp:652`

```diff
@@ -641,39 +641,30 @@ splitArgmaxReduction(RewriterBase &rewriter, linalg::GenericOp genericOp,
     }
   }
 
-  // Create partial linalg.generic op with global index computation.
-  Value tileSize = rewriter.create<arith::ConstantIndexOp>(loc, ratio);
-  auto partialOp = rewriter.create<linalg::GenericOp>(
+  // Step 1: Create a pure argmax to partially reduce the split dimension. The
+  // result will contain local indices within each reduction group, which need
+  // to be adjusted to the global index later.
+  auto partialArgmax = rewriter.create<linalg::GenericOp>(
       loc, TypeRange{identityValue.getType(), identityIndex.getType()},
       newInputs, ValueRange{identityValue, identityIndex}, newMaps,
-      newIteratorTypes);
-
-  rewriter.inlineRegionBefore(genericOp.getRegion(), partialOp.getRegion(),
-                              partialOp.getRegion().begin());
-
-  Block &body = partialOp.getRegion().front();
-  rewriter.setInsertionPointToStart(&body);
-
-  unsigned innerIdxDim = reductionDim + 1;
-  unsigned outerIdxDim = insertSplitDimension;
-
-  // Compute global index (gidx) for reduction when the original reduction
-  // dimension is split into [outerIdx, innerIdx] using `ratio`. This is used to
-  // correctly compute the global index for comparisons and index selection.
-  Value outerIdx = rewriter.create<linalg::IndexOp>(loc, outerIdxDim);
-  Value innerIdx = rewriter.create<linalg::IndexOp>(loc, innerIdxDim);
-  Value offset = rewriter.create<arith::MulIOp>(loc, outerIdx, tileSize);
-  Value gidx = rewriter.create<arith::AddIOp>(loc, offset, innerIdx);
-
-  auto selectOp = dyn_cast<arith::SelectOp>(combinerOps.selectOp);
-  Value oldIdx = selectOp.getTrueValue();
-  Value newIdx = gidx;
-  if (oldIdx.getType() != gidx.getType()) {
-    newIdx = rewriter.create<arith::IndexCastOp>(loc, oldIdx.getType(), gidx);
-  }
-  selectOp.setOperand(1, newIdx);
-  rewriter.setInsertionPointAfter(partialOp);
+      newIteratorTypes,
+      [reductionDim](OpBuilder &b, Location loc, ValueRange args) {
+        Value in = args[0], outVal = args[1], outIdx = args[2];
```

**Comment:**
nit: llvm discourages defining multiple variables on a single line.

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/Transforms/SplitReduction.cpp:694`

```diff
@@ -693,19 +684,26 @@ splitArgmaxReduction(RewriterBase &rewriter, linalg::GenericOp genericOp,
       AffineMap::get(intermRank, 0, resultExprs, rewriter.getContext());
   SmallVector<AffineMap> finalReductionMaps = {valueMap, indexMap, outputMap,
                                                outputMap};
-
-  // Create block for final reduction region.
   auto finalReduction = rewriter.create<linalg::GenericOp>(
       loc, genericOp.getResultTypes(),
-      ValueRange{partialOp.getResult(0), partialOp.getResult(1)},
+      ValueRange{partialArgmax.getResult(0), partialArgmax.getResult(1)},
       genericOp.getDpsInits(), finalReductionMaps, reductionIteratorTypes,
-      [combinerOps](OpBuilder &b, Location loc, ValueRange inputs) {
+      [combinerOps, tileSize, insertSplitDimension](OpBuilder &b, Location loc,
+                                                    ValueRange inputs) {
+        Value val = inputs[0], local = inputs[1];
+        Value outVal = inputs[2], outIdx = inputs[3];
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Dialect/LinalgExt/Utils/Utils.cpp:894`

```diff
@@ -891,6 +891,15 @@ bool isArgmaxOp(linalg::GenericOp genericOp) {
     if (!matchPattern(producer, m_Op<arith::SelectOp>())) {
       return false;
     }
+    auto selectOp = dyn_cast<arith::SelectOp>(producerOutput.getDefiningOp());
```

**Comment:**
Use `cast` if you require the result to be a select. Otherwise check if the result is not null. See https://llvm.org/docs/ProgrammersManual.html#the-isa-cast-and-dyn-cast-templates

---


---


## [PR #20438](https://github.com/iree-org/iree/pull/20438): [tuner] expose python binding for getting the tuner root ops

### Review Summary

**CHANGES_REQUESTED** (2025-04-02)

Although the logic is straightforward, I think this needs tests to make sure everything is threaded through properly across iree / c / python.


**COMMENTED** (2025-04-02)


**COMMENTED** (2025-04-02)


**APPROVED** (2025-04-09)

Looks good % nit


### Code Comments

**File:** `compiler/bindings/python/test/ir/dialects_test.py:329`

```diff
@@ -309,3 +309,22 @@ def compilation_info():
     assert compilation_info is not None
     assert compilation_info.lowering_config == lowering_config
     assert compilation_info.translation_info == translation_info
+
+
+@run
+def root_op():
+    module_str = """
+        module {
+            func.func @matmul(%arg0: tensor<4x4xf32>, %arg1: tensor<4x4xf32>) -> tensor<4x4xf32> {
+                %cst = arith.constant 0.000000e+00 : f32
+                %0 = tensor.empty() : tensor<4x4xf32>
+                %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<4x4xf32>) -> tensor<4x4xf32>
+                %2 = linalg.matmul { root_op } ins(%arg0, %arg1 : tensor<4x4xf32>, tensor<4x4xf32>) outs(%1 : tensor<4x4xf32>) -> tensor<4x4xf32>
+                return %2 : tensor<4x4xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
+    root_op_list = iree_codegen.get_tuner_root_ops(input_module)
+    assert len(root_op_list) == 1
```

**Comment:**
Could you add two more test cases:
1. no root ops
2. two or more root ops

This is so that we can exercise the vector population logic.

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:315`

```diff
@@ -309,3 +309,22 @@ def compilation_info():
     assert compilation_info is not None
     assert compilation_info.lowering_config == lowering_config
     assert compilation_info.translation_info == translation_info
+
+
+@run
+def root_op():
```

**Comment:**
This test is not a dialect test, we should move it to a different file

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1055`

```diff
@@ -1049,4 +1049,17 @@ queryMMAIntrinsics(IREE::HAL::ExecutableVariantOp executableOp) {
   return mmaIntrinsics;
 }
 
+SmallVector<Operation *> getTunerRootOps(mlir::ModuleOp moduleOp) {
+  SmallVector<Operation *> rootOps;
+
+  // Walk all operations in the module recursively
```

**Comment:**
https://llvm.org/docs/CodingStandards.html#commenting

---

**File:** `compiler/bindings/python/test/ir/tuner_test.py:39`

```diff
@@ -0,0 +1,46 @@
+# Copyright 2025 The IREE Authors
+#
+# Licensed under the Apache License v2.0 with LLVM Exceptions.
+# See https://llvm.org/LICENSE.txt for license information.
+# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+from iree.compiler import ir
+from iree.compiler.dialects import iree_codegen
+
+
+def run(fn):
+    with ir.Context(), ir.Location.unknown():
+        module = ir.Module.create()
+        with ir.InsertionPoint(module.body):
+            print("\nTEST:", fn.__name__)
+            fn()
+    return fn
+
+
+@run
+def root_op():
+    module_str = """
+        module {
+            func.func @matmul(%arg0: tensor<4x4xf32>, %arg1: tensor<4x4xf32>) -> tensor<4x4xf32> {
+                %cst = arith.constant 0.000000e+00 : f32
+                %0 = tensor.empty() : tensor<4x4xf32>
+                %1 = linalg.fill { root_op } ins(%cst : f32) outs(%0 : tensor<4x4xf32>) -> tensor<4x4xf32>
+                %2 = linalg.matmul { root_op } ins(%arg0, %arg1 : tensor<4x4xf32>, tensor<4x4xf32>) outs(%1 : tensor<4x4xf32>) -> tensor<4x4xf32>
+                return %2 : tensor<4x4xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
+    root_op_list = iree_codegen.get_tuner_root_ops(input_module)
+    assert len(root_op_list) == 2
+    assert root_op_list[0].name == "linalg.fill"
+    assert root_op_list[1].name == "linalg.matmul"
+
+    del root_op_list[0].attributes["root_op"]
```

**Comment:**
I don't think python is supposed to modify IR like this [1]: there may be some dangling references to IR objects. Instead, can you make it a few separate test cases with their own individual inputs?

[1] https://mlir.llvm.org/docs/Bindings/Python/#ownership-in-the-core-ir

---

**File:** `compiler/bindings/python/test/ir/tuner_test.py:21`

```diff
@@ -0,0 +1,46 @@
+# Copyright 2025 The IREE Authors
+#
+# Licensed under the Apache License v2.0 with LLVM Exceptions.
+# See https://llvm.org/LICENSE.txt for license information.
+# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+from iree.compiler import ir
+from iree.compiler.dialects import iree_codegen
+
+
+def run(fn):
+    with ir.Context(), ir.Location.unknown():
+        module = ir.Module.create()
+        with ir.InsertionPoint(module.body):
+            print("\nTEST:", fn.__name__)
+            fn()
+    return fn
+
+
+@run
+def root_op():
```

**Comment:**
I think this code should be in a directory outside of `ir`, like `compiler/bindings/python/test/api/tuner_api_test.py`

---

**File:** `compiler/bindings/python/test/api/tuner_api_test.py:23`

```diff
@@ -0,0 +1,68 @@
+# Copyright 2025 The IREE Authors
+#
+# Licensed under the Apache License v2.0 with LLVM Exceptions.
+# See https://llvm.org/LICENSE.txt for license information.
+# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+from iree.compiler import ir
+from iree.compiler.dialects import iree_codegen
+
+
+def run(fn):
+    with ir.Context(), ir.Location.unknown():
+        module = ir.Module.create()
+        with ir.InsertionPoint(module.body):
+            print("\nTEST:", fn.__name__)
+            fn()
+    return fn
+
+
+@run
+def root_op():
+    module_str = """
+        module {
```

**Comment:**
Can we use implicit modules here to make this a bit shorter?

---

**File:** `compiler/bindings/python/test/api/tuner_api_test.py:33`

```diff
@@ -0,0 +1,68 @@
+# Copyright 2025 The IREE Authors
+#
+# Licensed under the Apache License v2.0 with LLVM Exceptions.
+# See https://llvm.org/LICENSE.txt for license information.
+# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+from iree.compiler import ir
+from iree.compiler.dialects import iree_codegen
+
+
+def run(fn):
+    with ir.Context(), ir.Location.unknown():
+        module = ir.Module.create()
+        with ir.InsertionPoint(module.body):
+            print("\nTEST:", fn.__name__)
+            fn()
+    return fn
+
+
+@run
+def root_op():
+    module_str = """
+        module {
+            func.func @matmul(%arg0: tensor<4x4xf32>, %arg1: tensor<4x4xf32>) -> tensor<4x4xf32> {
+                %cst = arith.constant 0.000000e+00 : f32
+                %0 = tensor.empty() : tensor<4x4xf32>
+                %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<4x4xf32>) -> tensor<4x4xf32>
+                %2 = linalg.matmul ins(%arg0, %arg1 : tensor<4x4xf32>, tensor<4x4xf32>) outs(%1 : tensor<4x4xf32>) -> tensor<4x4xf32>
+                return %2 : tensor<4x4xf32>
+            }
+        }
+    """
+    input_module = ir.Module.parse(module_str)
```

**Comment:**
assert that the module parsed so that we can quickly identify issues with mlir syntax changes. Also in other testcases

---


---


## [PR #20246](https://github.com/iree-org/iree/pull/20246): Integrates/llvm 20250314 llvm/llvm-project@e45090e

### Review Summary

**APPROVED** (2025-03-15)



---


## [PR #20207](https://github.com/iree-org/iree/pull/20207): Integrates/llvm 20250310: Bump to llvm/llvm-project@967ab7e

### Review Summary

**COMMENTED** (2025-03-11)


**COMMENTED** (2025-03-11)


**COMMENTED** (2025-03-11)


**APPROVED** (2025-03-11)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/LLVMCPU/ConvertToLLVM.cpp:986`

```diff
@@ -975,15 +975,15 @@ void ConvertToLLVMPass::runOnOperation() {
     // TODO: doubtful that the "default" does what one want here, it is likely
     // better to use outerproduct.
     vector::populateVectorContractLoweringPatterns(
-        patterns, vector::VectorTransformsOptions());
+        patterns, vector::VectorContractLowering::Dot);
     vector::populateVectorMaskMaterializationPatterns(
         patterns, /*force32BitVectorIndices=*/false);
     vector::populateVectorMaskOpLoweringPatterns(patterns);
     vector::populateVectorShapeCastLoweringPatterns(patterns);
     // TODO: doubtful that the "default" does what one want here, it is likely
     // better to use shuffle.
     vector::populateVectorTransposeLoweringPatterns(
-        patterns, vector::VectorTransformsOptions());
+        patterns, vector::VectorTransposeLowering::EltWise);
```

**Comment:**
How did you decide these options? Could we also pick the default values for these new enums, e.g., `vector::VectorTransposeLowering()`?

Could you add the mapping to the PR description?

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMCPU/ConvertToLLVM.cpp:986`

```diff
@@ -975,15 +975,15 @@ void ConvertToLLVMPass::runOnOperation() {
     // TODO: doubtful that the "default" does what one want here, it is likely
     // better to use outerproduct.
     vector::populateVectorContractLoweringPatterns(
-        patterns, vector::VectorTransformsOptions());
+        patterns, vector::VectorContractLowering::Dot);
     vector::populateVectorMaskMaterializationPatterns(
         patterns, /*force32BitVectorIndices=*/false);
     vector::populateVectorMaskOpLoweringPatterns(patterns);
     vector::populateVectorShapeCastLoweringPatterns(patterns);
     // TODO: doubtful that the "default" does what one want here, it is likely
     // better to use shuffle.
     vector::populateVectorTransposeLoweringPatterns(
-        patterns, vector::VectorTransformsOptions());
+        patterns, vector::VectorTransposeLowering::EltWise);
```

**Comment:**
Why not do this then to pick up the defaults:
```c++
    VectorTransformsOptions defaultOptions;
    ...
    vector::populateVectorTransposeLoweringPatterns(
        patterns, defaultOptions.vectorTransposeLowering);
 ```

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMCPU/ConvertToLLVM.cpp:986`

```diff
@@ -975,15 +975,15 @@ void ConvertToLLVMPass::runOnOperation() {
     // TODO: doubtful that the "default" does what one want here, it is likely
     // better to use outerproduct.
     vector::populateVectorContractLoweringPatterns(
-        patterns, vector::VectorTransformsOptions());
+        patterns, vector::VectorContractLowering::Dot);
     vector::populateVectorMaskMaterializationPatterns(
         patterns, /*force32BitVectorIndices=*/false);
     vector::populateVectorMaskOpLoweringPatterns(patterns);
     vector::populateVectorShapeCastLoweringPatterns(patterns);
     // TODO: doubtful that the "default" does what one want here, it is likely
     // better to use shuffle.
     vector::populateVectorTransposeLoweringPatterns(
-        patterns, vector::VectorTransformsOptions());
+        patterns, vector::VectorTransposeLowering::EltWise);
```

**Comment:**
This is trivial to be optimized out for any c++ compiler -- just SROA and DCE

---


---


## [PR #20173](https://github.com/iree-org/iree/pull/20173): [Codegen][Tuner] improve verifier for the default attribute

### Review Summary

**CHANGES_REQUESTED** (2025-03-06)

Could you also update the documentation in https://iree.dev/reference/tuning/ ?


**COMMENTED** (2025-03-06)


**COMMENTED** (2025-03-06)


**COMMENTED** (2025-03-07)


**COMMENTED** (2025-03-07)


**CHANGES_REQUESTED** (2025-03-10)


**COMMENTED** (2025-03-10)


**COMMENTED** (2025-03-10)


**COMMENTED** (2025-03-12)

The new tests look solid, just a few remaining issues


**COMMENTED** (2025-03-12)


**COMMENTED** (2025-03-12)


**APPROVED** (2025-03-12)

LGTM


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:261`

```diff
@@ -103,3 +103,30 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
       transform.yield %res_b : !transform.any_op
   }
 }
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
```

**Comment:**
Do we need these to check for this error? Let's make sure we minimize the amount of unrelated code and keep the tests minimal

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:77`

```diff
@@ -58,17 +58,25 @@ IREECodegenDialect::verifyOperationAttribute(Operation *op,
   StringRef symbol = attribute.getName().strref();
   Attribute attr = attribute.getValue();
   // This function verifies the validity of a specific operation attribute.
-  // - If the attribute's name matches `kTuningDefaultSpecAttrName`, make
-  //   sure it contains a single named sequence op with name `__kernel_config`.
+  // - If the attribute's name matches kTuningSpecDefaultEntrypointAttrName
+  // (`iree_codegen.tuning_spec_with_default_entrypoint`):
+  //   1. Ensure that the module contains a single named sequence operation with
+  //   the name `__kernel_config`.
+  //   2. Verify that this `__kernel_config` named sequence operation has the
+  //   attribute `iree_codegen.tuning_spec_entrypoint`.
+  //   3. Ensure that the named sequence operation contains exactly **one**
+  //   `ForeachMatchOp`.
+  //   4. Ensure that only one named sequence operation with the
+  //   `iree_codegen.tuning_spec_entrypoint` attribute.
   // - If the attribute's name matches `kTuningSpecEntrypointAttrName`
-  // ("iree_codegen.tuning_spec_entrypoint"):
+  // (`iree_codegen.tuning_spec_entrypoint`):
   //   1. The attribute value must be a UnitAttr.
   //   2. If the operation is a transform::NamedSequenceOp:
   //      - The operation's function signature must satisfy the following:
-  //         a. It must have exactly one result type, and the result must be of
-  //         type `transform::AnyOpType`.
-  //         b. It must have exactly one argument type, and the argument must be
-  //         of type `transform::AnyOpType`.
+  //         a. It must have exactly one result type, and the result must be
+  //         of type `transform::AnyOpType`. b. It must have exactly one
```

**Comment:**
b. should be on a new line

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:99`

```diff
@@ -66,3 +66,67 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
```

**Comment:**
What if there are other ops like `transform.include`?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:99`

```diff
@@ -66,3 +66,67 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
```

**Comment:**
I'm thinking whether requiring a single for_each only would make merging simpler

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:99`

```diff
@@ -66,3 +66,67 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
```

**Comment:**
Let's check for this then

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/materialize_tuning_specs.mlir:14`

```diff
@@ -11,7 +11,6 @@
 // Check that the final tuning spec is as expected when the user tuning spec is provided.
 
 // CHECK-LABEL: module @iree_linked_tuning_spec
-// CHECK-SAME:    iree_codegen.tuning_spec_with_default_entrypoint
```

**Comment:**
I think this effectively break this test. This is fine, but we shoulda add a TODO to fix it once the merging logic can handle foreach_match ops.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:118`

```diff
@@ -66,3 +66,56 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      %res_b = transform.foreach_match in %res_a @match_b -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      transform.yield %res_b : !transform.any_op
+  }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly})
+      -> !transform.any_op attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
```

**Comment:**
We should add a test that has a foreach_match op in `__kernel_config` and some other op like print or include.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:112`

```diff
@@ -58,29 +58,75 @@ IREECodegenDialect::verifyOperationAttribute(Operation *op,
   StringRef symbol = attribute.getName().strref();
   Attribute attr = attribute.getValue();
   // This function verifies the validity of a specific operation attribute.
-  // - If the attribute's name matches `kTuningDefaultSpecAttrName`, make
-  //   sure it contains a single named sequence op with name `__kernel_config`.
+  // - If the attribute's name matches kTuningSpecDefaultEntrypointAttrName
+  // (`iree_codegen.tuning_spec_with_default_entrypoint`):
+  //   1. Ensure that the module contains a single named sequence operation with
+  //   the name `__kernel_config`.
+  //   2. Verify that this `__kernel_config` named sequence operation has the
+  //   attribute `iree_codegen.tuning_spec_entrypoint`.
+  //   3. Ensure that the named sequence operation contains exactly **one**
+  //   `ForeachMatchOp`.
+  //   4. Ensure that only one named sequence operation with the
+  //   `iree_codegen.tuning_spec_entrypoint` attribute.
   // - If the attribute's name matches `kTuningSpecEntrypointAttrName`
-  // ("iree_codegen.tuning_spec_entrypoint"):
+  // (`iree_codegen.tuning_spec_entrypoint`):
   //   1. The attribute value must be a UnitAttr.
   //   2. If the operation is a transform::NamedSequenceOp:
   //      - The operation's function signature must satisfy the following:
-  //         a. It must have exactly one result type, and the result must be of
-  //         type `transform::AnyOpType`.
+  //         a. It must have exactly one result type, and the result must be
+  //         of type `transform::AnyOpType`.
   //         b. It must have exactly one argument type, and the argument must be
   //         of type `transform::AnyOpType`.
 
   if (symbol == kTuningSpecDefaultEntrypointAttrName) {
     if (auto moduleOp = dyn_cast<ModuleOp>(op)) {
-      if (!llvm::any_of(moduleOp.getOps<transform::NamedSequenceOp>(),
+      auto kernelConfigOpIt =
+          llvm::find_if(moduleOp.getOps<transform::NamedSequenceOp>(),
                         [](transform::NamedSequenceOp op) {
                           return op.getName() == kKernelConfigSpecName;
-                        })) {
+                        });
+
+      if (kernelConfigOpIt ==
+          moduleOp.getOps<transform::NamedSequenceOp>().end()) {
         return moduleOp.emitError()
-               << "The tuning specification must include a named "
-                  "sequence with the symbol name '"
+               << "The tuning specification must include a named sequence with "
+                  "the symbol name '"
                << kKernelConfigSpecName << "'.";
       }
+
+      transform::NamedSequenceOp kernelConfigOp = *kernelConfigOpIt;
+
+      // Verify that the kernelConfigOp has the attribute
+      // `iree_codegen.tuning_spec_entrypoint`.
+      if (!kernelConfigOp->hasAttr(kTuningSpecEntrypointAttrName)) {
+        return kernelConfigOp.emitError()
+               << "The named sequence '" << kKernelConfigSpecName
+               << "' must have the attribute '" << kTuningSpecEntrypointAttrName
+               << "'.";
+      }
+
+      auto tuningSpecOps = llvm::filter_to_vector(
+          moduleOp.getOps<transform::NamedSequenceOp>(),
+          [](transform::NamedSequenceOp op) {
+            return op->hasAttr(kTuningSpecEntrypointAttrName);
+          });
```

**Comment:**
What if we find ops that are not named sequences?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:123`

```diff
@@ -58,29 +58,75 @@ IREECodegenDialect::verifyOperationAttribute(Operation *op,
   StringRef symbol = attribute.getName().strref();
   Attribute attr = attribute.getValue();
   // This function verifies the validity of a specific operation attribute.
-  // - If the attribute's name matches `kTuningDefaultSpecAttrName`, make
-  //   sure it contains a single named sequence op with name `__kernel_config`.
+  // - If the attribute's name matches kTuningSpecDefaultEntrypointAttrName
+  // (`iree_codegen.tuning_spec_with_default_entrypoint`):
+  //   1. Ensure that the module contains a single named sequence operation with
+  //   the name `__kernel_config`.
+  //   2. Verify that this `__kernel_config` named sequence operation has the
+  //   attribute `iree_codegen.tuning_spec_entrypoint`.
+  //   3. Ensure that the named sequence operation contains exactly **one**
+  //   `ForeachMatchOp`.
+  //   4. Ensure that only one named sequence operation with the
+  //   `iree_codegen.tuning_spec_entrypoint` attribute.
   // - If the attribute's name matches `kTuningSpecEntrypointAttrName`
-  // ("iree_codegen.tuning_spec_entrypoint"):
+  // (`iree_codegen.tuning_spec_entrypoint`):
   //   1. The attribute value must be a UnitAttr.
   //   2. If the operation is a transform::NamedSequenceOp:
   //      - The operation's function signature must satisfy the following:
-  //         a. It must have exactly one result type, and the result must be of
-  //         type `transform::AnyOpType`.
+  //         a. It must have exactly one result type, and the result must be
+  //         of type `transform::AnyOpType`.
   //         b. It must have exactly one argument type, and the argument must be
   //         of type `transform::AnyOpType`.
 
   if (symbol == kTuningSpecDefaultEntrypointAttrName) {
     if (auto moduleOp = dyn_cast<ModuleOp>(op)) {
-      if (!llvm::any_of(moduleOp.getOps<transform::NamedSequenceOp>(),
+      auto kernelConfigOpIt =
+          llvm::find_if(moduleOp.getOps<transform::NamedSequenceOp>(),
                         [](transform::NamedSequenceOp op) {
                           return op.getName() == kKernelConfigSpecName;
-                        })) {
+                        });
+
+      if (kernelConfigOpIt ==
+          moduleOp.getOps<transform::NamedSequenceOp>().end()) {
         return moduleOp.emitError()
-               << "The tuning specification must include a named "
-                  "sequence with the symbol name '"
+               << "The tuning specification must include a named sequence with "
+                  "the symbol name '"
                << kKernelConfigSpecName << "'.";
       }
+
+      transform::NamedSequenceOp kernelConfigOp = *kernelConfigOpIt;
+
+      // Verify that the kernelConfigOp has the attribute
+      // `iree_codegen.tuning_spec_entrypoint`.
+      if (!kernelConfigOp->hasAttr(kTuningSpecEntrypointAttrName)) {
+        return kernelConfigOp.emitError()
+               << "The named sequence '" << kKernelConfigSpecName
+               << "' must have the attribute '" << kTuningSpecEntrypointAttrName
+               << "'.";
+      }
+
+      auto tuningSpecOps = llvm::filter_to_vector(
+          moduleOp.getOps<transform::NamedSequenceOp>(),
+          [](transform::NamedSequenceOp op) {
+            return op->hasAttr(kTuningSpecEntrypointAttrName);
+          });
+
+      if (tuningSpecOps.size() != 1) {
+        return moduleOp.emitError()
+               << "Expected exactly one NamedSequenceOp with the attribute '"
+               << kTuningSpecEntrypointAttrName << "', but found "
+               << tuningSpecOps.size() << ".";
+      }
+
+      // Ensure there is exactly one ForeachMatchOp inside the kernelConfigOp.
+      auto foreachMatchOps =
+          llvm::to_vector(kernelConfigOp.getOps<transform::ForeachMatchOp>());
```

**Comment:**
What if there's a single foreach_match but also some other ops like `transform.include`?

---

**File:** `docs/website/docs/reference/tuning.md:129`

```diff
@@ -124,7 +124,9 @@ that conform to the following format:
 * All entry points in the final tuning specs must either read
   (`transform.readonly`) or consume (`transform.consumed`) the argument.
 * The `iree_codegen.tuning_spec_with_default_entrypoint` attribute ensures that
-  the tuning spec includes a named sequence op with name `__kernel_config`.
+  the tuning spec includes a named sequence op with name `__kernel_config`, which
+  must contain exactly one `foreach_match` op. Furthermore, only one tuning spec
+  entry point is allowed, and it must be `__kernel_config` op.
```

**Comment:**
```suggestion
  the tuning spec includes a named sequence op with name `__kernel_config`, which
  must contain exactly one `foreach_match` op.
```
this seems redundant to me

---

**File:** `compiler/plugins/target/ROCM/test/default_tuning_specs_amdgpu.mlir:39`

```diff
@@ -36,7 +36,6 @@
 // materialized. The user spec should have precedence over the default one.
 
 // BOTH-LABEL: module @iree_linked_tuning_spec
-// BOTH-SAME:    iree_codegen.tuning_spec_with_default_entrypoint
```

**Comment:**
Can we add a TODO to revisit this test and re-add this CHECK when the new linking lands?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:86`

```diff
@@ -83,7 +83,6 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
       0, hasConsumedSequences ? kArgConsumedAttrName : kArgReadOnlyAttrName,
       builder.getUnitAttr());
   newSpec->setAttr(kTuningSpecEntrypointAttrName, builder.getUnitAttr());
-  module->setAttr(kTuningSpecDefaultEntrypointAttrName, builder.getUnitAttr());
```

**Comment:**
Also here: let's keep track of re-enabling this

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/tuning_spec_default.mlir:14`

```diff
@@ -1,9 +1,18 @@
 // RUN: iree-opt %s
 
 module @user_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
-  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
-    attributes { iree_codegen.tuning_spec_entrypoint } {
-    transform.print {name = "Hello Tuning Spec", skip_regions}
-    transform.yield %arg0 : !transform.any_op
+  transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+    transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+    transform.yield
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+      %res = transform.foreach_match in %arg0 @match -> @apply_op_config
```

**Comment:**
nit: put match on its own line -- I find it weird to have the whole loop with its body on a single line

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:95`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
```

**Comment:**
Can we add some clue as to where this rule comes from? This is related to the attributes you set. Otherwise the `__kernel_config` name is not special on its own.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:85`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
```

**Comment:**
We don't need this named sequence in this test -- we can check the same thing by having two foreach_match ops that use the same match function

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:109`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      %res_b = transform.foreach_match in %res_a @match_b -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      transform.yield %res_b : !transform.any_op
+  }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
```

**Comment:**
Also here: we should print which attribute adds this verification rule. It's not `tuning_spec_entrypoint` that's at issue

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:140`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      %res_b = transform.foreach_match in %res_a @match_b -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      transform.yield %res_b : !transform.any_op
+  }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly})
+      -> !transform.any_op attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'transform::YieldOp', but found 2}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+        transform.yield %res : !transform.any_op
```

**Comment:**
Two yields don't make sense -- I'd expect this to be a terminator. Instead of checking that there is exact one yield and one foreach_match, we can check that the only two ops are foreach_match and yield without counting how many there are.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:170`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      %res_b = transform.foreach_match in %res_a @match_b -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      transform.yield %res_b : !transform.any_op
+  }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly})
+      -> !transform.any_op attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'transform::YieldOp', but found 2}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  module @extra_module attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.include}}
```

**Comment:**
The quote is not closed

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:193`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      %res_b = transform.foreach_match in %res_a @match_b -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      transform.yield %res_b : !transform.any_op
+  }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly})
+      -> !transform.any_op attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'transform::YieldOp', but found 2}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  module @extra_module attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.include}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %tmp = transform.include @dummy_func failures(suppress) (%arg0) : (!transform.any_op) -> (!transform.any_op)
+        %res = transform.foreach_match in %tmp @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.print}}
```

**Comment:**
Something is wrong with this error message -- the closing quote is missing and there's no space before 'but'.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:220`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      %res_b = transform.foreach_match in %res_a @match_b -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      transform.yield %res_b : !transform.any_op
+  }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly})
+      -> !transform.any_op attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'transform::YieldOp', but found 2}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  module @extra_module attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.include}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %tmp = transform.include @dummy_func failures(suppress) (%arg0) : (!transform.any_op) -> (!transform.any_op)
+        %res = transform.foreach_match in %tmp @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.print}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+         transform.print {name = "Hello"}
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op {transform.readonly})
+    -> (!transform.any_op, !transform.any_op) {
+    transform.yield %arg, %arg : !transform.any_op, !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op1: !transform.any_op {transform.readonly}, %op2: !transform.any_op {transform.readonly})
+    -> (!transform.any_op) {
+    transform.yield %op1 : !transform.any_op
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+    -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must return exactly one any_op result}}
+    %res1, %res2 = transform.foreach_match in %arg0 @match -> @apply_op_config
```

**Comment:**
Also here: let's print where this rule comes from

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:245`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      %res_b = transform.foreach_match in %res_a @match_b -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      transform.yield %res_b : !transform.any_op
+  }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly})
+      -> !transform.any_op attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'transform::YieldOp', but found 2}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  module @extra_module attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.include}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %tmp = transform.include @dummy_func failures(suppress) (%arg0) : (!transform.any_op) -> (!transform.any_op)
+        %res = transform.foreach_match in %tmp @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.print}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+         transform.print {name = "Hello"}
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op {transform.readonly})
+    -> (!transform.any_op, !transform.any_op) {
+    transform.yield %arg, %arg : !transform.any_op, !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op1: !transform.any_op {transform.readonly}, %op2: !transform.any_op {transform.readonly})
+    -> (!transform.any_op) {
+    transform.yield %op1 : !transform.any_op
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+    -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must return exactly one any_op result}}
+    %res1, %res2 = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
+
+    transform.yield %res1 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op) -> (!transform.any_op) {
+    transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op) {
+    transform.yield
+  }
+
+  // expected-error @+1 {{Tuning spec entry point expected to return any_op}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op)
+      -> (f32) attributes { iree_codegen.tuning_spec_entrypoint } {
+     // expected-error @+1 {{ForeachMatchOp must return exactly one any_op result}}
+    %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (!transform.any_op) -> (f32)
```

**Comment:**
This looks like an ill-formed forech_match op. It should check that the return type makes sense. Should we fix the `transform_match` verifier instead?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:263`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      %res_b = transform.foreach_match in %res_a @match_b -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      transform.yield %res_b : !transform.any_op
+  }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly})
+      -> !transform.any_op attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'transform::YieldOp', but found 2}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  module @extra_module attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.include}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %tmp = transform.include @dummy_func failures(suppress) (%arg0) : (!transform.any_op) -> (!transform.any_op)
+        %res = transform.foreach_match in %tmp @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.print}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+         transform.print {name = "Hello"}
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op {transform.readonly})
+    -> (!transform.any_op, !transform.any_op) {
+    transform.yield %arg, %arg : !transform.any_op, !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op1: !transform.any_op {transform.readonly}, %op2: !transform.any_op {transform.readonly})
+    -> (!transform.any_op) {
+    transform.yield %op1 : !transform.any_op
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+    -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must return exactly one any_op result}}
+    %res1, %res2 = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
+
+    transform.yield %res1 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op) -> (!transform.any_op) {
+    transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op) {
+    transform.yield
+  }
+
+  // expected-error @+1 {{Tuning spec entry point expected to return any_op}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op)
+      -> (f32) attributes { iree_codegen.tuning_spec_entrypoint } {
+     // expected-error @+1 {{ForeachMatchOp must return exactly one any_op result}}
+    %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (!transform.any_op) -> (f32)
+
+    transform.yield %res : f32
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg1: !transform.any_op {transform.readonly}, %arg2: !transform.any_op {transform.readonly})
+        -> (!transform.any_op) {
+        transform.yield %arg1 : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+    -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must take exactly one any_op argument}}
```

**Comment:**
Also here: we should either tighten the verifier on `transform.foreach_match` or say which attribute adds this verification rule

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:290`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      %res_b = transform.foreach_match in %res_a @match_b -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      transform.yield %res_b : !transform.any_op
+  }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly})
+      -> !transform.any_op attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'transform::YieldOp', but found 2}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  module @extra_module attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.include}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %tmp = transform.include @dummy_func failures(suppress) (%arg0) : (!transform.any_op) -> (!transform.any_op)
+        %res = transform.foreach_match in %tmp @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.print}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+         transform.print {name = "Hello"}
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op {transform.readonly})
+    -> (!transform.any_op, !transform.any_op) {
+    transform.yield %arg, %arg : !transform.any_op, !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op1: !transform.any_op {transform.readonly}, %op2: !transform.any_op {transform.readonly})
+    -> (!transform.any_op) {
+    transform.yield %op1 : !transform.any_op
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+    -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must return exactly one any_op result}}
+    %res1, %res2 = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
+
+    transform.yield %res1 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op) -> (!transform.any_op) {
+    transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op) {
+    transform.yield
+  }
+
+  // expected-error @+1 {{Tuning spec entry point expected to return any_op}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op)
+      -> (f32) attributes { iree_codegen.tuning_spec_entrypoint } {
+     // expected-error @+1 {{ForeachMatchOp must return exactly one any_op result}}
+    %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (!transform.any_op) -> (f32)
+
+    transform.yield %res : f32
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg1: !transform.any_op {transform.readonly}, %arg2: !transform.any_op {transform.readonly})
+        -> (!transform.any_op) {
+        transform.yield %arg1 : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+    -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must take exactly one any_op argument}}
+    %res = transform.foreach_match in %arg0, %arg0 @match -> @apply_op_config
+    : (!transform.any_op, !transform.any_op) -> (!transform.any_op)
+
+    transform.yield %res : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: index) -> (index) {
+    transform.yield %arg : index
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op) {
+    transform.yield
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: index)
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must take exactly one any_op argument}}
+    %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (index) -> (!transform.any_op)
```

**Comment:**
Also here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:305`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      %res_b = transform.foreach_match in %res_a @match_b -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      transform.yield %res_b : !transform.any_op
+  }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly})
+      -> !transform.any_op attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'transform::YieldOp', but found 2}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  module @extra_module attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.include}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %tmp = transform.include @dummy_func failures(suppress) (%arg0) : (!transform.any_op) -> (!transform.any_op)
+        %res = transform.foreach_match in %tmp @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.print}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+         transform.print {name = "Hello"}
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op {transform.readonly})
+    -> (!transform.any_op, !transform.any_op) {
+    transform.yield %arg, %arg : !transform.any_op, !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op1: !transform.any_op {transform.readonly}, %op2: !transform.any_op {transform.readonly})
+    -> (!transform.any_op) {
+    transform.yield %op1 : !transform.any_op
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+    -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must return exactly one any_op result}}
+    %res1, %res2 = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
+
+    transform.yield %res1 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op) -> (!transform.any_op) {
+    transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op) {
+    transform.yield
+  }
+
+  // expected-error @+1 {{Tuning spec entry point expected to return any_op}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op)
+      -> (f32) attributes { iree_codegen.tuning_spec_entrypoint } {
+     // expected-error @+1 {{ForeachMatchOp must return exactly one any_op result}}
+    %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (!transform.any_op) -> (f32)
+
+    transform.yield %res : f32
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg1: !transform.any_op {transform.readonly}, %arg2: !transform.any_op {transform.readonly})
+        -> (!transform.any_op) {
+        transform.yield %arg1 : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+    -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must take exactly one any_op argument}}
+    %res = transform.foreach_match in %arg0, %arg0 @match -> @apply_op_config
+    : (!transform.any_op, !transform.any_op) -> (!transform.any_op)
+
+    transform.yield %res : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: index) -> (index) {
+    transform.yield %arg : index
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op) {
+    transform.yield
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: index)
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must take exactly one any_op argument}}
+    %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (index) -> (!transform.any_op)
+
+    transform.yield %res : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op) -> (!transform.any_op) {
+    transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op) {
+    transform.yield
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op)
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+     // expected-error @+1{{ForeachMatchOp must not have the 'restrict_root' attribute}}
```

**Comment:**
Print which attribute adds this rule

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:326`

```diff
@@ -66,3 +66,267 @@ module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_defa
     return
   }
 }
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  // expected-error @+1{{The named sequence '__kernel_config' must have the attribute 'iree_codegen.tuning_spec_entrypoint'}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) {
+      transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match_a(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @match_b(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+      transform.yield
+  }
+
+  // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'ForeachMatchOp', but found 2}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+
+      %res_a = transform.foreach_match in %arg0 @match_a -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      %res_b = transform.foreach_match in %res_a @match_b -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+      transform.yield %res_b : !transform.any_op
+  }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly})
+      -> !transform.any_op attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_config' must contain exactly one 'transform::YieldOp', but found 2}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+// expected-error @+1{{Expected exactly one NamedSequenceOp with the attribute 'iree_codegen.tuning_spec_entrypoint', but found 2}}
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+
+  module @extra_module attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.include}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+        %tmp = transform.include @dummy_func failures(suppress) (%arg0) : (!transform.any_op) -> (!transform.any_op)
+        %res = transform.foreach_match in %tmp @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence private @dummy_func(!transform.any_op {transform.consumed}) -> !transform.any_op
+    transform.named_sequence @match(%arg: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+      transform.yield %arg : !transform.any_op
+    }
+
+    transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+    }
+
+    // expected-error @+1{{The named sequence '__kernel_configbut found an unsupported operation: transform.print}}
+    transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+        -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+         transform.print {name = "Hello"}
+        %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+        : (!transform.any_op) -> (!transform.any_op)
+
+        transform.yield %res : !transform.any_op
+    }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op {transform.readonly})
+    -> (!transform.any_op, !transform.any_op) {
+    transform.yield %arg, %arg : !transform.any_op, !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op1: !transform.any_op {transform.readonly}, %op2: !transform.any_op {transform.readonly})
+    -> (!transform.any_op) {
+    transform.yield %op1 : !transform.any_op
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+    -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must return exactly one any_op result}}
+    %res1, %res2 = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
+
+    transform.yield %res1 : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op) -> (!transform.any_op) {
+    transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op) {
+    transform.yield
+  }
+
+  // expected-error @+1 {{Tuning spec entry point expected to return any_op}}
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op)
+      -> (f32) attributes { iree_codegen.tuning_spec_entrypoint } {
+     // expected-error @+1 {{ForeachMatchOp must return exactly one any_op result}}
+    %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (!transform.any_op) -> (f32)
+
+    transform.yield %res : f32
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg1: !transform.any_op {transform.readonly}, %arg2: !transform.any_op {transform.readonly})
+        -> (!transform.any_op) {
+        transform.yield %arg1 : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly}) {
+        transform.yield
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op {transform.consumed})
+    -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must take exactly one any_op argument}}
+    %res = transform.foreach_match in %arg0, %arg0 @match -> @apply_op_config
+    : (!transform.any_op, !transform.any_op) -> (!transform.any_op)
+
+    transform.yield %res : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: index) -> (index) {
+    transform.yield %arg : index
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op) {
+    transform.yield
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: index)
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+    // expected-error @+1 {{ForeachMatchOp must take exactly one any_op argument}}
+    %res = transform.foreach_match in %arg0 @match -> @apply_op_config
+      : (index) -> (!transform.any_op)
+
+    transform.yield %res : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op) -> (!transform.any_op) {
+    transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op) {
+    transform.yield
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op)
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+     // expected-error @+1{{ForeachMatchOp must not have the 'restrict_root' attribute}}
+    %res = transform.foreach_match restrict_root in %arg0 @match -> @apply_op_config
+      : (!transform.any_op) -> (!transform.any_op)
+
+    transform.yield %res : !transform.any_op
+  }
+}
+
+// -----
+
+module @iree_default_tuning_spec attributes { iree_codegen.tuning_spec_with_default_entrypoint } {
+  transform.named_sequence @match(%arg: !transform.any_op) -> (!transform.any_op) {
+    transform.yield %arg : !transform.any_op
+  }
+
+  transform.named_sequence @apply_op_config(%op: !transform.any_op) {
+    transform.yield
+  }
+
+  transform.named_sequence @__kernel_config(%arg0: !transform.any_op)
+      -> (!transform.any_op) attributes { iree_codegen.tuning_spec_entrypoint } {
+     // expected-error @+1{{ForeachMatchOp must not have the 'flatten_results' attribute}}
```

**Comment:**
Also here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:134`

```diff
@@ -58,28 +58,130 @@ IREECodegenDialect::verifyOperationAttribute(Operation *op,
   StringRef symbol = attribute.getName().strref();
   Attribute attr = attribute.getValue();
   // This function verifies the validity of a specific operation attribute.
-  // - If the attribute's name matches `kTuningDefaultSpecAttrName`, make
-  //   sure it contains a single named sequence op with name `__kernel_config`.
+  // - If the attribute's name matches kTuningSpecDefaultEntrypointAttrName
+  // (`iree_codegen.tuning_spec_with_default_entrypoint`):
+  //   1. Ensure that the module contains a single named sequence operation with
+  //   the name `__kernel_config`.
+  //   2. Verify that this `__kernel_config` named sequence operation has the
+  //   attribute `iree_codegen.tuning_spec_entrypoint`.
+  //   3. Ensure that the named sequence operation contains exactly **one**
+  //   `ForeachMatchOp`.
+  //      - ForeachMatchOp must not have `flatten_results` and `restrict_root`
+  //        attributes.
+  //      - ForeachMatchOp must have exactly one argument of type any_op.
+  //      - ForeachMatchOp must have exactly one result of type any_op.
+  //   4. Ensure that only one named sequence operation with the
+  //   `iree_codegen.tuning_spec_entrypoint` attribute.
   // - If the attribute's name matches `kTuningSpecEntrypointAttrName`
-  // ("iree_codegen.tuning_spec_entrypoint"):
+  // (`iree_codegen.tuning_spec_entrypoint`):
   //   1. The attribute value must be a UnitAttr.
   //   2. If the operation is a transform::NamedSequenceOp:
   //      - The operation's function signature must satisfy the following:
-  //         a. It must have exactly one result type, and the result must be of
-  //         type `transform::AnyOpType`.
+  //         a. It must have exactly one result type, and the result must be
+  //         of type `transform::AnyOpType`.
   //         b. It must have exactly one argument type, and the argument must be
   //         of type `transform::AnyOpType`.
 
   if (symbol == kTuningSpecDefaultEntrypointAttrName) {
     if (auto moduleOp = dyn_cast<ModuleOp>(op)) {
-      if (!llvm::any_of(moduleOp.getOps<transform::NamedSequenceOp>(),
-                        [](transform::NamedSequenceOp op) {
-                          return op.getName() == kKernelConfigSpecName;
-                        })) {
+      transform::NamedSequenceOp kernelConfigOp;
+      int numTuningEntryPoints = 0;
+      for (Region &region : moduleOp->getRegions()) {
+        for (Block &block : region) {
+          for (Operation &op : block) {
+            if (auto namedSeqOp = dyn_cast<transform::NamedSequenceOp>(&op)) {
+              if (namedSeqOp.getName() == kKernelConfigSpecName) {
+                kernelConfigOp = namedSeqOp;
+              }
+            }
+
+            if (op.hasAttr(kTuningSpecEntrypointAttrName)) {
+              ++numTuningEntryPoints;
+            }
+          }
+        }
+      }
+
+      if (!kernelConfigOp) {
+        return moduleOp->emitError()
+               << "The tuning specification must include a named sequence with "
+               << "the symbol name '" << kKernelConfigSpecName << "'.";
+      }
+
+      // Verify that the kernelConfigOp has the attribute
+      // `iree_codegen.tuning_spec_entrypoint`.
+      if (!kernelConfigOp->hasAttr(kTuningSpecEntrypointAttrName)) {
+        return kernelConfigOp.emitError()
+               << "The named sequence '" << kKernelConfigSpecName
+               << "' must have the attribute '" << kTuningSpecEntrypointAttrName
+               << "'.";
+      }
+
+      if (numTuningEntryPoints != 1) {
         return moduleOp.emitError()
-               << "The tuning specification must include a named "
-                  "sequence with the symbol name '"
-               << kKernelConfigSpecName << "'.";
+               << "Expected exactly one NamedSequenceOp with the attribute '"
+               << kTuningSpecEntrypointAttrName << "', but found "
+               << numTuningEntryPoints << ".";
+      }
+
+      transform::ForeachMatchOp foreachMatchOp;
+      int numForeachMatchOps = 0;
+      int numYieldOps = 0;
+
+      for (Block &block : kernelConfigOp.getBlocks()) {
+        for (Operation &op : block) {
+          if (auto foreachOp = dyn_cast<transform::ForeachMatchOp>(op)) {
+            numForeachMatchOps++;
```

**Comment:**
https://llvm.org/docs/CodingStandards.html#prefer-preincrement

---


---


## [PR #20127](https://github.com/iree-org/iree/pull/20127): [Codegen][Tuner] merge the default td specs

### Review Summary

**CHANGES_REQUESTED** (2025-03-03)

To thoroughly test this code, we can start by assuming that tuning specs with default entypoints should always link, and then emitting warning whenever we notice they can't be linked. This is something we can test with `verify-diagnostics`.

 The way we usually do this in compilers is that we try to have two phases:
1. Analysis that determines transformation legality
2. Transformation that cannot bail out


**CHANGES_REQUESTED** (2025-03-07)


**COMMENTED** (2025-03-07)


**CHANGES_REQUESTED** (2025-03-10)


**COMMENTED** (2025-03-17)


**COMMENTED** (2025-03-17)


**COMMENTED** (2025-03-17)

Also, this PR doesn't actually fix https://github.com/nod-ai/shark-ai/issues/810 -- we also need to emit merged tuning specs in the tuner to close this issue


**COMMENTED** (2025-03-18)


**COMMENTED** (2025-03-18)


**COMMENTED** (2025-03-18)


**COMMENTED** (2025-03-18)


**COMMENTED** (2025-03-18)


### Code Comments

**File:** `compiler/plugins/target/ROCM/test/default_tuning_specs_amdgpu.mlir:49`

```diff
@@ -32,21 +32,22 @@
 
 // ============================================================================
 
-// Check that both the user tuning spec and the default spec get linked and
-// materialized. The user spec should have precedence over the default one.
+// Check that both the user tuning spec and the default spec get merged and
+// materialized, in which nested structure should not present and merged foreach_match op
+// should exist. The user spec should have precedence over the default one.
 
 // BOTH-LABEL: module @iree_linked_tuning_spec
 // BOTH-SAME:    iree_codegen.tuning_spec_with_default_entrypoint
 // BOTH-SAME:    transform.with_named_sequence
-// BOTH-LABEL:   module @mmt_tile_and_fuse_spec_0 attributes {transform.with_named_sequence}
-// BOTH-LABEL:     transform.named_sequence @main
-// BOTH-SAME:        attributes {iree_codegen.tuning_spec_entrypoint}
-// BOTH-LABEL:   module @iree_default_tuning_spec_gfx942_1 attributes {transform.with_named_sequence}
-// BOTH:           transform.named_sequence @__kernel_config
-// BOTH-SAME:        attributes {iree_codegen.tuning_spec_entrypoint}
+// BOTH-NOT:     module @mmt_tile_and_fuse_spec
+// BOTH-NOT:     module @iree_default_tuning_spec_gfx942
 // BOTH:         transform.named_sequence @__kernel_config
-// BOTH:           @mmt_tile_and_fuse_spec_0::@main
-// BOTH:           @iree_default_tuning_spec_gfx942_1::@__kernel_config
+// BOTH-SAME:    attributes {iree_codegen.tuning_spec_entrypoint}
+// BOTH:         transform.foreach_match
+// BOTH:         @match_mmt -> @apply_mmt_op_config
+// BOTH-NEXT:    @match_attention_2x10x4096x64x64x64_f16 -> @apply_attn_op_config
+// BOTH-NEXT:    @match_mmt_2048x1280x5120_f16_f16_f32 -> @apply_op_config
```

**Comment:**
nit: indent these to match the print format
```suggestion
// BOTH:           @match_mmt -> @apply_mmt_op_config
// BOTH-NEXT:      @match_attention_2x10x4096x64x64x64_f16 -> @apply_attn_op_config
// BOTH-NEXT:      @match_mmt_2048x1280x5120_f16_f16_f32 -> @apply_op_config
```

---

**File:** `compiler/plugins/target/ROCM/test/default_tuning_specs_amdgpu.mlir:49`

```diff
@@ -32,21 +32,22 @@
 
 // ============================================================================
 
-// Check that both the user tuning spec and the default spec get linked and
-// materialized. The user spec should have precedence over the default one.
+// Check that both the user tuning spec and the default spec get merged and
+// materialized, in which nested structure should not present and merged foreach_match op
+// should exist. The user spec should have precedence over the default one.
 
 // BOTH-LABEL: module @iree_linked_tuning_spec
 // BOTH-SAME:    iree_codegen.tuning_spec_with_default_entrypoint
 // BOTH-SAME:    transform.with_named_sequence
-// BOTH-LABEL:   module @mmt_tile_and_fuse_spec_0 attributes {transform.with_named_sequence}
-// BOTH-LABEL:     transform.named_sequence @main
-// BOTH-SAME:        attributes {iree_codegen.tuning_spec_entrypoint}
-// BOTH-LABEL:   module @iree_default_tuning_spec_gfx942_1 attributes {transform.with_named_sequence}
-// BOTH:           transform.named_sequence @__kernel_config
-// BOTH-SAME:        attributes {iree_codegen.tuning_spec_entrypoint}
+// BOTH-NOT:     module @mmt_tile_and_fuse_spec
+// BOTH-NOT:     module @iree_default_tuning_spec_gfx942
 // BOTH:         transform.named_sequence @__kernel_config
-// BOTH:           @mmt_tile_and_fuse_spec_0::@main
-// BOTH:           @iree_default_tuning_spec_gfx942_1::@__kernel_config
+// BOTH-SAME:    attributes {iree_codegen.tuning_spec_entrypoint}
+// BOTH:         transform.foreach_match
+// BOTH:         @match_mmt -> @apply_op_config
+// BOTH-NEXT:    @match_attention_2x10x4096x64x64x64_f16 -> @apply_attn_op_config
+// BOTH-NEXT:    @match_mmt_2048x1280x5120_f16_f16_f32 -> @apply_op_config_1
```

**Comment:**
nit: indent this like the printer does
```suggestion
// BOTH:         @match_mmt -> @apply_op_config
// BOTH-NEXT:    @match_attention_2x10x4096x64x64x64_f16 -> @apply_attn_op_config
// BOTH-NEXT:    @match_mmt_2048x1280x5120_f16_f16_f32 -> @apply_op_config_1
```
```suggestion
// BOTH:           @match_mmt -> @apply_op_config
// BOTH-NEXT:      @match_attention_2x10x4096x64x64x64_f16 -> @apply_attn_op_config
// BOTH-NEXT:      @match_mmt_2048x1280x5120_f16_f16_f32 -> @apply_op_config_1
```

---

**File:** `compiler/plugins/target/ROCM/test/tuning_spec_mmt_tile_and_fuse.mlir:4`

```diff
@@ -1,6 +1,6 @@
 // RUN: iree-opt %s
 
-module @mmt_tile_and_fuse_spec attributes { transform.with_named_sequence } {
+module @mmt_tile_and_fuse_spec attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint } {
 transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly},
```

**Comment:**
Why are we changing this? Do you think we should disallow tuning specs without default entrypoints?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:90`

```diff
@@ -85,6 +85,14 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   newSpec->setAttr(kTuningSpecEntrypointAttrName, builder.getUnitAttr());
   module->setAttr(kTuningSpecDefaultEntrypointAttrName, builder.getUnitAttr());
 
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    // Remove the default tuning spec attribute from inner modules,
+    // as the top-level module is attached with default attribute.
```

**Comment:**
Do we remove it because the entrypoint name can change? If that's the case, I think we'd want to iterate over the parents of `specsToLink` instead, since this functions doesn't assume a specific nesting structure.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:405`

```diff
@@ -154,6 +398,27 @@ struct LinkTuningSpecsPass final
 FailureOr<NamedSequenceOp> linkTuningSpecs(ModuleOp module) {
   SmallVector<NamedSequenceOp> tuningSpecs;
 
+  int matchingModules = 0;
+  int totalModules = 0;
+
+  for (auto module : module.getBody()->getOps<ModuleOp>()) {
+    totalModules++;
```

**Comment:**
Prefer pre-increment: https://llvm.org/docs/CodingStandards.html#prefer-preincrement

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:407`

```diff
@@ -154,6 +398,27 @@ struct LinkTuningSpecsPass final
 FailureOr<NamedSequenceOp> linkTuningSpecs(ModuleOp module) {
   SmallVector<NamedSequenceOp> tuningSpecs;
 
+  int matchingModules = 0;
+  int totalModules = 0;
+
+  for (auto module : module.getBody()->getOps<ModuleOp>()) {
+    totalModules++;
+    if (module->hasAttr(kTuningSpecDefaultEntrypointAttrName)) {
+      matchingModules++;
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:401`

```diff
@@ -154,6 +398,27 @@ struct LinkTuningSpecsPass final
 FailureOr<NamedSequenceOp> linkTuningSpecs(ModuleOp module) {
   SmallVector<NamedSequenceOp> tuningSpecs;
 
+  int matchingModules = 0;
```

**Comment:**
I don't understand this variable name. What do these modules match?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:402`

```diff
@@ -154,6 +398,27 @@ struct LinkTuningSpecsPass final
 FailureOr<NamedSequenceOp> linkTuningSpecs(ModuleOp module) {
   SmallVector<NamedSequenceOp> tuningSpecs;
 
+  int matchingModules = 0;
+  int totalModules = 0;
```

**Comment:**
I'd put this variable first since you update it unconditionally in the loop below.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:401`

```diff
@@ -154,6 +398,27 @@ struct LinkTuningSpecsPass final
 FailureOr<NamedSequenceOp> linkTuningSpecs(ModuleOp module) {
   SmallVector<NamedSequenceOp> tuningSpecs;
 
+  int matchingModules = 0;
```

**Comment:**
Maybe `numDefaultEntrypoint`?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:152`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
```

**Comment:**
Can we call the variable something like `namedSequenceToForeach` so that we don't need this comment?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:250`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
```

**Comment:**
This function is very long -- can we outline this loop to a helper function?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:163`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
```

**Comment:**
You don't need to explicitly initialize IR types to nullptr.
```suggestion
        transform::ForeachMatchOp foreachMatch;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:164`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
```

**Comment:**
This needs a more descriptive name

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:170`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:179`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
```

**Comment:**
```suggestion
        if (matchCount == 0 || matchCount > 1) {
          return failure();
        }
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:188`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
```

**Comment:**
Use `.contains(...)` for map types

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:199`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:187`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
```

**Comment:**
Can this ever not be a named sequence op?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:214`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
+                foreachMatchMap[actionOp] = foreachMatch;
+              }
+            }
+          }
+        }
+      } else {
+        namedSequenceOpsToMove.push_back(namedSequenceOp);
+      }
+    }
+  }
+
+  // Step 2-a: Ensure all ForeachMatchOps have the same result types before
+  // merging.
+  SmallVector<Type, 4> expectedResultTypes =
+      llvm::to_vector<4>(foreachMatchOps.front()->getResultTypes());
```

**Comment:**
```suggestion
  auto expectedResultTypes =
      llvm::to_vector_of<Type, 4>(foreachMatchOps.front()->getResultTypes());
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:257`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
+                foreachMatchMap[actionOp] = foreachMatch;
+              }
+            }
+          }
+        }
+      } else {
+        namedSequenceOpsToMove.push_back(namedSequenceOp);
+      }
+    }
+  }
+
+  // Step 2-a: Ensure all ForeachMatchOps have the same result types before
+  // merging.
```

**Comment:**
This could also be a helper function

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:262`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
+                foreachMatchMap[actionOp] = foreachMatch;
+              }
+            }
+          }
+        }
+      } else {
+        namedSequenceOpsToMove.push_back(namedSequenceOp);
+      }
+    }
+  }
+
+  // Step 2-a: Ensure all ForeachMatchOps have the same result types before
+  // merging.
+  SmallVector<Type, 4> expectedResultTypes =
+      llvm::to_vector<4>(foreachMatchOps.front()->getResultTypes());
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    SmallVector<Type, 4> currentResultTypes =
+        llvm::to_vector<4>(foreachMatchOp.getResultTypes());
+
+    if (!llvm::equal(currentResultTypes, expectedResultTypes)) {
+      return failure();
+    }
```

**Comment:**
Instead, should we check that the result type is exactly what we expect? I think it must take any_op and return any_op.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:228`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
+                foreachMatchMap[actionOp] = foreachMatch;
+              }
+            }
+          }
+        }
+      } else {
+        namedSequenceOpsToMove.push_back(namedSequenceOp);
+      }
+    }
+  }
+
+  // Step 2-a: Ensure all ForeachMatchOps have the same result types before
+  // merging.
+  SmallVector<Type, 4> expectedResultTypes =
+      llvm::to_vector<4>(foreachMatchOps.front()->getResultTypes());
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    SmallVector<Type, 4> currentResultTypes =
+        llvm::to_vector<4>(foreachMatchOp.getResultTypes());
+
+    if (!llvm::equal(currentResultTypes, expectedResultTypes)) {
+      return failure();
+    }
+  }
+
+  // Step 2-b: Ensure all ForeachMatchOps have the same `restrictRoot` and
+  // `flattenResults` attributes.
+  UnitAttr restrictRoot = nullptr;
+  UnitAttr flattenResults = nullptr;
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:270`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
+                foreachMatchMap[actionOp] = foreachMatch;
+              }
+            }
+          }
+        }
+      } else {
+        namedSequenceOpsToMove.push_back(namedSequenceOp);
+      }
+    }
+  }
+
+  // Step 2-a: Ensure all ForeachMatchOps have the same result types before
+  // merging.
+  SmallVector<Type, 4> expectedResultTypes =
+      llvm::to_vector<4>(foreachMatchOps.front()->getResultTypes());
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    SmallVector<Type, 4> currentResultTypes =
+        llvm::to_vector<4>(foreachMatchOp.getResultTypes());
+
+    if (!llvm::equal(currentResultTypes, expectedResultTypes)) {
+      return failure();
+    }
+  }
+
+  // Step 2-b: Ensure all ForeachMatchOps have the same `restrictRoot` and
+  // `flattenResults` attributes.
+  UnitAttr restrictRoot = nullptr;
+  UnitAttr flattenResults = nullptr;
+  bool hasMismatchAttr = false;
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    UnitAttr currentRestrictRoot = foreachMatchOp.getRestrictRootAttr();
+    UnitAttr currentFlattenResults = foreachMatchOp.getFlattenResultsAttr();
+
+    if (!restrictRoot) {
+      restrictRoot = currentRestrictRoot; // First encountered value.
+    } else if (restrictRoot != currentRestrictRoot) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+
+    if (!flattenResults) {
+      flattenResults = currentFlattenResults; // First encountered value.
+    } else if (flattenResults != currentFlattenResults) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+  }
+
+  // If there's a mismatch in attributes, do not merge.
+  if (hasMismatchAttr) {
+    return failure();
+  }
+
+  llvm::StringMap<unsigned> specNameCounts;
+  // Step 3-a: Make sure the name sequence names are unique, and then move
+  // collected NamedSequenceOps to the top-level module.
+  for (transform::NamedSequenceOp op : namedSequenceOpsToMove) {
+    StringRef specName = op.getSymName();
+    unsigned specNameSeenCount = specNameCounts[specName]++;
+    std::string newSpecName = specName.str();
+    if (specNameSeenCount > 0) {
+      newSpecName = llvm::formatv("{}_{}", specName, specNameSeenCount).str();
+      op.setSymName(newSpecName);
+    }
```

**Comment:**
We have the same logic elsewhere: could we make it a helper function?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:269`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
+                foreachMatchMap[actionOp] = foreachMatch;
+              }
+            }
+          }
+        }
+      } else {
+        namedSequenceOpsToMove.push_back(namedSequenceOp);
+      }
+    }
+  }
+
+  // Step 2-a: Ensure all ForeachMatchOps have the same result types before
+  // merging.
+  SmallVector<Type, 4> expectedResultTypes =
+      llvm::to_vector<4>(foreachMatchOps.front()->getResultTypes());
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    SmallVector<Type, 4> currentResultTypes =
+        llvm::to_vector<4>(foreachMatchOp.getResultTypes());
+
+    if (!llvm::equal(currentResultTypes, expectedResultTypes)) {
+      return failure();
+    }
+  }
+
+  // Step 2-b: Ensure all ForeachMatchOps have the same `restrictRoot` and
+  // `flattenResults` attributes.
+  UnitAttr restrictRoot = nullptr;
+  UnitAttr flattenResults = nullptr;
+  bool hasMismatchAttr = false;
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    UnitAttr currentRestrictRoot = foreachMatchOp.getRestrictRootAttr();
+    UnitAttr currentFlattenResults = foreachMatchOp.getFlattenResultsAttr();
+
+    if (!restrictRoot) {
+      restrictRoot = currentRestrictRoot; // First encountered value.
+    } else if (restrictRoot != currentRestrictRoot) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+
+    if (!flattenResults) {
+      flattenResults = currentFlattenResults; // First encountered value.
+    } else if (flattenResults != currentFlattenResults) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+  }
+
+  // If there's a mismatch in attributes, do not merge.
+  if (hasMismatchAttr) {
+    return failure();
+  }
+
+  llvm::StringMap<unsigned> specNameCounts;
+  // Step 3-a: Make sure the name sequence names are unique, and then move
+  // collected NamedSequenceOps to the top-level module.
+  for (transform::NamedSequenceOp op : namedSequenceOpsToMove) {
+    StringRef specName = op.getSymName();
+    unsigned specNameSeenCount = specNameCounts[specName]++;
+    std::string newSpecName = specName.str();
+    if (specNameSeenCount > 0) {
+      newSpecName = llvm::formatv("{}_{}", specName, specNameSeenCount).str();
+      op.setSymName(newSpecName);
+    }
+
+    // Only update ForeachMatchOp if there's a reference and the name has
+    // changed.
+    if (foreachMatchMap.count(op) && newSpecName != specName) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:267`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
+                foreachMatchMap[actionOp] = foreachMatch;
+              }
+            }
+          }
+        }
+      } else {
+        namedSequenceOpsToMove.push_back(namedSequenceOp);
+      }
+    }
+  }
+
+  // Step 2-a: Ensure all ForeachMatchOps have the same result types before
+  // merging.
+  SmallVector<Type, 4> expectedResultTypes =
+      llvm::to_vector<4>(foreachMatchOps.front()->getResultTypes());
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    SmallVector<Type, 4> currentResultTypes =
+        llvm::to_vector<4>(foreachMatchOp.getResultTypes());
+
+    if (!llvm::equal(currentResultTypes, expectedResultTypes)) {
+      return failure();
+    }
+  }
+
+  // Step 2-b: Ensure all ForeachMatchOps have the same `restrictRoot` and
+  // `flattenResults` attributes.
+  UnitAttr restrictRoot = nullptr;
+  UnitAttr flattenResults = nullptr;
+  bool hasMismatchAttr = false;
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    UnitAttr currentRestrictRoot = foreachMatchOp.getRestrictRootAttr();
+    UnitAttr currentFlattenResults = foreachMatchOp.getFlattenResultsAttr();
+
+    if (!restrictRoot) {
+      restrictRoot = currentRestrictRoot; // First encountered value.
+    } else if (restrictRoot != currentRestrictRoot) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+
+    if (!flattenResults) {
+      flattenResults = currentFlattenResults; // First encountered value.
+    } else if (flattenResults != currentFlattenResults) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+  }
+
+  // If there's a mismatch in attributes, do not merge.
+  if (hasMismatchAttr) {
+    return failure();
+  }
+
+  llvm::StringMap<unsigned> specNameCounts;
+  // Step 3-a: Make sure the name sequence names are unique, and then move
+  // collected NamedSequenceOps to the top-level module.
+  for (transform::NamedSequenceOp op : namedSequenceOpsToMove) {
+    StringRef specName = op.getSymName();
+    unsigned specNameSeenCount = specNameCounts[specName]++;
+    std::string newSpecName = specName.str();
+    if (specNameSeenCount > 0) {
+      newSpecName = llvm::formatv("{}_{}", specName, specNameSeenCount).str();
+      op.setSymName(newSpecName);
+    }
+
+    // Only update ForeachMatchOp if there's a reference and the name has
```

**Comment:**
What do you mean by 'if there's a reference'?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:324`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
+                foreachMatchMap[actionOp] = foreachMatch;
+              }
+            }
+          }
+        }
+      } else {
+        namedSequenceOpsToMove.push_back(namedSequenceOp);
+      }
+    }
+  }
+
+  // Step 2-a: Ensure all ForeachMatchOps have the same result types before
+  // merging.
+  SmallVector<Type, 4> expectedResultTypes =
+      llvm::to_vector<4>(foreachMatchOps.front()->getResultTypes());
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    SmallVector<Type, 4> currentResultTypes =
+        llvm::to_vector<4>(foreachMatchOp.getResultTypes());
+
+    if (!llvm::equal(currentResultTypes, expectedResultTypes)) {
+      return failure();
+    }
+  }
+
+  // Step 2-b: Ensure all ForeachMatchOps have the same `restrictRoot` and
+  // `flattenResults` attributes.
+  UnitAttr restrictRoot = nullptr;
+  UnitAttr flattenResults = nullptr;
+  bool hasMismatchAttr = false;
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    UnitAttr currentRestrictRoot = foreachMatchOp.getRestrictRootAttr();
+    UnitAttr currentFlattenResults = foreachMatchOp.getFlattenResultsAttr();
+
+    if (!restrictRoot) {
+      restrictRoot = currentRestrictRoot; // First encountered value.
+    } else if (restrictRoot != currentRestrictRoot) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+
+    if (!flattenResults) {
+      flattenResults = currentFlattenResults; // First encountered value.
+    } else if (flattenResults != currentFlattenResults) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+  }
+
+  // If there's a mismatch in attributes, do not merge.
+  if (hasMismatchAttr) {
+    return failure();
+  }
+
+  llvm::StringMap<unsigned> specNameCounts;
+  // Step 3-a: Make sure the name sequence names are unique, and then move
+  // collected NamedSequenceOps to the top-level module.
+  for (transform::NamedSequenceOp op : namedSequenceOpsToMove) {
+    StringRef specName = op.getSymName();
+    unsigned specNameSeenCount = specNameCounts[specName]++;
+    std::string newSpecName = specName.str();
+    if (specNameSeenCount > 0) {
+      newSpecName = llvm::formatv("{}_{}", specName, specNameSeenCount).str();
+      op.setSymName(newSpecName);
+    }
+
+    // Only update ForeachMatchOp if there's a reference and the name has
+    // changed.
+    if (foreachMatchMap.count(op) && newSpecName != specName) {
+      transform::ForeachMatchOp foreachMatchOp = foreachMatchMap[op];
+
+      SmallVector<Attribute> updatedMatchers, updatedActions;
+      for (auto matcherAttr : foreachMatchOp.getMatchers()) {
+        StringRef matcherName =
+            cast<SymbolRefAttr>(matcherAttr).getRootReference();
+        updatedMatchers.push_back(
+            (matcherName == specName)
+                ? SymbolRefAttr::get(builder.getContext(), newSpecName)
+                : matcherAttr);
+      }
+
+      for (auto actionAttr : foreachMatchOp.getActions()) {
+        StringRef actionName =
+            cast<SymbolRefAttr>(actionAttr).getRootReference();
+        updatedActions.push_back(
+            (actionName == specName)
+                ? SymbolRefAttr::get(builder.getContext(), newSpecName)
+                : actionAttr);
+      }
+
+      // Apply the updated matchers and actions.
+      foreachMatchOp.setMatchersAttr(builder.getArrayAttr(updatedMatchers));
+      foreachMatchOp.setActionsAttr(builder.getArrayAttr(updatedActions));
+    }
+    op.getOperation()->moveBefore(module.getBody(), module.getBody()->end());
+  }
+
+  // Step 3-b: Create a new NamedSequenceOp `__kernel_config` in the top-level
+  // module.
+  builder.setInsertionPointToEnd(module.getBody());
+  Location loc = module.getLoc();
+  Type anyOpType = builder.getType<transform::AnyOpType>();
+  FunctionType seqType =
+      builder.getFunctionType(TypeRange{anyOpType}, TypeRange{anyOpType});
+
+  auto newNamedSequence = builder.create<transform::NamedSequenceOp>(
+      loc, kKernelConfigSpecName, TypeAttr::get(seqType),
+      /*sym_visibility=*/StringAttr{},
+      /*arg_attrs=*/ArrayAttr{},
+      /*res_attrs*/ ArrayAttr{});
+
+  bool hasConsumedArg =
+      llvm::any_of(foreachMatchOps, [](transform::ForeachMatchOp op) {
+        Value operand = op->getOperand(0);
+        if (auto blockArg = mlir::dyn_cast<BlockArgument>(operand)) {
+          Operation *parentOp = blockArg.getOwner()->getParentOp();
+          if (auto namedSequenceOp =
+                  mlir::dyn_cast<transform::NamedSequenceOp>(parentOp)) {
+            return namedSequenceOp.getArgAttr(blockArg.getArgNumber(),
+                                              kArgConsumedAttrName) != nullptr;
+          }
+        }
+        return false;
+      });
```

**Comment:**
Make this a helper function

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:345`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
+                foreachMatchMap[actionOp] = foreachMatch;
+              }
+            }
+          }
+        }
+      } else {
+        namedSequenceOpsToMove.push_back(namedSequenceOp);
+      }
+    }
+  }
+
+  // Step 2-a: Ensure all ForeachMatchOps have the same result types before
+  // merging.
+  SmallVector<Type, 4> expectedResultTypes =
+      llvm::to_vector<4>(foreachMatchOps.front()->getResultTypes());
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    SmallVector<Type, 4> currentResultTypes =
+        llvm::to_vector<4>(foreachMatchOp.getResultTypes());
+
+    if (!llvm::equal(currentResultTypes, expectedResultTypes)) {
+      return failure();
+    }
+  }
+
+  // Step 2-b: Ensure all ForeachMatchOps have the same `restrictRoot` and
+  // `flattenResults` attributes.
+  UnitAttr restrictRoot = nullptr;
+  UnitAttr flattenResults = nullptr;
+  bool hasMismatchAttr = false;
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    UnitAttr currentRestrictRoot = foreachMatchOp.getRestrictRootAttr();
+    UnitAttr currentFlattenResults = foreachMatchOp.getFlattenResultsAttr();
+
+    if (!restrictRoot) {
+      restrictRoot = currentRestrictRoot; // First encountered value.
+    } else if (restrictRoot != currentRestrictRoot) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+
+    if (!flattenResults) {
+      flattenResults = currentFlattenResults; // First encountered value.
+    } else if (flattenResults != currentFlattenResults) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+  }
+
+  // If there's a mismatch in attributes, do not merge.
+  if (hasMismatchAttr) {
+    return failure();
+  }
+
+  llvm::StringMap<unsigned> specNameCounts;
+  // Step 3-a: Make sure the name sequence names are unique, and then move
+  // collected NamedSequenceOps to the top-level module.
+  for (transform::NamedSequenceOp op : namedSequenceOpsToMove) {
+    StringRef specName = op.getSymName();
+    unsigned specNameSeenCount = specNameCounts[specName]++;
+    std::string newSpecName = specName.str();
+    if (specNameSeenCount > 0) {
+      newSpecName = llvm::formatv("{}_{}", specName, specNameSeenCount).str();
+      op.setSymName(newSpecName);
+    }
+
+    // Only update ForeachMatchOp if there's a reference and the name has
+    // changed.
+    if (foreachMatchMap.count(op) && newSpecName != specName) {
+      transform::ForeachMatchOp foreachMatchOp = foreachMatchMap[op];
+
+      SmallVector<Attribute> updatedMatchers, updatedActions;
+      for (auto matcherAttr : foreachMatchOp.getMatchers()) {
+        StringRef matcherName =
+            cast<SymbolRefAttr>(matcherAttr).getRootReference();
+        updatedMatchers.push_back(
+            (matcherName == specName)
+                ? SymbolRefAttr::get(builder.getContext(), newSpecName)
+                : matcherAttr);
+      }
+
+      for (auto actionAttr : foreachMatchOp.getActions()) {
+        StringRef actionName =
+            cast<SymbolRefAttr>(actionAttr).getRootReference();
+        updatedActions.push_back(
+            (actionName == specName)
+                ? SymbolRefAttr::get(builder.getContext(), newSpecName)
+                : actionAttr);
+      }
+
+      // Apply the updated matchers and actions.
+      foreachMatchOp.setMatchersAttr(builder.getArrayAttr(updatedMatchers));
+      foreachMatchOp.setActionsAttr(builder.getArrayAttr(updatedActions));
+    }
+    op.getOperation()->moveBefore(module.getBody(), module.getBody()->end());
+  }
+
+  // Step 3-b: Create a new NamedSequenceOp `__kernel_config` in the top-level
+  // module.
+  builder.setInsertionPointToEnd(module.getBody());
+  Location loc = module.getLoc();
+  Type anyOpType = builder.getType<transform::AnyOpType>();
+  FunctionType seqType =
+      builder.getFunctionType(TypeRange{anyOpType}, TypeRange{anyOpType});
+
+  auto newNamedSequence = builder.create<transform::NamedSequenceOp>(
+      loc, kKernelConfigSpecName, TypeAttr::get(seqType),
+      /*sym_visibility=*/StringAttr{},
+      /*arg_attrs=*/ArrayAttr{},
+      /*res_attrs*/ ArrayAttr{});
+
+  bool hasConsumedArg =
+      llvm::any_of(foreachMatchOps, [](transform::ForeachMatchOp op) {
+        Value operand = op->getOperand(0);
+        if (auto blockArg = mlir::dyn_cast<BlockArgument>(operand)) {
+          Operation *parentOp = blockArg.getOwner()->getParentOp();
+          if (auto namedSequenceOp =
+                  mlir::dyn_cast<transform::NamedSequenceOp>(parentOp)) {
+            return namedSequenceOp.getArgAttr(blockArg.getArgNumber(),
+                                              kArgConsumedAttrName) != nullptr;
+          }
+        }
+        return false;
+      });
+
+  StringRef attrName =
+      hasConsumedArg ? kArgConsumedAttrName : kArgReadOnlyAttrName;
+  newNamedSequence.setArgAttr(0, attrName, builder.getUnitAttr());
+  newNamedSequence->setAttr(kTuningSpecEntrypointAttrName,
+                            builder.getUnitAttr());
+  // Indicate the output module is a default tuning spec after merging.
+  module->setAttr(kTuningSpecDefaultEntrypointAttrName, builder.getUnitAttr());
+
+  // Step 3-C: Create a new block inside the NamedSequenceOp and merging
+  // ForeachMatchOp from each inner modules into one ForachMatchOp.
+  SmallVector<Type, 4> resultTypes;
+  llvm::append_range(resultTypes, expectedResultTypes);
+
+  SmallVector<std::pair<SymbolRefAttr, SymbolRefAttr>> matcherActionPairs;
+  SmallVector<Value, 4> forwardedInputs;
+  for (auto foreachMatchOp : foreachMatchOps) {
+    ArrayAttr matchers = foreachMatchOp.getMatchers();
+    ArrayAttr actions = foreachMatchOp.getActions();
+
+    for (size_t i = 0; i < matchers.size(); i++) {
```

**Comment:**
Do not re-evaluate the end iterator: https://llvm.org/docs/CodingStandards.html#don-t-evaluate-end-every-time-through-a-loop

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:351`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
+                foreachMatchMap[actionOp] = foreachMatch;
+              }
+            }
+          }
+        }
+      } else {
+        namedSequenceOpsToMove.push_back(namedSequenceOp);
+      }
+    }
+  }
+
+  // Step 2-a: Ensure all ForeachMatchOps have the same result types before
+  // merging.
+  SmallVector<Type, 4> expectedResultTypes =
+      llvm::to_vector<4>(foreachMatchOps.front()->getResultTypes());
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    SmallVector<Type, 4> currentResultTypes =
+        llvm::to_vector<4>(foreachMatchOp.getResultTypes());
+
+    if (!llvm::equal(currentResultTypes, expectedResultTypes)) {
+      return failure();
+    }
+  }
+
+  // Step 2-b: Ensure all ForeachMatchOps have the same `restrictRoot` and
+  // `flattenResults` attributes.
+  UnitAttr restrictRoot = nullptr;
+  UnitAttr flattenResults = nullptr;
+  bool hasMismatchAttr = false;
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    UnitAttr currentRestrictRoot = foreachMatchOp.getRestrictRootAttr();
+    UnitAttr currentFlattenResults = foreachMatchOp.getFlattenResultsAttr();
+
+    if (!restrictRoot) {
+      restrictRoot = currentRestrictRoot; // First encountered value.
+    } else if (restrictRoot != currentRestrictRoot) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+
+    if (!flattenResults) {
+      flattenResults = currentFlattenResults; // First encountered value.
+    } else if (flattenResults != currentFlattenResults) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+  }
+
+  // If there's a mismatch in attributes, do not merge.
+  if (hasMismatchAttr) {
+    return failure();
+  }
+
+  llvm::StringMap<unsigned> specNameCounts;
+  // Step 3-a: Make sure the name sequence names are unique, and then move
+  // collected NamedSequenceOps to the top-level module.
+  for (transform::NamedSequenceOp op : namedSequenceOpsToMove) {
+    StringRef specName = op.getSymName();
+    unsigned specNameSeenCount = specNameCounts[specName]++;
+    std::string newSpecName = specName.str();
+    if (specNameSeenCount > 0) {
+      newSpecName = llvm::formatv("{}_{}", specName, specNameSeenCount).str();
+      op.setSymName(newSpecName);
+    }
+
+    // Only update ForeachMatchOp if there's a reference and the name has
+    // changed.
+    if (foreachMatchMap.count(op) && newSpecName != specName) {
+      transform::ForeachMatchOp foreachMatchOp = foreachMatchMap[op];
+
+      SmallVector<Attribute> updatedMatchers, updatedActions;
+      for (auto matcherAttr : foreachMatchOp.getMatchers()) {
+        StringRef matcherName =
+            cast<SymbolRefAttr>(matcherAttr).getRootReference();
+        updatedMatchers.push_back(
+            (matcherName == specName)
+                ? SymbolRefAttr::get(builder.getContext(), newSpecName)
+                : matcherAttr);
+      }
+
+      for (auto actionAttr : foreachMatchOp.getActions()) {
+        StringRef actionName =
+            cast<SymbolRefAttr>(actionAttr).getRootReference();
+        updatedActions.push_back(
+            (actionName == specName)
+                ? SymbolRefAttr::get(builder.getContext(), newSpecName)
+                : actionAttr);
+      }
+
+      // Apply the updated matchers and actions.
+      foreachMatchOp.setMatchersAttr(builder.getArrayAttr(updatedMatchers));
+      foreachMatchOp.setActionsAttr(builder.getArrayAttr(updatedActions));
+    }
+    op.getOperation()->moveBefore(module.getBody(), module.getBody()->end());
+  }
+
+  // Step 3-b: Create a new NamedSequenceOp `__kernel_config` in the top-level
+  // module.
+  builder.setInsertionPointToEnd(module.getBody());
+  Location loc = module.getLoc();
+  Type anyOpType = builder.getType<transform::AnyOpType>();
+  FunctionType seqType =
+      builder.getFunctionType(TypeRange{anyOpType}, TypeRange{anyOpType});
+
+  auto newNamedSequence = builder.create<transform::NamedSequenceOp>(
+      loc, kKernelConfigSpecName, TypeAttr::get(seqType),
+      /*sym_visibility=*/StringAttr{},
+      /*arg_attrs=*/ArrayAttr{},
+      /*res_attrs*/ ArrayAttr{});
+
+  bool hasConsumedArg =
+      llvm::any_of(foreachMatchOps, [](transform::ForeachMatchOp op) {
+        Value operand = op->getOperand(0);
+        if (auto blockArg = mlir::dyn_cast<BlockArgument>(operand)) {
+          Operation *parentOp = blockArg.getOwner()->getParentOp();
+          if (auto namedSequenceOp =
+                  mlir::dyn_cast<transform::NamedSequenceOp>(parentOp)) {
+            return namedSequenceOp.getArgAttr(blockArg.getArgNumber(),
+                                              kArgConsumedAttrName) != nullptr;
+          }
+        }
+        return false;
+      });
+
+  StringRef attrName =
+      hasConsumedArg ? kArgConsumedAttrName : kArgReadOnlyAttrName;
+  newNamedSequence.setArgAttr(0, attrName, builder.getUnitAttr());
+  newNamedSequence->setAttr(kTuningSpecEntrypointAttrName,
+                            builder.getUnitAttr());
+  // Indicate the output module is a default tuning spec after merging.
+  module->setAttr(kTuningSpecDefaultEntrypointAttrName, builder.getUnitAttr());
+
+  // Step 3-C: Create a new block inside the NamedSequenceOp and merging
+  // ForeachMatchOp from each inner modules into one ForachMatchOp.
+  SmallVector<Type, 4> resultTypes;
+  llvm::append_range(resultTypes, expectedResultTypes);
+
+  SmallVector<std::pair<SymbolRefAttr, SymbolRefAttr>> matcherActionPairs;
+  SmallVector<Value, 4> forwardedInputs;
+  for (auto foreachMatchOp : foreachMatchOps) {
+    ArrayAttr matchers = foreachMatchOp.getMatchers();
+    ArrayAttr actions = foreachMatchOp.getActions();
+
+    for (size_t i = 0; i < matchers.size(); i++) {
+      matcherActionPairs.push_back({mlir::cast<SymbolRefAttr>(matchers[i]),
+                                    mlir::cast<SymbolRefAttr>(actions[i])});
+    }
+    // Collect forwarded inputs (if any).
+    for (Value input : foreachMatchOp.getForwardedInputs()) {
+      if (llvm::find(forwardedInputs, input) == forwardedInputs.end()) {
```

**Comment:**
Use `llvm::is_contained`

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:344`

```diff
@@ -136,6 +144,242 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   return newSpec;
 }
 
+static FailureOr<NamedSequenceOp> emitLinkedDefaultTuningSpec(ModuleOp module) {
+  OpBuilder builder(module.getContext());
+  SmallVector<transform::NamedSequenceOp> namedSequenceOpsToMove;
+  SmallVector<transform::ForeachMatchOp> foreachMatchOps;
+  // foreachMatchMap: NamedSequenceOp -> ForeachMatchOps that reference it
+  // (either as a matcher or an action). It ensures
+  // that when a NamedSequenceOp is renamed for uniqueness, the corresponding
+  // ForeachMatchOp is also updated.
+  llvm::DenseMap<transform::NamedSequenceOp, transform::ForeachMatchOp>
+      foreachMatchMap;
+
+  // Step 1: Collect NamedSequenceOps and ForeachMatchOps from inner modules.
+  for (auto innerModule : module.getBody()->getOps<ModuleOp>()) {
+    for (auto namedSequenceOp :
+         innerModule.getBody()->getOps<transform::NamedSequenceOp>()) {
+      if (namedSequenceOp.getSymName() == kKernelConfigSpecName) {
+        transform::ForeachMatchOp foreachMatch = nullptr;
+        int matchCount = 0;
+        // Iterate directly over ForeachMatchOp within kernelConfig.
+        for (auto op : namedSequenceOp.getOps<transform::ForeachMatchOp>()) {
+          if (!foreachMatch) {
+            foreachMatch = op;
+          }
+          matchCount++;
+        }
+
+        // Return failure if multiple occurrences exist.
+        if (matchCount > 1) {
+          return failure();
+        }
+        // Return failure if not foreach match op found.
+        if (!foreachMatch)
+          return failure();
+
+        foreachMatchOps.push_back(foreachMatch);
+
+        for (auto matcher : foreachMatch.getMatchers()) {
+          if (auto matcherSymRef = dyn_cast<SymbolRefAttr>(matcher)) {
+            if (auto matcherOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         matcherSymRef))) {
+              if (!foreachMatchMap.count(matcherOp)) {
+                foreachMatchMap[matcherOp] = foreachMatch;
+              }
+            }
+          }
+        }
+        for (auto action : foreachMatch.getActions()) {
+          if (auto actionSymRef = dyn_cast<SymbolRefAttr>(action)) {
+            if (auto actionOp = dyn_cast_or_null<transform::NamedSequenceOp>(
+                    SymbolTable::lookupNearestSymbolFrom(innerModule,
+                                                         actionSymRef))) {
+              if (!foreachMatchMap.count(actionOp)) {
+                foreachMatchMap[actionOp] = foreachMatch;
+              }
+            }
+          }
+        }
+      } else {
+        namedSequenceOpsToMove.push_back(namedSequenceOp);
+      }
+    }
+  }
+
+  // Step 2-a: Ensure all ForeachMatchOps have the same result types before
+  // merging.
+  SmallVector<Type, 4> expectedResultTypes =
+      llvm::to_vector<4>(foreachMatchOps.front()->getResultTypes());
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    SmallVector<Type, 4> currentResultTypes =
+        llvm::to_vector<4>(foreachMatchOp.getResultTypes());
+
+    if (!llvm::equal(currentResultTypes, expectedResultTypes)) {
+      return failure();
+    }
+  }
+
+  // Step 2-b: Ensure all ForeachMatchOps have the same `restrictRoot` and
+  // `flattenResults` attributes.
+  UnitAttr restrictRoot = nullptr;
+  UnitAttr flattenResults = nullptr;
+  bool hasMismatchAttr = false;
+
+  for (auto foreachMatchOp : foreachMatchOps) {
+    UnitAttr currentRestrictRoot = foreachMatchOp.getRestrictRootAttr();
+    UnitAttr currentFlattenResults = foreachMatchOp.getFlattenResultsAttr();
+
+    if (!restrictRoot) {
+      restrictRoot = currentRestrictRoot; // First encountered value.
+    } else if (restrictRoot != currentRestrictRoot) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+
+    if (!flattenResults) {
+      flattenResults = currentFlattenResults; // First encountered value.
+    } else if (flattenResults != currentFlattenResults) {
+      hasMismatchAttr = true;
+      break; // Exit early when a mismatch is found.
+    }
+  }
+
+  // If there's a mismatch in attributes, do not merge.
+  if (hasMismatchAttr) {
+    return failure();
+  }
+
+  llvm::StringMap<unsigned> specNameCounts;
+  // Step 3-a: Make sure the name sequence names are unique, and then move
+  // collected NamedSequenceOps to the top-level module.
+  for (transform::NamedSequenceOp op : namedSequenceOpsToMove) {
+    StringRef specName = op.getSymName();
+    unsigned specNameSeenCount = specNameCounts[specName]++;
+    std::string newSpecName = specName.str();
+    if (specNameSeenCount > 0) {
+      newSpecName = llvm::formatv("{}_{}", specName, specNameSeenCount).str();
+      op.setSymName(newSpecName);
+    }
+
+    // Only update ForeachMatchOp if there's a reference and the name has
+    // changed.
+    if (foreachMatchMap.count(op) && newSpecName != specName) {
+      transform::ForeachMatchOp foreachMatchOp = foreachMatchMap[op];
+
+      SmallVector<Attribute> updatedMatchers, updatedActions;
+      for (auto matcherAttr : foreachMatchOp.getMatchers()) {
+        StringRef matcherName =
+            cast<SymbolRefAttr>(matcherAttr).getRootReference();
+        updatedMatchers.push_back(
+            (matcherName == specName)
+                ? SymbolRefAttr::get(builder.getContext(), newSpecName)
+                : matcherAttr);
+      }
+
+      for (auto actionAttr : foreachMatchOp.getActions()) {
+        StringRef actionName =
+            cast<SymbolRefAttr>(actionAttr).getRootReference();
+        updatedActions.push_back(
+            (actionName == specName)
+                ? SymbolRefAttr::get(builder.getContext(), newSpecName)
+                : actionAttr);
+      }
+
+      // Apply the updated matchers and actions.
+      foreachMatchOp.setMatchersAttr(builder.getArrayAttr(updatedMatchers));
+      foreachMatchOp.setActionsAttr(builder.getArrayAttr(updatedActions));
+    }
+    op.getOperation()->moveBefore(module.getBody(), module.getBody()->end());
+  }
+
+  // Step 3-b: Create a new NamedSequenceOp `__kernel_config` in the top-level
+  // module.
+  builder.setInsertionPointToEnd(module.getBody());
+  Location loc = module.getLoc();
+  Type anyOpType = builder.getType<transform::AnyOpType>();
+  FunctionType seqType =
+      builder.getFunctionType(TypeRange{anyOpType}, TypeRange{anyOpType});
+
+  auto newNamedSequence = builder.create<transform::NamedSequenceOp>(
+      loc, kKernelConfigSpecName, TypeAttr::get(seqType),
+      /*sym_visibility=*/StringAttr{},
+      /*arg_attrs=*/ArrayAttr{},
+      /*res_attrs*/ ArrayAttr{});
+
+  bool hasConsumedArg =
+      llvm::any_of(foreachMatchOps, [](transform::ForeachMatchOp op) {
+        Value operand = op->getOperand(0);
+        if (auto blockArg = mlir::dyn_cast<BlockArgument>(operand)) {
+          Operation *parentOp = blockArg.getOwner()->getParentOp();
+          if (auto namedSequenceOp =
+                  mlir::dyn_cast<transform::NamedSequenceOp>(parentOp)) {
+            return namedSequenceOp.getArgAttr(blockArg.getArgNumber(),
+                                              kArgConsumedAttrName) != nullptr;
+          }
+        }
+        return false;
+      });
+
+  StringRef attrName =
+      hasConsumedArg ? kArgConsumedAttrName : kArgReadOnlyAttrName;
+  newNamedSequence.setArgAttr(0, attrName, builder.getUnitAttr());
+  newNamedSequence->setAttr(kTuningSpecEntrypointAttrName,
+                            builder.getUnitAttr());
+  // Indicate the output module is a default tuning spec after merging.
+  module->setAttr(kTuningSpecDefaultEntrypointAttrName, builder.getUnitAttr());
+
+  // Step 3-C: Create a new block inside the NamedSequenceOp and merging
+  // ForeachMatchOp from each inner modules into one ForachMatchOp.
+  SmallVector<Type, 4> resultTypes;
+  llvm::append_range(resultTypes, expectedResultTypes);
+
+  SmallVector<std::pair<SymbolRefAttr, SymbolRefAttr>> matcherActionPairs;
+  SmallVector<Value, 4> forwardedInputs;
+  for (auto foreachMatchOp : foreachMatchOps) {
+    ArrayAttr matchers = foreachMatchOp.getMatchers();
+    ArrayAttr actions = foreachMatchOp.getActions();
+
+    for (size_t i = 0; i < matchers.size(); i++) {
+      matcherActionPairs.push_back({mlir::cast<SymbolRefAttr>(matchers[i]),
+                                    mlir::cast<SymbolRefAttr>(actions[i])});
+    }
+    // Collect forwarded inputs (if any).
+    for (Value input : foreachMatchOp.getForwardedInputs()) {
+      if (llvm::find(forwardedInputs, input) == forwardedInputs.end()) {
+        forwardedInputs.push_back(input); // Avoid duplicates
+      }
+    }
+  }
+
+  SmallVector<Attribute> mergedMatchers;
+  SmallVector<Attribute> mergedActions;
+
+  for (const auto &pair : matcherActionPairs) {
+    mergedMatchers.push_back(pair.first);
+    mergedActions.push_back(pair.second);
+  }
```

**Comment:**
You can make this more readable with structured bindings

---


---


## [PR #20081](https://github.com/iree-org/iree/pull/20081):  [Codegen][Tuner]: remove attrs inside decomposeConfig for attention op

### Review Summary

**COMMENTED** (2025-02-24)


**COMMENTED** (2025-02-25)

Looks good overall but let's wait from an approval from Kunwar


**APPROVED** (2025-02-25)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:74`

```diff
@@ -69,8 +69,19 @@ struct StripAttentionOpCompilationInfo final
       eraseLoweringConfig(attentionOp);
     }
 
-    if (attentionOp.getDecompositionConfigAttr()) {
-      attentionOp.removeDecompositionConfigAttr();
+    DictionaryAttr decompositionConfig =
+        attentionOp.getDecompositionConfigAttr();
+    if (decompositionConfig) {
```

**Comment:**
```suggestion
    if (DictionaryAttr decompositionConfig =
        attentionOp.getDecompositionConfigAttr()) {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:75`

```diff
@@ -69,8 +69,19 @@ struct StripAttentionOpCompilationInfo final
       eraseLoweringConfig(attentionOp);
     }
 
-    if (attentionOp.getDecompositionConfigAttr()) {
-      attentionOp.removeDecompositionConfigAttr();
+    DictionaryAttr decompositionConfig =
+        attentionOp.getDecompositionConfigAttr();
+    if (decompositionConfig) {
+      decompositionConfig = DictionaryAttr::get(
```

**Comment:**
It seems a little bit confusing to me to reuse the same variable even though it's not a reference and won't update the attribute itself. Can we create a new one instead?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:77`

```diff
@@ -69,8 +69,19 @@ struct StripAttentionOpCompilationInfo final
       eraseLoweringConfig(attentionOp);
     }
 
-    if (attentionOp.getDecompositionConfigAttr()) {
-      attentionOp.removeDecompositionConfigAttr();
+    DictionaryAttr decompositionConfig =
+        attentionOp.getDecompositionConfigAttr();
+    if (decompositionConfig) {
+      decompositionConfig = DictionaryAttr::get(
+          decompositionConfig.getContext(),
+          llvm::to_vector(llvm::make_filter_range(
```

**Comment:**
can we use `llvm::filter_to_vector`?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:77`

```diff
@@ -69,8 +69,19 @@ struct StripAttentionOpCompilationInfo final
       eraseLoweringConfig(attentionOp);
     }
 
-    if (attentionOp.getDecompositionConfigAttr()) {
-      attentionOp.removeDecompositionConfigAttr();
+    DictionaryAttr decompositionConfig =
+        attentionOp.getDecompositionConfigAttr();
+    if (decompositionConfig) {
+      decompositionConfig = DictionaryAttr::get(
+          decompositionConfig.getContext(),
+          llvm::to_vector(llvm::make_filter_range(
```

**Comment:**
I'd make this vector a local variable to reduce the overall nesting.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/strip_compilation_info.mlir:106`

```diff
@@ -103,7 +103,7 @@ func.func @attention(%arg0: tensor<2x10x6x4xf16>, %arg1 : tensor<2x10x4x4xf16>,
 #compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
 func.func @attention_1(%arg0: tensor<2x10x6x4xf16>, %arg1 : tensor<2x10x4x4xf16>, %arg2 : tensor<2x10x4x4xf16>, %arg3 : f16) -> tensor<2x10x6x4xf16> attributes {translation_info = #iree_codegen.translation_info<pipeline = None subgroup_size = 32>} {
   %init = tensor.empty() : tensor<2x10x6x4xf16>
-  %result = iree_linalg_ext.attention {decomposition_config = {x}, indexing_maps = [#map, #map1, #map2, #map3, #map4], compilation_info = #compilation} ins(%arg0, %arg1, %arg2, %arg3 : tensor<2x10x6x4xf16>, tensor<2x10x4x4xf16>, tensor<2x10x4x4xf16>, f16) outs(%init : tensor<2x10x6x4xf16>) {
+  %result = iree_linalg_ext.attention {decomposition_config = {pv_attrs = {x}, qk_attrs = {y}}, indexing_maps = [#map, #map1, #map2, #map3, #map4], compilation_info = #compilation} ins(%arg0, %arg1, %arg2, %arg3 : tensor<2x10x6x4xf16>, tensor<2x10x4x4xf16>, tensor<2x10x4x4xf16>, f16) outs(%init : tensor<2x10x6x4xf16>) {
```

**Comment:**
Should we check have a check with `use_exp2` to make sure we don't drop it? I assume that this is accomplished by the `z` unit attr above, but this seems like something easy to miss to me.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:76`

```diff
@@ -69,19 +69,17 @@ struct StripAttentionOpCompilationInfo final
       eraseLoweringConfig(attentionOp);
     }
 
-    DictionaryAttr decompositionConfig =
-        attentionOp.getDecompositionConfigAttr();
-    if (decompositionConfig) {
-      decompositionConfig = DictionaryAttr::get(
+    if (DictionaryAttr decompositionConfig =
+            attentionOp.getDecompositionConfigAttr()) {
+      DictionaryAttr modifiedDecompositionConfig = DictionaryAttr::get(
           decompositionConfig.getContext(),
-          llvm::to_vector(llvm::make_filter_range(
-              decompositionConfig, [&](NamedAttribute attr) {
-                return attr.getName() !=
-                           IREE::LinalgExt::AttentionOp::getQKAttrStr() &&
-                       attr.getName() !=
-                           IREE::LinalgExt::AttentionOp::getPVAttrStr();
-              })));
-      attentionOp.setDecompositionConfigAttr(decompositionConfig);
+          llvm::filter_to_vector(decompositionConfig, [&](NamedAttribute attr) {
```

**Comment:**
```suggestion
          llvm::filter_to_vector(decompositionConfig, [](NamedAttribute attr) {
```
AFAICT we don't need to capture anything

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:74`

```diff
@@ -69,19 +69,17 @@ struct StripAttentionOpCompilationInfo final
       eraseLoweringConfig(attentionOp);
     }
 
-    DictionaryAttr decompositionConfig =
-        attentionOp.getDecompositionConfigAttr();
-    if (decompositionConfig) {
-      decompositionConfig = DictionaryAttr::get(
+    if (DictionaryAttr decompositionConfig =
+            attentionOp.getDecompositionConfigAttr()) {
+      DictionaryAttr modifiedDecompositionConfig = DictionaryAttr::get(
```

**Comment:**
nit: this is a bit of a mouthful and I don't think we need to be that descriptive here
```suggestion
      DictionaryAttr newConfig = DictionaryAttr::get(
```

---


---


## [PR #20072](https://github.com/iree-org/iree/pull/20072): [Codegen][Tuner] add support for attention op in the StripCompilationInfoPass

### Review Summary

**COMMENTED** (2025-02-24)


**COMMENTED** (2025-02-24)


**COMMENTED** (2025-02-24)


**APPROVED** (2025-02-24)

LGTM


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:60`

```diff
@@ -57,13 +56,34 @@ struct StripLinalgOpCompilationInfo final
   }
 };
 
+struct StripAttentionOpCompilationInfo
+    : public OpRewritePattern<IREE::LinalgExt::AttentionOp> {
```

**Comment:**
```suggestion
struct StripAttentionOpCompilationInfo final
    : OpRewritePattern<IREE::LinalgExt::AttentionOp> {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:86`

```diff
@@ -57,13 +56,34 @@ struct StripLinalgOpCompilationInfo final
   }
 };
 
+struct StripAttentionOpCompilationInfo
+    : public OpRewritePattern<IREE::LinalgExt::AttentionOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(IREE::LinalgExt::AttentionOp attentionOp,
+                                PatternRewriter &rewriter) const override {
+    if (getCompilationInfo(attentionOp)) {
+      eraseCompilationInfo(attentionOp);
+    }
+
+    if (getLoweringConfig(attentionOp)) {
+      eraseLoweringConfig(attentionOp);
+    }
+
+    if (attentionOp.getDecompositionConfigAttr()) {
+      attentionOp.removeDecompositionConfigAttr();
+    }
+    return success();
+  }
+};
+
 struct StripCompilationInfoPass final
     : impl::StripCompilationInfoPassBase<StripCompilationInfoPass> {
   void runOnOperation() override {
     MLIRContext *ctx = &getContext();
     RewritePatternSet patterns(ctx);
     patterns.add<StripFuncOpTranslationInfo>(ctx);
     patterns.add<StripLinalgOpCompilationInfo>(ctx);
+    patterns.add<StripAttentionOpCompilationInfo>(ctx);
```

**Comment:**
```suggestion
    patterns.add<StripFuncOpTranslationInfo, StripLinalgOpCompilationInfo, StripAttentionOpCompilationInfo>(ctx);
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:66`

```diff
@@ -57,13 +56,34 @@ struct StripLinalgOpCompilationInfo final
   }
 };
 
+struct StripAttentionOpCompilationInfo
+    : public OpRewritePattern<IREE::LinalgExt::AttentionOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(IREE::LinalgExt::AttentionOp attentionOp,
+                                PatternRewriter &rewriter) const override {
+    if (getCompilationInfo(attentionOp)) {
+      eraseCompilationInfo(attentionOp);
+    }
```

**Comment:**
There's no test for this

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/strip_compilation_info.mlir:111`

```diff
@@ -82,5 +82,33 @@ func.func @attention(%arg0: tensor<2x10x6x4xf16>, %arg1 : tensor<2x10x4x4xf16>,
 
 // CHECK-LABEL: func.func @attention
 // CHECK-NOT:   iree_codegen.translation_info
+// CHECK-NOT:   iree_codegen.lowering_config
 // CHECK-NOT:   decomposition_config =
+
+
+// -----
+
+#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d4)>
+#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d5, d4)>
+#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d5)>
+#map3 = affine_map<(d0, d1, d2, d3, d4, d5) -> ()>
+#map4 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d3)>
+#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
+#map6 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d1, d3)>
+#config = #iree_codegen.lowering_config<tile_sizes = [[128, 256], [16, 16]]>
+#translation = #iree_codegen.translation_info<pipeline = None workgroup_size = [16, 8, 1] subgroup_size = 64>
+#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
+func.func @attention_1(%arg0: tensor<2x10x6x4xf16>, %arg1 : tensor<2x10x4x4xf16>, %arg2 : tensor<2x10x4x4xf16>, %arg3 : f16) -> tensor<2x10x6x4xf16> attributes {translation_info = #iree_codegen.translation_info<pipeline = None subgroup_size = 32>} {
+  %init = tensor.empty() : tensor<2x10x6x4xf16>
+  %result = iree_linalg_ext.attention {decomposition_config = {x}, indexing_maps = [#map, #map1, #map2, #map3, #map4], compilation_info = #compilation} ins(%arg0, %arg1, %arg2, %arg3 : tensor<2x10x6x4xf16>, tensor<2x10x4x4xf16>, tensor<2x10x4x4xf16>, f16) outs(%init : tensor<2x10x6x4xf16>) {
+        ^bb0(%arg: f32):
+          iree_linalg_ext.yield %arg : f32
+        } -> tensor<2x10x6x4xf16>
+  return %result : tensor<2x10x6x4xf16>
+}
+
+// CHECK-LABEL: func.func @attention_1
+// CHECK-NOT:   iree_codegen.compilation_info
```

**Comment:**
This checks for IREE attributes instead of keys in the attribute dictionary. A potential issue is that without `--mlir-print-ir-scope`, these attributes may be outlined **above the function** just like in the input IR.

Have you tried adding a new attribute, say `foo = #compilation`, and checking that the test does fail when the attribute remains in the output?

I'd think that we should check for the dictionary keys and/or print with local scope.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/strip_compilation_info.mlir:112`

```diff
@@ -60,3 +60,60 @@ func.func @matmul_128x1024x256_1(%lhs : tensor<128x256xf32>, %rhs: tensor<256x10
 
 // CHECK-LABEL: func.func @matmul_128x1024x256_1
 // CHECK-NOT:   iree_codegen.lowering_config
+
+// -----
+
+#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d4)>
+#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d5, d4)>
+#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d5)>
+#map3 = affine_map<(d0, d1, d2, d3, d4, d5) -> ()>
+#map4 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d3)>
+#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
+#map6 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d1, d3)>
+#config = #iree_codegen.lowering_config<tile_sizes = [[128, 256], [16, 16]]>
+func.func @attention(%arg0: tensor<2x10x6x4xf16>, %arg1 : tensor<2x10x4x4xf16>, %arg2 : tensor<2x10x4x4xf16>, %arg3 : f16) -> tensor<2x10x6x4xf16> attributes {translation_info = #iree_codegen.translation_info<pipeline = None subgroup_size = 32>} {
+  %init = tensor.empty() : tensor<2x10x6x4xf16>
+  %result = iree_linalg_ext.attention {decomposition_config = {x}, indexing_maps = [#map, #map1, #map2, #map3, #map4], lowering_config = #config} ins(%arg0, %arg1, %arg2, %arg3 : tensor<2x10x6x4xf16>, tensor<2x10x4x4xf16>, tensor<2x10x4x4xf16>, f16) outs(%init : tensor<2x10x6x4xf16>) {
+        ^bb0(%arg: f32):
+          iree_linalg_ext.yield %arg : f32
+        } -> tensor<2x10x6x4xf16>
+  return %result : tensor<2x10x6x4xf16>
+}
+
+// CHECK-LABEL: func.func @attention
+// CHECK-NOT:   iree_codegen.translation_info
+// CHECK-NOT:   iree_codegen.lowering_config
+// CHECK-NOT:   translation_info =
+// CHECK-NOT:   lowering_config =
+// CHECK-NOT:   decomposition_config =
+
+
+// -----
+
+#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d4)>
+#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d5, d4)>
+#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d5)>
+#map3 = affine_map<(d0, d1, d2, d3, d4, d5) -> ()>
+#map4 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d3)>
+#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
+#map6 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d1, d3)>
+#config = #iree_codegen.lowering_config<tile_sizes = [[128, 256], [16, 16]]>
+#translation = #iree_codegen.translation_info<pipeline = None workgroup_size = [16, 8, 1] subgroup_size = 64>
+#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
+func.func @attention_1(%arg0: tensor<2x10x6x4xf16>, %arg1 : tensor<2x10x4x4xf16>, %arg2 : tensor<2x10x4x4xf16>, %arg3 : f16) -> tensor<2x10x6x4xf16> attributes {translation_info = #iree_codegen.translation_info<pipeline = None subgroup_size = 32>} {
+  %init = tensor.empty() : tensor<2x10x6x4xf16>
+  %result = iree_linalg_ext.attention {decomposition_config = {x}, indexing_maps = [#map, #map1, #map2, #map3, #map4], compilation_info = #compilation} ins(%arg0, %arg1, %arg2, %arg3 : tensor<2x10x6x4xf16>, tensor<2x10x4x4xf16>, tensor<2x10x4x4xf16>, f16) outs(%init : tensor<2x10x6x4xf16>) {
+        ^bb0(%arg: f32):
+          iree_linalg_ext.yield %arg : f32
+        } -> tensor<2x10x6x4xf16>
+  return %result : tensor<2x10x6x4xf16>
+}
+
+// CHECK-LABEL: func.func @attention_1
```

**Comment:**
Can you also match the attention op?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/strip_compilation_info.mlir:76`

```diff
@@ -60,3 +60,60 @@ func.func @matmul_128x1024x256_1(%lhs : tensor<128x256xf32>, %rhs: tensor<256x10
 
 // CHECK-LABEL: func.func @matmul_128x1024x256_1
 // CHECK-NOT:   iree_codegen.lowering_config
+
+// -----
+
+#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d4)>
+#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d5, d4)>
+#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d5)>
+#map3 = affine_map<(d0, d1, d2, d3, d4, d5) -> ()>
+#map4 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d3)>
+#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
+#map6 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d1, d3)>
+#config = #iree_codegen.lowering_config<tile_sizes = [[128, 256], [16, 16]]>
+func.func @attention(%arg0: tensor<2x10x6x4xf16>, %arg1 : tensor<2x10x4x4xf16>, %arg2 : tensor<2x10x4x4xf16>, %arg3 : f16) -> tensor<2x10x6x4xf16> attributes {translation_info = #iree_codegen.translation_info<pipeline = None subgroup_size = 32>} {
+  %init = tensor.empty() : tensor<2x10x6x4xf16>
+  %result = iree_linalg_ext.attention {decomposition_config = {x}, indexing_maps = [#map, #map1, #map2, #map3, #map4], lowering_config = #config} ins(%arg0, %arg1, %arg2, %arg3 : tensor<2x10x6x4xf16>, tensor<2x10x4x4xf16>, tensor<2x10x4x4xf16>, f16) outs(%init : tensor<2x10x6x4xf16>) {
```

**Comment:**
Also here, it would be nice to check that the attention op is there

---


---


## [PR #20039](https://github.com/iree-org/iree/pull/20039): [Codegen][Tuner] add attention op into default tuning spec

### Review Summary

**CHANGES_REQUESTED** (2025-02-20)

This needs tests (both correctness and the expected performance improvements)


**COMMENTED** (2025-02-21)


**COMMENTED** (2025-02-21)


**COMMENTED** (2025-02-21)


**COMMENTED** (2025-02-21)


**COMMENTED** (2025-02-21)


**COMMENTED** (2025-02-24)

Looks good overall, thanks for the fixes. I wonder if we can reduce the test IR further.

Also, let's wait for review from @Groverkss before landing.


**COMMENTED** (2025-02-24)


**APPROVED** (2025-02-25)

Thanks for the cleanup, the tests look much more maintainable now.

Just a couple of remaining nits.


### Code Comments

**File:** `compiler/plugins/target/ROCM/builtins/tuning/iree_default_tuning_spec_gfx942.mlir:34`

```diff
@@ -19,6 +19,40 @@ transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.read
   transform.yield
 }
 
+transform.named_sequence @apply_attn_op_config(%attention: !transform.any_op {transform.readonly},
+                                                %config: !transform.any_param {transform.readonly},
+                                                %decomposition_config: !transform.any_param {transform.readonly}) {
+  transform.annotate %attention "compilation_info" = %config : !transform.any_op, !transform.any_param
+  transform.annotate %attention "decomposition_config" = %decomposition_config : !transform.any_op, !transform.any_param
+  transform.annotate %attention "__tuning_spec_applied__" : !transform.any_op
+  transform.yield
+}
+
+transform.named_sequence @match_attention_f16(%attention: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param, !transform.any_param) {
+    transform.match.operation_name %attention ["iree_linalg_ext.attention"] : !transform.any_op
+    %in0 = transform.get_operand %attention[0] : (!transform.any_op) -> !transform.any_value
+    transform.iree.match.cast_compatible_type %in0 = tensor<?x?x?x?xf16> : !transform.any_value
```

**Comment:**
Does this work for any attention size?

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/iree_default_tuning_spec_gfx942.mlir:34`

```diff
@@ -19,6 +19,40 @@ transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.read
   transform.yield
 }
 
+transform.named_sequence @apply_attn_op_config(%attention: !transform.any_op {transform.readonly},
+                                                %config: !transform.any_param {transform.readonly},
+                                                %decomposition_config: !transform.any_param {transform.readonly}) {
+  transform.annotate %attention "compilation_info" = %config : !transform.any_op, !transform.any_param
+  transform.annotate %attention "decomposition_config" = %decomposition_config : !transform.any_op, !transform.any_param
+  transform.annotate %attention "__tuning_spec_applied__" : !transform.any_op
+  transform.yield
+}
+
+transform.named_sequence @match_attention_f16(%attention: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param, !transform.any_param) {
+    transform.match.operation_name %attention ["iree_linalg_ext.attention"] : !transform.any_op
+    %in0 = transform.get_operand %attention[0] : (!transform.any_op) -> !transform.any_value
+    transform.iree.match.cast_compatible_type %in0 = tensor<?x?x?x?xf16> : !transform.any_value
```

**Comment:**
That's the source of my concern -- it applies to any attention size

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/iree_default_tuning_spec_gfx942.mlir:34`

```diff
@@ -19,6 +19,40 @@ transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.read
   transform.yield
 }
 
+transform.named_sequence @apply_attn_op_config(%attention: !transform.any_op {transform.readonly},
+                                                %config: !transform.any_param {transform.readonly},
+                                                %decomposition_config: !transform.any_param {transform.readonly}) {
+  transform.annotate %attention "compilation_info" = %config : !transform.any_op, !transform.any_param
+  transform.annotate %attention "decomposition_config" = %decomposition_config : !transform.any_op, !transform.any_param
+  transform.annotate %attention "__tuning_spec_applied__" : !transform.any_op
+  transform.yield
+}
+
+transform.named_sequence @match_attention_f16(%attention: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param, !transform.any_param) {
+    transform.match.operation_name %attention ["iree_linalg_ext.attention"] : !transform.any_op
+    %in0 = transform.get_operand %attention[0] : (!transform.any_op) -> !transform.any_value
+    transform.iree.match.cast_compatible_type %in0 = tensor<?x?x?x?xf16> : !transform.any_value
```

**Comment:**
> What is the expectation/goal here ?

To get good performance out of the box on attention in models we care about and don't error out on other attention variants we haven't seen.

This is the first step towards learning how to pick good default specs and how to test them, so that we figure out some process that we can use later on to add more default tuning specs for key contractions etc.

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/iree_default_tuning_spec_gfx942.mlir:34`

```diff
@@ -19,6 +19,40 @@ transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.read
   transform.yield
 }
 
+transform.named_sequence @apply_attn_op_config(%attention: !transform.any_op {transform.readonly},
+                                                %config: !transform.any_param {transform.readonly},
+                                                %decomposition_config: !transform.any_param {transform.readonly}) {
+  transform.annotate %attention "compilation_info" = %config : !transform.any_op, !transform.any_param
+  transform.annotate %attention "decomposition_config" = %decomposition_config : !transform.any_op, !transform.any_param
+  transform.annotate %attention "__tuning_spec_applied__" : !transform.any_op
+  transform.yield
+}
+
+transform.named_sequence @match_attention_f16(%attention: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param, !transform.any_param) {
+    transform.match.operation_name %attention ["iree_linalg_ext.attention"] : !transform.any_op
+    %in0 = transform.get_operand %attention[0] : (!transform.any_op) -> !transform.any_value
+    transform.iree.match.cast_compatible_type %in0 = tensor<?x?x?x?xf16> : !transform.any_value
```

**Comment:**
My intuition is to start small and match exactly the attention shapes we've tested this on. Later on we can do a sweep of attention sizes to see if we can have something more general.

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/iree_default_tuning_spec_gfx942.mlir:140`

```diff
@@ -66,7 +137,8 @@ transform.named_sequence
 @__kernel_config(%variant_op: !transform.any_op {transform.consumed}) -> !transform.any_op
   attributes { iree_codegen.tuning_spec_entrypoint } {
   %res = transform.foreach_match in %variant_op
-    @match_mmt_2048x1280x5120_f16_f16_f32 -> @apply_op_config
+    @match_attention_2x10x4096x64x64x64_f16 -> @apply_attn_op_config
```

**Comment:**
Can you add a comment with the expected speedup (for posterity)?

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/iree_default_tuning_spec_gfx942.mlir:74`

```diff
@@ -19,6 +19,77 @@ transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.read
   transform.yield
 }
 
+transform.named_sequence @apply_attn_op_config(%attention: !transform.any_op {transform.readonly},
+                                                %config: !transform.any_param {transform.readonly},
+                                                %decomposition_config: !transform.any_param {transform.readonly}) {
+  transform.annotate %attention "compilation_info" = %config : !transform.any_op, !transform.any_param
+  transform.annotate %attention "decomposition_config" = %decomposition_config : !transform.any_op, !transform.any_param
+  transform.annotate %attention "__tuning_spec_applied__" : !transform.any_op
+  transform.yield
+}
+
+transform.named_sequence @match_attention_f16(%root: !transform.any_op {transform.readonly})
+  -> !transform.any_op {
+  transform.match.operation_name %root ["iree_linalg_ext.attention"] : !transform.any_op
+  %ins, %outs = transform.iree.match.cast_compatible_dag_from_root %root {
+    ^bb0(%query: tensor<?x?x?x?xf16>,
+         %key: tensor<?x?x?x?xf16>,
+         %value: tensor<?x?x?x?xf16>,
+         %softmax_scale: f16,
+         %out: tensor<?x?x?x?xf16>):
+
+      %attn = iree_linalg_ext.attention {indexing_maps = [
+                                          affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d4)>,
+                                          affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d5, d4)>,
+                                          affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d5)>,
+                                          affine_map<(d0, d1, d2, d3, d4, d5) -> ()>,
+                                          affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d3)>]}
+        ins(%query, %key, %value, %softmax_scale :
+            tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>, f16)
+        outs(%out : tensor<?x?x?x?xf16>){
+          ^bb0(%arg0: f32):
+            iree_linalg_ext.yield %arg0 : f32
+        } -> tensor<?x?x?x?xf16>
+  } : (!transform.any_op) -> (!transform.any_value, !transform.any_value)
+
+  transform.yield %root : !transform.any_op
+}
+
+transform.named_sequence
+@match_attention_2x10x4096x64x64x64_f16(%attention: !transform.any_op {transform.readonly})
+  -> (!transform.any_op, !transform.any_param, !transform.any_param) {
+
+  %matched = transform.include @match_attention_f16 failures(propagate) (%attention)
+    : (!transform.any_op) -> !transform.any_op
+
+  %query = transform.get_operand %attention[0] : (!transform.any_op) -> !transform.any_value
+  %key = transform.get_operand %attention[1] : (!transform.any_op) -> !transform.any_value
+  %value = transform.get_operand %attention[2] : (!transform.any_op) -> !transform.any_value
+
+  transform.iree.match.cast_compatible_type %query = tensor<2x10x4096x64xf16> : !transform.any_value
+  transform.iree.match.cast_compatible_type %key = tensor<2x10x64x64xf16> : !transform.any_value
+  transform.iree.match.cast_compatible_type %value = tensor<2x10x64x64xf16> : !transform.any_value
+
+  %config = transform.param.constant #iree_codegen.compilation_info<
+          lowering_config = #iree_gpu.lowering_config<{workgroup = [1, 1, 64, 0, 0, 0], reduction=[0, 0, 0, 0, 0, 64], promote_operands = [1, 2]}>,
```

**Comment:**
I thought we wanted to do 1, 1, 128 for this shape?

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/test/spec_gfx942.mlir:88`

```diff
@@ -54,3 +54,58 @@ hal.executable public @main {
     }
   }
 }
+
+// -----
+
+// CHECK-LABEL:  func.func @attention_2x10x4096x64x64x64_f16
+// CHECK:          iree_linalg_ext.attention
+// CHECK-SAME:       __tuning_spec_applied__
+
+// MI300X-LABEL:  func.func @attention_2x10x4096x64x64x64_f16
+// MI300X:          iree_linalg_ext.attention
+// MI300X-SAME:       __tuning_spec_applied__
+
+hal.executable public @main {
+  hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb">) {
+    hal.executable.export public @attention ordinal(0) layout(#hal.pipeline.layout<constants = 2, bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) {
+    ^bb0(%arg0: !hal.device):
+      %x, %y, %z = flow.dispatch.workgroup_count_from_slice
+      hal.return %x, %y, %z : index, index, index
+    }
+    builtin.module {
+      // expected-remark@+1 {{Applied transform configuration strategy @iree_default_tuning_spec_gfx942::@__kernel_config}}
+      func.func @attention_2x10x4096x64x64x64_f16() {
+        %c85251584 = arith.constant 85251584 : index
+        %c283904 = arith.constant 283904 : index
+        %cst = arith.constant 1.250000e-01 : f16
+        %0 = hal.interface.constant.load layout(<constants = 2, bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) ordinal(0) : i32
+        %1 = hal.interface.constant.load layout(<constants = 2, bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) ordinal(1) : i32
+        %2 = arith.index_castui %0 : i32 to index
+        %3 = arith.index_castui %1 : i32 to index
+        %4:2 = util.assume.int
+            %2[<umin = 101640704, umax = 101640704, udiv = 101640704>, <umin = 101640704, umax = 101640704, udiv = 101640704>, <umin = 74765824, umax = 74765824, udiv = 74765824>],
+            %3[<umin = 91154944, umax = 91154944, udiv = 91154944>, <umin = 91154944, umax = 91154944, udiv = 91154944>, <umin = 64280064, umax = 64280064, udiv = 64280064>]
+          : index, index
```

**Comment:**
Can we simplify this test and remove parts that are not necessary?

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/test/spec_gfx942.mlir:77`

```diff
@@ -54,3 +54,58 @@ hal.executable public @main {
     }
   }
 }
+
+// -----
+
+// CHECK-LABEL:  func.func @attention_2x10x4096x64x64x64_f16
+// CHECK:          iree_linalg_ext.attention
+// CHECK-SAME:       __tuning_spec_applied__
+
+// MI300X-LABEL:  func.func @attention_2x10x4096x64x64x64_f16
+// MI300X:          iree_linalg_ext.attention
+// MI300X-SAME:       __tuning_spec_applied__
+
+hal.executable public @main {
+  hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb">) {
+    hal.executable.export public @attention ordinal(0) layout(#hal.pipeline.layout<constants = 2, bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) {
+    ^bb0(%arg0: !hal.device):
+      %x, %y, %z = flow.dispatch.workgroup_count_from_slice
+      hal.return %x, %y, %z : index, index, index
+    }
+    builtin.module {
+      // expected-remark@+1 {{Applied transform configuration strategy @iree_default_tuning_spec_gfx942::@__kernel_config}}
+      func.func @attention_2x10x4096x64x64x64_f16() {
```

**Comment:**
I think we should also have at least one negative test where the attention config is expected *not to apply*

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/test/spec_gfx942.mlir:98`

```diff
@@ -54,3 +54,107 @@ hal.executable public @main {
     }
   }
 }
+
+// -----
+
+// CHECK-LABEL:  func.func @attention_2x10x4096x64x64x64_f16
+// CHECK:          iree_linalg_ext.attention
+// CHECK-SAME:       __tuning_spec_applied__
+
+// MI300X-LABEL:  func.func @attention_2x10x4096x64x64x64_f16
+// MI300X:          iree_linalg_ext.attention
+// MI300X-SAME:       __tuning_spec_applied__
+
+#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d4)>
+#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d5, d4)>
+#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d5)>
+#map3 = affine_map<(d0, d1, d2, d3, d4, d5) -> ()>
+#map4 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d3)>
+#pipeline_layout = #hal.pipeline.layout<bindings = [
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>
+]>
+hal.executable public @main {
+  hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb">) {
+    hal.executable.export public @attention ordinal(0) layout(#hal.pipeline.layout<constants = 2, bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) {
+    ^bb0(%arg0: !hal.device):
+      %x, %y, %z = flow.dispatch.workgroup_count_from_slice
+      hal.return %x, %y, %z : index, index, index
+    }
+    builtin.module {
+      // expected-remark@+1 {{Applied transform configuration strategy @iree_default_tuning_spec_gfx942::@__kernel_config}}
+      func.func @attention_2x10x4096x64x64x64_f16() {
+        %cst = arith.constant 1.250000e-01 : f16
+        %c0 = arith.constant 0 : index
+        %0 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2x10x4096x64xf16>>
+        %1 = hal.interface.binding.subspan layout(#pipeline_layout) binding(1) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2x10x64x64xf16>>
+        %2 = hal.interface.binding.subspan layout(#pipeline_layout) binding(2) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2x10x64x64xf16>>
+        %3 = hal.interface.binding.subspan layout(#pipeline_layout) binding(3) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2x10x4096x64xf16>>
+        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0, 0, 0], sizes = [2, 10, 4096, 64], strides = [1, 1, 1, 1] : !flow.dispatch.tensor<readonly:tensor<2x10x4096x64xf16>> -> tensor<2x10x4096x64xf16>
+        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0, 0, 0], sizes = [2, 10, 64, 64], strides = [1, 1, 1, 1] : !flow.dispatch.tensor<readonly:tensor<2x10x64x64xf16>> -> tensor<2x10x64x64xf16>
+        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0, 0, 0], sizes = [2, 10, 64, 64], strides = [1, 1, 1, 1] : !flow.dispatch.tensor<readonly:tensor<2x10x64x64xf16>> -> tensor<2x10x64x64xf16>
+        %7 = tensor.empty() : tensor<2x10x4096x64xf16>
```

**Comment:**
I wonder if we could further reduce this by making these function arguments (even if it doesn't make sense in full compilation) and drop all of these hal and flow ops?

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/iree_default_tuning_spec_gfx942.mlir:140`

```diff
@@ -66,7 +137,9 @@ transform.named_sequence
 @__kernel_config(%variant_op: !transform.any_op {transform.consumed}) -> !transform.any_op
   attributes { iree_codegen.tuning_spec_entrypoint } {
   %res = transform.foreach_match in %variant_op
-    @match_mmt_2048x1280x5120_f16_f16_f32 -> @apply_op_config
+    // Notice approximately 1.22x speedup over the baseline by using this atten_op_config
```

**Comment:**
```suggestion
    // Expected speedup: 1.22x.
```

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/iree_default_tuning_spec_gfx942.mlir:22`

```diff
@@ -19,6 +19,77 @@ transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.read
   transform.yield
 }
 
+transform.named_sequence @apply_attn_op_config(%attention: !transform.any_op {transform.readonly},
```

**Comment:**
We have to start somewhere. I'd remove the warning or reword it to something along the lines that this is work in progress.

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/test/spec_gfx942.mlir:97`

```diff
@@ -54,3 +54,113 @@ hal.executable public @main {
     }
   }
 }
+
+// -----
+
+// CHECK-LABEL:  func.func @attention_2x10x4096x64x64x64_f16
+// CHECK:          iree_linalg_ext.attention
+// CHECK-SAME:       __tuning_spec_applied__
+
+// MI300X-LABEL:  func.func @attention_2x10x4096x64x64x64_f16
+// MI300X:          iree_linalg_ext.attention
+// MI300X-SAME:       __tuning_spec_applied__
+
+#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d4)>
+#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d5, d4)>
+#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d5)>
+#map3 = affine_map<(d0, d1, d2, d3, d4, d5) -> ()>
+#map4 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d3)>
+#pipeline_layout = #hal.pipeline.layout<bindings = [
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>
+]>
+hal.executable public @main {
+  hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb">) {
+    hal.executable.export public @attention ordinal(0) layout(#pipeline_layout) {
+    ^bb0(%arg0: !hal.device):
+      %x, %y, %z = flow.dispatch.workgroup_count_from_slice
+      hal.return %x, %y, %z : index, index, index
+    }
+    builtin.module {
+      // expected-remark@+1 {{Applied transform configuration strategy @iree_default_tuning_spec_gfx942::@__kernel_config}}
+      func.func @attention_2x10x4096x64x64x64_f16(
+        %query: tensor<2x10x4096x64xf16>,
+        %key: tensor<2x10x64x64xf16>,
+        %value: tensor<2x10x64x64xf16>
+      ) -> tensor<2x10x4096x64xf16> {
+
+        %cst = arith.constant 1.250000e-01 : f16
+        %output = tensor.empty() : tensor<2x10x4096x64xf16>
+
+        // Apply the attention operation directly to function inputs
```

**Comment:**
```suggestion
        // Apply the attention operation directly to function inputs.
```
https://llvm.org/docs/CodingStandards.html#commenting

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/test/spec_gfx942.mlir:152`

```diff
@@ -54,3 +54,113 @@ hal.executable public @main {
     }
   }
 }
+
+// -----
+
+// CHECK-LABEL:  func.func @attention_2x10x4096x64x64x64_f16
+// CHECK:          iree_linalg_ext.attention
+// CHECK-SAME:       __tuning_spec_applied__
+
+// MI300X-LABEL:  func.func @attention_2x10x4096x64x64x64_f16
+// MI300X:          iree_linalg_ext.attention
+// MI300X-SAME:       __tuning_spec_applied__
+
+#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d4)>
+#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d5, d4)>
+#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d5)>
+#map3 = affine_map<(d0, d1, d2, d3, d4, d5) -> ()>
+#map4 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d3)>
+#pipeline_layout = #hal.pipeline.layout<bindings = [
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>
+]>
+hal.executable public @main {
+  hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb">) {
+    hal.executable.export public @attention ordinal(0) layout(#pipeline_layout) {
+    ^bb0(%arg0: !hal.device):
+      %x, %y, %z = flow.dispatch.workgroup_count_from_slice
+      hal.return %x, %y, %z : index, index, index
+    }
+    builtin.module {
+      // expected-remark@+1 {{Applied transform configuration strategy @iree_default_tuning_spec_gfx942::@__kernel_config}}
+      func.func @attention_2x10x4096x64x64x64_f16(
+        %query: tensor<2x10x4096x64xf16>,
+        %key: tensor<2x10x64x64xf16>,
+        %value: tensor<2x10x64x64xf16>
+      ) -> tensor<2x10x4096x64xf16> {
+
+        %cst = arith.constant 1.250000e-01 : f16
+        %output = tensor.empty() : tensor<2x10x4096x64xf16>
+
+        // Apply the attention operation directly to function inputs
+        %result = iree_linalg_ext.attention {
+            indexing_maps = [#map, #map1, #map2, #map3, #map4]
+        } ins(%query, %key, %value, %cst :
+            tensor<2x10x4096x64xf16>, tensor<2x10x64x64xf16>, tensor<2x10x64x64xf16>, f16)
+          outs(%output : tensor<2x10x4096x64xf16>) {
+            ^bb0(%arg0: f32):
+              iree_linalg_ext.yield %arg0 : f32
+          } -> tensor<2x10x4096x64xf16>
+
+        return %result : tensor<2x10x4096x64xf16>
+      }
+    }
+  }
+}
+
+// -----
+
+// CHECK-LABEL:  func.func @attention_3x10x4096x64x64x32_f16
+// CHECK:          iree_linalg_ext.attention
+// CHECK-NOT:       __tuning_spec_applied__
+
+// MI300X-LABEL:  func.func @attention_3x10x4096x64x64x32_f16
+// MI300X:          iree_linalg_ext.attention
+// MI300X-NOT:       __tuning_spec_applied__
+
+#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d4)>
+#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d5, d4)>
+#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d5)>
+#map3 = affine_map<(d0, d1, d2, d3, d4, d5) -> ()>
+#map4 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d3)>
+#pipeline_layout = #hal.pipeline.layout<bindings = [
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>
+]>
+hal.executable public @main {
+  hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb">) {
+    hal.executable.export public @attention ordinal(0) layout(#pipeline_layout) {
+    ^bb0(%arg0: !hal.device):
+      %x, %y, %z = flow.dispatch.workgroup_count_from_slice
+      hal.return %x, %y, %z : index, index, index
+    }
+    builtin.module {
+      // expected-remark@+1 {{Applied transform configuration strategy @iree_default_tuning_spec_gfx942::@__kernel_config}}
+      func.func @attention_3x10x4096x64x64x32_f16(
+        %query: tensor<3x10x4096x64xf16>,
+        %key: tensor<3x10x32x64xf16>,
+        %value: tensor<3x10x64x32xf16>
+      ) -> tensor<3x10x4096x64xf16> {
+
+        %cst = arith.constant 1.250000e-01 : f16
+        %output = tensor.empty() : tensor<3x10x4096x64xf16>
+
+        // Apply the attention operation directly to function inputs
```

**Comment:**
```suggestion
        // Apply the attention operation directly to function inputs.
```

---


---


## [PR #19762](https://github.com/iree-org/iree/pull/19762): [Codegen][Tuner] Add support for per-sku tuning spec

### Review Summary

**CHANGES_REQUESTED** (2025-01-22)


**COMMENTED** (2025-01-22)


**COMMENTED** (2025-01-22)


**COMMENTED** (2025-01-22)


**COMMENTED** (2025-01-23)


**COMMENTED** (2025-01-23)

Looks much better now


**COMMENTED** (2025-01-23)


**COMMENTED** (2025-01-23)


**COMMENTED** (2025-01-23)


**APPROVED** (2025-01-23)

This LGTM % the minor issue above. But let's wait for one more approval before landing, maybe from @MaheshRavishankar or @qedawkins.


**COMMENTED** (2025-01-23)


**COMMENTED** (2025-01-24)


**APPROVED** (2025-01-24)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:130`

```diff
@@ -125,12 +125,25 @@ getDefaultTuningSpec(ModuleOp module,
 
   // Try to look up the default tuning spec for this architecture, if any.
   StringRef arch = gpuTarget.getArch();
+  std::optional<std::string> sku = gpuTarget.getSKU();
   std::string defaultTuningSpecName =
       llvm::formatv("iree_default_tuning_spec_{}.mlir", arch);
```

**Comment:**
This can now be moved into the code block where it's used.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.cpp:1097`

```diff
@@ -1094,6 +1094,23 @@ bool TargetAttr::supportsSyncMMAOps() const {
   return false;
 }
 
+std::optional<std::string> TargetAttr::getSKU() const {
```

**Comment:**
This is not the best location for this code -- generic gpu attributes shouldn't know about the exact hardware details.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.cpp:1097`

```diff
@@ -1094,6 +1094,23 @@ bool TargetAttr::supportsSyncMMAOps() const {
   return false;
 }
 
+std::optional<std::string> TargetAttr::getSKU() const {
```

**Comment:**
Why not return `optional<StringRef>`?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.cpp:1111`

```diff
@@ -1094,6 +1094,23 @@ bool TargetAttr::supportsSyncMMAOps() const {
   return false;
 }
 
+std::optional<std::string> TargetAttr::getSKU() const {
+  StringRef arch = getArch();
+  if (arch == "gfx942") {
+    TargetChipAttr chip = getChip();
+    if (chip) {
+      if (chip.getWgpCount() == 304) {
+        return "mi300x";
+      } else if (chip.getWgpCount() == 228) {
+        return "mi300a";
+      } else if (chip.getWgpCount() == 80) {
+        return "mi308x";
+      }
+    }
+  }
+  return std::nullopt;
```

**Comment:**
Please update this code to follow the llvm coding standards: https://llvm.org/docs/CodingStandards.html

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:958`

```diff
@@ -954,6 +954,8 @@ IREE::GPU::TargetAttr getCLGPUTarget(MLIRContext *context) {
       backend = "cuda";
     else if (StringRef(clTestTarget).starts_with("gfx"))
       backend = "hip";
+    else if (StringRef(clTestTarget).starts_with("mi"))
+      backend = "hip";
```

**Comment:**
I don't think we need to change this -- we can use the existing way of specifying targets and backends separately.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.td:504`

```diff
@@ -499,6 +499,28 @@ def IREEGPU_TargetAttr : AttrDef<IREEGPU_Dialect, "Target"> {
     bool supportsTF32InputMMAOps() const;
     // Returns true if this target supports TensorCore synchronized MMA ops.
     bool supportsSyncMMAOps() const;
+    // Returns the SKU of the target GPU if available.
+    std::optional<StringRef> getSKU() const {
+        llvm::StringRef arch = getArch();
```

**Comment:**
I think we should move this code somewhere close to or in `KnownTargets.cpp`; The generic GPU attributes shouldn't know anything about the rocm backend details and hip targets.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:146`

```diff
@@ -123,14 +123,28 @@ getDefaultTuningSpec(ModuleOp module,
     return failure();
   }
 
-  // Try to look up the default tuning spec for this architecture, if any.
-  StringRef arch = gpuTarget.getArch();
-  std::string defaultTuningSpecName =
-      llvm::formatv("iree_default_tuning_spec_{}.mlir", arch);
+  std::optional<StringRef> sku = gpuTarget.getSKU();
+  std::string defaultTuningSpecName;
   std::optional<StringRef> defaultTuningSpecSource;
-  EmbeddedDataDirectory::withGlobal([&](EmbeddedDataDirectory &dir) {
-    defaultTuningSpecSource = dir.getFile(defaultTuningSpecName);
-  });
+  if (sku) {
+    // Try to look up the default tuning spec for this sku.
+    defaultTuningSpecName =
+        llvm::formatv("iree_default_tuning_spec_{}.mlir", sku);
+    EmbeddedDataDirectory::withGlobal([&](EmbeddedDataDirectory &dir) {
+      defaultTuningSpecSource = dir.getFile(defaultTuningSpecName);
+    });
+  }
+  if (!defaultTuningSpecSource) {
+    // If SKU-specific spec is not found, fall back to the default
+    // architecture-based tuning spec.
+    StringRef arch = gpuTarget.getArch();
+    defaultTuningSpecName =
+        llvm::formatv("iree_default_tuning_spec_{}.mlir", arch);
+    EmbeddedDataDirectory::withGlobal([&](EmbeddedDataDirectory &dir) {
+      defaultTuningSpecSource = dir.getFile(defaultTuningSpecName);
+    });
+  }
```

**Comment:**
This function is getting a bit long, I think it'd be better to outline this code to a helper function.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.td:506`

```diff
@@ -499,6 +499,28 @@ def IREEGPU_TargetAttr : AttrDef<IREEGPU_Dialect, "Target"> {
     bool supportsTF32InputMMAOps() const;
     // Returns true if this target supports TensorCore synchronized MMA ops.
     bool supportsSyncMMAOps() const;
+    // Returns the SKU of the target GPU if available.
+    std::optional<StringRef> getSKU() const {
+        llvm::StringRef arch = getArch();
+
+        if (arch == "gfx942") {
```

**Comment:**
Invert this condition and use an early return: https://llvm.org/docs/CodingStandards.html#use-early-exits-and-continue-to-simplify-code

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/KnownTargets.cpp:708`

```diff
@@ -693,6 +693,30 @@ StringRef normalizeHIPTarget(StringRef target) {
   return normalizeAMDGPUTarget(target);
 }
 
+std::optional<StringRef> getAMDSKU(TargetAttr target) {
+  StringRef arch = target.getArch();
+
+  if (arch != "gfx942") {
+    return std::nullopt;
+  }
+
+  TargetChipAttr chip = target.getChip();
+  if (chip) {
+    uint32_t wgpCount = chip.getWgpCount();
+    if (wgpCount == 304) {
+      return "mi300x";
+    }
```

**Comment:**
Thanks for all the fixes around this code. I spend some time thinking about this approach of raising the target attribute back to the sku and wasn't sure if it's a good idea or not. It seems simple and thought that it may be good enough for amdgpu for now, but I found a counterexample: mi325. It has the same number of CUs as mi300, but the performance characteristics are different.

I think that to make this robust, we have to go back to [what I suggested previously](https://github.com/iree-org/iree/pull/19748#discussion_r1924336701) and record the sku in the target attribute itself, similar to how we keep the target arch around today.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/KnownTargets.cpp:57`

```diff
@@ -54,6 +54,7 @@ struct WgpDetails {
 // Chip level feature/limit details
 struct ChipDetails {
   uint32_t wgpCount;
+  std::optional<llvm::StringRef> sku;
```

**Comment:**
```suggestion
  std::optional<StringRef> sku;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/KnownTargets.cpp:123`

```diff
@@ -116,9 +117,13 @@ TargetAttr createTargetAttr(const TargetDetails &details, StringRef arch,
       DictionaryAttr{});
 
   TargetChipAttr targetChip;
-  if (details.chip)
-    targetChip =
-        TargetChipAttr::get(context, details.chip->wgpCount, DictionaryAttr{});
+  if (details.chip) {
+    StringAttr skuAttr = details.chip->sku
+                             ? StringAttr::get(context, *(details.chip->sku))
+                             : StringAttr::get(context, "");
```

**Comment:**
optional has a `.value_or` function, we should use it here. https://en.cppreference.com/w/cpp/utility/optional/value_or

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:140`

```diff
@@ -123,14 +135,28 @@ getDefaultTuningSpec(ModuleOp module,
     return failure();
   }
 
-  // Try to look up the default tuning spec for this architecture, if any.
-  StringRef arch = gpuTarget.getArch();
-  std::string defaultTuningSpecName =
-      llvm::formatv("iree_default_tuning_spec_{}.mlir", arch);
+  std::optional<StringRef> sku;
+  if (IREE::GPU::TargetChipAttr chip = gpuTarget.getChip()) {
+    StringAttr chipSku = chip.getSku();
```

**Comment:**
Since chipSKU is already an optional attribute, I'd not expect to also find empty strings here. We can add an assertion for this.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:142`

```diff
@@ -123,14 +135,28 @@ getDefaultTuningSpec(ModuleOp module,
     return failure();
   }
 
-  // Try to look up the default tuning spec for this architecture, if any.
-  StringRef arch = gpuTarget.getArch();
-  std::string defaultTuningSpecName =
-      llvm::formatv("iree_default_tuning_spec_{}.mlir", arch);
+  std::optional<StringRef> sku;
+  if (IREE::GPU::TargetChipAttr chip = gpuTarget.getChip()) {
+    std::optional<StringAttr> chipSku = chip.getSku();
+    if (chipSku) {
+      sku = (*chipSku).getValue();
```

**Comment:**
```suggestion
    if (std::optional<StringAttr> chipSku = chip.getSku()) {
      sku = chipSku->getValue();
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:149`

```diff
@@ -123,14 +135,28 @@ getDefaultTuningSpec(ModuleOp module,
     return failure();
   }
 
-  // Try to look up the default tuning spec for this architecture, if any.
-  StringRef arch = gpuTarget.getArch();
-  std::string defaultTuningSpecName =
-      llvm::formatv("iree_default_tuning_spec_{}.mlir", arch);
+  std::optional<StringRef> sku;
+  if (IREE::GPU::TargetChipAttr chip = gpuTarget.getChip()) {
+    std::optional<StringAttr> chipSku = chip.getSku();
+    if (chipSku) {
+      sku = (*chipSku).getValue();
+    }
+  }
+
+  std::string defaultTuningSpecName;
   std::optional<StringRef> defaultTuningSpecSource;
-  EmbeddedDataDirectory::withGlobal([&](EmbeddedDataDirectory &dir) {
-    defaultTuningSpecSource = dir.getFile(defaultTuningSpecName);
-  });
+  if (sku) {
+    // Try to look up the default tuning spec for this sku.
```

**Comment:**
This comment doesn't clarify much beyond what the code does. Focus on **why** when writing comments, not **what**.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:155`

```diff
@@ -123,14 +135,28 @@ getDefaultTuningSpec(ModuleOp module,
     return failure();
   }
 
-  // Try to look up the default tuning spec for this architecture, if any.
-  StringRef arch = gpuTarget.getArch();
-  std::string defaultTuningSpecName =
-      llvm::formatv("iree_default_tuning_spec_{}.mlir", arch);
+  std::optional<StringRef> sku;
+  if (IREE::GPU::TargetChipAttr chip = gpuTarget.getChip()) {
+    std::optional<StringAttr> chipSku = chip.getSku();
+    if (chipSku) {
+      sku = (*chipSku).getValue();
+    }
+  }
+
+  std::string defaultTuningSpecName;
   std::optional<StringRef> defaultTuningSpecSource;
-  EmbeddedDataDirectory::withGlobal([&](EmbeddedDataDirectory &dir) {
-    defaultTuningSpecSource = dir.getFile(defaultTuningSpecName);
-  });
+  if (sku) {
+    // Try to look up the default tuning spec for this sku.
+    defaultTuningSpecSource = fetchDefaultTuningSpec(*sku);
+  }
+
+  if (!defaultTuningSpecSource) {
+    // If SKU-specific spec is not found, fall back to the default
+    // architecture-based tuning spec.
```

**Comment:**
Here, the comment is very useful because it explains why we are attempting to fetch this spec.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/KnownTargets.cpp:125`

```diff
@@ -116,9 +117,15 @@ TargetAttr createTargetAttr(const TargetDetails &details, StringRef arch,
       DictionaryAttr{});
 
   TargetChipAttr targetChip;
-  if (details.chip)
-    targetChip =
-        TargetChipAttr::get(context, details.chip->wgpCount, DictionaryAttr{});
+  if (details.chip) {
+    std::optional<StringAttr> skuAttr =
+        details.chip->sku && !details.chip->sku->empty()
+            ? std::optional<StringAttr>(
+                  StringAttr::get(context, *details.chip->sku))
+            : std::nullopt;
```

**Comment:**
This ternary is very complex. Imo this should be an if statement.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.td:420`

```diff
@@ -416,6 +416,8 @@ def IREEGPU_TargetChipAttr : AttrDef<IREEGPU_Dialect, "TargetChip"> {
   let parameters = (ins
     "uint32_t":$wgp_count,
 
+    // An optional SKU identifier to distinguish different models.
+    OptionalParameter<"std::optional<StringAttr>">:$sku,
```

**Comment:**
I don't think this should be double-optional

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/KnownTargets.cpp:122`

```diff
@@ -116,9 +117,14 @@ TargetAttr createTargetAttr(const TargetDetails &details, StringRef arch,
       DictionaryAttr{});
 
   TargetChipAttr targetChip;
-  if (details.chip)
-    targetChip =
-        TargetChipAttr::get(context, details.chip->wgpCount, DictionaryAttr{});
+  if (details.chip) {
+    std::optional<StringAttr> skuAttr = std::nullopt;
+    if (details.chip->sku && !details.chip->sku->empty()) {
```

**Comment:**
When can the sku be present but empty? I don't think this happens with the current code.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/KnownTargets.cpp:122`

```diff
@@ -116,9 +117,14 @@ TargetAttr createTargetAttr(const TargetDetails &details, StringRef arch,
       DictionaryAttr{});
 
   TargetChipAttr targetChip;
-  if (details.chip)
-    targetChip =
-        TargetChipAttr::get(context, details.chip->wgpCount, DictionaryAttr{});
+  if (details.chip) {
+    std::optional<StringAttr> skuAttr = std::nullopt;
+    if (details.chip->sku && !details.chip->sku->empty()) {
```

**Comment:**
I don't think this is something we have to check at all

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/KnownTargets.cpp:123`

```diff
@@ -116,9 +117,12 @@ TargetAttr createTargetAttr(const TargetDetails &details, StringRef arch,
       DictionaryAttr{});
 
   TargetChipAttr targetChip;
-  if (details.chip)
-    targetChip =
-        TargetChipAttr::get(context, details.chip->wgpCount, DictionaryAttr{});
+  if (details.chip) {
+    StringAttr skuAttr =
+        StringAttr::get(context, details.chip->sku.value_or(""));
+    targetChip = TargetChipAttr::get(context, details.chip->wgpCount, skuAttr,
```

**Comment:**
If sku is optional, why are we setting it with an empty string? Is empty string considered the same as `nullptr` in `StringAttr`?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/KnownTargets.cpp:122`

```diff
@@ -116,9 +117,12 @@ TargetAttr createTargetAttr(const TargetDetails &details, StringRef arch,
       DictionaryAttr{});
 
   TargetChipAttr targetChip;
-  if (details.chip)
-    targetChip =
-        TargetChipAttr::get(context, details.chip->wgpCount, DictionaryAttr{});
+  if (details.chip) {
+    StringAttr skuAttr =
+        StringAttr::get(context, details.chip->sku.value_or(""));
```

**Comment:**
```suggestion
    auto skuAttr =
        StringAttr::get(context, details.chip->sku.value_or(""));
```
The type is obvious based on the RHS: https://llvm.org/docs/CodingStandards.html#use-auto-type-deduction-to-make-code-more-readable

---


---


## [PR #19756](https://github.com/iree-org/iree/pull/19756): [Codegen] add mi308x target

### Review Summary

**COMMENTED** (2025-01-22)


**COMMENTED** (2025-01-22)

Can you link to the relevant issue in the PR description?


**APPROVED** (2025-01-22)

LGTM


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/KnownTargets.cpp:343`

```diff
@@ -338,7 +340,7 @@ StringRef normalizeAMDGPUTarget(StringRef target) {
     return target;
 
   return llvm::StringSwitch<StringRef>(target.lower())
-      .Cases("mi300x", "mi300a", "gfx942")
+      .Cases("mi300x", "mi300a", "mi308x", "gfx942")
```

**Comment:**
I'd think we should append them to keep consistent with `.Cases("cdna3", "gfx940", "gfx941", "gfx942",` above where newer targets are placed towards the end.

Ultimately I don't care as long as we maintain some consistent ordering.

---


---


## [PR #19748](https://github.com/iree-org/iree/pull/19748): [Codegen][Tuner] default tuning spec available per-SKU

### Review Summary

**CHANGES_REQUESTED** (2025-01-21)

We should split this into three (or more) PRs:
* Add a known target for mi308
* Implement support for per-sku tuning specs
* Populate tuning specs for mi308


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:37`

```diff
@@ -34,6 +34,8 @@
 #define DBGS() (llvm::dbgs() << "[" DEBUG_TYPE "]: ")
 #define LDBG(X) LLVM_DEBUG(DBGS() << X << "\n")
 
+extern llvm::cl::opt<std::string> clTestTarget;
```

**Comment:**
In general, we should no reference global variables like this. This makes the code hard to maintain and LLVM converged on external storage for flags meant to be external, and having these flag storage variables declared in headers.

Here specifically, we should not rely on test flags in this code. All the information we use must come from the gpu target attr. If the information we need there is not available, we should work on adding it.

---

**File:** `compiler/plugins/target/ROCM/builtins/tuning/iree_default_tuning_spec_mi308x.mlir:243`

```diff
@@ -0,0 +1,263 @@
+// RUN: iree-opt %s
+
+// This is just an initial tuning spec for mi308x and is not intended for
+// production use.
+// TODO(https://github.com/iree-org/iree/issues/19214): Add missing
+// configurations to this spec.
+
+module @iree_default_tuning_spec_mi308x attributes { transform.with_named_sequence, iree_codegen.tuning_spec_with_default_entrypoint} {
+//===----------------------------------------------------------------------===//
+// Tuning infra
+//===----------------------------------------------------------------------===//
+
+transform.named_sequence @apply_op_config(%op: !transform.any_op {transform.readonly},
+                                          %config: !transform.any_param {transform.readonly}) {
+  transform.annotate %op "compilation_info" = %config : !transform.any_op, !transform.any_param
+  transform.annotate %op "__tuning_spec_applied__" : !transform.any_op
+  transform.yield
+}
+
+transform.named_sequence @apply_attn_op_config(%attention: !transform.any_op {transform.readonly},
+                                                %config: !transform.any_param {transform.readonly},
+                                                %decomposition_config: !transform.any_param {transform.readonly}) {
+  transform.annotate %attention "compilation_info" = %config : !transform.any_op, !transform.any_param
+  transform.annotate %attention "decomposition_config" = %decomposition_config : !transform.any_op, !transform.any_param
+  // transform.print %attention {name = "Applied attention config"} : !transform.any_op
+  transform.yield
+}
+
+transform.named_sequence @match_attention_f16(%attention: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param, !transform.any_param) {
+  transform.match.operation_name %attention ["iree_linalg_ext.attention"] : !transform.any_op
+  %in0 = transform.get_operand %attention[0] : (!transform.any_op) -> !transform.any_value
+  transform.iree.match.cast_compatible_type %in0 = tensor<?x?x?x?xf16> : !transform.any_value
+
+  %config = transform.param.constant #iree_codegen.compilation_info<
+          lowering_config = #iree_gpu.lowering_config<{workgroup = [1, 1, 64, 0, 0, 0], reduction=[0, 0, 0, 0, 0, 64], promote_operands = [1, 2]}>,
+          translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUVectorDistribute
+                                                            workgroup_size = [64, 4]
+                                                            subgroup_size = 64 ,
+            {llvm_func_attrs = { "amdgpu-waves-per-eu" = "2", "denormal-fp-math-f32" = "preserve-sign" }}>>
+  -> !transform.any_param
+
+  %decomposition_config = transform.param.constant {
+    qk_attrs = {attention_qk_matmul,
+                lowering_config = #iree_gpu.lowering_config<{mma_kind = #iree_gpu.virtual_mma_layout<intrinsic = VMFMA_F32_32x32x16_F16>,
+                                                              subgroup_m_count = 4, subgroup_n_count = 1, promote_operands = [1] }>},
+    pv_attrs = {attention_pv_matmul,
+                lowering_config = #iree_gpu.lowering_config<{mma_kind = #iree_gpu.mma_layout<MFMA_F32_32x32x8_F16>,
+                                                              subgroup_m_count = 4, subgroup_n_count = 1, promote_operands = [1] }>}
+  } -> !transform.any_param
+
+  transform.yield %attention, %config, %decomposition_config : !transform.any_op, !transform.any_param, !transform.any_param
+}
+
+transform.named_sequence @match_mmt_f16_f16_f32(%root: !transform.any_op {transform.readonly}) -> (!transform.any_op) {
+  transform.match.operation_name %root ["linalg.generic"] : !transform.any_op
+  // transform.print %root {name = "Generic"} : !transform.any_op
+  %ins, %outs = transform.iree.match.cast_compatible_dag_from_root %root {
+    ^bb0(%lhs: tensor<?x?xf16>, %rhs: tensor<?x?xf16>, %out: tensor<?x?xf32>):
+    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>,
+                                          affine_map<(d0, d1, d2) -> (d1, d2)>,
+                                          affine_map<(d0, d1, d2) -> (d0, d1)>],
+                         iterator_types = ["parallel", "parallel", "reduction"]}
+        ins(%lhs, %rhs : tensor<?x?xf16>, tensor<?x?xf16>) outs(%out : tensor<?x?xf32>) {
+      ^bb0(%in: f16, %in_0: f16, %acc: f32):
+        %18 = arith.extf %in : f16 to f32
+        %19 = arith.extf %in_0 : f16 to f32
+        %20 = arith.mulf %18, %19 : f32
+        %21 = arith.addf %acc, %20 : f32
+        linalg.yield %21 : f32
+      } -> tensor<?x?xf32>
+  } : (!transform.any_op) -> (!transform.any_value, !transform.any_value)
+  transform.yield %root : !transform.any_op
+}
+
+// TUNING_SPEC_BEGIN DO NOT REMOVE
+
+//===----------------------------------------------------------------------===//
+// Matmul tuning
+//===----------------------------------------------------------------------===//
+
+transform.named_sequence @match_mmt_1920x10240x1280(%matmul: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param) {
+  %mmt = transform.include @match_mmt_f16_f16_f32 failures(propagate) (%matmul) : (!transform.any_op) -> !transform.any_op
+  %lhs = transform.get_operand %matmul[0] : (!transform.any_op) -> !transform.any_value
+  %rhs = transform.get_operand %matmul[1] : (!transform.any_op) -> !transform.any_value
+  transform.iree.match.cast_compatible_type %lhs = tensor<1920x1280xf16> : !transform.any_value
+  transform.iree.match.cast_compatible_type %rhs = tensor<10240x1280xf16> : !transform.any_value
+  %config = transform.param.constant #iree_codegen.compilation_info<
+  lowering_config = #iree_gpu.lowering_config<{promote_operands = [0, 1],
+                                                mma_kind = #iree_gpu.mma_layout<MFMA_F32_16x16x16_F16>,
+                                                subgroup_m_count = 4, subgroup_n_count = 2,
+                                                reduction = [0, 0, 32],
+                                                workgroup = [128, 128, 0]}>,
+  translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUVectorDistribute
+    workgroup_size = [128, 4, 1] subgroup_size = 64,
+    {gpu_pipeline_options = #iree_gpu.pipeline_options<prefetch_shared_memory = true>,
+     llvm_func_attrs = {"amdgpu-waves-per-eu" = "2"}
+    }>> -> !transform.any_param
+  transform.yield %matmul, %config : !transform.any_op, !transform.any_param
+}
+
+transform.named_sequence @match_mmt_1920x1280x1280(%matmul: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param) {
+  %mmt = transform.include @match_mmt_f16_f16_f32 failures(propagate) (%matmul) : (!transform.any_op) -> !transform.any_op
+  %lhs = transform.get_operand %matmul[0] : (!transform.any_op) -> !transform.any_value
+  %rhs = transform.get_operand %matmul[1] : (!transform.any_op) -> !transform.any_value
+  transform.iree.match.cast_compatible_type %lhs = tensor<1920x1280xf16> : !transform.any_value
+  transform.iree.match.cast_compatible_type %rhs = tensor<1280x1280xf16> : !transform.any_value
+  %config = transform.param.constant #iree_codegen.compilation_info<
+  lowering_config = #iree_gpu.lowering_config<{promote_operands = [0, 1],
+                                                mma_kind = #iree_gpu.mma_layout<MFMA_F32_16x16x16_F16>,
+                                                subgroup_m_count = 4, subgroup_n_count = 2,
+                                                reduction = [0, 0, 32],
+                                                workgroup = [128, 128, 0]}>,
+  translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUVectorDistribute
+    workgroup_size = [128, 4, 1] subgroup_size = 64,
+    {gpu_pipeline_options = #iree_gpu.pipeline_options<prefetch_shared_memory = true>,
+     llvm_func_attrs = {"amdgpu-waves-per-eu" = "2"}
+    }>> -> !transform.any_param
+  transform.yield %matmul, %config : !transform.any_op, !transform.any_param
+}
+
+transform.named_sequence @match_mmt_1920x1280x5120(%matmul: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param) {
+  %mmt = transform.include @match_mmt_f16_f16_f32 failures(propagate) (%matmul) : (!transform.any_op) -> !transform.any_op
+  %lhs = transform.get_operand %matmul[0] : (!transform.any_op) -> !transform.any_value
+  %rhs = transform.get_operand %matmul[1] : (!transform.any_op) -> !transform.any_value
+  transform.iree.match.cast_compatible_type %lhs = tensor<1920x5120xf16> : !transform.any_value
+  transform.iree.match.cast_compatible_type %rhs = tensor<1280x5120xf16> : !transform.any_value
+  %config = transform.param.constant #iree_codegen.compilation_info<
+  lowering_config = #iree_gpu.lowering_config<{promote_operands = [0, 1],
+                                                mma_kind = #iree_gpu.mma_layout<MFMA_F32_16x16x16_F16>,
+                                                subgroup_m_count = 4, subgroup_n_count = 2,
+                                                reduction = [0, 0, 32],
+                                                workgroup = [128, 128, 0]}>,
+  translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUVectorDistribute
+    workgroup_size = [128, 4, 1] subgroup_size = 64,
+    {gpu_pipeline_options = #iree_gpu.pipeline_options<prefetch_shared_memory = true>,
+     llvm_func_attrs = {"amdgpu-waves-per-eu" = "2"}
+    }>> -> !transform.any_param
+  transform.yield %matmul, %config : !transform.any_op, !transform.any_param
+}
+
+transform.named_sequence @match_mmt_7680x5120x640(%matmul: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param) {
+  %mmt = transform.include @match_mmt_f16_f16_f32 failures(propagate) (%matmul) : (!transform.any_op) -> !transform.any_op
+  %lhs = transform.get_operand %matmul[0] : (!transform.any_op) -> !transform.any_value
+  %rhs = transform.get_operand %matmul[1] : (!transform.any_op) -> !transform.any_value
+  transform.iree.match.cast_compatible_type %lhs = tensor<7680x640xf16> : !transform.any_value
+  transform.iree.match.cast_compatible_type %rhs = tensor<5120x640xf16> : !transform.any_value
+  %config = transform.param.constant #iree_codegen.compilation_info<
+  lowering_config = #iree_gpu.lowering_config<{promote_operands = [0, 1],
+                                                mma_kind = #iree_gpu.mma_layout<MFMA_F32_16x16x16_F16>,
+                                                subgroup_m_count = 2, subgroup_n_count = 4,
+                                                reduction = [0, 0, 32],
+                                                workgroup = [128, 256, 0]}>,
+  translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUVectorDistribute
+    workgroup_size = [256, 2, 1] subgroup_size = 64,
+    {gpu_pipeline_options = #iree_gpu.pipeline_options<prefetch_shared_memory = true>,
+     llvm_func_attrs = {"amdgpu-waves-per-eu" = "2"}
+    }>> -> !transform.any_param
+  transform.yield %matmul, %config : !transform.any_op, !transform.any_param
+}
+
+transform.named_sequence @match_mmt_128x1280x2048(%matmul: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param) {
+  %mmt = transform.include @match_mmt_f16_f16_f32 failures(propagate) (%matmul) : (!transform.any_op) -> !transform.any_op
+  %lhs = transform.get_operand %matmul[0] : (!transform.any_op) -> !transform.any_value
+  %rhs = transform.get_operand %matmul[1] : (!transform.any_op) -> !transform.any_value
+  transform.iree.match.cast_compatible_type %lhs = tensor<1280x2048xf16> : !transform.any_value
+  transform.iree.match.cast_compatible_type %rhs = tensor<1280x2048xf16> : !transform.any_value
+  %config = transform.param.constant #iree_codegen.compilation_info<
+  lowering_config = #iree_gpu.lowering_config<{promote_operands = [0, 1],
+                                                mma_kind = #iree_gpu.mma_layout<MFMA_F32_16x16x16_F16>,
+                                                subgroup_m_count = 2, subgroup_n_count = 1,
+                                                reduction = [0, 0, 128],
+                                                workgroup = [64, 16, 0]}>,
+  translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUVectorDistribute
+    workgroup_size = [64, 2, 1] subgroup_size = 64,
+    {gpu_pipeline_options = #iree_gpu.pipeline_options<prefetch_shared_memory = true>,
+     llvm_func_attrs = {"amdgpu-waves-per-eu" = "2"}
+    }>> -> !transform.any_param
+  transform.yield %matmul, %config : !transform.any_op, !transform.any_param
+}
+
+transform.named_sequence @match_mmt_7680x640x640(%matmul: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param) {
+  %mmt = transform.include @match_mmt_f16_f16_f32 failures(propagate) (%matmul) : (!transform.any_op) -> !transform.any_op
+  %lhs = transform.get_operand %matmul[0] : (!transform.any_op) -> !transform.any_value
+  %rhs = transform.get_operand %matmul[1] : (!transform.any_op) -> !transform.any_value
+  transform.iree.match.cast_compatible_type %lhs = tensor<7680x640xf16> : !transform.any_value
+  transform.iree.match.cast_compatible_type %rhs = tensor<640x640xf16> : !transform.any_value
+  %config = transform.param.constant #iree_codegen.compilation_info<
+  lowering_config = #iree_gpu.lowering_config<{promote_operands = [0, 1],
+                                                mma_kind = #iree_gpu.mma_layout<MFMA_F32_32x32x8_F16>,
+                                                subgroup_m_count = 1, subgroup_n_count = 4,
+                                                reduction = [0, 0, 32],
+                                                workgroup = [256, 128, 0]}>,
+  translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUVectorDistribute
+    workgroup_size = [256, 1, 1] subgroup_size = 64,
+    {gpu_pipeline_options = #iree_gpu.pipeline_options<prefetch_shared_memory = true>,
+     llvm_func_attrs = {"amdgpu-waves-per-eu" = "1"}
+    }>> -> !transform.any_param
+  transform.yield %matmul, %config : !transform.any_op, !transform.any_param
+}
+
+transform.named_sequence @match_mmt_7680x640x2560(%matmul: !transform.any_op {transform.readonly}) -> (!transform.any_op, !transform.any_param) {
+  %mmt = transform.include @match_mmt_f16_f16_f32 failures(propagate) (%matmul) : (!transform.any_op) -> !transform.any_op
+  %lhs = transform.get_operand %matmul[0] : (!transform.any_op) -> !transform.any_value
+  %rhs = transform.get_operand %matmul[1] : (!transform.any_op) -> !transform.any_value
+  transform.iree.match.cast_compatible_type %lhs = tensor<7680x2560xf16> : !transform.any_value
+  transform.iree.match.cast_compatible_type %rhs = tensor<640x2560xf16> : !transform.any_value
+  %config = transform.param.constant #iree_codegen.compilation_info<
+  lowering_config = #iree_gpu.lowering_config<{promote_operands = [0, 1],
+                                                mma_kind = #iree_gpu.mma_layout<MFMA_F32_32x32x8_F16>,
+                                                subgroup_m_count = 4, subgroup_n_count = 2,
+                                                reduction = [0, 0, 32],
+                                                workgroup = [256, 128, 0]}>,
+  translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUVectorDistribute
+    workgroup_size = [128, 4, 1] subgroup_size = 64,
+    {gpu_pipeline_options = #iree_gpu.pipeline_options<prefetch_shared_memory = true>,
+     llvm_func_attrs = {"amdgpu-waves-per-eu" = "4"}
+    }>> -> !transform.any_param
+  transform.yield %matmul, %config : !transform.any_op, !transform.any_param
+}
+
+//===----------------------------------------------------------------------===//
+// Convolution tuning
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// Batch matmul tuning
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// Broadcast rhs mmt tuning
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// Contraction tuning
+//===----------------------------------------------------------------------===//
+
+// TUNING_SPEC_END DO NOT REMOVE
+
+//===----------------------------------------------------------------------===//
+// Entry point
+//===----------------------------------------------------------------------===//
+
+  transform.named_sequence @__kernel_config(%variant_op: !transform.any_op {transform.consumed}) -> !transform.any_op
```

**Comment:**
We can stage this PR such that we first commit a simple tuning spec to make sure everything works, and then separately work on adding configs we care about.

These configs should be tested to make sure they apply on the intended code, and we should quantify the improvements making sure we don't populate these specs with configs that give us only marginal improvements. This needs to be backed by data.

---


---


## [PR #19603](https://github.com/iree-org/iree/pull/19603): [Codegen][Tuner] skip linking based on the default entry point attribute

### Review Summary

**CHANGES_REQUESTED** (2025-01-06)


**APPROVED** (2025-01-06)

LGTM


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:201`

```diff
@@ -197,14 +197,30 @@ struct MaterializeTuningSpecsPass final
       return;
     }
 
-    // If only the default tuning spec is available, use it directly and skip
-    // the linking stage.
-    if (!hasUserTuningSpec) {
-      if (failed(dumpFinalTuningSpecToDir(*defaultTuningSpec))) {
+    // Check if the user-provided tuning spec has the default entry point
+    // attribute.
```

**Comment:**
The code below is self-explanatory. I think the comment a few lines below should be enough.
```suggestion
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:202`

```diff
@@ -197,14 +197,30 @@ struct MaterializeTuningSpecsPass final
       return;
     }
 
-    // If only the default tuning spec is available, use it directly and skip
-    // the linking stage.
-    if (!hasUserTuningSpec) {
-      if (failed(dumpFinalTuningSpecToDir(*defaultTuningSpec))) {
+    // Check if the user-provided tuning spec has the default entry point
+    // attribute.
+    bool userTuningSpecWithDefaultAttr =
```

**Comment:**
```suggestion
    bool isUserTuningSpecWithDefaultAttr =
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:212`

```diff
@@ -197,14 +197,30 @@ struct MaterializeTuningSpecsPass final
       return;
     }
 
-    // If only the default tuning spec is available, use it directly and skip
-    // the linking stage.
-    if (!hasUserTuningSpec) {
-      if (failed(dumpFinalTuningSpecToDir(*defaultTuningSpec))) {
+    // Check if the user-provided tuning spec has the default entry point
+    // attribute.
+    bool userTuningSpecWithDefaultAttr =
+        hasUserTuningSpec &&
+        (*userTuningSpec)->hasAttr(kTuningSpecDefaultEntrypointAttrName);
+
+    // Determine if the linking pass should be skipped.
+    // Skip if there is a user-provided spec with the default attribute but no
+    // default tuning spec, or if there is no user-provided spec but a default
+    // tuning spec is available.
+    bool skipLinkPass = (hasUserTuningSpec && !hasDefaultTuningSpec &&
+                         userTuningSpecWithDefaultAttr) ||
+                        (!hasUserTuningSpec && hasDefaultTuningSpec);
```

**Comment:**
Instead of this logic, can we move up some code from down below to perform this check?
```c++
    SmallVector<ModuleOp, 2> allSpecs = {*userTuningSpec};
    if (hasDefaultTuningSpec) {
      allSpecs.push_back(*defaultTuningSpec);
    }
```
(and handle the missing user spec case too.)

Then we can check that there's a single element in this vector and that it has the default entrypoint attr.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/materialize_tuning_specs.mlir:33`

```diff
@@ -19,6 +24,16 @@
 // CHECK-SAME:     iree_codegen.tuning_spec_mlirbc = dense<{{.+}}> : vector<{{[0-9]+}}xi8>
 // CHECK-LABEL:    func.func @main_0
 
+
+// CHECK that the user-provided tuning spec is materized without linking when default tuing spec
+// is missing and the user-provided tuning spec is marked the default attribute.
+
+// SKIPLINK-LABEL: module  @user_spec
+// SKIPLINK-SAME:    iree_codegen.tuning_spec_with_default_entrypoint
+// SKIPLINK-SAME:    transform.with_named_sequence
```

**Comment:**
Can we also check that there are no nested modules?

---


---


## [PR #19525](https://github.com/iree-org/iree/pull/19525): [Codegen][Tuner] verifier for the default tuning spec

### Review Summary

**CHANGES_REQUESTED** (2024-12-19)

Can you explain what are the alternatives you considered and why you decided on this way of verifying the default specs? IIRC, we first discussed a dedicated verification pass.

Also, please update the documentation in https://iree.dev/reference/tuning/


**COMMENTED** (2025-01-02)


**COMMENTED** (2025-01-02)


**COMMENTED** (2025-01-03)


**APPROVED** (2025-01-03)

LGTM


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenAttrs.h:49`

```diff
@@ -45,6 +45,8 @@ namespace mlir::iree_compiler {
 // Constant names.
 //===----------------------------------------------------------------------===//
 constexpr StringLiteral kConfigAttrName = "lowering_config";
+constexpr StringLiteral kTuningDefaultSpecAttrName =
+    "iree_codegen.default_tuning_spec";
```

**Comment:**
I'm not sure this is the best name to use -- we should also allow user specs to specify that they have a single entry point. Maybe `iree_codegen.tuning_spec_with_default_entrypoint`?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:77`

```diff
@@ -63,6 +66,19 @@ IREECodegenDialect::verifyOperationAttribute(Operation *op,
   //         b. It must have exactly one argument type, and the argument must be
   //         of type `transform::AnyOpType`.
 
+  if (symbol == kTuningDefaultSpecAttrName) {
+    if (auto moduleOp = dyn_cast<ModuleOp>(op)) {
+      if (!llvm::any_of(moduleOp.getOps(), [](auto &op) {
+            return SymbolTable::getSymbolName(&op).getValue() ==
+                   kKernelConfigSpecName;
+          })) {
+        return moduleOp.emitError()
+               << "The default tuning specification must include an "
+                  "operation with the symbol name '__kernel_config'.";
```

**Comment:**
Don't hardcode the name here in case we want to rename it in the future.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:73`

```diff
@@ -63,6 +66,19 @@ IREECodegenDialect::verifyOperationAttribute(Operation *op,
   //         b. It must have exactly one argument type, and the argument must be
   //         of type `transform::AnyOpType`.
 
+  if (symbol == kTuningDefaultSpecAttrName) {
+    if (auto moduleOp = dyn_cast<ModuleOp>(op)) {
+      if (!llvm::any_of(moduleOp.getOps(), [](auto &op) {
+            return SymbolTable::getSymbolName(&op).getValue() ==
+                   kKernelConfigSpecName;
```

**Comment:**
Can you enumerate the named sequence ops and check only these instead? The check here doesn't guarantee that the symbol is a named sequence, it could be something else.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:58`

```diff
@@ -51,8 +51,11 @@ IREECodegenDialect::verifyOperationAttribute(Operation *op,
                                              NamedAttribute attribute) {
   StringRef symbol = attribute.getName().strref();
   Attribute attr = attribute.getValue();
-
   // This function verifies the validity of a specific operation attribute.
+  // - If the attribute's name matches `kTuningDefaultSpecAttrName` :
+  //   - For the `ModuleOp` operation ( representing the default spec):
+  //     - Ensure the module contains one operation with the symbol
+  //       name `__kernel_config`. If not, emit an error.
```

**Comment:**
```suggestion
  // - If the attribute's name matches `kTuningDefaultSpecAttrName`, make
  //   sure it contains a single named sequence op with name `__kernel_config`.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:59`

```diff
@@ -51,3 +51,9 @@ module @foo_module attributes { transform.with_named_sequence } {
   transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly})
     attributes { iree_codegen.tuning_spec_entrypoint } {}
 }
+
+// -----
+
+// expected-error @+1{{The default tuning specification must include an operation with the symbol name '__kernel_config'}}
+module @iree_default_tuning_spec attributes { iree_codegen.default_tuning_spec } {
+}
```

**Comment:**
We should also have a test for when there's some other op with named `__kernel_config`, e.g., `func.func`.

---

**File:** `compiler/plugins/target/ROCM/test/default_tuning_specs_amdgpu.mlir:36`

```diff
@@ -33,11 +33,11 @@
 // Check that both the user tuning spec and the default spec get linked and
 // materialized. The user spec should have precedence over the default one.
 
-// BOTH-LABEL: module @iree_linked_tuning_spec attributes {transform.with_named_sequence}
+// BOTH-LABEL: module @iree_linked_tuning_spec attributes {iree_codegen.tuning_spec_with_default_entrypoint, transform.with_named_sequence}
```

**Comment:**
Can you use `BOTH-SAME` to make this match work for any ordering of these module-level attributes?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:86`

```diff
@@ -81,6 +81,10 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
       0, hasConsumedSequences ? kArgConsumedAttrName : kArgReadOnlyAttrName,
       builder.getUnitAttr());
   newSpec->setAttr(kTuningSpecEntrypointAttrName, builder.getUnitAttr());
+  // As the newSpec is a named sequence operation with the symbol name
+  // '__kernel_config', the module should add the unit attribute
+  // 'iree_codegen.tuning_spec_with_default_entrypoint' to indicate this change.
```

**Comment:**
It's not clear to me what `this change` refers to. Instead, I'd add a comment higher up that this will create a named sequence op that conforms to the requirements of tuning specs with default entrypoint (not just the name).

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:72`

```diff
@@ -68,9 +68,8 @@ IREECodegenDialect::verifyOperationAttribute(Operation *op,
     if (auto moduleOp = dyn_cast<ModuleOp>(op)) {
       if (!llvm::any_of(moduleOp.getOps(), [](auto &op) {
             if (auto namedSeqOp = dyn_cast<transform::NamedSequenceOp>(&op)) {
-              return SymbolTable::getSymbolName(namedSeqOp)
-                  .getValue()
-                  .contains(kKernelConfigSpecName);
+              return SymbolTable::getSymbolName(namedSeqOp).getValue() ==
+                     kKernelConfigSpecName;
```

**Comment:**
Can we check the name directly (`.getName` or similar) instead of using the symbol table?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:88`

```diff
@@ -81,6 +83,10 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
       0, hasConsumedSequences ? kArgConsumedAttrName : kArgReadOnlyAttrName,
       builder.getUnitAttr());
   newSpec->setAttr(kTuningSpecEntrypointAttrName, builder.getUnitAttr());
+  // As the newSpec is a named sequence operation with the symbol name
+  // '__kernel_config', the module should add the unit attribute
+  // 'iree_codegen.tuning_spec_with_default_entrypoint' to indicate this change.
```

**Comment:**
I don't think we need this comment anymore
```suggestion
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:69`

```diff
@@ -63,6 +64,22 @@ IREECodegenDialect::verifyOperationAttribute(Operation *op,
   //         b. It must have exactly one argument type, and the argument must be
   //         of type `transform::AnyOpType`.
 
+  if (symbol == kTuningSpecDefaultEntrypointAttrName) {
+    if (auto moduleOp = dyn_cast<ModuleOp>(op)) {
+      if (!llvm::any_of(moduleOp.getOps(), [](auto &op) {
```

**Comment:**
We don't need this to be generic. Also, let's not shadow the `op` variable from the parent scope.
```suggestion
      if (!llvm::any_of(moduleOp.getOps(), [](Operation *nestedOp) {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:69`

```diff
@@ -63,6 +64,22 @@ IREECodegenDialect::verifyOperationAttribute(Operation *op,
   //         b. It must have exactly one argument type, and the argument must be
   //         of type `transform::AnyOpType`.
 
+  if (symbol == kTuningSpecDefaultEntrypointAttrName) {
+    if (auto moduleOp = dyn_cast<ModuleOp>(op)) {
+      if (!llvm::any_of(moduleOp.getOps(), [](auto &op) {
```

**Comment:**
Isn't there a helper that gets the ops of the specified type? Something like `getOps<SomeOp>()`?

---

**File:** `docs/website/docs/reference/tuning.md:127`

```diff
@@ -123,6 +123,8 @@ that conform to the following format:
   `!transform.any_op`.
 * All entry points in the final tuning specs must either read
   (`transform.readonly`) or consume (`transform.consumed`) the argument.
+* The `iree_codegen.tuning_spec_with_default_entrypoint` attribute ensures that
+  the tuning spec includes a named sequence op marked with `__kernel_config`.
```

**Comment:**
```suggestion
  the tuning spec includes a named sequence op with name `__kernel_config`.
```

---


---


## [PR #19486](https://github.com/iree-org/iree/pull/19486): [Codegen][Tuner] attr verifier for tuning specs

### Review Summary

**CHANGES_REQUESTED** (2024-12-17)


**COMMENTED** (2024-12-18)


**COMMENTED** (2024-12-18)


**APPROVED** (2024-12-18)

Mostly LGTM now


**CHANGES_REQUESTED** (2024-12-18)


**COMMENTED** (2024-12-18)


**COMMENTED** (2024-12-18)


**APPROVED** (2024-12-18)

LGTM % one nit


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:118`

```diff
@@ -107,6 +107,16 @@ getUserTuningSpec(ModuleOp module, IREE::Codegen::IREECodegenDialect &dialect) {
            << clCodegenTuningSpecPath;
   }
 
+  // Iterate through all operations in the module to verify attributes
+  for (Operation &op : (*maybeTransformLibrary).getBody()->getOperations()) {
+    for (NamedAttribute attr : op.getAttrs()) {
+      if (failed(dialect.verifyOperationAttribute(&op, attr))) {
+        return op.emitError() << "Attribute verification failed for operation "
+                                 "in the user tuning spec";
+      }
+    }
+  }
```

**Comment:**
We should call verify on the whole module. You can see this used here: https://github.com/llvm/llvm-project/blob/57c161a6479fb70a31553e2f9bc1efa46262aa92/mlir/lib/Dialect/Transform/Transforms/TransformInterpreterUtils.cpp#L118

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:110`

```diff
@@ -107,6 +107,16 @@ getUserTuningSpec(ModuleOp module, IREE::Codegen::IREECodegenDialect &dialect) {
            << clCodegenTuningSpecPath;
   }
 
+  // Iterate through all operations in the module to verify attributes
```

**Comment:**
Use proper punctuation.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:118`

```diff
@@ -107,6 +107,16 @@ getUserTuningSpec(ModuleOp module, IREE::Codegen::IREECodegenDialect &dialect) {
            << clCodegenTuningSpecPath;
   }
 
+  // Iterate through all operations in the module to verify attributes
+  for (Operation &op : (*maybeTransformLibrary).getBody()->getOperations()) {
+    for (NamedAttribute attr : op.getAttrs()) {
+      if (failed(dialect.verifyOperationAttribute(&op, attr))) {
+        return op.emitError() << "Attribute verification failed for operation "
+                                 "in the user tuning spec";
+      }
+    }
+  }
```

**Comment:**
Actually, I don't think this is needed along this code path because the user spec verification happens in `parseTransformModuleFromFile`. We should verify linked specs after linking though.l

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:167`

```diff
@@ -138,8 +148,25 @@ getDefaultTuningSpec(ModuleOp module,
 
   // Load the library through the codegen dialect so that we cache the parsed
   // module.
-  return dialect.getOrParseTransformLibraryModule(defaultTuningSpecName,
-                                                  *defaultTuningSpecSource);
+  FailureOr<ModuleOp> defaultTransformLibrary =
+      dialect.getOrParseTransformLibraryModule(defaultTuningSpecName,
+                                               *defaultTuningSpecSource);
+  if (failed(defaultTransformLibrary)) {
+    return module->emitError()
+           << "Failed to parse default tuning spec transform library for '"
+           << arch << "'";
+  }
+  // Iterate through operations and validate their attributes
+  for (Operation &op : (*defaultTransformLibrary).getBody()->getOperations()) {
+    for (NamedAttribute attr : op.getAttrs()) {
+      if (failed(dialect.verifyOperationAttribute(&op, attr))) {
+        return op.emitError() << "Attribute verification failed for operation "
+                                 "in default tuning spec";
+      }
+    }
+  }
```

**Comment:**
I don't think we should verify default specs -- these are already verified by our tests when building the compiler. We could do that but under debug builds only.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:1`

```diff
@@ -0,0 +1,18 @@
+// RUN: iree-opt --no-implicit-module --verify-diagnostics -split-input-file --mlir-disable-threading %s
```

**Comment:**
```suggestion
// RUN: iree-opt  --verify-diagnostics  %s
```

This test does not rely on the other flags AFAICT

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:18`

```diff
@@ -0,0 +1,18 @@
+// RUN: iree-opt --no-implicit-module --verify-diagnostics -split-input-file --mlir-disable-threading %s
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{'iree_codegen.tuning_spec_entrypoint' attribute must be a UnitAttr}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.tuning_spec_entrypoint = "foo" } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    transform.named_sequence @bar(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.tuning_spec_entrypoint } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    func.func @baz(%arg0: i32) -> () {
+      return
+    }
+  }
+}
```

**Comment:**
We should also check what happens there's some other attribute attached to `named_sequence` and documents whether that's allowed or not (by the virtue of having a testcase).

We should also add tests that check that the attribute is present but the `named_sequence` signature is wrong.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:76`

```diff
@@ -45,4 +46,39 @@ void IREECodegenDialect::initialize() {
       >();
 }
 
+LogicalResult
+IREECodegenDialect::verifyOperationAttribute(Operation *op,
+                                             NamedAttribute attribute) {
+  StringRef symbol = attribute.getName().strref();
+  Attribute attr = attribute.getValue();
+
+  // Early return if the symbol is not "iree_codegen.tuning_spec_entrypoint"
+  if (symbol != kTuningSpecEntrypointAttrName)
+    return success();
+
+  // Verify that the attribute is a UnitAttr
+  if (!llvm::isa<UnitAttr>(attr)) {
+    return op->emitError("'") << symbol << "' attribute must be a UnitAttr";
+  }
+
+  if (auto namedSeqOp = dyn_cast<transform::NamedSequenceOp>(op)) {
+    ArrayRef<Type> resTypes = namedSeqOp.getFunctionType().getResults();
+    if (resTypes.size() != 1 || !isa<transform::AnyOpType>(resTypes[0])) {
+      return namedSeqOp.emitWarning()
+             << "Tuning spec entry point expected to return any_op";
+    }
+
+    ArrayRef<Type> argTypes = namedSeqOp.getArgumentTypes();
+    if (argTypes.size() != 1 || !isa<transform::AnyOpType>(argTypes[0])) {
+      return namedSeqOp.emitWarning()
+             << "Tuning spec entry point expected to have a "
+                "single any_op argument";
+    }
```

**Comment:**
We should remove the validation from the other code where this was copied from -- no need to validate twice.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:55`

```diff
@@ -45,4 +46,39 @@ void IREECodegenDialect::initialize() {
       >();
 }
 
+LogicalResult
+IREECodegenDialect::verifyOperationAttribute(Operation *op,
+                                             NamedAttribute attribute) {
+  StringRef symbol = attribute.getName().strref();
+  Attribute attr = attribute.getValue();
+
+  // Early return if the symbol is not "iree_codegen.tuning_spec_entrypoint"
```

**Comment:**
I don't think this comment adds clarity, I'd drop it. Instead, you can summarize the validity criteria in one longer comment.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:59`

```diff
@@ -45,4 +46,39 @@ void IREECodegenDialect::initialize() {
       >();
 }
 
+LogicalResult
+IREECodegenDialect::verifyOperationAttribute(Operation *op,
+                                             NamedAttribute attribute) {
+  StringRef symbol = attribute.getName().strref();
+  Attribute attr = attribute.getValue();
+
+  // Early return if the symbol is not "iree_codegen.tuning_spec_entrypoint"
+  if (symbol != kTuningSpecEntrypointAttrName)
+    return success();
+
+  // Verify that the attribute is a UnitAttr
```

**Comment:**
Same here

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:60`

```diff
@@ -45,4 +46,39 @@ void IREECodegenDialect::initialize() {
       >();
 }
 
+LogicalResult
+IREECodegenDialect::verifyOperationAttribute(Operation *op,
+                                             NamedAttribute attribute) {
+  StringRef symbol = attribute.getName().strref();
+  Attribute attr = attribute.getValue();
+
+  // Early return if the symbol is not "iree_codegen.tuning_spec_entrypoint"
+  if (symbol != kTuningSpecEntrypointAttrName)
+    return success();
+
+  // Verify that the attribute is a UnitAttr
+  if (!llvm::isa<UnitAttr>(attr)) {
```

**Comment:**
```suggestion
  if (!isa<UnitAttr>(attr)) {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:150`

```diff
@@ -165,17 +145,13 @@ struct LinkTuningSpecsPass final
 FailureOr<NamedSequenceOp> linkTuningSpecs(ModuleOp module) {
   SmallVector<NamedSequenceOp> tuningSpecs;
 
+  if (failed(mlir::verify(module)))
+    return failure();
+
```

**Comment:**
We should verify the result of linking, not the input. It is assumed that the input would have been verifier by the parser or something else that created it.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:147`

```diff
@@ -138,8 +139,17 @@ getDefaultTuningSpec(ModuleOp module,
 
   // Load the library through the codegen dialect so that we cache the parsed
   // module.
-  return dialect.getOrParseTransformLibraryModule(defaultTuningSpecName,
-                                                  *defaultTuningSpecSource);
+  FailureOr<ModuleOp> defaultTransformLibrary =
+      dialect.getOrParseTransformLibraryModule(defaultTuningSpecName,
+                                               *defaultTuningSpecSource);
+
+#ifndef NDEBUG
+  if (failed(mlir::verify(*defaultTransformLibrary)))
```

**Comment:**
This doesn't handle the case then the loaded module is a failure or nullptr

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:257`

```diff
@@ -240,6 +250,13 @@ struct MaterializeTuningSpecsPass final
       return signalPassFailure();
     }
 
+    if (failed(mlir::verify(linkedTuningSpec.get()))) {
+      linkedTuningSpec.get().emitError(
+          "Attribute verification failed for operation in linked "
+          "tuning spec");
+      return signalPassFailure();
```

**Comment:**
This should be checked in the code that does linking

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:1`

```diff
@@ -0,0 +1,64 @@
+// RUN: iree-opt  --verify-diagnostics --split-input-file  %s
```

**Comment:**
```suggestion
// RUN: iree-opt --verify-diagnostics --split-input-file %s
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:39`

```diff
@@ -0,0 +1,64 @@
+// RUN: iree-opt  --verify-diagnostics --split-input-file  %s
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{'iree_codegen.tuning_spec_entrypoint' attribute must be a UnitAttr}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.tuning_spec_entrypoint = "foo" } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    transform.named_sequence @bar(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.something } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    func.func @baz(%arg0: i32) -> () {
+      return
+    }
+  }
+}
+
+// -----
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{Tuning spec entry point expected to have a single any_op argument}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}, %arg1: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.tuning_spec_entrypoint } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    func.func @baz(%arg0: i32) -> () {
+      return
+    }
+  }
+}
+
+// -----
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{Tuning spec entry point expected to return any_op}}
```

**Comment:**
We should have a testcase for when there are no return values

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:6`

```diff
@@ -0,0 +1,64 @@
+// RUN: iree-opt  --verify-diagnostics --split-input-file  %s
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{'iree_codegen.tuning_spec_entrypoint' attribute must be a UnitAttr}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
```

**Comment:**
We don't need the nested module in this test -- a single level of nesting is sufficient

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:23`

```diff
@@ -0,0 +1,64 @@
+// RUN: iree-opt  --verify-diagnostics --split-input-file  %s
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{'iree_codegen.tuning_spec_entrypoint' attribute must be a UnitAttr}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.tuning_spec_entrypoint = "foo" } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    transform.named_sequence @bar(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.something } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    func.func @baz(%arg0: i32) -> () {
+      return
+    }
+  }
+}
+
+// -----
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
```

**Comment:**
Also here, we don't need to nest

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:31`

```diff
@@ -0,0 +1,64 @@
+// RUN: iree-opt  --verify-diagnostics --split-input-file  %s
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{'iree_codegen.tuning_spec_entrypoint' attribute must be a UnitAttr}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.tuning_spec_entrypoint = "foo" } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    transform.named_sequence @bar(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.something } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    func.func @baz(%arg0: i32) -> () {
+      return
+    }
+  }
+}
+
+// -----
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{Tuning spec entry point expected to have a single any_op argument}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}, %arg1: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.tuning_spec_entrypoint } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    func.func @baz(%arg0: i32) -> () {
+      return
+    }
```

**Comment:**
Other tests already check that function ops are allowed, I don't think we need this here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:31`

```diff
@@ -0,0 +1,64 @@
+// RUN: iree-opt  --verify-diagnostics --split-input-file  %s
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{'iree_codegen.tuning_spec_entrypoint' attribute must be a UnitAttr}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.tuning_spec_entrypoint = "foo" } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    transform.named_sequence @bar(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.something } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    func.func @baz(%arg0: i32) -> () {
+      return
+    }
+  }
+}
+
+// -----
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{Tuning spec entry point expected to have a single any_op argument}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}, %arg1: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.tuning_spec_entrypoint } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    func.func @baz(%arg0: i32) -> () {
+      return
+    }
```

**Comment:**
Same in the other test cases below

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:53`

```diff
@@ -0,0 +1,64 @@
+// RUN: iree-opt  --verify-diagnostics --split-input-file  %s
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{'iree_codegen.tuning_spec_entrypoint' attribute must be a UnitAttr}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.tuning_spec_entrypoint = "foo" } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    transform.named_sequence @bar(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.something } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    func.func @baz(%arg0: i32) -> () {
+      return
+    }
+  }
+}
+
+// -----
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{Tuning spec entry point expected to have a single any_op argument}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}, %arg1: !transform.any_op {transform.readonly}) -> !transform.any_op
+      attributes { iree_codegen.tuning_spec_entrypoint } {
+      transform.yield %arg0 : !transform.any_op
+    }
+    func.func @baz(%arg0: i32) -> () {
+      return
+    }
+  }
+}
+
+// -----
+
+module @td_module attributes { transform.with_named_sequence } {
+  module @foo_module attributes { transform.with_named_sequence } {
+    // expected-error @+1{{Tuning spec entry point expected to return any_op}}
+    transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> i32
+      attributes { iree_codegen.tuning_spec_entrypoint } {
+      %0 = arith.constant 0 : i32
+      transform.yield %0 : i32
+    }
+    func.func @baz(%arg0: i32) -> () {
+      return
+    }
+  }
+}
+
+// -----
+
+module @td_module attributes { transform.with_named_sequence } {
```

**Comment:**
I'd move this up just after the first testcase that checked for the wrong number of arguments

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/Codegen/IR/IREECodegenDialect.cpp:88`

```diff
@@ -45,4 +46,49 @@ void IREECodegenDialect::initialize() {
       >();
 }
 
+LogicalResult
+IREECodegenDialect::verifyOperationAttribute(Operation *op,
+                                             NamedAttribute attribute) {
+  StringRef symbol = attribute.getName().strref();
+  Attribute attr = attribute.getValue();
+
+  // This function verifies the validity of a specific operation attribute.
+  // - If the attribute's name matches `kTuningSpecEntrypointAttrName`
+  // ("iree_codegen.tuning_spec_entrypoint"):
+  //   1. The attribute value must be a UnitAttr.
+  //   2. If the operation is a transform::NamedSequenceOp:
+  //      - The operation's function signature must satisfy the following:
+  //         a. It must have exactly one result type, and the result must be of
+  //         type `transform::AnyOpType`.
+  //         b. It must have exactly one argument type, and the argument must be
+  //         of type `transform::AnyOpType`.
+
+  if (symbol != kTuningSpecEntrypointAttrName)
+    return success();
+
+  // Verify that the attribute is a UnitAttr.
+  if (!isa<UnitAttr>(attr)) {
+    return op->emitError("'") << symbol << "' attribute must be a UnitAttr";
+  }
+
+  if (auto namedSeqOp = dyn_cast<transform::NamedSequenceOp>(op)) {
+    ArrayRef<Type> resTypes = namedSeqOp.getFunctionType().getResults();
+    if (resTypes.size() != 1 || !isa<transform::AnyOpType>(resTypes[0])) {
+      return namedSeqOp.emitError()
+             << "Tuning spec entry point expected to return any_op";
+    }
+
+    ArrayRef<Type> argTypes = namedSeqOp.getArgumentTypes();
+    if (argTypes.size() != 1 || !isa<transform::AnyOpType>(argTypes[0])) {
+      return namedSeqOp.emitError()
+             << "Tuning spec entry point expected to have a "
+                "single any_op argument";
+    }
+
+    return success();
```

**Comment:**
```suggestion
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:130`

```diff
@@ -124,6 +124,12 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   }
 
   builder.create<transform::YieldOp>(loc, operand);
+
+  if (failed(mlir::verify(module))) {
+    module.emitError("verification failed for operation in linked "
+                     "tuning spec");
```

**Comment:**
We should return failure so that nothing uses this module.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/LinkTuningSpecsPass.cpp:130`

```diff
@@ -144,6 +124,12 @@ emitLinkedTuningSpec(ModuleOp module, ArrayRef<NamedSequenceOp> specsToLink) {
   }
 
   builder.create<transform::YieldOp>(loc, operand);
+
+  if (failed(mlir::verify(module))) {
+    module.emitError("verification failed for operation in linked "
+                     "tuning spec");
```

**Comment:**
```suggestion
    module.emitError("Linked tuning spec failed to verify");
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:154`

```diff
@@ -138,8 +139,22 @@ getDefaultTuningSpec(ModuleOp module,
 
   // Load the library through the codegen dialect so that we cache the parsed
   // module.
-  return dialect.getOrParseTransformLibraryModule(defaultTuningSpecName,
-                                                  *defaultTuningSpecSource);
+  FailureOr<ModuleOp> defaultTransformLibrary =
+      dialect.getOrParseTransformLibraryModule(defaultTuningSpecName,
+                                               *defaultTuningSpecSource);
+
+  if (failed(defaultTransformLibrary)) {
+    return module->emitError()
+           << "Failed to load  default tuning spec" << defaultTuningSpecName;
+  }
+
+#ifndef NDEBUG
+  if (failed(mlir::verify(*defaultTransformLibrary)))
+    return (*defaultTransformLibrary).emitError()
+           << "Verification failed for default tuning spec";
```

**Comment:**
```suggestion
           << "Default tuning spec " << defaultTuningSpecName << " failed to verify";
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:148`

```diff
@@ -138,8 +139,22 @@ getDefaultTuningSpec(ModuleOp module,
 
   // Load the library through the codegen dialect so that we cache the parsed
   // module.
-  return dialect.getOrParseTransformLibraryModule(defaultTuningSpecName,
-                                                  *defaultTuningSpecSource);
+  FailureOr<ModuleOp> defaultTransformLibrary =
+      dialect.getOrParseTransformLibraryModule(defaultTuningSpecName,
+                                               *defaultTuningSpecSource);
+
+  if (failed(defaultTransformLibrary)) {
+    return module->emitError()
+           << "Failed to load  default tuning spec" << defaultTuningSpecName;
```

**Comment:**
We shouldn't emit this error here. The reason is that `getOrParseTransformLibraryModule` already does the reporting and it knows do it only when the parsing fails for the first time.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/MaterializeTuningSpecsPass.cpp:152`

```diff
@@ -138,8 +139,22 @@ getDefaultTuningSpec(ModuleOp module,
 
   // Load the library through the codegen dialect so that we cache the parsed
   // module.
-  return dialect.getOrParseTransformLibraryModule(defaultTuningSpecName,
-                                                  *defaultTuningSpecSource);
+  FailureOr<ModuleOp> defaultTransformLibrary =
+      dialect.getOrParseTransformLibraryModule(defaultTuningSpecName,
+                                               *defaultTuningSpecSource);
+
+  if (failed(defaultTransformLibrary)) {
+    return module->emitError()
+           << "Failed to load  default tuning spec" << defaultTuningSpecName;
+  }
+
+#ifndef NDEBUG
+  if (failed(mlir::verify(*defaultTransformLibrary)))
```

**Comment:**
Instead of the check above, we can do this:
```suggestion
  if (succeded(defaultTransformLibrary) && failed(mlir::verify(*defaultTransformLibrary)))
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:35`

```diff
@@ -0,0 +1,57 @@
+// RUN: iree-opt --verify-diagnostics --split-input-file %s
+
+module @foo_module attributes { transform.with_named_sequence } {
+  // expected-error @+1{{'iree_codegen.tuning_spec_entrypoint' attribute must be a UnitAttr}}
+  transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+    attributes { iree_codegen.tuning_spec_entrypoint = "foo" } {
+    transform.yield %arg0 : !transform.any_op
+  }
+  transform.named_sequence @bar(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+    attributes { iree_codegen.something } {
+    transform.yield %arg0 : !transform.any_op
+  }
+  func.func @baz(%arg0: i32) -> () {
+    return
+  }
+}
+
+// -----
+
+module @foo_module attributes { transform.with_named_sequence } {
+  // expected-error @+1{{Tuning spec entry point expected to have a single any_op argument}}
+  transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}, %arg1: !transform.any_op {transform.readonly}) -> !transform.any_op
+    attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @foo_module attributes { transform.with_named_sequence } {
+  // expected-error @+1{{Tuning spec entry point expected to have a single any_op argument}}
+  transform.named_sequence @foo(%arg0: i32) -> !transform.any_op
+    attributes { iree_codegen.tuning_spec_entrypoint } {
+
+  }
```

**Comment:**
```suggestion
    attributes { iree_codegen.tuning_spec_entrypoint } {}
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:56`

```diff
@@ -0,0 +1,57 @@
+// RUN: iree-opt --verify-diagnostics --split-input-file %s
+
+module @foo_module attributes { transform.with_named_sequence } {
+  // expected-error @+1{{'iree_codegen.tuning_spec_entrypoint' attribute must be a UnitAttr}}
+  transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+    attributes { iree_codegen.tuning_spec_entrypoint = "foo" } {
+    transform.yield %arg0 : !transform.any_op
+  }
+  transform.named_sequence @bar(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+    attributes { iree_codegen.something } {
+    transform.yield %arg0 : !transform.any_op
+  }
+  func.func @baz(%arg0: i32) -> () {
+    return
+  }
+}
+
+// -----
+
+module @foo_module attributes { transform.with_named_sequence } {
+  // expected-error @+1{{Tuning spec entry point expected to have a single any_op argument}}
+  transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}, %arg1: !transform.any_op {transform.readonly}) -> !transform.any_op
+    attributes { iree_codegen.tuning_spec_entrypoint } {
+    transform.yield %arg0 : !transform.any_op
+  }
+}
+
+// -----
+
+module @foo_module attributes { transform.with_named_sequence } {
+  // expected-error @+1{{Tuning spec entry point expected to have a single any_op argument}}
+  transform.named_sequence @foo(%arg0: i32) -> !transform.any_op
+    attributes { iree_codegen.tuning_spec_entrypoint } {
+
+  }
+}
+
+// -----
+
+module @foo_module attributes { transform.with_named_sequence } {
+  // expected-error @+1{{Tuning spec entry point expected to return any_op}}
+  transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> i32
+    attributes { iree_codegen.tuning_spec_entrypoint } {
+    %0 = arith.constant 0 : i32
+    transform.yield %0 : i32
+  }
+}
+
+// -----
+
+module @foo_module attributes { transform.with_named_sequence } {
+  // expected-error @+1{{Tuning spec entry point expected to return any_op}}
+  transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly})
+    attributes { iree_codegen.tuning_spec_entrypoint } {
+
+  }
```

**Comment:**
```suggestion
    attributes { iree_codegen.tuning_spec_entrypoint } {}
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:12`

```diff
@@ -0,0 +1,57 @@
+// RUN: iree-opt --verify-diagnostics --split-input-file %s
+
+module @foo_module attributes { transform.with_named_sequence } {
+  // expected-error @+1{{'iree_codegen.tuning_spec_entrypoint' attribute must be a UnitAttr}}
+  transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+    attributes { iree_codegen.tuning_spec_entrypoint = "foo" } {
+    transform.yield %arg0 : !transform.any_op
+  }
+  transform.named_sequence @bar(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+    attributes { iree_codegen.something } {
+    transform.yield %arg0 : !transform.any_op
+  }
```

**Comment:**
This won't be verified anyway because of the previous error. We should move it before the erroneous spec.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/verify_tuning_specs.mlir:15`

```diff
@@ -0,0 +1,57 @@
+// RUN: iree-opt --verify-diagnostics --split-input-file %s
+
+module @foo_module attributes { transform.with_named_sequence } {
+  // expected-error @+1{{'iree_codegen.tuning_spec_entrypoint' attribute must be a UnitAttr}}
+  transform.named_sequence @foo(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+    attributes { iree_codegen.tuning_spec_entrypoint = "foo" } {
+    transform.yield %arg0 : !transform.any_op
+  }
+  transform.named_sequence @bar(%arg0: !transform.any_op {transform.readonly}) -> !transform.any_op
+    attributes { iree_codegen.something } {
+    transform.yield %arg0 : !transform.any_op
+  }
+  func.func @baz(%arg0: i32) -> () {
+    return
+  }
```

**Comment:**
Also here

---


---


## [PR #19376](https://github.com/iree-org/iree/pull/19376): [tuner]: add property functions to lowering config python binding

### Review Summary

**COMMENTED** (2024-12-05)


**CHANGES_REQUESTED** (2024-12-05)


**CHANGES_REQUESTED** (2024-12-05)


**COMMENTED** (2024-12-06)


**COMMENTED** (2024-12-06)


**COMMENTED** (2024-12-06)


**APPROVED** (2024-12-07)

LGTM % one nit


**COMMENTED** (2024-12-07)


### Code Comments

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:102`

```diff
@@ -93,6 +93,23 @@ MLIR_CAPI_EXPORTED MlirAttribute ireeGPULoweringConfigAttrGet(
 MLIR_CAPI_EXPORTED MlirAttribute
 ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr);
 
+MLIR_CAPI_EXPORTED void
+ireeGPULoweringConfigAttrGetReductionTileSizes(MlirAttribute attr, size_t *len,
+                                               int64_t *reductionTileSizes);
+
+MLIR_CAPI_EXPORTED void
+ireeGPULoweringConfigAttrGetWorkgroupTileSizes(MlirAttribute attr, size_t *len,
+                                               int64_t *workgroupTileSizes);
```

**Comment:**
I'd put these in a single function that returns all tile sizes in a struct. You can see an example above (`ireeGPUMMAAttrGetInfo`). The reason is that this is expands the API surface area by quite a lot, and adding more levels of tiling would exacerbate this further. With a struct, we can keep extending it with more fields.

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:108`

```diff
@@ -93,6 +93,23 @@ MLIR_CAPI_EXPORTED MlirAttribute ireeGPULoweringConfigAttrGet(
 MLIR_CAPI_EXPORTED MlirAttribute
 ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr);
 
+MLIR_CAPI_EXPORTED void
+ireeGPULoweringConfigAttrGetReductionTileSizes(MlirAttribute attr, size_t *len,
+                                               int64_t *reductionTileSizes);
+
+MLIR_CAPI_EXPORTED void
+ireeGPULoweringConfigAttrGetWorkgroupTileSizes(MlirAttribute attr, size_t *len,
+                                               int64_t *workgroupTileSizes);
+
+MLIR_CAPI_EXPORTED MlirAttribute
+ireeGPULoweringConfigAttrGetSubgroupMCount(MlirAttribute attr);
+
+MLIR_CAPI_EXPORTED MlirAttribute
+ireeGPULoweringConfigAttrGetSubgroupNCount(MlirAttribute attr);
```

**Comment:**
This can be a single function that returns two integers.

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:63`

```diff
@@ -48,6 +48,36 @@ ireeCodegenQueryMMAIntrinsicsBinding(MlirOperation op) {
   return mmaList;
 }
 
+static std::optional<std::vector<int64_t>>
+ireeGPULoweringConfigAttrGetReductionTileSizesBinding(
+    MlirAttribute lowering_config) {
+  size_t tileSizes = 0;
+  ireeGPULoweringConfigAttrGetReductionTileSizes(lowering_config, &tileSizes,
+                                                 nullptr);
+  if (tileSizes == -1) {
+    return std::nullopt;
+  }
+  std::vector<int64_t> reductionTileSizes(tileSizes);
+  ireeGPULoweringConfigAttrGetReductionTileSizes(lowering_config, &tileSizes,
+                                                 reductionTileSizes.data());
+  return reductionTileSizes;
```

**Comment:**
We don't need the buffer to be caller-allocated -- we can return a pointer to the attribute storage.

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:77`

```diff
@@ -48,6 +48,36 @@ ireeCodegenQueryMMAIntrinsicsBinding(MlirOperation op) {
   return mmaList;
 }
 
+static std::optional<std::vector<int64_t>>
+ireeGPULoweringConfigAttrGetReductionTileSizesBinding(
+    MlirAttribute lowering_config) {
+  size_t tileSizes = 0;
+  ireeGPULoweringConfigAttrGetReductionTileSizes(lowering_config, &tileSizes,
+                                                 nullptr);
+  if (tileSizes == -1) {
+    return std::nullopt;
+  }
+  std::vector<int64_t> reductionTileSizes(tileSizes);
+  ireeGPULoweringConfigAttrGetReductionTileSizes(lowering_config, &tileSizes,
+                                                 reductionTileSizes.data());
+  return reductionTileSizes;
+}
+
+static std::optional<std::vector<int64_t>>
+ireeGPULoweringConfigAttrGetWorkgroupTileSizesBinding(
+    MlirAttribute lowering_config) {
+  size_t tileSizes = 0;
+  ireeGPULoweringConfigAttrGetWorkgroupTileSizes(lowering_config, &tileSizes,
+                                                 nullptr);
+  if (tileSizes == -1) {
+    return std::nullopt;
+  }
+  std::vector<int64_t> workgroupTileSizes(tileSizes);
+  ireeGPULoweringConfigAttrGetWorkgroupTileSizes(lowering_config, &tileSizes,
+                                                 workgroupTileSizes.data());
```

**Comment:**
Also here

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:231`

```diff
@@ -210,11 +210,40 @@ def mma_intrinsic_attr():
 
 @run
 def lowering_config_attr():
-    attributes = ir.DictAttr.get({"reduction": ir.ArrayAttr.get([])})
+    attributes = ir.DictAttr.get(
+        {
+            "reduction": ir.ArrayAttr.get([]),
+        }
+    )
     lowering_config = iree_gpu.LoweringConfigAttr.get(attributes)
     assert lowering_config is not None
 
     assert lowering_config.attributes == attributes
+    assert lowering_config.workgroup_tile_sizes == None
+    assert lowering_config.reduction_tile_sizes == []
+    assert lowering_config.subgroup_m_count == None
+    assert lowering_config.mma_kind == None
+
+    mma_intrinsic = iree_gpu.MMAIntrinsic.MFMA_F32_16x16x16_F16
+    mma_attr = iree_gpu.MMAAttr.get(mma_intrinsic)
+    attributes = ir.DictAttr.get(
+        {
+            "reduction": ir.ArrayAttr.get([ir.IntegerAttr.get(ir.IndexType.get(), 1)]),
```

**Comment:**
You can add a helper to `tuner_ctx` to help with this, e.g.: `tuner_ctx.type.getIndexArray([1])`

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:109`

```diff
@@ -93,6 +93,32 @@ MLIR_CAPI_EXPORTED MlirAttribute ireeGPULoweringConfigAttrGet(
 MLIR_CAPI_EXPORTED MlirAttribute
 ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr);
 
+struct ireeGPUTileSizes {
+  const int64_t *workgroupTileSizes;
+  size_t numWorkgroupTileSizes;
+  const int64_t *reductionTileSizes;
+  size_t numReductionTileSizes;
+};
+
+MLIR_CAPI_EXPORTED ireeGPUTileSizes
+ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr);
+
+// MLIR_CAPI_EXPORTED void
+// ireeGPULoweringConfigAttrGetWorkgroupTileSizes(MlirAttribute attr, size_t
+// *len,
+//                                                int64_t *workgroupTileSizes);
```

**Comment:**
Some leftover comment

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_gpu.h:106`

```diff
@@ -93,6 +93,32 @@ MLIR_CAPI_EXPORTED MlirAttribute ireeGPULoweringConfigAttrGet(
 MLIR_CAPI_EXPORTED MlirAttribute
 ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr);
 
+struct ireeGPUTileSizes {
+  const int64_t *workgroupTileSizes;
+  size_t numWorkgroupTileSizes;
+  const int64_t *reductionTileSizes;
+  size_t numReductionTileSizes;
+};
+
+MLIR_CAPI_EXPORTED ireeGPUTileSizes
+ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr);
+
+// MLIR_CAPI_EXPORTED void
+// ireeGPULoweringConfigAttrGetWorkgroupTileSizes(MlirAttribute attr, size_t
+// *len,
+//                                                int64_t *workgroupTileSizes);
+
+struct ireeGPUSubgroupCountInfo {
+  MlirAttribute subgroupMCountAttr;
+  MlirAttribute subgroupNCountAttr;
```

**Comment:**
I'd return these as integers int64_t

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:224`

```diff
@@ -213,3 +213,61 @@ MlirAttribute ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr) {
                   unwrap(attr))
                   .getAttributes());
 }
+
+ireeGPUTileSizes ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  ireeGPUTileSizes tilesizes = {};
+  auto loweringConfigAttr =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr));
+
+  auto workgroups = loweringConfigAttr.getWorkgroupTileSizes();
```

**Comment:**
Don't use `auto` when the type is not obvious based on the RHS only

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:228`

```diff
@@ -213,3 +213,61 @@ MlirAttribute ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr) {
                   unwrap(attr))
                   .getAttributes());
 }
+
+ireeGPUTileSizes ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  ireeGPUTileSizes tilesizes = {};
+  auto loweringConfigAttr =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr));
+
+  auto workgroups = loweringConfigAttr.getWorkgroupTileSizes();
+  tilesizes.workgroupTileSizes = workgroups.data();
+  tilesizes.numWorkgroupTileSizes = workgroups.size();
+
+  auto reduction = loweringConfigAttr.getStaticTilingLevelSizes(
```

**Comment:**
Also here

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:252`

```diff
@@ -213,3 +213,61 @@ MlirAttribute ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr) {
                   unwrap(attr))
                   .getAttributes());
 }
+
+ireeGPUTileSizes ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  ireeGPUTileSizes tilesizes = {};
+  auto loweringConfigAttr =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr));
+
+  auto workgroups = loweringConfigAttr.getWorkgroupTileSizes();
+  tilesizes.workgroupTileSizes = workgroups.data();
+  tilesizes.numWorkgroupTileSizes = workgroups.size();
+
+  auto reduction = loweringConfigAttr.getStaticTilingLevelSizes(
+      static_cast<int64_t>(
+          mlir::iree_compiler::IREE::GPU::TilingLevel::Reduction),
+      nullptr);
+  tilesizes.reductionTileSizes = reduction.data();
+  tilesizes.numReductionTileSizes = reduction.size();
+
+  return tilesizes;
+}
+
+ireeGPUSubgroupCountInfo
+ireeGPULoweringConfigAttrGetSubgroupCount(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  mlir::DictionaryAttr dict =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr))
+          .getAttributes();
+
+  constexpr mlir::StringLiteral kSubgroupMCountName = "subgroup_m_count";
+  constexpr mlir::StringLiteral kSubgroupNCountName = "subgroup_n_count";
+
+  mlir::IntegerAttr subgroup_m_count_attr =
+      dict.getAs<mlir::IntegerAttr>(kSubgroupMCountName);
+  mlir::IntegerAttr subgroup_n_count_attr =
+      dict.getAs<mlir::IntegerAttr>(kSubgroupNCountName);
```

**Comment:**
Don't we have helpers for this in the dialect headers or the attr interface?

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:252`

```diff
@@ -213,3 +213,61 @@ MlirAttribute ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr) {
                   unwrap(attr))
                   .getAttributes());
 }
+
+ireeGPUTileSizes ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  ireeGPUTileSizes tilesizes = {};
+  auto loweringConfigAttr =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr));
+
+  auto workgroups = loweringConfigAttr.getWorkgroupTileSizes();
+  tilesizes.workgroupTileSizes = workgroups.data();
+  tilesizes.numWorkgroupTileSizes = workgroups.size();
+
+  auto reduction = loweringConfigAttr.getStaticTilingLevelSizes(
+      static_cast<int64_t>(
+          mlir::iree_compiler::IREE::GPU::TilingLevel::Reduction),
+      nullptr);
+  tilesizes.reductionTileSizes = reduction.data();
+  tilesizes.numReductionTileSizes = reduction.size();
+
+  return tilesizes;
+}
+
+ireeGPUSubgroupCountInfo
+ireeGPULoweringConfigAttrGetSubgroupCount(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  mlir::DictionaryAttr dict =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr))
+          .getAttributes();
+
+  constexpr mlir::StringLiteral kSubgroupMCountName = "subgroup_m_count";
+  constexpr mlir::StringLiteral kSubgroupNCountName = "subgroup_n_count";
+
+  mlir::IntegerAttr subgroup_m_count_attr =
+      dict.getAs<mlir::IntegerAttr>(kSubgroupMCountName);
+  mlir::IntegerAttr subgroup_n_count_attr =
+      dict.getAs<mlir::IntegerAttr>(kSubgroupNCountName);
```

**Comment:**
Here: https://github.com/iree-org/iree/blob/a1664e30a6a53850f689f32086a8b0c45bed327b/compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/GPULoweringConfigUtils.h#L22-L23

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:269`

```diff
@@ -213,3 +213,61 @@ MlirAttribute ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr) {
                   unwrap(attr))
                   .getAttributes());
 }
+
+ireeGPUTileSizes ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  ireeGPUTileSizes tilesizes = {};
+  auto loweringConfigAttr =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr));
+
+  auto workgroups = loweringConfigAttr.getWorkgroupTileSizes();
+  tilesizes.workgroupTileSizes = workgroups.data();
+  tilesizes.numWorkgroupTileSizes = workgroups.size();
+
+  auto reduction = loweringConfigAttr.getStaticTilingLevelSizes(
+      static_cast<int64_t>(
+          mlir::iree_compiler::IREE::GPU::TilingLevel::Reduction),
+      nullptr);
+  tilesizes.reductionTileSizes = reduction.data();
+  tilesizes.numReductionTileSizes = reduction.size();
+
+  return tilesizes;
+}
+
+ireeGPUSubgroupCountInfo
+ireeGPULoweringConfigAttrGetSubgroupCount(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  mlir::DictionaryAttr dict =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr))
+          .getAttributes();
+
+  constexpr mlir::StringLiteral kSubgroupMCountName = "subgroup_m_count";
+  constexpr mlir::StringLiteral kSubgroupNCountName = "subgroup_n_count";
+
+  mlir::IntegerAttr subgroup_m_count_attr =
+      dict.getAs<mlir::IntegerAttr>(kSubgroupMCountName);
+  mlir::IntegerAttr subgroup_n_count_attr =
+      dict.getAs<mlir::IntegerAttr>(kSubgroupNCountName);
+
+  ireeGPUSubgroupCountInfo info = {};
+  info.subgroupMCountAttr = wrap(subgroup_m_count_attr);
+  info.subgroupNCountAttr = wrap(subgroup_n_count_attr);
+  return info;
+}
+
+MlirAttribute ireeGPULoweringConfigAttrGetMmaKind(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  mlir::DictionaryAttr dict =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr))
+          .getAttributes();
+
+  constexpr mlir::StringLiteral kMmaKindName = "mma_kind";
+  mlir::iree_compiler::IREE::GPU::MmaInterfaceAttr mma_attr =
+      dict.getAs<mlir::iree_compiler::IREE::GPU::MmaInterfaceAttr>(
```

**Comment:**
We also have a helper for this

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:373`

```diff
@@ -352,7 +352,49 @@ PYBIND11_MODULE(_ireeCompilerDialects, m) {
           "cls"_a, "value"_a, "ctx"_a = py::none(),
           "Gets an #iree_gpu.lowering_config from parameters.")
       .def_property_readonly("attributes",
-                             ireeGPULoweringConfigAttrGetAttributes);
+                             ireeGPULoweringConfigAttrGetAttributes)
+      .def_property_readonly("workgroup_tile_sizes",
+                             [](MlirAttribute self) -> std::vector<int64_t> {
+                               auto tilesizes =
+                                   ireeGPULoweringConfigAttrGetTileSizes(self);
+                               return {tilesizes.workgroupTileSizes,
+                                       tilesizes.workgroupTileSizes +
+                                           tilesizes.numWorkgroupTileSizes};
+                             })
+      .def_property_readonly("reduction_tile_sizes",
+                             [](MlirAttribute self) -> std::vector<int64_t> {
+                               auto tilesizes =
+                                   ireeGPULoweringConfigAttrGetTileSizes(self);
+                               return {tilesizes.reductionTileSizes,
+                                       tilesizes.reductionTileSizes +
+                                           tilesizes.numReductionTileSizes};
+                             })
+      .def_property_readonly(
+          "subgroup_count",
```

**Comment:**
To me it's not clear how these map to mnk dimensions. Maybe call it `subgroup_count_mn`?

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:13`

```diff
@@ -10,6 +10,16 @@
 from iree.compiler.dialects import flow, hal, stream, vm, util, iree_codegen, iree_gpu
 
 
+def get_array_attr(vals: list[int]) -> ir.ArrayAttr:
```

**Comment:**
This function name doesn't specify the element type

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:19`

```diff
@@ -10,6 +10,16 @@
 from iree.compiler.dialects import flow, hal, stream, vm, util, iree_codegen, iree_gpu
 
 
+def get_array_attr(vals: list[int]) -> ir.ArrayAttr:
+    return ir.ArrayAttr.get(
+        [ir.IntegerAttr.get(ir.IndexType.get(), val) for val in vals]
+    )
+
+
+def get_integer_attr(val: int) -> ir.IntegerAttr:
```

**Comment:**
This returns the index type

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:15`

```diff
@@ -10,6 +10,16 @@
 from iree.compiler.dialects import flow, hal, stream, vm, util, iree_codegen, iree_gpu
 
 
+def get_array_attr(vals: list[int]) -> ir.ArrayAttr:
+    return ir.ArrayAttr.get(
+        [ir.IntegerAttr.get(ir.IndexType.get(), val) for val in vals]
```

**Comment:**
You can move the `get_index_attr` helper above and use here

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:388`

```diff
@@ -352,7 +352,49 @@ PYBIND11_MODULE(_ireeCompilerDialects, m) {
           "cls"_a, "value"_a, "ctx"_a = py::none(),
           "Gets an #iree_gpu.lowering_config from parameters.")
       .def_property_readonly("attributes",
-                             ireeGPULoweringConfigAttrGetAttributes);
+                             ireeGPULoweringConfigAttrGetAttributes)
+      .def_property_readonly("workgroup_tile_sizes",
+                             [](MlirAttribute self) -> std::vector<int64_t> {
+                               auto tilesizes =
+                                   ireeGPULoweringConfigAttrGetTileSizes(self);
+                               return {tilesizes.workgroupTileSizes,
+                                       tilesizes.workgroupTileSizes +
+                                           tilesizes.numWorkgroupTileSizes};
+                             })
+      .def_property_readonly("reduction_tile_sizes",
+                             [](MlirAttribute self) -> std::vector<int64_t> {
+                               auto tilesizes =
+                                   ireeGPULoweringConfigAttrGetTileSizes(self);
+                               return {tilesizes.reductionTileSizes,
+                                       tilesizes.reductionTileSizes +
+                                           tilesizes.numReductionTileSizes};
+                             })
+      .def_property_readonly(
+          "subgroup_count_mn",
+          [](MlirAttribute self) -> py::tuple {
+            ireeGPUSubgroupCountInfo info =
+                ireeGPULoweringConfigAttrGetSubgroupCount(self);
+            MlirAttribute mCountAttr = info.subgroupMCountAttr;
+            MlirAttribute nCountAttr = info.subgroupNCountAttr;
+            std::optional<int64_t> mCount =
+                mlirAttributeIsNull(mCountAttr)
+                    ? std::nullopt
+                    : std::optional<int64_t>(
+                          mlirIntegerAttrGetValueInt(mCountAttr));
+            std::optional<int64_t> nCount =
+                mlirAttributeIsNull(nCountAttr)
+                    ? std::nullopt
+                    : std::optional<int64_t>(
+                          mlirIntegerAttrGetValueInt(nCountAttr));
```

**Comment:**
These ternaries get pretty lengthy. Instead, I'd put the assignment in an if condition.

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:233`

```diff
@@ -213,3 +214,65 @@ MlirAttribute ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr) {
                   unwrap(attr))
                   .getAttributes());
 }
+
+ireeGPUTileSizes ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  ireeGPUTileSizes tilesizes = {};
+  auto loweringConfigAttr =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr));
+
+  llvm::SmallVector<int64_t> workgroups =
+      loweringConfigAttr.getWorkgroupTileSizes();
+  tilesizes.workgroupTileSizes = workgroups.data();
+  tilesizes.numWorkgroupTileSizes = workgroups.size();
+
+  llvm::SmallVector<int64_t> reductions =
+      loweringConfigAttr.getStaticTilingLevelSizes(
+          static_cast<int64_t>(
+              mlir::iree_compiler::IREE::GPU::TilingLevel::Reduction),
```

**Comment:**
use `llvm::to_underlying` to make sure we get the type right

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:236`

```diff
@@ -213,3 +214,65 @@ MlirAttribute ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr) {
                   unwrap(attr))
                   .getAttributes());
 }
+
+ireeGPUTileSizes ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  ireeGPUTileSizes tilesizes = {};
+  auto loweringConfigAttr =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr));
+
+  llvm::SmallVector<int64_t> workgroups =
+      loweringConfigAttr.getWorkgroupTileSizes();
+  tilesizes.workgroupTileSizes = workgroups.data();
+  tilesizes.numWorkgroupTileSizes = workgroups.size();
+
+  llvm::SmallVector<int64_t> reductions =
+      loweringConfigAttr.getStaticTilingLevelSizes(
+          llvm::to_underlying(
+              mlir::iree_compiler::IREE::GPU::TilingLevel::Reduction),
+          nullptr);
+  tilesizes.reductionTileSizes = reductions.data();
+  tilesizes.numReductionTileSizes = reductions.size();
```

**Comment:**
This returns dangling pointers. We should use the data from the attr storage -- these two should be array attributes.

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:367`

```diff
@@ -352,7 +352,66 @@ PYBIND11_MODULE(_ireeCompilerDialects, m) {
           "cls"_a, "value"_a, "ctx"_a = py::none(),
           "Gets an #iree_gpu.lowering_config from parameters.")
       .def_property_readonly("attributes",
-                             ireeGPULoweringConfigAttrGetAttributes);
+                             ireeGPULoweringConfigAttrGetAttributes)
+      .def_property_readonly(
+          "workgroup_tile_sizes",
+          [](MlirAttribute self) -> std::vector<int64_t> {
+            auto tilesizes = ireeGPULoweringConfigAttrGetTileSizes(self);
+            MlirAttribute workgroupAttr = tilesizes.workgroupAttr;
+            if (mlirAttributeIsNull(workgroupAttr)) {
+              return {};
+            }
+
+            size_t len = mlirArrayAttrGetNumElements(workgroupAttr);
+            std::vector<int64_t> workgroup(len);
+            for (size_t i = 0, e = len; i < e; ++i) {
```

**Comment:**
```suggestion
            for (size_t i = 0; i < len; ++i) {
```

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:384`

```diff
@@ -352,7 +352,66 @@ PYBIND11_MODULE(_ireeCompilerDialects, m) {
           "cls"_a, "value"_a, "ctx"_a = py::none(),
           "Gets an #iree_gpu.lowering_config from parameters.")
       .def_property_readonly("attributes",
-                             ireeGPULoweringConfigAttrGetAttributes);
+                             ireeGPULoweringConfigAttrGetAttributes)
+      .def_property_readonly(
+          "workgroup_tile_sizes",
+          [](MlirAttribute self) -> std::vector<int64_t> {
+            auto tilesizes = ireeGPULoweringConfigAttrGetTileSizes(self);
+            MlirAttribute workgroupAttr = tilesizes.workgroupAttr;
+            if (mlirAttributeIsNull(workgroupAttr)) {
+              return {};
+            }
+
+            size_t len = mlirArrayAttrGetNumElements(workgroupAttr);
+            std::vector<int64_t> workgroup(len);
+            for (size_t i = 0, e = len; i < e; ++i) {
+              MlirAttribute attr = mlirArrayAttrGetElement(workgroupAttr, i);
+              workgroup[i] = mlirIntegerAttrGetValueInt(attr);
+            }
+            return workgroup;
+          })
+      .def_property_readonly(
+          "reduction_tile_sizes",
+          [](MlirAttribute self) -> std::vector<int64_t> {
+            auto tilesizes = ireeGPULoweringConfigAttrGetTileSizes(self);
+            MlirAttribute reductionAttr = tilesizes.reductionAttr;
+            if (mlirAttributeIsNull(reductionAttr)) {
+              return {};
+            }
+
+            size_t len = mlirArrayAttrGetNumElements(reductionAttr);
+            std::vector<int64_t> reduction(len);
+            for (size_t i = 0, e = len; i < e; ++i) {
```

**Comment:**
```suggestion
            for (size_t i = 0; i < len; ++i) {
```

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:230`

```diff
@@ -213,3 +214,62 @@ MlirAttribute ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr) {
                   unwrap(attr))
                   .getAttributes());
 }
+
+ireeGPUTileSizes ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  ireeGPUTileSizes tilesizes = {};
+  mlir::DictionaryAttr dict =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr))
+          .getAttributes();
+
+  constexpr mlir::StringLiteral workgroupName = "workgroup";
+  if (auto workgroupArray = dict.getAs<mlir::ArrayAttr>(workgroupName)) {
```

**Comment:**
We should expose these string literals in the dialect headers.

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:231`

```diff
@@ -213,3 +214,62 @@ MlirAttribute ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr) {
                   unwrap(attr))
                   .getAttributes());
 }
+
+ireeGPUTileSizes ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  ireeGPUTileSizes tilesizes = {};
+  mlir::DictionaryAttr dict =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr))
+          .getAttributes();
+
+  constexpr mlir::StringLiteral workgroupName = "workgroup";
+  if (auto workgroupArray = dict.getAs<mlir::ArrayAttr>(workgroupName)) {
+    tilesizes.workgroupAttr = wrap(workgroupArray);
+  }
+
+  constexpr mlir::StringLiteral reductionName = "reduction";
```

**Comment:**
Also this one.

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.h:89`

```diff
@@ -84,6 +84,11 @@ struct OpaqueMmaLayout {
 OpaqueMmaLayout getOpaqueMMALayout(MLIRContext *context,
                                    IREE::GPU::MMAIntrinsic intrinsic);
 
+/// Maps a GPU tiling level to its corresponding string representation. (e.g.,
+/// workgroup, reduction, etc.). If an invalid or unknown tiling level is
+/// provided, the function triggers an assertion failure.
```

**Comment:**
I'd say something more concise like:
```c++
/// Returns the name of the tilling `level`, as used in the `lowering_config` attribute.
```

---

**File:** `compiler/src/iree/compiler/API/Internal/IREEGPUDialectCAPI.cpp:228`

```diff
@@ -213,3 +214,67 @@ MlirAttribute ireeGPULoweringConfigAttrGetAttributes(MlirAttribute attr) {
                   unwrap(attr))
                   .getAttributes());
 }
+
+ireeGPUTileSizes ireeGPULoweringConfigAttrGetTileSizes(MlirAttribute attr) {
+  assert(ireeAttributeIsAGPULoweringConfigAttr(attr));
+  ireeGPUTileSizes tilesizes = {};
+  mlir::DictionaryAttr dict =
+      llvm::cast<mlir::iree_compiler::IREE::GPU::LoweringConfigAttr>(
+          unwrap(attr))
+          .getAttributes();
+
+  llvm::StringRef workgroupName =
+      mlir::iree_compiler::IREE::GPU::getTilingLevelName(
+          mlir::iree_compiler::IREE::GPU::TilingLevel::Workgroup);
```

**Comment:**
looks much cleaner now!

---


---


## [PR #19218](https://github.com/iree-org/iree/pull/19218): [tuner]: add c/python binding for querying mma intrinsic

### Review Summary

**CHANGES_REQUESTED** (2024-11-20)

Nice. Looks good, just need to clean up the code a bit.

Let's keep the test around for now to make sure it continues working as we iterate on this code, but I think we should drop it before landing.


**COMMENTED** (2024-11-20)


**COMMENTED** (2024-11-20)


**COMMENTED** (2024-11-20)


**COMMENTED** (2024-11-20)


**APPROVED** (2024-11-20)

LGTM. Have you checked locally that your previous python test still works?


### Code Comments

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_codegen.h:71`

```diff
@@ -66,6 +66,14 @@ MLIR_CAPI_EXPORTED MlirAttribute ireeCodegenCompilationInfoAttrGet(
 MLIR_CAPI_EXPORTED ireeCodegenCompilationInfoParameters
 ireeCodegenCompilationInfoAttrGetParameters(MlirAttribute attr);
 
+MLIR_CAPI_EXPORTED void
+ireeCodegenGetExecutableVariantOps(MlirModule module, int *num_ops,
+                                   MlirOperation *executableOps);
```

**Comment:**
Let's keep the types consistent with how the other functions handle containers in this file.
```suggestion
ireeCodegenGetExecutableVariantOps(MlirModule module,  size_t *numOps,
                                   MlirOperation *executableOps);
```
also use `thisCase` for function arguments instead of the `snake_case`

---

**File:** `compiler/bindings/c/iree/compiler/dialects/iree_codegen.h:75`

```diff
@@ -66,6 +66,14 @@ MLIR_CAPI_EXPORTED MlirAttribute ireeCodegenCompilationInfoAttrGet(
 MLIR_CAPI_EXPORTED ireeCodegenCompilationInfoParameters
 ireeCodegenCompilationInfoAttrGetParameters(MlirAttribute attr);
 
+MLIR_CAPI_EXPORTED void
+ireeCodegenGetExecutableVariantOps(MlirModule module, int *num_ops,
+                                   MlirOperation *executableOps);
+
+MLIR_CAPI_EXPORTED void ireeCodegenQueryMMAIntrinsics(MlirOperation op,
+                                                      int *num_intrinsics,
+                                                      uint32_t *mma_intrinsics);
```

**Comment:**
same here

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:24`

```diff
@@ -21,6 +21,38 @@ static const char *kGpuModuleImportPath =
 namespace py = pybind11;
 using namespace mlir::python::adaptors;
 
+py::list ireeCodegenGetExecutableVariantOpsBinding(MlirModule module) {
```

**Comment:**
You can return a vector: `std::vector<MlirOperation>`

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:53`

```diff
@@ -21,6 +21,38 @@ static const char *kGpuModuleImportPath =
 namespace py = pybind11;
 using namespace mlir::python::adaptors;
 
+py::list ireeCodegenGetExecutableVariantOpsBinding(MlirModule module) {
+  int numOps = 0;
+  ireeCodegenGetExecutableVariantOps(module, &numOps, nullptr);
+
+  std::vector<MlirOperation> ops(numOps);
+
+  ireeCodegenGetExecutableVariantOps(module, &numOps, ops.data());
+
+  py::list opsList;
+  for (int i = 0; i < numOps; ++i) {
+    opsList.append(ops[i]);
+  }
+
+  return opsList;
+}
+
+py::list ireeCodegenQueryMMAIntrinsicsBinding(MlirOperation op) {
+  int numMMAs = 0;
+  ireeCodegenQueryMMAIntrinsics(op, &numMMAs, nullptr);
+
+  std::vector<uint32_t> mmaIntrinsics(numMMAs);
+
+  ireeCodegenQueryMMAIntrinsics(op, &numMMAs, mmaIntrinsics.data());
+
+  py::list opsList;
+  for (int i = 0; i < numMMAs; ++i) {
+    opsList.append(mmaIntrinsics[i]);
+  }
+
+  return opsList;
```

**Comment:**
Here, we should return a list of enums, not integers. You can see how to construct an enum in the code that handles enum attributes below.

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:377`

```diff
@@ -326,4 +358,21 @@ PYBIND11_MODULE(_ireeCompilerDialects, m) {
           "Gets an #iree_gpu.lowering_config from parameters.")
       .def_property_readonly("attributes",
                              ireeGPULoweringConfigAttrGetAttributes);
+
+  //===-------------------------------------------------------------------===//
+  // Binding to utility function getExecutableVariantOps
+  //===-------------------------------------------------------------------===//
+
+  iree_codegen_module.def(
+      "get_executable_variant_ops", &ireeCodegenGetExecutableVariantOpsBinding,
+      "Gets the executable variant operations from a module.",
+      py::arg("module"));
+
+  //===-------------------------------------------------------------------===//
+  // Binding to utility function queryMMAIntrinsics
+  //===-------------------------------------------------------------------===//
+
+  iree_codegen_module.def(
+      "query_mma_intrinsics", &ireeCodegenQueryMMAIntrinsicsBinding,
+      "Querys the mma intrinsics from a executable variant op.", py::arg("op"));
```

**Comment:**
Please run your PR through a spell checker. I have a vscode extension for that.

---

**File:** `compiler/bindings/python/test/ir/dialects_test.py:240`

```diff
@@ -232,3 +232,44 @@ def compilation_info():
     assert compilation_info is not None
     assert compilation_info.lowering_config == lowering_config
     assert compilation_info.translation_info == translation_info
+
+
+@run
+def test_query_mma():
+    test_module = ir.Module.parse(
+        """
```

**Comment:**
I'm concerned this is a 'change detector' test. Every time we update any of the related dialects, we will have to come back to this test and also update it. It's worse than lit tests were at least you have the lsp helping you with syntax highlighting etc. IMO we can do without a test here -- the logic is tested on the C++ side with your test pass.

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:157`

```diff
@@ -149,3 +150,41 @@ ireeCodegenCompilationInfoAttrGetParameters(MlirAttribute attr) {
   parameters.translationInfo = wrap(compilationInfo.getTranslationInfo());
   return parameters;
 }
+
+void ireeCodegenGetExecutableVariantOps(MlirModule module, int *num_ops,
+                                        MlirOperation *executableOps) {
+  mlir::ModuleOp moduleOp = unwrap(module);
+  llvm::SmallVector<mlir::iree_compiler::IREE::HAL::ExecutableVariantOp>
```

**Comment:**
You can make a typedef for this op type to define these longs namespaces

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:156`

```diff
@@ -149,3 +150,41 @@ ireeCodegenCompilationInfoAttrGetParameters(MlirAttribute attr) {
   parameters.translationInfo = wrap(compilationInfo.getTranslationInfo());
   return parameters;
 }
+
+void ireeCodegenGetExecutableVariantOps(MlirModule module, int *num_ops,
+                                        MlirOperation *executableOps) {
+  mlir::ModuleOp moduleOp = unwrap(module);
```

**Comment:**
We should check that `num_ops` is not `nullptr`

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:166`

```diff
@@ -149,3 +150,41 @@ ireeCodegenCompilationInfoAttrGetParameters(MlirAttribute attr) {
   parameters.translationInfo = wrap(compilationInfo.getTranslationInfo());
   return parameters;
 }
+
+void ireeCodegenGetExecutableVariantOps(MlirModule module, int *num_ops,
+                                        MlirOperation *executableOps) {
+  mlir::ModuleOp moduleOp = unwrap(module);
+  llvm::SmallVector<mlir::iree_compiler::IREE::HAL::ExecutableVariantOp>
+      executableVariantOps =
+          mlir::iree_compiler::getExecutableVariantOps(moduleOp);
+
+  if (!executableOps) {
+    *num_ops = executableVariantOps.size();
+    return;
+  }
+
+  for (size_t i = 0; i < executableVariantOps.size(); i++) {
```

**Comment:**
We should check that `num_ops` matches the number of variant ops.

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:172`

```diff
@@ -149,3 +150,41 @@ ireeCodegenCompilationInfoAttrGetParameters(MlirAttribute attr) {
   parameters.translationInfo = wrap(compilationInfo.getTranslationInfo());
   return parameters;
 }
+
+void ireeCodegenGetExecutableVariantOps(MlirModule module, int *num_ops,
+                                        MlirOperation *executableOps) {
+  mlir::ModuleOp moduleOp = unwrap(module);
+  llvm::SmallVector<mlir::iree_compiler::IREE::HAL::ExecutableVariantOp>
+      executableVariantOps =
+          mlir::iree_compiler::getExecutableVariantOps(moduleOp);
+
+  if (!executableOps) {
+    *num_ops = executableVariantOps.size();
+    return;
+  }
+
+  for (size_t i = 0; i < executableVariantOps.size(); i++) {
+    executableOps[i] = wrap(executableVariantOps[i]);
+  }
+}
+
+void ireeCodegenQueryMMAIntrinsics(MlirOperation op, int *num_intrinsics,
+                                   uint32_t *mma_intrinsics) {
```

**Comment:**
Similar in this function

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:175`

```diff
@@ -149,3 +150,41 @@ ireeCodegenCompilationInfoAttrGetParameters(MlirAttribute attr) {
   parameters.translationInfo = wrap(compilationInfo.getTranslationInfo());
   return parameters;
 }
+
+void ireeCodegenGetExecutableVariantOps(MlirModule module, int *num_ops,
+                                        MlirOperation *executableOps) {
+  mlir::ModuleOp moduleOp = unwrap(module);
+  llvm::SmallVector<mlir::iree_compiler::IREE::HAL::ExecutableVariantOp>
+      executableVariantOps =
+          mlir::iree_compiler::getExecutableVariantOps(moduleOp);
+
+  if (!executableOps) {
+    *num_ops = executableVariantOps.size();
+    return;
+  }
+
+  for (size_t i = 0; i < executableVariantOps.size(); i++) {
+    executableOps[i] = wrap(executableVariantOps[i]);
+  }
+}
+
+void ireeCodegenQueryMMAIntrinsics(MlirOperation op, int *num_intrinsics,
+                                   uint32_t *mma_intrinsics) {
+  mlir::Operation *mlirOp = unwrap(op);
+  auto variantOp =
+      llvm::dyn_cast<mlir::iree_compiler::IREE::HAL::ExecutableVariantOp>(
```

**Comment:**
```suggestion
      llvm::dyn_cast_if_present<mlir::iree_compiler::IREE::HAL::ExecutableVariantOp>(
```

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:189`

```diff
@@ -149,3 +150,41 @@ ireeCodegenCompilationInfoAttrGetParameters(MlirAttribute attr) {
   parameters.translationInfo = wrap(compilationInfo.getTranslationInfo());
   return parameters;
 }
+
+void ireeCodegenGetExecutableVariantOps(MlirModule module, int *num_ops,
+                                        MlirOperation *executableOps) {
+  mlir::ModuleOp moduleOp = unwrap(module);
+  llvm::SmallVector<mlir::iree_compiler::IREE::HAL::ExecutableVariantOp>
+      executableVariantOps =
+          mlir::iree_compiler::getExecutableVariantOps(moduleOp);
+
+  if (!executableOps) {
+    *num_ops = executableVariantOps.size();
+    return;
+  }
+
+  for (size_t i = 0; i < executableVariantOps.size(); i++) {
+    executableOps[i] = wrap(executableVariantOps[i]);
+  }
+}
+
+void ireeCodegenQueryMMAIntrinsics(MlirOperation op, int *num_intrinsics,
+                                   uint32_t *mma_intrinsics) {
+  mlir::Operation *mlirOp = unwrap(op);
+  auto variantOp =
+      llvm::dyn_cast<mlir::iree_compiler::IREE::HAL::ExecutableVariantOp>(
+          mlirOp);
+
+  assert(variantOp && "operation is not a ExecutableVariantOp");
+
+  llvm::SmallVector<mlir::iree_compiler::IREE::GPU::MMAIntrinsic>
+      mmaIntrinsics = mlir::iree_compiler::queryMMAIntrinsics(variantOp);
+  if (!mma_intrinsics) {
+    *num_intrinsics = mmaIntrinsics.size();
+    return;
+  }
+
+  for (size_t i = 0; i < mmaIntrinsics.size(); i++) {
+    mma_intrinsics[i] = static_cast<uint32_t>(mmaIntrinsics[i]);
+  }
```

**Comment:**
Follow the llvm coding style for loops: https://llvm.org/docs/CodingStandards.html#don-t-evaluate-end-every-time-through-a-loop

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1033`

```diff
@@ -1030,7 +1030,7 @@ std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func) {
 
 SmallVector<IREE::HAL::ExecutableVariantOp>
 getExecutableVariantOps(mlir::ModuleOp moduleOp) {
-  llvm::SmallVector<IREE::HAL::ExecutableVariantOp> executableVariantOps;
+  SmallVector<IREE::HAL::ExecutableVariantOp> executableVariantOps;
```

**Comment:**
This looks like an old change?

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1042`

```diff
@@ -1039,7 +1039,7 @@ getExecutableVariantOps(mlir::ModuleOp moduleOp) {
 
 SmallVector<IREE::GPU::MMAIntrinsic>
 queryMMAIntrinsics(IREE::HAL::ExecutableVariantOp executableOp) {
-  llvm::SmallVector<IREE::GPU::MMAIntrinsic> mmaIntrinsics;
+  SmallVector<IREE::GPU::MMAIntrinsic> mmaIntrinsics;
```

**Comment:**
also here

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:41`

```diff
@@ -21,36 +21,33 @@ static const char *kGpuModuleImportPath =
 namespace py = pybind11;
 using namespace mlir::python::adaptors;
 
-py::list ireeCodegenGetExecutableVariantOpsBinding(MlirModule module) {
-  int numOps = 0;
+std::vector<MlirOperation>
+ireeCodegenGetExecutableVariantOpsBinding(MlirModule module) {
+  size_t numOps = 0;
   ireeCodegenGetExecutableVariantOps(module, &numOps, nullptr);
 
   std::vector<MlirOperation> ops(numOps);
 
   ireeCodegenGetExecutableVariantOps(module, &numOps, ops.data());
 
-  py::list opsList;
-  for (int i = 0; i < numOps; ++i) {
-    opsList.append(ops[i]);
-  }
-
-  return opsList;
+  return ops;
 }
 
-py::list ireeCodegenQueryMMAIntrinsicsBinding(MlirOperation op) {
-  int numMMAs = 0;
+std::vector<py::object> ireeCodegenQueryMMAIntrinsicsBinding(MlirOperation op) {
+  size_t numMMAs = 0;
   ireeCodegenQueryMMAIntrinsics(op, &numMMAs, nullptr);
 
   std::vector<uint32_t> mmaIntrinsics(numMMAs);
 
```

**Comment:**
```suggestion
  std::vector<uint32_t> mmaIntrinsics(numMMAs);
```

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:48`

```diff
@@ -21,36 +21,33 @@ static const char *kGpuModuleImportPath =
 namespace py = pybind11;
 using namespace mlir::python::adaptors;
 
-py::list ireeCodegenGetExecutableVariantOpsBinding(MlirModule module) {
-  int numOps = 0;
+std::vector<MlirOperation>
+ireeCodegenGetExecutableVariantOpsBinding(MlirModule module) {
+  size_t numOps = 0;
   ireeCodegenGetExecutableVariantOps(module, &numOps, nullptr);
 
   std::vector<MlirOperation> ops(numOps);
 
   ireeCodegenGetExecutableVariantOps(module, &numOps, ops.data());
 
-  py::list opsList;
-  for (int i = 0; i < numOps; ++i) {
-    opsList.append(ops[i]);
-  }
-
-  return opsList;
+  return ops;
 }
 
-py::list ireeCodegenQueryMMAIntrinsicsBinding(MlirOperation op) {
-  int numMMAs = 0;
+std::vector<py::object> ireeCodegenQueryMMAIntrinsicsBinding(MlirOperation op) {
+  size_t numMMAs = 0;
   ireeCodegenQueryMMAIntrinsics(op, &numMMAs, nullptr);
 
   std::vector<uint32_t> mmaIntrinsics(numMMAs);
 
   ireeCodegenQueryMMAIntrinsics(op, &numMMAs, mmaIntrinsics.data());
 
-  py::list opsList;
-  for (int i = 0; i < numMMAs; ++i) {
-    opsList.append(mmaIntrinsics[i]);
-  }
-
-  return opsList;
+  std::vector<py::object> mmaList(numMMAs);
+  std::transform(mmaIntrinsics.begin(), mmaIntrinsics.end(), mmaList.begin(),
+                 [&](uint32_t rawValue) {
+                   return py::module_::import(kGpuModuleImportPath)
+                       .attr("MMAIntrinsic")(rawValue);
```

**Comment:**
We should get the enum att once and reuse it instead of importing the module N times.

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:30`

```diff
@@ -21,6 +21,35 @@ static const char *kGpuModuleImportPath =
 namespace py = pybind11;
 using namespace mlir::python::adaptors;
 
+std::vector<MlirOperation>
+ireeCodegenGetExecutableVariantOpsBinding(MlirModule module) {
+  size_t numOps = 0;
+  ireeCodegenGetExecutableVariantOps(module, &numOps, nullptr);
+
+  std::vector<MlirOperation> ops(numOps);
+
```

**Comment:**
```suggestion
  std::vector<MlirOperation> ops(numOps);
```

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:42`

```diff
@@ -21,6 +21,35 @@ static const char *kGpuModuleImportPath =
 namespace py = pybind11;
 using namespace mlir::python::adaptors;
 
+std::vector<MlirOperation>
+ireeCodegenGetExecutableVariantOpsBinding(MlirModule module) {
+  size_t numOps = 0;
+  ireeCodegenGetExecutableVariantOps(module, &numOps, nullptr);
+
+  std::vector<MlirOperation> ops(numOps);
+
+  ireeCodegenGetExecutableVariantOps(module, &numOps, ops.data());
+
+  return ops;
+}
+
+std::vector<py::object> ireeCodegenQueryMMAIntrinsicsBinding(MlirOperation op) {
+  size_t numMMAs = 0;
+  ireeCodegenQueryMMAIntrinsics(op, &numMMAs, nullptr);
+
+  std::vector<uint32_t> mmaIntrinsics(numMMAs);
+
+  ireeCodegenQueryMMAIntrinsics(op, &numMMAs, mmaIntrinsics.data());
```

**Comment:**
```suggestion
  ireeCodegenQueryMMAIntrinsics(op, &numMMAs, nullptr);
  std::vector<uint32_t> mmaIntrinsics(numMMAs);
  ireeCodegenQueryMMAIntrinsics(op, &numMMAs, mmaIntrinsics.data());
```

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:158`

```diff
@@ -149,3 +152,49 @@ ireeCodegenCompilationInfoAttrGetParameters(MlirAttribute attr) {
   parameters.translationInfo = wrap(compilationInfo.getTranslationInfo());
   return parameters;
 }
+
+void ireeCodegenGetExecutableVariantOps(MlirModule module, size_t *numOps,
+                                        MlirOperation *executableOps) {
+  assert(numOps && "numOps cannot be nullptr");
```

**Comment:**
We should also assert that `module` is not null

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:184`

```diff
@@ -149,3 +152,49 @@ ireeCodegenCompilationInfoAttrGetParameters(MlirAttribute attr) {
   parameters.translationInfo = wrap(compilationInfo.getTranslationInfo());
   return parameters;
 }
+
+void ireeCodegenGetExecutableVariantOps(MlirModule module, size_t *numOps,
+                                        MlirOperation *executableOps) {
+  assert(numOps && "numOps cannot be nullptr");
+
+  mlir::ModuleOp moduleOp = unwrap(module);
+  llvm::SmallVector<ExecutableVariantOp> executableVariantOps =
+      mlir::iree_compiler::getExecutableVariantOps(moduleOp);
+
+  if (!executableOps) {
+    *numOps = executableVariantOps.size();
+    return;
+  }
+
+  assert(
+      *numOps == executableVariantOps.size() &&
+      "*numOps must match the number of elements in the executableVariantOps");
+
+  for (size_t i = 0, e = executableVariantOps.size(); i < e; ++i) {
+    executableOps[i] = wrap(executableVariantOps[i]);
+  }
+}
+
+void ireeCodegenQueryMMAIntrinsics(MlirOperation op, size_t *numIntrinsics,
+                                   uint32_t *mmaIntrinsics) {
+  assert(numIntrinsics && "numIntrinsics cannot be nullptr");
+
+  mlir::Operation *mlirOp = unwrap(op);
+  auto variantOp = llvm::dyn_cast_if_present<ExecutableVariantOp>(mlirOp);
+
```

**Comment:**
```suggestion
```

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1033`

```diff
@@ -1030,7 +1030,7 @@ std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func) {
 
 SmallVector<IREE::HAL::ExecutableVariantOp>
 getExecutableVariantOps(mlir::ModuleOp moduleOp) {
-  llvm::SmallVector<IREE::HAL::ExecutableVariantOp> executableVariantOps;
+  SmallVector<IREE::HAL::ExecutableVariantOp> executableVariantOps;
```

**Comment:**
This is not resolved. If these are old changes, we should rebase this PR to be reasonably up-to-date with main.

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1042`

```diff
@@ -1039,7 +1039,7 @@ getExecutableVariantOps(mlir::ModuleOp moduleOp) {
 
 SmallVector<IREE::GPU::MMAIntrinsic>
 queryMMAIntrinsics(IREE::HAL::ExecutableVariantOp executableOp) {
-  llvm::SmallVector<IREE::GPU::MMAIntrinsic> mmaIntrinsics;
+  SmallVector<IREE::GPU::MMAIntrinsic> mmaIntrinsics;
```

**Comment:**
same here

---

**File:** `compiler/src/iree/compiler/API/Internal/IREECodegenDialectCAPI.cpp:158`

```diff
@@ -155,6 +155,7 @@ ireeCodegenCompilationInfoAttrGetParameters(MlirAttribute attr) {
 
 void ireeCodegenGetExecutableVariantOps(MlirModule module, size_t *numOps,
                                         MlirOperation *executableOps) {
+  assert(module.ptr && "module cannot be nullptr");
```

**Comment:**
use the c function to check this instead of accessing `.ptr` directly

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:46`

```diff
@@ -21,6 +21,32 @@ static const char *kGpuModuleImportPath =
 namespace py = pybind11;
 using namespace mlir::python::adaptors;
 
+std::vector<MlirOperation>
+ireeCodegenGetExecutableVariantOpsBinding(MlirModule module) {
+  size_t numOps = 0;
+  ireeCodegenGetExecutableVariantOps(module, &numOps, nullptr);
+
+  std::vector<MlirOperation> ops(numOps);
+
+  ireeCodegenGetExecutableVariantOps(module, &numOps, ops.data());
+
+  return ops;
+}
+
+std::vector<py::object> ireeCodegenQueryMMAIntrinsicsBinding(MlirOperation op) {
+  size_t numMMAs = 0;
+  ireeCodegenQueryMMAIntrinsics(op, &numMMAs, nullptr);
+  std::vector<uint32_t> mmaIntrinsics(numMMAs);
+  ireeCodegenQueryMMAIntrinsics(op, &numMMAs, mmaIntrinsics.data());
+
+  py::object mmaIntrinsicEnum =
+      py::module_::import(kGpuModuleImportPath).attr("MMAIntrinsic");
+  std::vector<py::object> mmaList(numMMAs);
+  std::transform(mmaIntrinsics.begin(), mmaIntrinsics.end(), mmaList.begin(),
+                 [&](uint32_t rawValue) { return mmaIntrinsicEnum(rawValue); });
```

**Comment:**
nit: I'd make this a for loop, I don't think the transform helps the readability much...

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:24`

```diff
@@ -21,6 +21,32 @@ static const char *kGpuModuleImportPath =
 namespace py = pybind11;
 using namespace mlir::python::adaptors;
 
+std::vector<MlirOperation>
```

**Comment:**
```suggestion
static std::vector<MlirOperation>
```

---

**File:** `compiler/bindings/python/IREECompilerDialectsModule.cpp:36`

```diff
@@ -21,6 +21,32 @@ static const char *kGpuModuleImportPath =
 namespace py = pybind11;
 using namespace mlir::python::adaptors;
 
+std::vector<MlirOperation>
+ireeCodegenGetExecutableVariantOpsBinding(MlirModule module) {
+  size_t numOps = 0;
+  ireeCodegenGetExecutableVariantOps(module, &numOps, nullptr);
+
+  std::vector<MlirOperation> ops(numOps);
+
+  ireeCodegenGetExecutableVariantOps(module, &numOps, ops.data());
+
+  return ops;
+}
+
+std::vector<py::object> ireeCodegenQueryMMAIntrinsicsBinding(MlirOperation op) {
```

**Comment:**
```suggestion
static std::vector<py::object> ireeCodegenQueryMMAIntrinsicsBinding(MlirOperation op) {
```

---


---


## [PR #19199](https://github.com/iree-org/iree/pull/19199): [tuner]: two new utility functions which are more friendly for c binding

### Review Summary

**COMMENTED** (2024-11-19)


**APPROVED** (2024-11-19)

LGTM % one nit


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1034`

```diff
@@ -1028,22 +1028,25 @@ std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func) {
   return std::nullopt;
 }
 
-llvm::SmallDenseMap<IREE::HAL::ExecutableVariantOp,
-                    SmallVector<IREE::GPU::MMAIntrinsic>>
-queryMMAIntrinsics(mlir::ModuleOp moduleOp) {
-  llvm::SmallDenseMap<IREE::HAL::ExecutableVariantOp,
-                      SmallVector<IREE::GPU::MMAIntrinsic>>
-      mmaAttributesMap;
-  moduleOp.walk([&](IREE::HAL::ExecutableVariantOp executableOp) {
-    if (IREE::GPU::TargetAttr target = getGPUTargetAttr(executableOp)) {
-      auto mmaIntrinsics = llvm::map_to_vector(
-          target.getWgp().getMma(), [](IREE::GPU::MMAAttr attr) {
-            return attr.getIntrinsic().getValue();
-          });
-      mmaAttributesMap[executableOp] = std::move(mmaIntrinsics);
-    }
-  });
-  return mmaAttributesMap;
+llvm::SmallVector<IREE::HAL::ExecutableVariantOp>
+getExecutableVariantOps(mlir::ModuleOp moduleOp) {
+  llvm::SmallVector<IREE::HAL::ExecutableVariantOp> executableVariantOps;
+  moduleOp.walk<WalkOrder::PreOrder>(
```

**Comment:**
Why did you change it to pre-order?

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1033`

```diff
@@ -1028,22 +1028,25 @@ std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func) {
   return std::nullopt;
 }
 
-llvm::SmallDenseMap<IREE::HAL::ExecutableVariantOp,
-                    SmallVector<IREE::GPU::MMAIntrinsic>>
-queryMMAIntrinsics(mlir::ModuleOp moduleOp) {
-  llvm::SmallDenseMap<IREE::HAL::ExecutableVariantOp,
-                      SmallVector<IREE::GPU::MMAIntrinsic>>
-      mmaAttributesMap;
-  moduleOp.walk([&](IREE::HAL::ExecutableVariantOp executableOp) {
-    if (IREE::GPU::TargetAttr target = getGPUTargetAttr(executableOp)) {
-      auto mmaIntrinsics = llvm::map_to_vector(
-          target.getWgp().getMma(), [](IREE::GPU::MMAAttr attr) {
-            return attr.getIntrinsic().getValue();
-          });
-      mmaAttributesMap[executableOp] = std::move(mmaIntrinsics);
-    }
-  });
-  return mmaAttributesMap;
+llvm::SmallVector<IREE::HAL::ExecutableVariantOp>
+getExecutableVariantOps(mlir::ModuleOp moduleOp) {
+  llvm::SmallVector<IREE::HAL::ExecutableVariantOp> executableVariantOps;
```

**Comment:**
```suggestion
SmallVector<IREE::HAL::ExecutableVariantOp>
getExecutableVariantOps(mlir::ModuleOp moduleOp) {
  SmallVector<IREE::HAL::ExecutableVariantOp> executableVariantOps;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.h:211`

```diff
@@ -207,13 +207,16 @@ IREE::GPU::TargetAttr getGPUTargetAttr(Operation *op);
 /// Returns std::nullopt if none found.
 std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func);
 
-/// Returns a map of supported MMA intrinsic instructions based on the
-/// GPU target descriptions in `moduleOp`. Each entry in the map associates
-/// an `IREE::HAL::ExecutableVariantOp` with a vector of
-/// `IREE::GPU::MMAIntrinsic` attributes.
-llvm::SmallDenseMap<IREE::HAL::ExecutableVariantOp,
-                    SmallVector<IREE::GPU::MMAIntrinsic>>
-queryMMAIntrinsics(mlir::ModuleOp moduleOp);
+/// Returns all `IREE::HAL::ExecutableVariantOp` operations from the
+/// given `mlir::ModuleOp`,  ensuring they are returned in their original IR
```

**Comment:**
```suggestion
/// given `mlir::ModuleOp`, ensuring they are returned in their original IR
```

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.h:213`

```diff
@@ -207,13 +207,16 @@ IREE::GPU::TargetAttr getGPUTargetAttr(Operation *op);
 /// Returns std::nullopt if none found.
 std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func);
 
-/// Returns a map of supported MMA intrinsic instructions based on the
-/// GPU target descriptions in `moduleOp`. Each entry in the map associates
-/// an `IREE::HAL::ExecutableVariantOp` with a vector of
-/// `IREE::GPU::MMAIntrinsic` attributes.
-llvm::SmallDenseMap<IREE::HAL::ExecutableVariantOp,
-                    SmallVector<IREE::GPU::MMAIntrinsic>>
-queryMMAIntrinsics(mlir::ModuleOp moduleOp);
+/// Returns all `IREE::HAL::ExecutableVariantOp` operations from the
+/// given `mlir::ModuleOp`,  ensuring they are returned in their original IR
+/// order.
+llvm::SmallVector<IREE::HAL::ExecutableVariantOp>
```

**Comment:**
```suggestion
SmallVector<IREE::HAL::ExecutableVariantOp>
```

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.h:218`

```diff
@@ -207,13 +207,16 @@ IREE::GPU::TargetAttr getGPUTargetAttr(Operation *op);
 /// Returns std::nullopt if none found.
 std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func);
 
-/// Returns a map of supported MMA intrinsic instructions based on the
-/// GPU target descriptions in `moduleOp`. Each entry in the map associates
-/// an `IREE::HAL::ExecutableVariantOp` with a vector of
-/// `IREE::GPU::MMAIntrinsic` attributes.
-llvm::SmallDenseMap<IREE::HAL::ExecutableVariantOp,
-                    SmallVector<IREE::GPU::MMAIntrinsic>>
-queryMMAIntrinsics(mlir::ModuleOp moduleOp);
+/// Returns all `IREE::HAL::ExecutableVariantOp` operations from the
+/// given `mlir::ModuleOp`,  ensuring they are returned in their original IR
+/// order.
+llvm::SmallVector<IREE::HAL::ExecutableVariantOp>
+getExecutableVariantOps(mlir::ModuleOp moduleOp);
+
+// Returns the MMA intrinsics associated with the given
+// `IREE::HAL::ExecutableVariantOp`.
+llvm::SmallVector<IREE::GPU::MMAIntrinsic>
```

**Comment:**
```suggestion
SmallVector<IREE::GPU::MMAIntrinsic>
```

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/TestLLVMGPUQueryMMAPass.cpp:28`

```diff
@@ -23,15 +23,16 @@ struct TestLLVMGPUQueryMMAPass final
     : impl::TestLLVMGPUQueryMMAPassBase<TestLLVMGPUQueryMMAPass> {
   void runOnOperation() override {
     ModuleOp moduleOp = getOperation();
-    llvm::SmallDenseMap<IREE::HAL::ExecutableVariantOp,
-                        SmallVector<IREE::GPU::MMAIntrinsic>>
-        mmaMap = queryMMAIntrinsics(moduleOp);
-    for (const auto &[op, mmaAttrs] : mmaMap) {
+    SmallVector<IREE::HAL::ExecutableVariantOp> executableVariantOps =
+        getExecutableVariantOps(moduleOp);
+    for (const auto &op : executableVariantOps) {
```

**Comment:**
You can use values for IR constructs -- they are designed to be cheap to pass by value. Also use the actual type, since the type is not obvious based on the RHS only.

---


---


## [PR #19124](https://github.com/iree-org/iree/pull/19124): [tuner]: Add a utility function to query supported MMA intrinsics

### Review Summary

**CHANGES_REQUESTED** (2024-11-13)

> and expose it to C API and python

This doesn't expose the new helper to C or python. Did you forget to add some files when pushing or is that coming in a future PR?


**COMMENTED** (2024-11-13)


**COMMENTED** (2024-11-14)


**COMMENTED** (2024-11-14)

Looks pretty good now, just a few more comments.

We should also update the title of the PR so that we don't advertise C API  and Python bindings which are not in the PR.


**COMMENTED** (2024-11-14)


**COMMENTED** (2024-11-14)


**COMMENTED** (2024-11-14)


**COMMENTED** (2024-11-14)


**COMMENTED** (2024-11-14)


**APPROVED** (2024-11-14)

LGTM


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/TestLLVMGPUQueryMMAPass.cpp:27`

```diff
@@ -0,0 +1,48 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/LLVMGPU/Passes.h"
+#include "iree/compiler/Codegen/Utils/GPUUtils.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "iree-test-llvmgpu-query-mma"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_TESTLLVMGPUQUERYMMAPASS
+#include "iree/compiler/Codegen/LLVMGPU/Passes.h.inc"
+
+static void printMMAVector(SmallVector<IREE::GPU::MMAAttr> &mmaAttrs,
+                           const std::string &extraMessage = {}) {
+  llvm::outs() << "Printing MMA Collection" << extraMessage
+               << ", size: " << mmaAttrs.size() << "\n";
+  for (const auto &mma : mmaAttrs) {
+    llvm::outs() << mma << " ";
+  }
+  llvm::outs() << "\n";
```

**Comment:**
This helper doesn't really do anything -- we can inline it into the pass use `llvm::interleaveComma` instead of the for loop.

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/TestLLVMGPUQueryMMAPass.cpp:39`

```diff
@@ -0,0 +1,48 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/LLVMGPU/Passes.h"
+#include "iree/compiler/Codegen/Utils/GPUUtils.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "iree-test-llvmgpu-query-mma"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_TESTLLVMGPUQUERYMMAPASS
+#include "iree/compiler/Codegen/LLVMGPU/Passes.h.inc"
+
+static void printMMAVector(SmallVector<IREE::GPU::MMAAttr> &mmaAttrs,
+                           const std::string &extraMessage = {}) {
+  llvm::outs() << "Printing MMA Collection" << extraMessage
+               << ", size: " << mmaAttrs.size() << "\n";
+  for (const auto &mma : mmaAttrs) {
+    llvm::outs() << mma << " ";
+  }
+  llvm::outs() << "\n";
+}
+
+namespace {
+
+struct TestLLVMGPUQueryMMAPass final
+    : impl::TestLLVMGPUQueryMMAPassBase<TestLLVMGPUQueryMMAPass> {
+  void runOnOperation() override {
+    ModuleOp moduleOp = getOperation();
+    SmallVector<IREE::GPU::MMAAttr> mmaCollecton;
+    // Print mma vector before collection.
+    printMMAVector(mmaCollecton,
+                   " Before querying supported mma instrinsic instructions");
```

**Comment:**
There's no point in printing an empty vector

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/test/test_query_mma.mlir:5`

```diff
@@ -0,0 +1,60 @@
+// RUN: iree-opt --split-input-file --iree-test-llvmgpu-query-mma %s | FileCheck %s
+
+#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb",
+{abi = "hip", iree.gpu.target = #iree_gpu.target<arch = "gfx942", features = "",
+wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8,
```

**Comment:**
We don't need an exhaustive list all the other wgp properties -- we can trim it down to something minimal like `compute = int32, storage = b32, ...`

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1037`

```diff
@@ -1028,4 +1029,27 @@ std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func) {
   return std::nullopt;
 }
 
+void QueryMMAIntrinsics(mlir::ModuleOp moduleOp,
+                        SmallVector<IREE::GPU::MMAAttr> &mmaAttrs) {
+  IREE::GPU::TargetAttr target;
+
+  // Walk through all `func::FuncOp` operations in `moduleOp`.
+  moduleOp.walk([&](func::FuncOp funcOp) {
```

**Comment:**
Wouldn't it be enough to look up `hal.executable.variant` only? This is where the attribute is attached to.

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1036`

```diff
@@ -1028,4 +1029,27 @@ std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func) {
   return std::nullopt;
 }
 
+void QueryMMAIntrinsics(mlir::ModuleOp moduleOp,
+                        SmallVector<IREE::GPU::MMAAttr> &mmaAttrs) {
+  IREE::GPU::TargetAttr target;
+
+  // Walk through all `func::FuncOp` operations in `moduleOp`.
```

**Comment:**
This comment doesn't clarify anything -- `moduleOp.walk` is a very basic function and the intention is obvious here.

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1033`

```diff
@@ -1028,4 +1029,27 @@ std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func) {
   return std::nullopt;
 }
 
+void QueryMMAIntrinsics(mlir::ModuleOp moduleOp,
+                        SmallVector<IREE::GPU::MMAAttr> &mmaAttrs) {
```

**Comment:**
I think this should probably return a mapping of executable variants to their mma attrs. Should should also have a test of a module with two variants.

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1050`

```diff
@@ -1028,4 +1029,27 @@ std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func) {
   return std::nullopt;
 }
 
+void QueryMMAIntrinsics(mlir::ModuleOp moduleOp,
+                        SmallVector<IREE::GPU::MMAAttr> &mmaAttrs) {
+  IREE::GPU::TargetAttr target;
+
+  // Walk through all `func::FuncOp` operations in `moduleOp`.
+  moduleOp.walk([&](func::FuncOp funcOp) {
+    if (auto attr = getGPUTargetAttr(funcOp)) {
+      // Store the target attribute if found.
+      target = attr;
+      return WalkResult::interrupt();
+    }
+    return WalkResult::advance();
+  });
+
+  if (target) {
+    // Append each MMA attribute from the target's `Wgp` configuration to
+    // `mmaAttrs`.
+    for (IREE::GPU::MMAAttr mma : target.getWgp().getMma()) {
+      mmaAttrs.emplace_back(mma);
```

**Comment:**
we should be able to append all of them at once with `llvm::append_range`

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.h:211`

```diff
@@ -206,6 +206,11 @@ IREE::GPU::TargetAttr getGPUTargetAttr(Operation *op);
 /// Returns std::nullopt if none found.
 std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func);
 
+/// Returns supported MMA intrinsic instructions based on the GPU target
+/// description stored in `moduleOp` and populates them in `mmaAttrs`.
+void QueryMMAIntrinsics(mlir::ModuleOp moduleOp,
```

**Comment:**
```suggestion
void queryMMAIntrinsics(mlir::ModuleOp moduleOp,
```

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.h:212`

```diff
@@ -206,6 +206,11 @@ IREE::GPU::TargetAttr getGPUTargetAttr(Operation *op);
 /// Returns std::nullopt if none found.
 std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func);
 
+/// Returns supported MMA intrinsic instructions based on the GPU target
+/// description stored in `moduleOp` and populates them in `mmaAttrs`.
+void QueryMMAIntrinsics(mlir::ModuleOp moduleOp,
+                        SmallVector<IREE::GPU::MMAAttr> &mmaAttrs);
```

**Comment:**
Can we return mma intrinsics attrs instead of mma attr? I think we can always go from an intrsic to an mma but not the other way round?

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/TestLLVMGPUQueryMMAPass.cpp:31`

```diff
@@ -17,31 +17,31 @@ namespace mlir::iree_compiler {
 #define GEN_PASS_DEF_TESTLLVMGPUQUERYMMAPASS
 #include "iree/compiler/Codegen/LLVMGPU/Passes.h.inc"
 
-static void printMMAVector(SmallVector<IREE::GPU::MMAAttr> &mmaAttrs,
-                           const std::string &extraMessage = {}) {
-  llvm::outs() << "Printing MMA Collection" << extraMessage
-               << ", size: " << mmaAttrs.size() << "\n";
-  for (const auto &mma : mmaAttrs) {
-    llvm::outs() << mma << " ";
-  }
-  llvm::outs() << "\n";
-}
-
 namespace {
 
 struct TestLLVMGPUQueryMMAPass final
     : impl::TestLLVMGPUQueryMMAPassBase<TestLLVMGPUQueryMMAPass> {
   void runOnOperation() override {
     ModuleOp moduleOp = getOperation();
-    SmallVector<IREE::GPU::MMAAttr> mmaCollecton;
-    // Print mma vector before collection.
-    printMMAVector(mmaCollecton,
-                   " Before querying supported mma instrinsic instructions");
-    // Collect mma intrinsic instructions.
-    QueryMMAIntrinsics(moduleOp, mmaCollecton);
-    // Print mma vector after collection.
-    printMMAVector(mmaCollecton,
-                   " After querying supported mma instrinsic instructions");
+    llvm::SmallDenseMap<Operation *, SmallVector<IREE::GPU::MMAIntrinsic>>
+        mmaMap;
+    queryMMAIntrinsics(moduleOp, mmaMap);
+    for (const auto &entry : mmaMap) {
+      Operation *op = entry.first;
+      const SmallVector<IREE::GPU::MMAIntrinsic> &mmaAttrs = entry.second;
```

**Comment:**
You can use structured bindings to unpack these two members to variables

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/TestLLVMGPUQueryMMAPass.cpp:42`

```diff
@@ -17,31 +17,31 @@ namespace mlir::iree_compiler {
 #define GEN_PASS_DEF_TESTLLVMGPUQUERYMMAPASS
 #include "iree/compiler/Codegen/LLVMGPU/Passes.h.inc"
 
-static void printMMAVector(SmallVector<IREE::GPU::MMAAttr> &mmaAttrs,
-                           const std::string &extraMessage = {}) {
-  llvm::outs() << "Printing MMA Collection" << extraMessage
-               << ", size: " << mmaAttrs.size() << "\n";
-  for (const auto &mma : mmaAttrs) {
-    llvm::outs() << mma << " ";
-  }
-  llvm::outs() << "\n";
-}
-
 namespace {
 
 struct TestLLVMGPUQueryMMAPass final
     : impl::TestLLVMGPUQueryMMAPassBase<TestLLVMGPUQueryMMAPass> {
   void runOnOperation() override {
     ModuleOp moduleOp = getOperation();
-    SmallVector<IREE::GPU::MMAAttr> mmaCollecton;
-    // Print mma vector before collection.
-    printMMAVector(mmaCollecton,
-                   " Before querying supported mma instrinsic instructions");
-    // Collect mma intrinsic instructions.
-    QueryMMAIntrinsics(moduleOp, mmaCollecton);
-    // Print mma vector after collection.
-    printMMAVector(mmaCollecton,
-                   " After querying supported mma instrinsic instructions");
+    llvm::SmallDenseMap<Operation *, SmallVector<IREE::GPU::MMAIntrinsic>>
+        mmaMap;
+    queryMMAIntrinsics(moduleOp, mmaMap);
+    for (const auto &entry : mmaMap) {
+      Operation *op = entry.first;
+      const SmallVector<IREE::GPU::MMAIntrinsic> &mmaAttrs = entry.second;
+      if (auto variantOp = llvm::dyn_cast<IREE::HAL::ExecutableVariantOp>(op)) {
+        llvm::outs() << "Executable Variant Name: " << variantOp.getName()
+                     << "\n";
+      } else {
+        llvm::outs() << "Executable Variant Name: " << "Unnamed Operation"
+                     << "\n";
+      }
+      llvm::outs() << "MMA Intrinsics: ";
+      for (const auto &mma : mmaAttrs) {
+        llvm::outs() << mma << " ";
+      }
```

**Comment:**
use  `llvm::interleave` -- you can give it the desired separator

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/test/test_query_mma.mlir:40`

```diff
@@ -25,18 +22,65 @@ module {
   }
 }
 
-// CHECK: Printing MMA Collection Before querying supported mma instrinsic instructions, size: 0
-// CHECK: Printing MMA Collection After querying supported mma instrinsic instructions, size: 9
-// CHECK: MFMA_F32_16x16x4_F32
+// CHECK:       Executable Variant Name
+// CHECK-SAME:  main
+// CHECK: MMA   Intrinsics
+// CHECK-SAME:  MFMA_F32_16x16x4_F32
+// CHECK-SAME:  MFMA_F32_16x16x16_F16
+// CHECK-LABEL: func.func @fn
+
+// -----
+
+#executable_target_rocm_hsaco_fb0 = #hal.executable.target<"rocm", "rocm-hsaco-fb",
+{abi = "hip", iree.gpu.target = #iree_gpu.target<arch = "gfx942", features = "",
+wgp = <compute = int32, storage =  b32,
+subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32,
+mma = [<MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>],
+subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024],
+max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536,
```

**Comment:**
could we further trim this down but skipping some of these `wgp` properties out entirely?

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.h:214`

```diff
@@ -207,9 +207,11 @@ IREE::GPU::TargetAttr getGPUTargetAttr(Operation *op);
 std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func);
 
 /// Returns supported MMA intrinsic instructions based on the GPU target
-/// description stored in `moduleOp` and populates them in `mmaAttrs`.
-void QueryMMAIntrinsics(mlir::ModuleOp moduleOp,
-                        SmallVector<IREE::GPU::MMAAttr> &mmaAttrs);
+/// description stored in `moduleOp` and populates them in `MMAIntrinsic`.
+void queryMMAIntrinsics(
+    mlir::ModuleOp moduleOp,
+    llvm::SmallDenseMap<Operation *, SmallVector<IREE::GPU::MMAIntrinsic>>
+        &mmaAttributesMap);
```

**Comment:**
Why not return this map?

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1041`

```diff
@@ -1028,4 +1029,22 @@ std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func) {
   return std::nullopt;
 }
 
+void queryMMAIntrinsics(
+    mlir::ModuleOp moduleOp,
+    llvm::SmallDenseMap<Operation *, SmallVector<IREE::GPU::MMAIntrinsic>>
+        &mmaAttributesMap) {
+  moduleOp.walk([&](IREE::HAL::ExecutableVariantOp executableOp) {
+    if (IREE::GPU::TargetAttr target = getGPUTargetAttr(executableOp)) {
+      SmallVector<IREE::GPU::MMAIntrinsic> mmaIntrinsics;
+      llvm::append_range(
+          mmaIntrinsics,
+          llvm::map_range(target.getWgp().getMma(),
```

**Comment:**
Instead of appending to an empty range, use `llvm::map_to_vector`.

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.h:213`

```diff
@@ -206,6 +206,13 @@ IREE::GPU::TargetAttr getGPUTargetAttr(Operation *op);
 /// Returns std::nullopt if none found.
 std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func);
 
+/// Returns a map of supported MMA intrinsic instructions based on the
+/// GPU target descriptions in `moduleOp`. Each entry in the map associates
+/// an `Operation*` ( an `IREE::HAL::ExecutableVariantOp`) with a
+/// vector of `IREE::GPU::MMAIntrinsic` attributes.
+llvm::SmallDenseMap<Operation *, SmallVector<IREE::GPU::MMAIntrinsic>>
```

**Comment:**
Why do we want to return `Operation *` instead of `ExecutableVariantOp`? I think this would simplify both code and the documentation.

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/TestLLVMGPUQueryMMAPass.cpp:35`

```diff
@@ -0,0 +1,43 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/LLVMGPU/Passes.h"
+#include "iree/compiler/Codegen/Utils/GPUUtils.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "iree-test-llvmgpu-query-mma"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_TESTLLVMGPUQUERYMMAPASS
+#include "iree/compiler/Codegen/LLVMGPU/Passes.h.inc"
+
+namespace {
+
+struct TestLLVMGPUQueryMMAPass final
+    : impl::TestLLVMGPUQueryMMAPassBase<TestLLVMGPUQueryMMAPass> {
+  void runOnOperation() override {
+    ModuleOp moduleOp = getOperation();
+    llvm::SmallDenseMap<Operation *, SmallVector<IREE::GPU::MMAIntrinsic>>
+        mmaMap = queryMMAIntrinsics(moduleOp);
+    for (const auto &[op, mmaAttrs] : mmaMap) {
+      if (auto variantOp = llvm::dyn_cast<IREE::HAL::ExecutableVariantOp>(op)) {
+        llvm::outs() << "Executable Variant Name: " << variantOp.getName()
+                     << "\n";
+      } else {
+        llvm::outs() << "Executable Variant Name: " << "Unnamed Operation"
+                     << "\n";
+      }
```

**Comment:**
Can this actually happen? I don't see it in the LIT test.

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:1042`

```diff
@@ -1028,4 +1029,20 @@ std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func) {
   return std::nullopt;
 }
 
+llvm::SmallDenseMap<Operation *, SmallVector<IREE::GPU::MMAIntrinsic>>
+queryMMAIntrinsics(mlir::ModuleOp moduleOp) {
+  llvm::SmallDenseMap<Operation *, SmallVector<IREE::GPU::MMAIntrinsic>>
+      mmaAttributesMap;
+  moduleOp.walk([&](IREE::HAL::ExecutableVariantOp executableOp) {
+    if (IREE::GPU::TargetAttr target = getGPUTargetAttr(executableOp)) {
+      auto mmaIntrinsics = llvm::map_to_vector(
+          target.getWgp().getMma(), [](IREE::GPU::MMAAttr attr) {
+            return attr.getIntrinsic().getValue();
+          });
+      mmaAttributesMap[executableOp] = mmaIntrinsics;
```

**Comment:**
```suggestion
      mmaAttributesMap[executableOp] = std::move(mmaIntrinsics);
```
to avoid needless copying

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:21`

```diff
@@ -18,6 +18,7 @@
 #include "llvm/Support/ErrorHandling.h"
 #include "mlir/Dialect/Affine/IR/AffineOps.h"
 #include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
```

**Comment:**
Is this include still necessary?

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/TestLLVMGPUQueryMMAPass.cpp:31`

```diff
@@ -23,16 +23,13 @@ struct TestLLVMGPUQueryMMAPass final
     : impl::TestLLVMGPUQueryMMAPassBase<TestLLVMGPUQueryMMAPass> {
   void runOnOperation() override {
     ModuleOp moduleOp = getOperation();
-    llvm::SmallDenseMap<Operation *, SmallVector<IREE::GPU::MMAIntrinsic>>
+    llvm::SmallDenseMap<IREE::HAL::ExecutableVariantOp,
+                        SmallVector<IREE::GPU::MMAIntrinsic>>
         mmaMap = queryMMAIntrinsics(moduleOp);
     for (const auto &[op, mmaAttrs] : mmaMap) {
-      if (auto variantOp = llvm::dyn_cast<IREE::HAL::ExecutableVariantOp>(op)) {
-        llvm::outs() << "Executable Variant Name: " << variantOp.getName()
-                     << "\n";
-      } else {
-        llvm::outs() << "Executable Variant Name: " << "Unnamed Operation"
-                     << "\n";
-      }
+      llvm::outs() << "Executable Variant Name: "
+                   << cast<IREE::HAL::ExecutableVariantOp>(*op).getName()
```

**Comment:**
Why do we need the cast? I'd think the type of op is already executable variant op, no?

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.h:213`

```diff
@@ -210,7 +211,8 @@ std::optional<int> getGPUSubgroupSize(mlir::FunctionOpInterface func);
 /// GPU target descriptions in `moduleOp`. Each entry in the map associates
 /// an `Operation*` ( an `IREE::HAL::ExecutableVariantOp`) with a
 /// vector of `IREE::GPU::MMAIntrinsic` attributes.
```

**Comment:**
This is out of date

---


---


## [PR #19069](https://github.com/iree-org/iree/pull/19069): [tuner] add an iree-opt pass to strip configuration from executable sources

### Review Summary

**COMMENTED** (2024-11-08)

Thanks for taking on this task. It seems to be functional in the current state already, now just need to clean it up a bit and make it landable.


**COMMENTED** (2024-11-08)


**COMMENTED** (2024-11-09)


**COMMENTED** (2024-11-09)


**COMMENTED** (2024-11-09)


**COMMENTED** (2024-11-11)

Just one remaining issue, looks good to me otherwise


**COMMENTED** (2024-11-11)


**COMMENTED** (2024-11-11)


**APPROVED** (2024-11-11)

LGTM, thanks for the fixes.


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/Passes.td:495`

```diff
@@ -490,6 +490,11 @@ def RemoveSingleIterationLoopPass :
   let summary = "Remove distributed loop with single iteration.";
 }
 
+def StripConfigInfoPass :
+    InterfacePass<"iree-codegen-strip-config-info", "mlir::FunctionOpInterface">{
+   let summary = "Remove all the the lowering configuration and translation info.";
```

**Comment:**
I'd call it `StripCompilationInfo`. The reason is that the `#iree_codegen.compilation_info` attribute contains both lowering config and translation info, so that'd encompass all 3 attributes

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripConfigInfoPass.cpp:20`

```diff
@@ -0,0 +1,44 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-config-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCONFIGINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+class StripConfigInfoPass final
+    : public impl::StripConfigInfoPassBase<StripConfigInfoPass> {
+  using impl::StripConfigInfoPassBase<
+      StripConfigInfoPass>::StripConfigInfoPassBase;
```

**Comment:**
```suggestion
  using impl::StripConfigInfoPassBass::StripConfigInfoPassBase;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripConfigInfoPass.cpp:18`

```diff
@@ -0,0 +1,44 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-config-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCONFIGINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+class StripConfigInfoPass final
+    : public impl::StripConfigInfoPassBase<StripConfigInfoPass> {
```

**Comment:**
nit: `struct` will save us some typing, since we don't need to hide data members from anybody anyway -- this pass is defined in an anonymous namespace, so only this file knows its full type anyway
```suggestion
struct StripConfigInfoPass final
    : impl::StripConfigInfoPassBase<StripConfigInfoPass> {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripConfigInfoPass.cpp:22`

```diff
@@ -0,0 +1,44 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-config-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCONFIGINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+class StripConfigInfoPass final
+    : public impl::StripConfigInfoPassBase<StripConfigInfoPass> {
+  using impl::StripConfigInfoPassBase<
+      StripConfigInfoPass>::StripConfigInfoPassBase;
+
+public:
```

**Comment:**
```suggestion
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripConfigInfoPass.cpp:27`

```diff
@@ -0,0 +1,44 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-config-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCONFIGINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+class StripConfigInfoPass final
+    : public impl::StripConfigInfoPassBase<StripConfigInfoPass> {
+  using impl::StripConfigInfoPassBase<
+      StripConfigInfoPass>::StripConfigInfoPassBase;
+
+public:
+  void runOnOperation() override;
+};
+} // namespace
+
+void StripConfigInfoPass::runOnOperation() {
```

**Comment:**
I'd define this inline -- I don't think we gain much by outlining this function

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripConfigInfoPass.cpp:31`

```diff
@@ -0,0 +1,44 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-config-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCONFIGINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+class StripConfigInfoPass final
+    : public impl::StripConfigInfoPassBase<StripConfigInfoPass> {
+  using impl::StripConfigInfoPassBase<
+      StripConfigInfoPass>::StripConfigInfoPassBase;
+
+public:
+  void runOnOperation() override;
+};
+} // namespace
+
+void StripConfigInfoPass::runOnOperation() {
+  auto funcOp = getOperation();
+  IREE::Codegen::TranslationInfoAttr translationInfo =
+      getTranslationInfo(funcOp);
+  if (translationInfo) {
```

**Comment:**
We don't have to keep `translationInfo` as a local variable -- we never use it beyond checking that it's there. I'd do the same thing as you do with lowering config below.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripConfigInfoPass.cpp:37`

```diff
@@ -0,0 +1,44 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-config-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCONFIGINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+class StripConfigInfoPass final
+    : public impl::StripConfigInfoPassBase<StripConfigInfoPass> {
+  using impl::StripConfigInfoPassBase<
+      StripConfigInfoPass>::StripConfigInfoPassBase;
+
+public:
+  void runOnOperation() override;
+};
+} // namespace
+
+void StripConfigInfoPass::runOnOperation() {
+  auto funcOp = getOperation();
+  IREE::Codegen::TranslationInfoAttr translationInfo =
+      getTranslationInfo(funcOp);
+  if (translationInfo) {
+    // Erase the translation info from function if it exists.
+    eraseTranslationInfo(funcOp);
+  }
+
+  funcOp->walk([&](Operation *op) {
+    if (getLoweringConfig(op)) {
```

**Comment:**
We should be also stripping compilation info attributes for completeness sake IMO. These won't be produced by the compiler in the default flow, but would appear as the result of applying the tuning specs (transform dialect library).

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/strip_config_info.mlir:3`

```diff
@@ -0,0 +1,103 @@
+// RUN: iree-opt --split-input-file --pass-pipeline='builtin.module(hal.executable(hal.executable.variant(builtin.module(func.func(iree-codegen-strip-config-info)))))' %s | FileCheck %s
+
+#config = #iree_gpu.lowering_config<{mma_kind = #iree_gpu.mma_layout<MFMA_F32_16x16x16_F16>, promote_operands = [0, 1], reduction = [0, 0, 64], subgroup_m_count = 2 : i64, subgroup_n_count = 2 : i64, workgroup = [64, 128, 0]}>
```

**Comment:**
+1, we can write a small test case by hand that doesn't use the actual dumps from iree-opt. For example, setting `lowering_config` to a `StringAttr` should exercise the code just as well.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/strip_config_info.mlir:1`

```diff
@@ -0,0 +1,103 @@
+// RUN: iree-opt --split-input-file --pass-pipeline='builtin.module(hal.executable(hal.executable.variant(builtin.module(func.func(iree-codegen-strip-config-info)))))' %s | FileCheck %s
```

**Comment:**
Ah, now I think we should make this pass over `Operation` so that we don't have to manually specify the nesting. We can walk whatever the input op is and find functions ops first.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:22`

```diff
@@ -0,0 +1,80 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+// Checks whether the funcOp has any compilation info.
+LogicalResult hasCompilationInfo(func::FuncOp funcOp) {
+  if (getTranslationInfo(funcOp))
+    return success();
```

**Comment:**
This assumption is not correct. We may have lowering config / compilation info but not translation info. This would be true for modules configured by TD scripts.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:20`

```diff
@@ -0,0 +1,80 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+// Checks whether the funcOp has any compilation info.
+LogicalResult hasCompilationInfo(func::FuncOp funcOp) {
```

**Comment:**
IMO this can return `bool` -- it's very clear what the meaning of `true`/`false` is.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:37`

```diff
@@ -0,0 +1,80 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+// Checks whether the funcOp has any compilation info.
+LogicalResult hasCompilationInfo(func::FuncOp funcOp) {
+  if (getTranslationInfo(funcOp))
+    return success();
+
+  bool hasAttrConfig = false;
+  funcOp.walk([&](Operation *op) {
+    if (getCompilationInfo(op) || getLoweringConfig(op)) {
+      hasAttrConfig = true;
+      return;
+    }
+  });
+
+  // Return success if any relevant attributes were found.
+  return hasAttrConfig ? success() : failure();
+}
+
+class StripCompilationInfo : public OpRewritePattern<func::FuncOp> {
+public:
```

**Comment:**
Previous comments not addressed

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:45`

```diff
@@ -0,0 +1,80 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+// Checks whether the funcOp has any compilation info.
+LogicalResult hasCompilationInfo(func::FuncOp funcOp) {
+  if (getTranslationInfo(funcOp))
+    return success();
+
+  bool hasAttrConfig = false;
+  funcOp.walk([&](Operation *op) {
+    if (getCompilationInfo(op) || getLoweringConfig(op)) {
+      hasAttrConfig = true;
+      return;
+    }
+  });
+
+  // Return success if any relevant attributes were found.
+  return hasAttrConfig ? success() : failure();
+}
+
+class StripCompilationInfo : public OpRewritePattern<func::FuncOp> {
+public:
+  using OpRewritePattern<func::FuncOp>::OpRewritePattern;
+  LogicalResult matchAndRewrite(func::FuncOp funcOp,
+                                PatternRewriter &rewriter) const final {
+    if (failed(hasCompilationInfo(funcOp)))
+      return failure();
+
+    func::FuncOp newFuncOp =
+        dyn_cast<func::FuncOp>(rewriter.clone(*funcOp.getOperation()));
```

**Comment:**
Do we really need to clone the function to make this work? Can't we modify it in place?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:75`

```diff
@@ -0,0 +1,80 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+// Checks whether the funcOp has any compilation info.
+LogicalResult hasCompilationInfo(func::FuncOp funcOp) {
+  if (getTranslationInfo(funcOp))
+    return success();
+
+  bool hasAttrConfig = false;
+  funcOp.walk([&](Operation *op) {
+    if (getCompilationInfo(op) || getLoweringConfig(op)) {
+      hasAttrConfig = true;
+      return;
+    }
+  });
+
+  // Return success if any relevant attributes were found.
+  return hasAttrConfig ? success() : failure();
+}
+
+class StripCompilationInfo : public OpRewritePattern<func::FuncOp> {
+public:
+  using OpRewritePattern<func::FuncOp>::OpRewritePattern;
+  LogicalResult matchAndRewrite(func::FuncOp funcOp,
+                                PatternRewriter &rewriter) const final {
+    if (failed(hasCompilationInfo(funcOp)))
+      return failure();
+
+    func::FuncOp newFuncOp =
+        dyn_cast<func::FuncOp>(rewriter.clone(*funcOp.getOperation()));
+
+    // if the cloned function has translation info, erase it
+    if (getTranslationInfo(newFuncOp)) {
+      eraseTranslationInfo(newFuncOp);
+    }
+
+    newFuncOp->walk([&](Operation *op) {
+      if (getCompilationInfo(op)) {
+        // Erase the compilation info configuration if it exists
+        eraseCompilationInfo(op);
+      }
+      if (getLoweringConfig(op)) {
+        // Erase the lowering configuration from root operation if it
+        // exists.
+        eraseLoweringConfig(op);
+      }
+    });
+
+    rewriter.replaceOp(funcOp, newFuncOp);
+    return success();
+  }
+};
+
+struct StripCompilationInfoPass final
+    : impl::StripCompilationInfoPassBase<StripCompilationInfoPass> {
+  void runOnOperation() override {
+    RewritePatternSet patterns(&getContext());
+    patterns.add<StripCompilationInfo>(&getContext());
+    if (failed(
+            applyPatternsAndFoldGreedily(getOperation(), std::move(patterns))))
```

**Comment:**
You don't need the greedy rewriter here, you can use `walkAndApplyPatterns` which should be much cheaper.

But I'm not sure we need it any case -- the previous approach with a manual `walk` seemed fine to me

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/strip_compilation_info.mlir:57`

```diff
@@ -0,0 +1,64 @@
+// RUN: iree-opt --split-input-file --iree-codegen-strip-compilation-info %s | FileCheck %s
+
+#translation_info = #iree_codegen.translation_info<LLVMGPUVectorDistribute workgroup_size = [64, 1, 1] subgroup_size = 64>
+func.func @main() attributes {translation_info = #translation_info} {
+    return
+}
+
+// CHECK-LABEL: func.func @main
+// CHECK-NOT:   #translation_info =
+// CHECK-NOT:   LLVMGPUVectorDistribute
+
+#pipeline_layout = #hal.pipeline.layout<bindings = [
+  #hal.pipeline.binding<storage_buffer>
+]>
+hal.executable private @strip_main {
+  hal.executable.variant public @strip_main target(#hal.executable.target<"", "", {}>) {
+    hal.executable.export public @entry_point layout(#pipeline_layout)
+    builtin.module {
+      func.func @fn1() attributes {translation_info = #iree_codegen.translation_info<None subgroup_size = 32>}  {
+        return
+      }
+      func.func @fn2() attributes {translation_info = #iree_codegen.translation_info<None subgroup_size = 32>} {
+        return
+      }
+    }
+  }
+}
+
+// CHECK-LABEL: hal.executable private @strip_main
+// CHECK: @fn1
+// CHECK-NOT:   #translation_info =
+// CHECK: @fn2
+// CHECK-NOT:   #translation_info =
+// CHECK: return
+
+#layout = #hal.pipeline.layout<bindings = [
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>
+]>
+#config = #iree_codegen.lowering_config<tile_sizes = [[128, 256], [16, 16]]>
+#translation = #iree_codegen.translation_info<None workgroup_size = [16, 8, 1] subgroup_size = 64>
+#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
+func.func @matmul_128x1024x256() {
+  %cst = arith.constant 0.000000e+00 : f32
+  %c128 = arith.constant 128 : index
+  %c1024 = arith.constant 1024 : index
+  %c0 = arith.constant 0 : index
+  %0 = hal.interface.binding.subspan layout(#layout) binding(0) : !flow.dispatch.tensor<readonly:tensor<128x256xf32>>
+  %1 = hal.interface.binding.subspan layout(#layout) binding(1) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
+  %2 = hal.interface.binding.subspan layout(#layout) binding(2) : !flow.dispatch.tensor<writeonly:tensor<128x1024xf32>>
+  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [128, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x256xf32>> -> tensor<128x256xf32>
+  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
+  %5 = tensor.empty() : tensor<128x1024xf32>
+  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<128x1024xf32>) -> tensor<128x1024xf32>
+  %7 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<128x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<128x1024xf32>) -> tensor<128x1024xf32>
+  flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [128, 1024], strides = [1, 1] : tensor<128x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<128x1024xf32>>
```

**Comment:**
We can further simplify this. We can have a function with just a couple of ops that accepts tensors and does matmul on the operands.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:36`

```diff
@@ -0,0 +1,73 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+// Checks whether the funcOp has any compilation info.
+bool hasCompilationInfo(func::FuncOp funcOp) {
+  if (getTranslationInfo(funcOp))
+    return true;
+
+  bool hasAttrConfig = false;
+  funcOp.walk([&](Operation *op) {
+    if (getCompilationInfo(op) || getLoweringConfig(op)) {
+      hasAttrConfig = true;
+      return;
+    }
+  });
+
+  // Return success if any relevant attributes were found.
+  return hasAttrConfig;
+}
+
+struct StripCompilationInfo : public OpRewritePattern<func::FuncOp> {
```

**Comment:**
```suggestion
struct StripCompilationInfo final : OpRewritePattern<func::FuncOp> {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:48`

```diff
@@ -0,0 +1,73 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+// Checks whether the funcOp has any compilation info.
+bool hasCompilationInfo(func::FuncOp funcOp) {
+  if (getTranslationInfo(funcOp))
+    return true;
+
+  bool hasAttrConfig = false;
+  funcOp.walk([&](Operation *op) {
+    if (getCompilationInfo(op) || getLoweringConfig(op)) {
+      hasAttrConfig = true;
+      return;
+    }
+  });
+
+  // Return success if any relevant attributes were found.
+  return hasAttrConfig;
+}
+
+struct StripCompilationInfo : public OpRewritePattern<func::FuncOp> {
+  using OpRewritePattern<func::FuncOp>::OpRewritePattern;
+  LogicalResult matchAndRewrite(func::FuncOp funcOp,
+                                PatternRewriter &rewriter) const final {
+    if (!hasCompilationInfo(funcOp))
+      return failure();
+
+    // if the function has translation info, erase it
+    if (getTranslationInfo(funcOp)) {
+      eraseTranslationInfo(funcOp);
+    }
+
+    funcOp->walk([&](Operation *op) {
```

**Comment:**
The `hasCompilationInfo` check seems redundant -- we may just as well remember if any modification were performed or not.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:43`

```diff
@@ -0,0 +1,73 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+// Checks whether the funcOp has any compilation info.
+bool hasCompilationInfo(func::FuncOp funcOp) {
+  if (getTranslationInfo(funcOp))
+    return true;
+
+  bool hasAttrConfig = false;
+  funcOp.walk([&](Operation *op) {
+    if (getCompilationInfo(op) || getLoweringConfig(op)) {
+      hasAttrConfig = true;
+      return;
+    }
+  });
+
+  // Return success if any relevant attributes were found.
+  return hasAttrConfig;
+}
+
+struct StripCompilationInfo : public OpRewritePattern<func::FuncOp> {
+  using OpRewritePattern<func::FuncOp>::OpRewritePattern;
+  LogicalResult matchAndRewrite(func::FuncOp funcOp,
+                                PatternRewriter &rewriter) const final {
+    if (!hasCompilationInfo(funcOp))
+      return failure();
+
+    // if the function has translation info, erase it
```

**Comment:**
Use proper casing and punctuation

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/strip_compilation_info.mlir:45`

```diff
@@ -0,0 +1,52 @@
+// RUN: iree-opt --split-input-file --iree-codegen-strip-compilation-info %s | FileCheck %s
+
+#translation_info = #iree_codegen.translation_info<LLVMGPUVectorDistribute workgroup_size = [64, 1, 1] subgroup_size = 64>
+func.func @main() attributes {translation_info = #translation_info} {
+    return
+}
+
+// CHECK-LABEL: func.func @main
+// CHECK-NOT:   #translation_info =
+// CHECK-NOT:   LLVMGPUVectorDistribute
+
+#pipeline_layout = #hal.pipeline.layout<bindings = [
+  #hal.pipeline.binding<storage_buffer>
+]>
+hal.executable private @strip_main {
+  hal.executable.variant public @strip_main target(#hal.executable.target<"", "", {}>) {
+    hal.executable.export public @entry_point layout(#pipeline_layout)
+    builtin.module {
+      func.func @fn1() attributes {translation_info = #iree_codegen.translation_info<None subgroup_size = 32>}  {
+        return
+      }
+      func.func @fn2() attributes {translation_info = #iree_codegen.translation_info<None subgroup_size = 32>} {
+        return
+      }
+    }
+  }
+}
+
+// CHECK-LABEL: hal.executable private @strip_main
+// CHECK: @fn1
+// CHECK-NOT:   #translation_info =
+// CHECK: @fn2
+// CHECK-NOT:   #translation_info =
+// CHECK: return
+
+#layout = #hal.pipeline.layout<bindings = [
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>,
+  #hal.pipeline.binding<storage_buffer>
+]>
+#config = #iree_codegen.lowering_config<tile_sizes = [[128, 256], [16, 16]]>
+#translation = #iree_codegen.translation_info<None workgroup_size = [16, 8, 1] subgroup_size = 64>
+#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
+func.func @matmul_128x1024x256(%lhs : tensor<128x256xf32>, %rhs: tensor<256x1024xf32>, %init: tensor<128x1024xf32>) -> tensor<128x1024xf32> {
+    %result =  linalg.matmul {compilation_info = #compilation} ins(%lhs, %rhs : tensor<128x256xf32>, tensor<256x1024xf32>) outs(%init : tensor<128x1024xf32>) -> tensor<128x1024xf32>
```

**Comment:**
We should also add one test with just lowering config attached to an op

---

**File:** `compiler/src/iree/compiler/Codegen/Common/test/strip_compilation_info.mlir:9`

```diff
@@ -0,0 +1,52 @@
+// RUN: iree-opt --split-input-file --iree-codegen-strip-compilation-info %s | FileCheck %s
+
+#translation_info = #iree_codegen.translation_info<LLVMGPUVectorDistribute workgroup_size = [64, 1, 1] subgroup_size = 64>
+func.func @main() attributes {translation_info = #translation_info} {
+    return
+}
+
+// CHECK-LABEL: func.func @main
+// CHECK-NOT:   #translation_info =
```

**Comment:**
Most of the checks in this file seem off: there's no `#` symbol on the attribute name

---

**File:** `compiler/src/iree/compiler/Codegen/Common/Passes.td:495`

```diff
@@ -490,6 +490,11 @@ def RemoveSingleIterationLoopPass :
   let summary = "Remove distributed loop with single iteration.";
 }
 
+def StripCompilationInfoPass :
+    Pass<"iree-codegen-strip-compilation-info", "">{
+   let summary = "Remove all the the lowering configuration and translation info.";
```

**Comment:**
```suggestion
   let summary = "Remove all the the lowering configuration and translation info attributes.";
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:36`

```diff
@@ -0,0 +1,73 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+// Checks whether the funcOp has any compilation info.
+bool hasCompilationInfo(func::FuncOp funcOp) {
+  if (getTranslationInfo(funcOp))
+    return true;
+
+  bool hasAttrConfig = false;
+  funcOp.walk([&](Operation *op) {
+    if (getCompilationInfo(op) || getLoweringConfig(op)) {
+      hasAttrConfig = true;
+      return;
+    }
+  });
+
+  // Return success if any relevant attributes were found.
+  return hasAttrConfig;
+}
+
+struct StripCompilationInfo : public OpRewritePattern<func::FuncOp> {
```

**Comment:**
Can we make this pattern be over the func op interface?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:53`

```diff
@@ -0,0 +1,73 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+// Checks whether the funcOp has any compilation info.
+bool hasCompilationInfo(func::FuncOp funcOp) {
+  if (getTranslationInfo(funcOp))
+    return true;
+
+  bool hasAttrConfig = false;
+  funcOp.walk([&](Operation *op) {
+    if (getCompilationInfo(op) || getLoweringConfig(op)) {
+      hasAttrConfig = true;
+      return;
+    }
+  });
+
+  // Return success if any relevant attributes were found.
+  return hasAttrConfig;
+}
+
+struct StripCompilationInfo : public OpRewritePattern<func::FuncOp> {
+  using OpRewritePattern<func::FuncOp>::OpRewritePattern;
+  LogicalResult matchAndRewrite(func::FuncOp funcOp,
+                                PatternRewriter &rewriter) const final {
+    if (!hasCompilationInfo(funcOp))
+      return failure();
+
+    // if the function has translation info, erase it
+    if (getTranslationInfo(funcOp)) {
+      eraseTranslationInfo(funcOp);
+    }
+
+    funcOp->walk([&](Operation *op) {
+      if (getCompilationInfo(op)) {
+        // Erase the compilation info configuration if it exists
+        eraseCompilationInfo(op);
+      }
+      if (getLoweringConfig(op)) {
+        // Erase the lowering configuration from root operation if it
+        // exists.
+        eraseLoweringConfig(op);
+      }
```

**Comment:**
Also, in rewrite patterns, it's invalid to mutate the IR without going through the rewriter: https://mlir.llvm.org/docs/PatternRewriter/#common-pattern-drivers. You'd need to use `modifyOpInPlace`.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:40`

```diff
@@ -0,0 +1,57 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+struct StripCompilationInfo final
+    : OpInterfaceRewritePattern<mlir::FunctionOpInterface> {
+  using OpInterfaceRewritePattern<
+      mlir::FunctionOpInterface>::OpInterfaceRewritePattern;
+  LogicalResult matchAndRewrite(mlir::FunctionOpInterface funcOp,
+                                PatternRewriter &rewriter) const final {
+    rewriter.modifyOpInPlace(funcOp, [&]() {
+      // If the function has translation info, erase it.
+      if (getTranslationInfo(funcOp)) {
+        eraseTranslationInfo(funcOp);
+      }
+
+      funcOp->walk([&](Operation *op) {
+        if (getCompilationInfo(op)) {
+          // Erase the compilation info configuration if it exists
+          eraseCompilationInfo(op);
+        }
+        if (getLoweringConfig(op)) {
+          // Erase the lowering configuration from root operation if it
+          // exists.
+          eraseLoweringConfig(op);
+        }
```

**Comment:**
I think the nested walk makes this pass quadratic. Maybe we should have a separate pattern for these?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:22`

```diff
@@ -0,0 +1,57 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+struct StripCompilationInfo final
+    : OpInterfaceRewritePattern<mlir::FunctionOpInterface> {
+  using OpInterfaceRewritePattern<
+      mlir::FunctionOpInterface>::OpInterfaceRewritePattern;
```

**Comment:**
```suggestion
  using OpInterfaceRewritePattern::OpInterfaceRewritePattern;
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/StripCompilationInfoPass.cpp:33`

```diff
@@ -0,0 +1,57 @@
+// Copyright 2024 The IREE Authors
+//
+// Licensed under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+
+#include "iree/compiler/Codegen/Common/Passes.h"
+#include "mlir/Transforms/WalkPatternRewriteDriver.h"
+
+#define DEBUG_TYPE "iree-codegen-strip-compilation-info"
+
+namespace mlir::iree_compiler {
+
+#define GEN_PASS_DEF_STRIPCOMPILATIONINFOPASS
+#include "iree/compiler/Codegen/Common/Passes.h.inc"
+
+namespace {
+
+struct StripCompilationInfo final
+    : OpInterfaceRewritePattern<mlir::FunctionOpInterface> {
+  using OpInterfaceRewritePattern<
+      mlir::FunctionOpInterface>::OpInterfaceRewritePattern;
+  LogicalResult matchAndRewrite(mlir::FunctionOpInterface funcOp,
+                                PatternRewriter &rewriter) const final {
+    rewriter.modifyOpInPlace(funcOp, [&]() {
+      // If the function has translation info, erase it.
+      if (getTranslationInfo(funcOp)) {
+        eraseTranslationInfo(funcOp);
+      }
+
+      funcOp->walk([&](Operation *op) {
+        if (getCompilationInfo(op)) {
+          // Erase the compilation info configuration if it exists
```

**Comment:**
```suggestion
          // Erase the compilation info configuration if it exists.
```

---


---


## [PR #18952](https://github.com/iree-org/iree/pull/18952): [VectorDistribution] Add distribution pattern for vector::ContractionOp

### Review Summary

**COMMENTED** (2024-10-30)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:573`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
```

**Comment:**
Butterfly shuffle is an implementation detail that's not observable from this pattern. I'd just say we perform subgroup reduction.

Also nit: the formatting is weird.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:571`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
```

**Comment:**
I'd call it something like: subgroup reduction or inter-thread reduction.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:576`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
```

**Comment:**
let's try to stick to the portable naming like in the gpu dialect

```suggestion
/// Currently, reduction across multiple subgroups is not supported.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:578`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
```

**Comment:**
Can you describe why we'd need it?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:598`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
+struct DistributeContract final : OpDistributionPattern<vector::ContractionOp> {
+  using OpDistributionPattern::OpDistributionPattern;
+
+  DistributeContract(MLIRContext *context, int64_t subgroupSize,
+                     int64_t maxBitsPerShuffle, int64_t benefit = 1)
+      : OpDistributionPattern(context, benefit), subgroupSize(subgroupSize),
+        maxBitsPerShuffle(maxBitsPerShuffle) {}
+
+  LogicalResult matchAndRewrite(vector::ContractionOp contractOp,
+                                DistributionSignature &signature,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<VectorContractOpInfo> maybeOpInfo =
+        VectorContractOpInfo::inferFromIndexingMaps(
+            contractOp.getIndexingMapsArray());
+    if (failed(maybeOpInfo)) {
+      return rewriter.notifyMatchFailure(contractOp, "not a contraction");
+    }
+    // If mmaAttr exists, defer the lowering to use MMA.
+    // Notify failure if the "iree.amdgpu.mma" intrinsic attribute is present.
+    IREE::GPU::MMAAttr mmaAttr =
```

**Comment:**
```suggestion
    auto mmaAttr =
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:605`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
+struct DistributeContract final : OpDistributionPattern<vector::ContractionOp> {
+  using OpDistributionPattern::OpDistributionPattern;
+
+  DistributeContract(MLIRContext *context, int64_t subgroupSize,
+                     int64_t maxBitsPerShuffle, int64_t benefit = 1)
+      : OpDistributionPattern(context, benefit), subgroupSize(subgroupSize),
+        maxBitsPerShuffle(maxBitsPerShuffle) {}
+
+  LogicalResult matchAndRewrite(vector::ContractionOp contractOp,
+                                DistributionSignature &signature,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<VectorContractOpInfo> maybeOpInfo =
+        VectorContractOpInfo::inferFromIndexingMaps(
+            contractOp.getIndexingMapsArray());
+    if (failed(maybeOpInfo)) {
+      return rewriter.notifyMatchFailure(contractOp, "not a contraction");
+    }
+    // If mmaAttr exists, defer the lowering to use MMA.
+    // Notify failure if the "iree.amdgpu.mma" intrinsic attribute is present.
+    IREE::GPU::MMAAttr mmaAttr =
+        contractOp->getAttrOfType<IREE::GPU::MMAAttr>("iree.amdgpu.mma");
+    if (mmaAttr) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "iree.amdgpu.mma intrinsic attribute exists");
+    }
+
+    NestedLayoutAttr lhsLayout =
```

**Comment:**
```suggestion
    auto lhsLayout =
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:611`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
+struct DistributeContract final : OpDistributionPattern<vector::ContractionOp> {
+  using OpDistributionPattern::OpDistributionPattern;
+
+  DistributeContract(MLIRContext *context, int64_t subgroupSize,
+                     int64_t maxBitsPerShuffle, int64_t benefit = 1)
+      : OpDistributionPattern(context, benefit), subgroupSize(subgroupSize),
+        maxBitsPerShuffle(maxBitsPerShuffle) {}
+
+  LogicalResult matchAndRewrite(vector::ContractionOp contractOp,
+                                DistributionSignature &signature,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<VectorContractOpInfo> maybeOpInfo =
+        VectorContractOpInfo::inferFromIndexingMaps(
+            contractOp.getIndexingMapsArray());
+    if (failed(maybeOpInfo)) {
+      return rewriter.notifyMatchFailure(contractOp, "not a contraction");
+    }
+    // If mmaAttr exists, defer the lowering to use MMA.
+    // Notify failure if the "iree.amdgpu.mma" intrinsic attribute is present.
+    IREE::GPU::MMAAttr mmaAttr =
+        contractOp->getAttrOfType<IREE::GPU::MMAAttr>("iree.amdgpu.mma");
+    if (mmaAttr) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "iree.amdgpu.mma intrinsic attribute exists");
+    }
+
+    NestedLayoutAttr lhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getLhs()]);
+    if (!lhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction lhs");
+    }
+    NestedLayoutAttr rhsLayout =
```

**Comment:**
```suggestion
    auto rhsLayout =
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:624`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
+struct DistributeContract final : OpDistributionPattern<vector::ContractionOp> {
+  using OpDistributionPattern::OpDistributionPattern;
+
+  DistributeContract(MLIRContext *context, int64_t subgroupSize,
+                     int64_t maxBitsPerShuffle, int64_t benefit = 1)
+      : OpDistributionPattern(context, benefit), subgroupSize(subgroupSize),
+        maxBitsPerShuffle(maxBitsPerShuffle) {}
+
+  LogicalResult matchAndRewrite(vector::ContractionOp contractOp,
+                                DistributionSignature &signature,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<VectorContractOpInfo> maybeOpInfo =
+        VectorContractOpInfo::inferFromIndexingMaps(
+            contractOp.getIndexingMapsArray());
+    if (failed(maybeOpInfo)) {
+      return rewriter.notifyMatchFailure(contractOp, "not a contraction");
+    }
+    // If mmaAttr exists, defer the lowering to use MMA.
+    // Notify failure if the "iree.amdgpu.mma" intrinsic attribute is present.
+    IREE::GPU::MMAAttr mmaAttr =
+        contractOp->getAttrOfType<IREE::GPU::MMAAttr>("iree.amdgpu.mma");
+    if (mmaAttr) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "iree.amdgpu.mma intrinsic attribute exists");
+    }
+
+    NestedLayoutAttr lhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getLhs()]);
+    if (!lhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction lhs");
+    }
+    NestedLayoutAttr rhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getRhs()]);
+    if (!rhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction rhs");
+    }
+
+    Value disLhs = getDistributed(rewriter, contractOp.getLhs(), lhsLayout);
+    Value disRhs = getDistributed(rewriter, contractOp.getRhs(), rhsLayout);
+
+    Value acc = contractOp.getAcc();
+    Value res = contractOp.getResult();
+    VectorValue accVector = dyn_cast<VectorValue>(acc);
+    VectorValue resVector = dyn_cast<VectorValue>(res);
```

**Comment:**
```suggestion
    auto accVector = dyn_cast<VectorValue>(acc);
    auto resVector = dyn_cast<VectorValue>(res);
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:636`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
+struct DistributeContract final : OpDistributionPattern<vector::ContractionOp> {
+  using OpDistributionPattern::OpDistributionPattern;
+
+  DistributeContract(MLIRContext *context, int64_t subgroupSize,
+                     int64_t maxBitsPerShuffle, int64_t benefit = 1)
+      : OpDistributionPattern(context, benefit), subgroupSize(subgroupSize),
+        maxBitsPerShuffle(maxBitsPerShuffle) {}
+
+  LogicalResult matchAndRewrite(vector::ContractionOp contractOp,
+                                DistributionSignature &signature,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<VectorContractOpInfo> maybeOpInfo =
+        VectorContractOpInfo::inferFromIndexingMaps(
+            contractOp.getIndexingMapsArray());
+    if (failed(maybeOpInfo)) {
+      return rewriter.notifyMatchFailure(contractOp, "not a contraction");
+    }
+    // If mmaAttr exists, defer the lowering to use MMA.
+    // Notify failure if the "iree.amdgpu.mma" intrinsic attribute is present.
+    IREE::GPU::MMAAttr mmaAttr =
+        contractOp->getAttrOfType<IREE::GPU::MMAAttr>("iree.amdgpu.mma");
+    if (mmaAttr) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "iree.amdgpu.mma intrinsic attribute exists");
+    }
+
+    NestedLayoutAttr lhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getLhs()]);
+    if (!lhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction lhs");
+    }
+    NestedLayoutAttr rhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getRhs()]);
+    if (!rhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction rhs");
+    }
+
+    Value disLhs = getDistributed(rewriter, contractOp.getLhs(), lhsLayout);
+    Value disRhs = getDistributed(rewriter, contractOp.getRhs(), rhsLayout);
+
+    Value acc = contractOp.getAcc();
+    Value res = contractOp.getResult();
+    VectorValue accVector = dyn_cast<VectorValue>(acc);
+    VectorValue resVector = dyn_cast<VectorValue>(res);
+    Value disAcc;
+    if (accVector) {
+      disAcc = getDistributed(rewriter, accVector, signature[accVector]);
+    } else {
+      disAcc = contractOp.getAcc();
+    }
+
+    Location loc = contractOp.getLoc();
+    int64_t rank = lhsLayout.getRank();
+
+    SmallVector<bool> reducedDims(rank, false);
+    auto maps = contractOp.getIndexingMapsArray();
```

**Comment:**
Use the actual type since it's not obvious based on the RHS only

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:639`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
+struct DistributeContract final : OpDistributionPattern<vector::ContractionOp> {
+  using OpDistributionPattern::OpDistributionPattern;
+
+  DistributeContract(MLIRContext *context, int64_t subgroupSize,
+                     int64_t maxBitsPerShuffle, int64_t benefit = 1)
+      : OpDistributionPattern(context, benefit), subgroupSize(subgroupSize),
+        maxBitsPerShuffle(maxBitsPerShuffle) {}
+
+  LogicalResult matchAndRewrite(vector::ContractionOp contractOp,
+                                DistributionSignature &signature,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<VectorContractOpInfo> maybeOpInfo =
+        VectorContractOpInfo::inferFromIndexingMaps(
+            contractOp.getIndexingMapsArray());
+    if (failed(maybeOpInfo)) {
+      return rewriter.notifyMatchFailure(contractOp, "not a contraction");
+    }
+    // If mmaAttr exists, defer the lowering to use MMA.
+    // Notify failure if the "iree.amdgpu.mma" intrinsic attribute is present.
+    IREE::GPU::MMAAttr mmaAttr =
+        contractOp->getAttrOfType<IREE::GPU::MMAAttr>("iree.amdgpu.mma");
+    if (mmaAttr) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "iree.amdgpu.mma intrinsic attribute exists");
+    }
+
+    NestedLayoutAttr lhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getLhs()]);
+    if (!lhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction lhs");
+    }
+    NestedLayoutAttr rhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getRhs()]);
+    if (!rhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction rhs");
+    }
+
+    Value disLhs = getDistributed(rewriter, contractOp.getLhs(), lhsLayout);
+    Value disRhs = getDistributed(rewriter, contractOp.getRhs(), rhsLayout);
+
+    Value acc = contractOp.getAcc();
+    Value res = contractOp.getResult();
+    VectorValue accVector = dyn_cast<VectorValue>(acc);
+    VectorValue resVector = dyn_cast<VectorValue>(res);
+    Value disAcc;
+    if (accVector) {
+      disAcc = getDistributed(rewriter, accVector, signature[accVector]);
+    } else {
+      disAcc = contractOp.getAcc();
+    }
+
+    Location loc = contractOp.getLoc();
+    int64_t rank = lhsLayout.getRank();
+
+    SmallVector<bool> reducedDims(rank, false);
+    auto maps = contractOp.getIndexingMapsArray();
+
+    // Identify the reduction dimension and apply it for cross-thread reduction.
+    MLIRContext *ctx = maps[0].getContext();
```

**Comment:**
I'd put it just before loc and extract from `contractOp` instead.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:657`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
+struct DistributeContract final : OpDistributionPattern<vector::ContractionOp> {
+  using OpDistributionPattern::OpDistributionPattern;
+
+  DistributeContract(MLIRContext *context, int64_t subgroupSize,
+                     int64_t maxBitsPerShuffle, int64_t benefit = 1)
+      : OpDistributionPattern(context, benefit), subgroupSize(subgroupSize),
+        maxBitsPerShuffle(maxBitsPerShuffle) {}
+
+  LogicalResult matchAndRewrite(vector::ContractionOp contractOp,
+                                DistributionSignature &signature,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<VectorContractOpInfo> maybeOpInfo =
+        VectorContractOpInfo::inferFromIndexingMaps(
+            contractOp.getIndexingMapsArray());
+    if (failed(maybeOpInfo)) {
+      return rewriter.notifyMatchFailure(contractOp, "not a contraction");
+    }
+    // If mmaAttr exists, defer the lowering to use MMA.
+    // Notify failure if the "iree.amdgpu.mma" intrinsic attribute is present.
+    IREE::GPU::MMAAttr mmaAttr =
+        contractOp->getAttrOfType<IREE::GPU::MMAAttr>("iree.amdgpu.mma");
+    if (mmaAttr) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "iree.amdgpu.mma intrinsic attribute exists");
+    }
+
+    NestedLayoutAttr lhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getLhs()]);
+    if (!lhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction lhs");
+    }
+    NestedLayoutAttr rhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getRhs()]);
+    if (!rhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction rhs");
+    }
+
+    Value disLhs = getDistributed(rewriter, contractOp.getLhs(), lhsLayout);
+    Value disRhs = getDistributed(rewriter, contractOp.getRhs(), rhsLayout);
+
+    Value acc = contractOp.getAcc();
+    Value res = contractOp.getResult();
+    VectorValue accVector = dyn_cast<VectorValue>(acc);
+    VectorValue resVector = dyn_cast<VectorValue>(res);
+    Value disAcc;
+    if (accVector) {
+      disAcc = getDistributed(rewriter, accVector, signature[accVector]);
+    } else {
+      disAcc = contractOp.getAcc();
+    }
+
+    Location loc = contractOp.getLoc();
+    int64_t rank = lhsLayout.getRank();
+
+    SmallVector<bool> reducedDims(rank, false);
+    auto maps = contractOp.getIndexingMapsArray();
+
+    // Identify the reduction dimension and apply it for cross-thread reduction.
+    MLIRContext *ctx = maps[0].getContext();
+    for (auto [index, iteratorType] :
+         llvm::enumerate(contractOp.getIteratorTypes())) {
+      if (vector::isReductionIterator(iteratorType)) {
+        int64_t redIdx =
+            *maps[0].getResultPosition(getAffineDimExpr(index, ctx));
+        reducedDims[redIdx] = true;
+      }
+    }
+
+    ArrayRef<Attribute> iteratorTypes =
+        contractOp.getIteratorTypes().getValue();
+    SmallVector<Attribute> newIterators;
+
+    // Given that the distribution format is <BATCH x OUTER x ELEMENT>,
+    // the iterations and affine maps need to be replicated three times.
+
+    // Replicate the iterators for local vector.contract
+    for (int i = 0; i < 3; i++) {
```

**Comment:**
https://llvm.org/docs/CodingStandards.html#prefer-preincrement

```suggestion
    for (int i = 0; i < 3; ++i) {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:663`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
+struct DistributeContract final : OpDistributionPattern<vector::ContractionOp> {
+  using OpDistributionPattern::OpDistributionPattern;
+
+  DistributeContract(MLIRContext *context, int64_t subgroupSize,
+                     int64_t maxBitsPerShuffle, int64_t benefit = 1)
+      : OpDistributionPattern(context, benefit), subgroupSize(subgroupSize),
+        maxBitsPerShuffle(maxBitsPerShuffle) {}
+
+  LogicalResult matchAndRewrite(vector::ContractionOp contractOp,
+                                DistributionSignature &signature,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<VectorContractOpInfo> maybeOpInfo =
+        VectorContractOpInfo::inferFromIndexingMaps(
+            contractOp.getIndexingMapsArray());
+    if (failed(maybeOpInfo)) {
+      return rewriter.notifyMatchFailure(contractOp, "not a contraction");
+    }
+    // If mmaAttr exists, defer the lowering to use MMA.
+    // Notify failure if the "iree.amdgpu.mma" intrinsic attribute is present.
+    IREE::GPU::MMAAttr mmaAttr =
+        contractOp->getAttrOfType<IREE::GPU::MMAAttr>("iree.amdgpu.mma");
+    if (mmaAttr) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "iree.amdgpu.mma intrinsic attribute exists");
+    }
+
+    NestedLayoutAttr lhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getLhs()]);
+    if (!lhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction lhs");
+    }
+    NestedLayoutAttr rhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getRhs()]);
+    if (!rhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction rhs");
+    }
+
+    Value disLhs = getDistributed(rewriter, contractOp.getLhs(), lhsLayout);
+    Value disRhs = getDistributed(rewriter, contractOp.getRhs(), rhsLayout);
+
+    Value acc = contractOp.getAcc();
+    Value res = contractOp.getResult();
+    VectorValue accVector = dyn_cast<VectorValue>(acc);
+    VectorValue resVector = dyn_cast<VectorValue>(res);
+    Value disAcc;
+    if (accVector) {
+      disAcc = getDistributed(rewriter, accVector, signature[accVector]);
+    } else {
+      disAcc = contractOp.getAcc();
+    }
+
+    Location loc = contractOp.getLoc();
+    int64_t rank = lhsLayout.getRank();
+
+    SmallVector<bool> reducedDims(rank, false);
+    auto maps = contractOp.getIndexingMapsArray();
+
+    // Identify the reduction dimension and apply it for cross-thread reduction.
+    MLIRContext *ctx = maps[0].getContext();
+    for (auto [index, iteratorType] :
+         llvm::enumerate(contractOp.getIteratorTypes())) {
+      if (vector::isReductionIterator(iteratorType)) {
+        int64_t redIdx =
+            *maps[0].getResultPosition(getAffineDimExpr(index, ctx));
+        reducedDims[redIdx] = true;
+      }
+    }
+
+    ArrayRef<Attribute> iteratorTypes =
+        contractOp.getIteratorTypes().getValue();
+    SmallVector<Attribute> newIterators;
+
+    // Given that the distribution format is <BATCH x OUTER x ELEMENT>,
+    // the iterations and affine maps need to be replicated three times.
+
+    // Replicate the iterators for local vector.contract
+    for (int i = 0; i < 3; i++) {
+      newIterators.append(iteratorTypes.begin(), iteratorTypes.end());
+    }
+
+    // Replicate the affine maps for local vector.contract
+    SmallVector<AffineMap> newMaps;
+    for (auto map : maps) {
```

**Comment:**
Use the actual type since it's not obvious based on the RHS only

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:667`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
+struct DistributeContract final : OpDistributionPattern<vector::ContractionOp> {
+  using OpDistributionPattern::OpDistributionPattern;
+
+  DistributeContract(MLIRContext *context, int64_t subgroupSize,
+                     int64_t maxBitsPerShuffle, int64_t benefit = 1)
+      : OpDistributionPattern(context, benefit), subgroupSize(subgroupSize),
+        maxBitsPerShuffle(maxBitsPerShuffle) {}
+
+  LogicalResult matchAndRewrite(vector::ContractionOp contractOp,
+                                DistributionSignature &signature,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<VectorContractOpInfo> maybeOpInfo =
+        VectorContractOpInfo::inferFromIndexingMaps(
+            contractOp.getIndexingMapsArray());
+    if (failed(maybeOpInfo)) {
+      return rewriter.notifyMatchFailure(contractOp, "not a contraction");
+    }
+    // If mmaAttr exists, defer the lowering to use MMA.
+    // Notify failure if the "iree.amdgpu.mma" intrinsic attribute is present.
+    IREE::GPU::MMAAttr mmaAttr =
+        contractOp->getAttrOfType<IREE::GPU::MMAAttr>("iree.amdgpu.mma");
+    if (mmaAttr) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "iree.amdgpu.mma intrinsic attribute exists");
+    }
+
+    NestedLayoutAttr lhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getLhs()]);
+    if (!lhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction lhs");
+    }
+    NestedLayoutAttr rhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getRhs()]);
+    if (!rhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction rhs");
+    }
+
+    Value disLhs = getDistributed(rewriter, contractOp.getLhs(), lhsLayout);
+    Value disRhs = getDistributed(rewriter, contractOp.getRhs(), rhsLayout);
+
+    Value acc = contractOp.getAcc();
+    Value res = contractOp.getResult();
+    VectorValue accVector = dyn_cast<VectorValue>(acc);
+    VectorValue resVector = dyn_cast<VectorValue>(res);
+    Value disAcc;
+    if (accVector) {
+      disAcc = getDistributed(rewriter, accVector, signature[accVector]);
+    } else {
+      disAcc = contractOp.getAcc();
+    }
+
+    Location loc = contractOp.getLoc();
+    int64_t rank = lhsLayout.getRank();
+
+    SmallVector<bool> reducedDims(rank, false);
+    auto maps = contractOp.getIndexingMapsArray();
+
+    // Identify the reduction dimension and apply it for cross-thread reduction.
+    MLIRContext *ctx = maps[0].getContext();
+    for (auto [index, iteratorType] :
+         llvm::enumerate(contractOp.getIteratorTypes())) {
+      if (vector::isReductionIterator(iteratorType)) {
+        int64_t redIdx =
+            *maps[0].getResultPosition(getAffineDimExpr(index, ctx));
+        reducedDims[redIdx] = true;
+      }
+    }
+
+    ArrayRef<Attribute> iteratorTypes =
+        contractOp.getIteratorTypes().getValue();
+    SmallVector<Attribute> newIterators;
+
+    // Given that the distribution format is <BATCH x OUTER x ELEMENT>,
+    // the iterations and affine maps need to be replicated three times.
+
+    // Replicate the iterators for local vector.contract
+    for (int i = 0; i < 3; i++) {
+      newIterators.append(iteratorTypes.begin(), iteratorTypes.end());
+    }
+
+    // Replicate the affine maps for local vector.contract
+    SmallVector<AffineMap> newMaps;
+    for (auto map : maps) {
+      int64_t numDims = map.getNumDims();
+      int64_t numResults = map.getNumResults();
+      SmallVector<AffineExpr> exprs;
+      for (int i = 0; i < 3; i++) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:669`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
+struct DistributeContract final : OpDistributionPattern<vector::ContractionOp> {
+  using OpDistributionPattern::OpDistributionPattern;
+
+  DistributeContract(MLIRContext *context, int64_t subgroupSize,
+                     int64_t maxBitsPerShuffle, int64_t benefit = 1)
+      : OpDistributionPattern(context, benefit), subgroupSize(subgroupSize),
+        maxBitsPerShuffle(maxBitsPerShuffle) {}
+
+  LogicalResult matchAndRewrite(vector::ContractionOp contractOp,
+                                DistributionSignature &signature,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<VectorContractOpInfo> maybeOpInfo =
+        VectorContractOpInfo::inferFromIndexingMaps(
+            contractOp.getIndexingMapsArray());
+    if (failed(maybeOpInfo)) {
+      return rewriter.notifyMatchFailure(contractOp, "not a contraction");
+    }
+    // If mmaAttr exists, defer the lowering to use MMA.
+    // Notify failure if the "iree.amdgpu.mma" intrinsic attribute is present.
+    IREE::GPU::MMAAttr mmaAttr =
+        contractOp->getAttrOfType<IREE::GPU::MMAAttr>("iree.amdgpu.mma");
+    if (mmaAttr) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "iree.amdgpu.mma intrinsic attribute exists");
+    }
+
+    NestedLayoutAttr lhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getLhs()]);
+    if (!lhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction lhs");
+    }
+    NestedLayoutAttr rhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getRhs()]);
+    if (!rhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction rhs");
+    }
+
+    Value disLhs = getDistributed(rewriter, contractOp.getLhs(), lhsLayout);
+    Value disRhs = getDistributed(rewriter, contractOp.getRhs(), rhsLayout);
+
+    Value acc = contractOp.getAcc();
+    Value res = contractOp.getResult();
+    VectorValue accVector = dyn_cast<VectorValue>(acc);
+    VectorValue resVector = dyn_cast<VectorValue>(res);
+    Value disAcc;
+    if (accVector) {
+      disAcc = getDistributed(rewriter, accVector, signature[accVector]);
+    } else {
+      disAcc = contractOp.getAcc();
+    }
+
+    Location loc = contractOp.getLoc();
+    int64_t rank = lhsLayout.getRank();
+
+    SmallVector<bool> reducedDims(rank, false);
+    auto maps = contractOp.getIndexingMapsArray();
+
+    // Identify the reduction dimension and apply it for cross-thread reduction.
+    MLIRContext *ctx = maps[0].getContext();
+    for (auto [index, iteratorType] :
+         llvm::enumerate(contractOp.getIteratorTypes())) {
+      if (vector::isReductionIterator(iteratorType)) {
+        int64_t redIdx =
+            *maps[0].getResultPosition(getAffineDimExpr(index, ctx));
+        reducedDims[redIdx] = true;
+      }
+    }
+
+    ArrayRef<Attribute> iteratorTypes =
+        contractOp.getIteratorTypes().getValue();
+    SmallVector<Attribute> newIterators;
+
+    // Given that the distribution format is <BATCH x OUTER x ELEMENT>,
+    // the iterations and affine maps need to be replicated three times.
+
+    // Replicate the iterators for local vector.contract
+    for (int i = 0; i < 3; i++) {
+      newIterators.append(iteratorTypes.begin(), iteratorTypes.end());
+    }
+
+    // Replicate the affine maps for local vector.contract
+    SmallVector<AffineMap> newMaps;
+    for (auto map : maps) {
+      int64_t numDims = map.getNumDims();
+      int64_t numResults = map.getNumResults();
+      SmallVector<AffineExpr> exprs;
+      for (int i = 0; i < 3; i++) {
+        AffineMap shiftedMap = map.shiftDims(i * numDims);
+        for (int j = 0; j < numResults; j++) {
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:701`

```diff
@@ -563,6 +563,223 @@ struct DistributeMultiReduction final
   int64_t maxBitsPerShuffle;
 };
 
+/// The lowering for Contract is performed in three steps (similar to above
+/// multi_reduction):
+///   1. Local Contract: Each thread performs operations on its locally
+///   distributed elements.
+///   2. Thread Reduction: Threads collectively reduce the results from step 1
+///   across threads,
+///      using a butterfly shuffle if distribution occurs along the reduction
+///      dimension.
+///   3. Accumulator Reduction: Each thread combines its intermediate results
+///   with its held accumulator.
+///
+/// Currently, reduction across multiple warps is not supported.
+/// TODO: Add support for reductions across multiple warps.
+struct DistributeContract final : OpDistributionPattern<vector::ContractionOp> {
+  using OpDistributionPattern::OpDistributionPattern;
+
+  DistributeContract(MLIRContext *context, int64_t subgroupSize,
+                     int64_t maxBitsPerShuffle, int64_t benefit = 1)
+      : OpDistributionPattern(context, benefit), subgroupSize(subgroupSize),
+        maxBitsPerShuffle(maxBitsPerShuffle) {}
+
+  LogicalResult matchAndRewrite(vector::ContractionOp contractOp,
+                                DistributionSignature &signature,
+                                PatternRewriter &rewriter) const override {
+    FailureOr<VectorContractOpInfo> maybeOpInfo =
+        VectorContractOpInfo::inferFromIndexingMaps(
+            contractOp.getIndexingMapsArray());
+    if (failed(maybeOpInfo)) {
+      return rewriter.notifyMatchFailure(contractOp, "not a contraction");
+    }
+    // If mmaAttr exists, defer the lowering to use MMA.
+    // Notify failure if the "iree.amdgpu.mma" intrinsic attribute is present.
+    IREE::GPU::MMAAttr mmaAttr =
+        contractOp->getAttrOfType<IREE::GPU::MMAAttr>("iree.amdgpu.mma");
+    if (mmaAttr) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "iree.amdgpu.mma intrinsic attribute exists");
+    }
+
+    NestedLayoutAttr lhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getLhs()]);
+    if (!lhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction lhs");
+    }
+    NestedLayoutAttr rhsLayout =
+        dyn_cast<NestedLayoutAttr>(signature[contractOp.getRhs()]);
+    if (!rhsLayout) {
+      return rewriter.notifyMatchFailure(
+          contractOp, "missing nested layout for contraction rhs");
+    }
+
+    Value disLhs = getDistributed(rewriter, contractOp.getLhs(), lhsLayout);
+    Value disRhs = getDistributed(rewriter, contractOp.getRhs(), rhsLayout);
+
+    Value acc = contractOp.getAcc();
+    Value res = contractOp.getResult();
+    VectorValue accVector = dyn_cast<VectorValue>(acc);
+    VectorValue resVector = dyn_cast<VectorValue>(res);
+    Value disAcc;
+    if (accVector) {
+      disAcc = getDistributed(rewriter, accVector, signature[accVector]);
+    } else {
+      disAcc = contractOp.getAcc();
+    }
+
+    Location loc = contractOp.getLoc();
+    int64_t rank = lhsLayout.getRank();
+
+    SmallVector<bool> reducedDims(rank, false);
+    auto maps = contractOp.getIndexingMapsArray();
+
+    // Identify the reduction dimension and apply it for cross-thread reduction.
+    MLIRContext *ctx = maps[0].getContext();
+    for (auto [index, iteratorType] :
+         llvm::enumerate(contractOp.getIteratorTypes())) {
+      if (vector::isReductionIterator(iteratorType)) {
+        int64_t redIdx =
+            *maps[0].getResultPosition(getAffineDimExpr(index, ctx));
+        reducedDims[redIdx] = true;
+      }
+    }
+
+    ArrayRef<Attribute> iteratorTypes =
+        contractOp.getIteratorTypes().getValue();
+    SmallVector<Attribute> newIterators;
+
+    // Given that the distribution format is <BATCH x OUTER x ELEMENT>,
+    // the iterations and affine maps need to be replicated three times.
+
+    // Replicate the iterators for local vector.contract
+    for (int i = 0; i < 3; i++) {
+      newIterators.append(iteratorTypes.begin(), iteratorTypes.end());
+    }
+
+    // Replicate the affine maps for local vector.contract
+    SmallVector<AffineMap> newMaps;
+    for (auto map : maps) {
+      int64_t numDims = map.getNumDims();
+      int64_t numResults = map.getNumResults();
+      SmallVector<AffineExpr> exprs;
+      for (int i = 0; i < 3; i++) {
+        AffineMap shiftedMap = map.shiftDims(i * numDims);
+        for (int j = 0; j < numResults; j++) {
+          exprs.push_back(shiftedMap.getResult(j));
+        }
+      }
+      AffineMap newMap =
+          AffineMap::get(/*dimCount=*/3 * numDims,
+                         /*symbolCount=*/map.getNumSymbols(), exprs, ctx);
+      newMaps.push_back(newMap);
+    }
+
+    Type accElemTy = getElementTypeOrSelf(acc.getType());
+    Value localInit = getCombiningIdentityValue(
+        loc, rewriter, contractOp.getKind(), disAcc.getType());
+
+    auto localContractOp = rewriter.create<vector::ContractionOp>(
+        loc, disLhs, disRhs, localInit, rewriter.getAffineMapArrayAttr(newMaps),
+        rewriter.getArrayAttr(newIterators), contractOp.getKind());
+    localContractOp->setDiscardableAttrs(
+        contractOp->getDiscardableAttrDictionary());
+
+    VectorValue localContractValue;
+    if (accVector) {
+      localContractValue = dyn_cast<VectorValue>(localContractOp.getResult());
+    } else {
+      VectorType vecType = VectorType::get(ArrayRef{int64_t(1)}, accElemTy);
+      localContractValue = rewriter.create<vector::BroadcastOp>(
+          loc, vecType, localContractOp.getResult());
+    }
+
+    assert(localContractValue && "result should have been a vector");
+
+    // Flatten the locally result value.
+    VectorType shaped = localContractValue.getType();
+    int64_t numElements = shaped.getNumElements();
+    SmallVector<int64_t> flatShape(1, numElements);
+    VectorType flatVecType = VectorType::get(flatShape, accElemTy);
+    VectorValue flat = rewriter.create<vector::ShapeCastOp>(loc, flatVecType,
+                                                            localContractValue);
+
+    // Do inter-thread/warp reduce.
+    FailureOr<VectorValue> threadReduced = doThreadReduction(
+        rewriter, lhsLayout, flat, contractOp.getKind(), reducedDims);
+    if (failed(threadReduced)) {
+      return failure();
+    }
+
+    // Do reduction against accumulator, which needs to be done after thread
+    // reduction.
+    VectorValue unflattened = rewriter.create<vector::ShapeCastOp>(
+        loc, shaped, threadReduced.value());
+
+    if (!accVector) {
+      disAcc = rewriter.create<vector::BroadcastOp>(loc, shaped, disAcc);
+    }
+
+    Value accReduction = vector::makeArithReduction(
+        rewriter, loc, contractOp.getKind(), unflattened, disAcc);
+    auto accReduced = dyn_cast<VectorValue>(accReduction);
+    if (!accReduced) {
+      return failure();
+    }
+
+    if (resVector) {
+      replaceOpWithDistributedValues(rewriter, contractOp, accReduced);
+    } else {
+      Value accReducedVal = rewriter.create<vector::ExtractOp>(
+          loc, accReduction, SmallVector<int64_t>{0});
+      replaceOpWithDistributedValues(rewriter, contractOp, accReducedVal);
+    }
+
+    return success();
```

**Comment:**
Would it be possible to split this function into a couple smaller helpers matching the 3-stage lowering described in the comment? This implementation is a bit long and hard to follow.

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPUConfigureTensorLayouts.cpp:344`

```diff
@@ -341,8 +341,10 @@ struct LLVMGPUConfigureTensorLayoutsPass final
     llvm::StringLiteral scheduleAttrName =
         IREE::GPU::MMAScheduleAttr::getMnemonic();
     DictionaryAttr configDict = getTranslationInfo(func).getConfiguration();
-    auto scheduleAttr = dyn_cast_or_null<IREE::GPU::MMAScheduleAttr>(
-        configDict.get(scheduleAttrName));
+    IREE::GPU::MMAScheduleAttr scheduleAttr = nullptr;
```

**Comment:**
```suggestion
    IREE::GPU::MMAScheduleAttr scheduleAttr;
```

---


---


## [PR #18825](https://github.com/iree-org/iree/pull/18825): [VectorDistribution] Add kernel configs for root reduction operations (4/4)

### Review Summary

**COMMENTED** (2024-10-18)

Looks OK


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/KernelConfig.cpp:932`

```diff
@@ -814,6 +820,156 @@ setAttentionVectorDistributionConfig(IREE::GPU::TargetAttr target,
                                                targetSubgroupSize, configDict);
 }
 
+// Checks if 'outputOperand' is a reduction with a single combiner operation.
+// Returns the combiner operation of the reduction, or nullptr if it is not a
+// valid reduction. This function is adapted from the implementation of Linalg
+// vectorization.
+static Operation *matchLinalgReduction(OpOperand *outputOperand) {
+  auto linalgOp = cast<linalg::LinalgOp>(outputOperand->getOwner());
+  unsigned outputPos =
+      outputOperand->getOperandNumber() - linalgOp.getNumDpsInputs();
+  // Only single combiner operations are supported for now.
+  SmallVector<Operation *, 4> combinerOps;
+  if (!matchReduction(linalgOp.getRegionOutputArgs(), outputPos, combinerOps) ||
+      combinerOps.size() != 1)
+    return nullptr;
+
+  // Return the combiner operation.
+  return combinerOps[0];
+}
+
+static LogicalResult reductionPrecondition(linalg::LinalgOp op) {
+  SmallVector<unsigned> parallelDims;
+  SmallVector<unsigned> reductionDims;
+  op.getParallelDims(parallelDims);
+  op.getReductionDims(reductionDims);
+  if (reductionDims.empty())
+    return failure();
+
+  SmallVector<int64_t, 4> bounds = op.getStaticLoopRanges();
+  int64_t numParallelDims = op.getNumParallelLoops();
+
+  // Make sure reduction dimensions are static and innermost ones.
+  for (unsigned dim : reductionDims) {
+    if (ShapedType::isDynamic(bounds[dim]))
+      return failure();
+    if (dim < numParallelDims) {
+      LDBG("Not innermost ones");
+      return failure();
+    }
+  }
+
+  // Make sure parallel dimensions are static
+  for (unsigned dim : parallelDims) {
+    if (ShapedType::isDynamic(bounds[dim]))
+      return failure();
+  }
+
+  for (OpOperand &opOperand : op.getDpsInitsMutable()) {
+    AffineMap indexingMap = op.getMatchingIndexingMap(&opOperand);
+    if (indexingMap.isPermutation())
+      continue;
+
+    Operation *reduceOp = matchLinalgReduction(&opOperand);
+    if (!reduceOp || !linalg::getCombinerOpKind(reduceOp)) {
+      LDBG("reduction precondition failed: reduction detection failed\n");
+      return failure();
+    }
+  }
+  return success();
+}
+
+static LogicalResult
+setReductionVectorDistributionConfig(IREE::GPU::TargetAttr target,
+                                     mlir::FunctionOpInterface entryPoint,
+                                     linalg::LinalgOp op) {
+  if (!target.supportsSubgroupShuffle())
+    return failure();
+
+  SmallVector<unsigned> parallelDims;
+  SmallVector<unsigned> reductionDims;
+  op.getParallelDims(parallelDims);
+  op.getReductionDims(reductionDims);
+
+  SmallVector<int64_t, 4> bounds = op.getStaticLoopRanges();
+
+  int64_t reductionSize = 1;
+  for (int64_t dim : reductionDims)
+    reductionSize *= bounds[dim];
+
+  int64_t subgroupSize = 0;
+  for (int s : target.getWgp().getSubgroupSizeChoices().asArrayRef()) {
+    if (reductionSize % s == 0) {
+      subgroupSize = s;
+      break;
+    }
+  }
+  if (subgroupSize == 0)
+    return failure();
+
+  Value init = op.getDpsInitOperand(0)->get();
+  Value src = op.getDpsInputOperand(0)->get();
+  Type initElemType = getElementTypeOrSelf(init);
+  Type srcElemType = getElementTypeOrSelf(src);
+
+  if (auto initOp = init.getDefiningOp<linalg::GenericOp>()) {
+    if (IREE::LinalgExt::isBitExtendOp(initOp))
+      initElemType = getElementTypeOrSelf(initOp.getDpsInputs()[0]);
+  }
+
+  if (auto srcOp = src.getDefiningOp<linalg::GenericOp>()) {
+    if (IREE::LinalgExt::isBitExtendOp(srcOp))
+      srcElemType = getElementTypeOrSelf(srcOp.getDpsInputs()[0]);
+  }
+
+  if (!initElemType.isIntOrFloat() || !srcElemType.isIntOrFloat())
+    return failure();
+
+  unsigned bitWidth = std::min(initElemType.getIntOrFloatBitWidth(),
+                               srcElemType.getIntOrFloatBitWidth());
+
+  // Reduction distribution only supports 8/16/32 bit types now.
+  if (bitWidth != 32 && bitWidth != 16 && bitWidth != 8)
```

**Comment:**
nit: you can use `llvm::is_contained`

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/KernelConfig.cpp:1103`

```diff
@@ -814,6 +820,156 @@ setAttentionVectorDistributionConfig(IREE::GPU::TargetAttr target,
                                                targetSubgroupSize, configDict);
 }
 
+// Checks if 'outputOperand' is a reduction with a single combiner operation.
+// Returns the combiner operation of the reduction, or nullptr if it is not a
+// valid reduction. This function is adapted from the implementation of Linalg
+// vectorization.
+static Operation *matchLinalgReduction(OpOperand *outputOperand) {
+  auto linalgOp = cast<linalg::LinalgOp>(outputOperand->getOwner());
+  unsigned outputPos =
+      outputOperand->getOperandNumber() - linalgOp.getNumDpsInputs();
+  // Only single combiner operations are supported for now.
+  SmallVector<Operation *, 4> combinerOps;
+  if (!matchReduction(linalgOp.getRegionOutputArgs(), outputPos, combinerOps) ||
+      combinerOps.size() != 1)
+    return nullptr;
+
+  // Return the combiner operation.
+  return combinerOps[0];
+}
+
+static LogicalResult reductionPrecondition(linalg::LinalgOp op) {
+  SmallVector<unsigned> parallelDims;
+  SmallVector<unsigned> reductionDims;
+  op.getParallelDims(parallelDims);
+  op.getReductionDims(reductionDims);
+  if (reductionDims.empty())
+    return failure();
+
+  SmallVector<int64_t, 4> bounds = op.getStaticLoopRanges();
+  int64_t numParallelDims = op.getNumParallelLoops();
+
+  // Make sure reduction dimensions are static and innermost ones.
+  for (unsigned dim : reductionDims) {
+    if (ShapedType::isDynamic(bounds[dim]))
+      return failure();
+    if (dim < numParallelDims) {
+      LDBG("Not innermost ones");
+      return failure();
+    }
+  }
+
+  // Make sure parallel dimensions are static
+  for (unsigned dim : parallelDims) {
+    if (ShapedType::isDynamic(bounds[dim]))
+      return failure();
+  }
+
+  for (OpOperand &opOperand : op.getDpsInitsMutable()) {
+    AffineMap indexingMap = op.getMatchingIndexingMap(&opOperand);
+    if (indexingMap.isPermutation())
+      continue;
+
+    Operation *reduceOp = matchLinalgReduction(&opOperand);
+    if (!reduceOp || !linalg::getCombinerOpKind(reduceOp)) {
+      LDBG("reduction precondition failed: reduction detection failed\n");
+      return failure();
+    }
+  }
+  return success();
+}
+
+static LogicalResult
+setReductionVectorDistributionConfig(IREE::GPU::TargetAttr target,
+                                     mlir::FunctionOpInterface entryPoint,
+                                     linalg::LinalgOp op) {
+  if (!target.supportsSubgroupShuffle())
+    return failure();
+
+  SmallVector<unsigned> parallelDims;
+  SmallVector<unsigned> reductionDims;
+  op.getParallelDims(parallelDims);
+  op.getReductionDims(reductionDims);
+
+  SmallVector<int64_t, 4> bounds = op.getStaticLoopRanges();
+
+  int64_t reductionSize = 1;
+  for (int64_t dim : reductionDims)
+    reductionSize *= bounds[dim];
+
+  int64_t subgroupSize = 0;
+  for (int s : target.getWgp().getSubgroupSizeChoices().asArrayRef()) {
+    if (reductionSize % s == 0) {
+      subgroupSize = s;
+      break;
+    }
+  }
+  if (subgroupSize == 0)
+    return failure();
+
+  Value init = op.getDpsInitOperand(0)->get();
+  Value src = op.getDpsInputOperand(0)->get();
+  Type initElemType = getElementTypeOrSelf(init);
+  Type srcElemType = getElementTypeOrSelf(src);
+
+  if (auto initOp = init.getDefiningOp<linalg::GenericOp>()) {
+    if (IREE::LinalgExt::isBitExtendOp(initOp))
+      initElemType = getElementTypeOrSelf(initOp.getDpsInputs()[0]);
+  }
+
+  if (auto srcOp = src.getDefiningOp<linalg::GenericOp>()) {
+    if (IREE::LinalgExt::isBitExtendOp(srcOp))
+      srcElemType = getElementTypeOrSelf(srcOp.getDpsInputs()[0]);
+  }
+
+  if (!initElemType.isIntOrFloat() || !srcElemType.isIntOrFloat())
+    return failure();
+
+  unsigned bitWidth = std::min(initElemType.getIntOrFloatBitWidth(),
+                               srcElemType.getIntOrFloatBitWidth());
+
+  // Reduction distribution only supports 8/16/32 bit types now.
+  if (bitWidth != 32 && bitWidth != 16 && bitWidth != 8)
+    return failure();
+
+  const int64_t targetSubgroupSize = target.getPreferredSubgroupSize();
+
+  const unsigned largestLoadSizeInBits = 128;
+  unsigned vectorSize = largestLoadSizeInBits / bitWidth;
```

**Comment:**
We should probably make this a target property like discussed here: https://discord.com/channels/689900678990135345/1254843174111678555/1296841473249251439.
This is not a blocker IMO.

---


---


## [PR #18822](https://github.com/iree-org/iree/pull/18822): [VectorDistribution] Plumb the VectorDistribute pipeline to support reduction operations (3/4)

### Review Summary

**COMMENTED** (2024-10-17)

Why do we need the new pipeline option?


**APPROVED** (2024-11-04)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/TargetUtils/ConfigUtils.h:47`

```diff
@@ -44,6 +44,7 @@ struct GPUPipelineOptions {
   bool enableReduceSharedMemoryBankConflicts = true;
   bool prefetchSharedMemory = false;
   bool enableUkernels = false;
+  bool generateContract = true;
```

**Comment:**
I think changes to these options need to be reflected on the tablegen side (in the matching attribute)? @Max191

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.cpp:232`

```diff
@@ -227,6 +228,9 @@ static void addGPUVectorizationPasses(OpPassManager &funcPassManager) {
   options.vectorizeGatherAccesses = true;
   options.enableCleanup = false;
   options.foldCastIntoContract = true;
+  // used for supporting reduction along VectorDistribute pipeline
+  // disable conversion from reduction ops to contraction ops.
```

**Comment:**
Use proper capitalization

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.cpp:867`

```diff
@@ -857,7 +861,12 @@ void addGPUVectorDistributePassPipeline(OpPassManager &funcPassManager,
   funcPassManager.addPass(createOptimizeTensorInsertExtractSlicesPass());
 
   // Linalg -> Vector
-  addGPUVectorizationPasses(funcPassManager);
+  if (options.generateContract) {
+    addGPUVectorizationPasses(funcPassManager);
+  } else {
+    // disable conversion from reductions ops to contraction ops.
```

**Comment:**
Also here

---


---


## [PR #18800](https://github.com/iree-org/iree/pull/18800): [VectorDistribution] Add vector distribution support multi-dim reduction with scalars

### Review Summary

**COMMENTED** (2024-10-17)


**COMMENTED** (2024-10-23)


**APPROVED** (2024-10-25)

LGTM % one comment


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.h:112`

```diff
@@ -105,6 +107,10 @@ FailureOr<SmallVector<int64_t>> getGPUTileSize(mlir::FunctionOpInterface funcOp,
 FailureOr<scf::SCFTileSizeComputationFunction>
 getGPUScfTileSizeComputeFn(mlir::FunctionOpInterface funcOp, int tilingLevel);
 
+// Returns a boolean flag indicating whether the input value 'val' is a
+// vector, determined by checking its rank.
+bool isVector(VectorValue val);
```

**Comment:**
This strikes me as an odd helper: you give 'vector' a new meaning without introducing a name. Instead, I'd either flip it and add a helper like `isRank0(VectorValue val)`, or just expand the check where you need it.

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.cpp:219`

```diff
@@ -212,6 +212,12 @@ getGPUScfTileSizeComputeFn(mlir::FunctionOpInterface funcOp, int tilingLevel) {
   return computeFn;
 }
 
+bool isVector(VectorValue val) {
+  if (val.getType().getRank() != 0)
+    return true;
+  return false;
+}
```

**Comment:**
```suggestion
bool isVector(VectorValue val) {
  return val.getType().getRank() != 0;
}
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/VectorLayoutAnalysis.cpp:1063`

```diff
@@ -1027,6 +1040,47 @@ void EnforceLayout::visitRegionSuccessors(RegionBranchOpInterface branch,
   }
 }
 
+void EnforceLayout::visitRegionBranchTerminatorOpInterface(
+    RegionBranchOpInterface branch, RegionBranchPoint branchPoint) {
+  SmallVector<RegionSuccessor> successors;
+  branch.getSuccessorRegions(branchPoint, successors);
+  if (!branch.hasLoop())
+    return;
+  SmallVector<DistributionLayout *> resultLattices;
+  for (Value result : branch->getResults()) {
+    DistributionLayout *resultLattice = getLatticeElement(result);
+    if (resultLattice->isUninitialized())
+      continue;
+    resultLattices.push_back(resultLattice);
+  }
+
+  // Result lattice not has a layout yet.
+  if (resultLattices.empty())
+    return;
+
+  // We do not support multiple results yet.
+  if (resultLattices.size() != 1)
+    return;
```

**Comment:**
The first check is redundant

---

**File:** `compiler/src/iree/compiler/Codegen/Common/VectorLayoutAnalysis.cpp:1066`

```diff
@@ -1027,6 +1040,47 @@ void EnforceLayout::visitRegionSuccessors(RegionBranchOpInterface branch,
   }
 }
 
+void EnforceLayout::visitRegionBranchTerminatorOpInterface(
+    RegionBranchOpInterface branch, RegionBranchPoint branchPoint) {
+  SmallVector<RegionSuccessor> successors;
+  branch.getSuccessorRegions(branchPoint, successors);
+  if (!branch.hasLoop())
+    return;
+  SmallVector<DistributionLayout *> resultLattices;
+  for (Value result : branch->getResults()) {
+    DistributionLayout *resultLattice = getLatticeElement(result);
+    if (resultLattice->isUninitialized())
+      continue;
+    resultLattices.push_back(resultLattice);
+  }
+
+  // Result lattice not has a layout yet.
+  if (resultLattices.empty())
+    return;
+
+  // We do not support multiple results yet.
+  if (resultLattices.size() != 1)
+    return;
+
+  for (RegionSuccessor successor : successors) {
+    if (auto succ = successor.getSuccessor()) {
```

**Comment:**
Can you use the actual type here instead of `auto`? It's not clear based on the RHS

---

**File:** `compiler/src/iree/compiler/Codegen/Common/VectorLayoutAnalysis.cpp:713`

```diff
@@ -699,6 +706,9 @@ static void enforceLayoutToBroadcastOp(
 
   auto resultShape = broadcast.getResultVectorType().getShape();
   auto inputType = broadcast.getSourceType();
+  if (!isa<VectorType>(inputType)) {
+    return;
+  }
   assert(isa<VectorType>(inputType) &&
          "Scalar broadcast not supported for now.");
```

**Comment:**
This assertion is obsolete now.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUVectorDistribution.cpp:136`

```diff
@@ -132,7 +132,8 @@ void DistributionPattern::replaceOpWithDistributedValues(
   for (auto [opResult, replacement] :
        llvm::zip_equal(op->getOpResults(), values)) {
     // If this value is a vector type, it must be converted back to simd.
-    if (isa<VectorType>(replacement.getType())) {
+    if (isa<VectorType>(replacement.getType()) &&
+        cast<ShapedType>(replacement.getType()).getRank() != 0) {
```

**Comment:**
use dyn_cast instead:
```c++
if (auto x = dyn_cast<Y>(y)) {
  if (x.something() == Z) {
 ```
 
 See https://llvm.org/docs/ProgrammersManual.html#the-isa-cast-and-dyn-cast-templates:~:text=Note%20that%20you%20should%20not%20use%20an%20isa%3C%3E%20test%20followed%20by%20a%20cast%3C%3E%2C%20for%20that%20use%20the%20dyn_cast%3C%3E%20operator.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:535`

```diff
@@ -485,15 +516,27 @@ struct DistributeMultiReduction final
     // reduction.
     VectorValue unflattened = rewriter.create<vector::ShapeCastOp>(
         loc, shaped, threadReduced.value());
+
+    if (!accVector) {
+      disAcc = rewriter.create<vector::BroadcastOp>(loc, shaped, disAcc);
+    }
+
     Value accReduction = vector::makeArithReduction(
         rewriter, loc, multiReduceOp.getKind(), unflattened, disAcc);
     auto accReduced = dyn_cast<VectorValue>(accReduction);
     if (!accReduced) {
       return failure();
     }
-    replaceOpWithDistributedValues(rewriter, multiReduceOp, accReduced);
 
-    return failure();
+    if (resVector) {
+      replaceOpWithDistributedValues(rewriter, multiReduceOp, accReduced);
+    } else {
+      Value accReducedVal = rewriter.create<vector::ExtractOp>(
+          loc, accReduction, SmallVector<int64_t>{0});
```

**Comment:**
You don't need a vector here, you can do: `ArrayRef{int64_t(0)}`

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:493`

```diff
@@ -462,7 +485,15 @@ struct DistributeMultiReduction final
     auto localReduction = rewriter.create<vector::MultiDimReductionOp>(
         loc, disSrc, localInit, distributedReductionMask,
         multiReduceOp.getKind());
-    auto locallyReduced = dyn_cast<VectorValue>(localReduction.getResult());
+
+    VectorValue locallyReduced;
+    if (accVector) {
+      locallyReduced = dyn_cast<VectorValue>(localReduction.getResult());
+    } else {
+      VectorType vecType = VectorType::get(SmallVector<int64_t>{1}, elemTy);
```

**Comment:**
Also here, no need to use a vector

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:428`

```diff
@@ -413,15 +417,29 @@ struct DistributeMultiReduction final
                                 DistributionSignature &signature,
                                 PatternRewriter &rewriter) const override {
     VectorValue srcVector = multiReduceOp.getSource();
-    auto accVector = dyn_cast<VectorValue>(multiReduceOp.getAcc());
-    if (!accVector) {
-      return rewriter.notifyMatchFailure(
-          multiReduceOp, "unimplemented: scalar accumulator distribution");
-    }
-    auto resVector = dyn_cast<VectorValue>(multiReduceOp.getResult());
-    if (!resVector) {
-      return rewriter.notifyMatchFailure(
-          multiReduceOp, "unimplemented: scalar result distribution");
+    Value acc = multiReduceOp.getAcc();
+    Value res = multiReduceOp.getResult();
+    auto accVector = dyn_cast<VectorValue>(acc);
+    auto resVector = dyn_cast<VectorValue>(res);
+    Type accType = acc.getType();
+    Type resType = res.getType();
+    Type accElemTy;
+    if (accVector) {
+      accElemTy = accVector.getType().getElementType();
```

**Comment:**
You can use `getElementTypeOrSelf`. Also below.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:312`

```diff
@@ -305,7 +305,11 @@ struct DistributeBroadcast final : OpDistributionPattern<vector::BroadcastOp> {
     auto vectorType = VectorType::get(distShape, elementType);
 
     VectorValue srcVector = dyn_cast<VectorValue>(broadcastOp.getSource());
-    if (!srcVector) {
+    // Types such as vector<f32> can return a valid pointer. An additional
+    // rank check is added to ensure that the type is indeed a vector
+    // value.
+    bool isSrcVector = (srcVector) && (isVector(srcVector));
+    if (!isSrcVector) {
```

**Comment:**
This is the only use of `isSrcVector`, so it makes sense to inline it
```suggestion
    if (!srcVector || !isVector(srcVector)) {
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUDistributionPatterns.cpp:588`

```diff
@@ -582,8 +584,12 @@ struct DistributeScfFor final : OpDistributionPattern<scf::ForOp> {
     SmallVector<Value> operands;
     for (Value operand : yieldOp->getOperands()) {
       if (auto vectorOperand = dyn_cast<VectorValue>(operand)) {
-        operand = DistributionPattern::getDistributed(rewriter, vectorOperand,
-                                                      signature[vectorOperand]);
+        // Types such as vector<f32> can pass this condition. An additional rank
+        // check is added here to ensure that the type is indeed a vector value.
```

**Comment:**
`vector<f32>` is a vector value so the comment doesn't make sense to me

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:310`

```diff
@@ -305,7 +305,11 @@ struct DistributeBroadcast final : OpDistributionPattern<vector::BroadcastOp> {
     auto vectorType = VectorType::get(distShape, elementType);
 
     VectorValue srcVector = dyn_cast<VectorValue>(broadcastOp.getSource());
-    if (!srcVector) {
+    // Types such as vector<f32> can return a valid pointer. An additional
+    // rank check is added to ensure that the type is indeed a vector
+    // value.
```

**Comment:**
Similar here -- the comment doesn't make sense to me. What does it mean for `vector<f32>` to return a pointer?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:312`

```diff
@@ -305,7 +305,11 @@ struct DistributeBroadcast final : OpDistributionPattern<vector::BroadcastOp> {
     auto vectorType = VectorType::get(distShape, elementType);
 
     VectorValue srcVector = dyn_cast<VectorValue>(broadcastOp.getSource());
-    if (!srcVector) {
+    // Types such as vector<f32> can return a valid pointer. An additional
+    // rank check is added to ensure that the type is indeed a vector
+    // value.
+    bool isSrcVector = (srcVector) && (isNonZeroRank(srcVector));
+    if (!isSrcVector) {
```

**Comment:**
We should inline this condition into the `if`

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:426`

```diff
@@ -413,15 +417,17 @@ struct DistributeMultiReduction final
                                 DistributionSignature &signature,
                                 PatternRewriter &rewriter) const override {
     VectorValue srcVector = multiReduceOp.getSource();
-    auto accVector = dyn_cast<VectorValue>(multiReduceOp.getAcc());
-    if (!accVector) {
-      return rewriter.notifyMatchFailure(
-          multiReduceOp, "unimplemented: scalar accumulator distribution");
-    }
-    auto resVector = dyn_cast<VectorValue>(multiReduceOp.getResult());
-    if (!resVector) {
-      return rewriter.notifyMatchFailure(
-          multiReduceOp, "unimplemented: scalar result distribution");
+    Value acc = multiReduceOp.getAcc();
+    Value res = multiReduceOp.getResult();
+    auto accVector = dyn_cast<VectorValue>(acc);
+    auto resVector = dyn_cast<VectorValue>(res);
+
+    Type accElemTy = getElementTypeOrSelf(acc.getType());
+    Type resElemTy = getElementTypeOrSelf(res.getType());
```

**Comment:**
Either check that the `dyn_cast`s succeeded (and return an error if not) or use `cast` to assert on cast failures

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUVectorDistribution.cpp:135`

```diff
@@ -132,14 +132,17 @@ void DistributionPattern::replaceOpWithDistributedValues(
   for (auto [opResult, replacement] :
        llvm::zip_equal(op->getOpResults(), values)) {
     // If this value is a vector type, it must be converted back to simd.
-    if (isa<VectorType>(replacement.getType())) {
-      auto oldResult = cast<VectorValue>(opResult);
-      // Create a toSIMD op to convert the value back to the simd.
-      rewriter.setInsertionPointAfterValue(oldResult);
-      Value toSIMD = rewriter.create<IREE::VectorExt::ToSIMDOp>(
-          oldResult.getLoc(), oldResult.getType(), replacement);
-      // Add to replacements.
-      replacement = toSIMD;
+    if (VectorType replacementType =
```

**Comment:**
```suggestion
    if (auto replacementType =
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/VectorLayoutAnalysis.cpp:712`

```diff
@@ -699,8 +706,10 @@ static void enforceLayoutToBroadcastOp(
 
   auto resultShape = broadcast.getResultVectorType().getShape();
   auto inputType = broadcast.getSourceType();
-  assert(isa<VectorType>(inputType) &&
-         "Scalar broadcast not supported for now.");
+  if (!isa<VectorType>(inputType)) {
+    return;
+  }
+
```

**Comment:**
Follow the llvm coding style and use `dyn_cast` here to avoid repeated type checking below

---

**File:** `compiler/src/iree/compiler/Codegen/Common/VectorLayoutAnalysis.cpp:948`

```diff
@@ -935,6 +944,9 @@ void EnforceLayout::visitOperation(Operation *op) {
   if (auto branch = dyn_cast<RegionBranchOpInterface>(op)) {
     visitRegionSuccessors(branch, RegionBranchPoint::parent(),
                           branch->getOpOperands());
+
+    // Handle the propation from scf.for to yield op
```

**Comment:**
typo and punctuation
```suggestion
    // Handle the propagation from scf.for to yield op.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Utils/GPUUtils.h:111`

```diff
@@ -105,6 +107,10 @@ FailureOr<SmallVector<int64_t>> getGPUTileSize(mlir::FunctionOpInterface funcOp,
 FailureOr<scf::SCFTileSizeComputationFunction>
 getGPUScfTileSizeComputeFn(mlir::FunctionOpInterface funcOp, int tilingLevel);
 
+// Determines whether the rank of the input value 'val' is non-zero.
+// Returns true if the rank is non-zero; otherwise, returns false.
```

**Comment:**
```suggestion
// Returns true iff the rank of the input value 'val' is non-zero.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUDistributionPatterns.cpp:592`

```diff
@@ -584,8 +584,12 @@ struct DistributeScfFor final : OpDistributionPattern<scf::ForOp> {
     SmallVector<Value> operands;
     for (Value operand : yieldOp->getOperands()) {
       if (auto vectorOperand = dyn_cast<VectorValue>(operand)) {
-        // Types such as vector<f32> can pass this condition. An additional rank
-        // check is added here to ensure that the type is indeed a vector value.
+        // Check if the operand is a vector type (e.g., vector<f32>), which
+        // passes this condition as it is indeed a vector. However, distributing
+        // the operand requires it to have a non-zero rank, meaning it must have
+        // at least one dimension. To ensure this, we add a necessary rank
+        // check. If the vector has a non-zero rank, the operand is distributed
+        // according to the provided layout signature.
```

**Comment:**
Thanks for revising this. Now the comment is very clear but also very verbose. I think we can simplify this.

```suggestion
        // Distributing the operand requires it to have a non-zero rank, meaning it must have
        // at least one dimension. If the vector has a non-zero rank, the operand is distributed
        // according to the provided layout signature.
```

---


---


## [PR #18784](https://github.com/iree-org/iree/pull/18784): [VectorDistribution] Add scalar support for distributing multi-dim reduction (1/4)

### Review Summary

**COMMENTED** (2024-10-17)

I reviewed your PRs out of ordered and left the comments here: https://github.com/iree-org/iree/pull/18800


**COMMENTED** (2024-10-30)


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:484`

```diff
@@ -462,7 +476,15 @@ struct DistributeMultiReduction final
     auto localReduction = rewriter.create<vector::MultiDimReductionOp>(
         loc, disSrc, localInit, distributedReductionMask,
         multiReduceOp.getKind());
-    auto locallyReduced = dyn_cast<VectorValue>(localReduction.getResult());
+
+    VectorValue locallyReduced;
+    if (accVector) {
+      locallyReduced = dyn_cast<VectorValue>(localReduction.getResult());
+    } else {
+      VectorType vecType = VectorType::get(SmallVector<int64_t>{1}, elemTy);
```

**Comment:**
You should be able to use ArrayRef here instead of constructing a vector

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/GPUNestedLayoutDistributionPatterns.cpp:526`

```diff
@@ -485,15 +507,27 @@ struct DistributeMultiReduction final
     // reduction.
     VectorValue unflattened = rewriter.create<vector::ShapeCastOp>(
         loc, shaped, threadReduced.value());
+
+    if (!accVector) {
+      disAcc = rewriter.create<vector::BroadcastOp>(loc, shaped, disAcc);
+    }
+
     Value accReduction = vector::makeArithReduction(
         rewriter, loc, multiReduceOp.getKind(), unflattened, disAcc);
     auto accReduced = dyn_cast<VectorValue>(accReduction);
     if (!accReduced) {
       return failure();
     }
-    replaceOpWithDistributedValues(rewriter, multiReduceOp, accReduced);
 
-    return failure();
+    if (resVector) {
+      replaceOpWithDistributedValues(rewriter, multiReduceOp, accReduced);
+    } else {
+      Value accReducedVal = rewriter.create<vector::ExtractOp>(
+          loc, accReduction, SmallVector<int64_t>{0});
```

**Comment:**
also here

---


---


## [PR #18012](https://github.com/iree-org/iree/pull/18012): use tile 16

### Review Summary

**APPROVED** (2024-07-26)

I tested this locally and it looks fine. We also checked that this only applies to those two convs across the whole unet.



---


## [PR #17811](https://github.com/iree-org/iree/pull/17811): Add workgroup chipletgroup strategy to workgroup reordering pass

### Review Summary

**CHANGES_REQUESTED** (2024-07-08)


**COMMENTED** (2024-07-10)


**COMMENTED** (2024-07-10)

Just some remaining issues with comments. Looks good otherwise.


**COMMENTED** (2024-07-10)


**COMMENTED** (2024-07-10)


**COMMENTED** (2024-07-11)


**COMMENTED** (2024-07-11)


**APPROVED** (2024-07-11)

LGTM, thanks for all the fixes


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/Passes.td:205`

```diff
@@ -199,10 +199,11 @@ def ReorderWorkgroupsPass :
   let dependentDialects = ["::mlir::affine::AffineDialect"];
   let options = [
     Option<"strategy", "strategy", "std::string", /*default=*/"",
-           "Workgroup reordering strategy, one of: '' (none),  'transpose', 'swizzle'">,
+           "Workgroup reordering strategy, one of: '' (none),  'transpose', 'swizzle', 'chipletgroup'">,
     Option<"logTile", "logTile", "unsigned",
             /*default=*/"0",
-           "The log2 of the tile size used for swizzling. (0: disabled, non-0: swizzling enabled)">,
+           "The log2 of the tile size used for swizzling and chipletgroup. "
```

**Comment:**
The name doesn't match the chiplet-group strategy. We should either rename it or add a second option if that makes more sense.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:71`

```diff
@@ -68,6 +68,95 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// reoredering to make workgroup ids move slowly between chiplet groups
```

**Comment:**
```suggestion
// Reordering to make workgroup ids move slowly between chiplet groups.
```

Could you also give an example? IE pick some topology and show how the math works out.

Also say what the return value is.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:76`

```diff
@@ -68,6 +68,95 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// reoredering to make workgroup ids move slowly between chiplet groups
+static Value chipletAwareWorkgroupReordering(Location loc, OpBuilder b,
+                                             Value linearizedId,
+                                             Value workgroupCountX,
+                                             Value workgroupCountY,
+                                             int64_t numChipletsPerGroup) {
```

**Comment:**
Is this the number of chiplets per *work*group or something else?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:90`

```diff
@@ -68,6 +68,95 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// reoredering to make workgroup ids move slowly between chiplet groups
+static Value chipletAwareWorkgroupReordering(Location loc, OpBuilder b,
+                                             Value linearizedId,
+                                             Value workgroupCountX,
+                                             Value workgroupCountY,
+                                             int64_t numChipletsPerGroup) {
+  Value numChipletsVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, numChipletsPerGroup);
+  Value workgroupCount =
+      b.create<arith::MulIOp>(loc, workgroupCountX, workgroupCountY);
+  Value workgroupCountPerChiplet =
+      b.create<arith::DivUIOp>(loc, workgroupCount, numChipletsVal);
+  Value chipletId = b.create<arith::RemUIOp>(loc, linearizedId, numChipletsVal);
+  Value wgIdWithinChiplet =
+      b.create<arith::DivUIOp>(loc, linearizedId, numChipletsVal);
+  Value reorderedId = b.create<arith::AddIOp>(
+      loc, wgIdWithinChiplet,
+      b.create<arith::MulIOp>(loc, chipletId, workgroupCountPerChiplet));
+
+  // The following code is used to handle the remainder part
```

**Comment:**
```suggestion
  // Handle the remainder part.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:100`

```diff
@@ -68,6 +68,95 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// reoredering to make workgroup ids move slowly between chiplet groups
+static Value chipletAwareWorkgroupReordering(Location loc, OpBuilder b,
+                                             Value linearizedId,
+                                             Value workgroupCountX,
+                                             Value workgroupCountY,
+                                             int64_t numChipletsPerGroup) {
+  Value numChipletsVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, numChipletsPerGroup);
+  Value workgroupCount =
+      b.create<arith::MulIOp>(loc, workgroupCountX, workgroupCountY);
+  Value workgroupCountPerChiplet =
+      b.create<arith::DivUIOp>(loc, workgroupCount, numChipletsVal);
+  Value chipletId = b.create<arith::RemUIOp>(loc, linearizedId, numChipletsVal);
+  Value wgIdWithinChiplet =
+      b.create<arith::DivUIOp>(loc, linearizedId, numChipletsVal);
+  Value reorderedId = b.create<arith::AddIOp>(
+      loc, wgIdWithinChiplet,
+      b.create<arith::MulIOp>(loc, chipletId, workgroupCountPerChiplet));
+
+  // The following code is used to handle the remainder part
+
+  Value constOne = b.createOrFold<arith::ConstantIndexOp>(loc, 1);
+  Value lastWorkgroupId =
+      b.create<arith::SubIOp>(loc, workgroupCount, constOne);
+  Value modulatedLastWorkgroupId = b.create<arith::SubIOp>(
+      loc, lastWorkgroupId,
+      b.create<arith::RemUIOp>(loc, workgroupCount, numChipletsVal));
+  Value isGreaterThanFinalWorkgroupId = b.create<arith::CmpIOp>(
+      loc, arith::CmpIPredicate::ugt, linearizedId, modulatedLastWorkgroupId);
+  linearizedId = b.create<arith::SelectOp>(loc, isGreaterThanFinalWorkgroupId,
```

**Comment:**
nit: Do not reassign the function arguments, it makes the logic harder to follow

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:110`

```diff
@@ -68,6 +68,95 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// reoredering to make workgroup ids move slowly between chiplet groups
+static Value chipletAwareWorkgroupReordering(Location loc, OpBuilder b,
+                                             Value linearizedId,
+                                             Value workgroupCountX,
+                                             Value workgroupCountY,
+                                             int64_t numChipletsPerGroup) {
+  Value numChipletsVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, numChipletsPerGroup);
+  Value workgroupCount =
+      b.create<arith::MulIOp>(loc, workgroupCountX, workgroupCountY);
+  Value workgroupCountPerChiplet =
+      b.create<arith::DivUIOp>(loc, workgroupCount, numChipletsVal);
+  Value chipletId = b.create<arith::RemUIOp>(loc, linearizedId, numChipletsVal);
+  Value wgIdWithinChiplet =
+      b.create<arith::DivUIOp>(loc, linearizedId, numChipletsVal);
+  Value reorderedId = b.create<arith::AddIOp>(
+      loc, wgIdWithinChiplet,
+      b.create<arith::MulIOp>(loc, chipletId, workgroupCountPerChiplet));
+
+  // The following code is used to handle the remainder part
+
+  Value constOne = b.createOrFold<arith::ConstantIndexOp>(loc, 1);
+  Value lastWorkgroupId =
+      b.create<arith::SubIOp>(loc, workgroupCount, constOne);
+  Value modulatedLastWorkgroupId = b.create<arith::SubIOp>(
+      loc, lastWorkgroupId,
+      b.create<arith::RemUIOp>(loc, workgroupCount, numChipletsVal));
+  Value isGreaterThanFinalWorkgroupId = b.create<arith::CmpIOp>(
+      loc, arith::CmpIPredicate::ugt, linearizedId, modulatedLastWorkgroupId);
+  linearizedId = b.create<arith::SelectOp>(loc, isGreaterThanFinalWorkgroupId,
+                                           linearizedId, reorderedId);
+
+  return linearizedId;
+}
+
+// Chiplet-aware workgroup reordering strategy: reordering + super-grouping.
+// Step 1: Reorder the workgroup grid to move slowly between
+// chiplet groups (Function: chipletAwareWorkgroupReordering).
+// Step 2: Implement 'super-grouping' of workgroups before switching to the next
+// column.
```

**Comment:**
Say what the return value is.

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:115`

```diff
@@ -68,6 +68,95 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// reoredering to make workgroup ids move slowly between chiplet groups
+static Value chipletAwareWorkgroupReordering(Location loc, OpBuilder b,
+                                             Value linearizedId,
+                                             Value workgroupCountX,
+                                             Value workgroupCountY,
+                                             int64_t numChipletsPerGroup) {
+  Value numChipletsVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, numChipletsPerGroup);
+  Value workgroupCount =
+      b.create<arith::MulIOp>(loc, workgroupCountX, workgroupCountY);
+  Value workgroupCountPerChiplet =
+      b.create<arith::DivUIOp>(loc, workgroupCount, numChipletsVal);
+  Value chipletId = b.create<arith::RemUIOp>(loc, linearizedId, numChipletsVal);
+  Value wgIdWithinChiplet =
+      b.create<arith::DivUIOp>(loc, linearizedId, numChipletsVal);
+  Value reorderedId = b.create<arith::AddIOp>(
+      loc, wgIdWithinChiplet,
+      b.create<arith::MulIOp>(loc, chipletId, workgroupCountPerChiplet));
+
+  // The following code is used to handle the remainder part
+
+  Value constOne = b.createOrFold<arith::ConstantIndexOp>(loc, 1);
+  Value lastWorkgroupId =
+      b.create<arith::SubIOp>(loc, workgroupCount, constOne);
+  Value modulatedLastWorkgroupId = b.create<arith::SubIOp>(
+      loc, lastWorkgroupId,
+      b.create<arith::RemUIOp>(loc, workgroupCount, numChipletsVal));
+  Value isGreaterThanFinalWorkgroupId = b.create<arith::CmpIOp>(
+      loc, arith::CmpIPredicate::ugt, linearizedId, modulatedLastWorkgroupId);
+  linearizedId = b.create<arith::SelectOp>(loc, isGreaterThanFinalWorkgroupId,
+                                           linearizedId, reorderedId);
+
+  return linearizedId;
+}
+
+// Chiplet-aware workgroup reordering strategy: reordering + super-grouping.
+// Step 1: Reorder the workgroup grid to move slowly between
+// chiplet groups (Function: chipletAwareWorkgroupReordering).
+// Step 2: Implement 'super-grouping' of workgroups before switching to the next
+// column.
+static std::pair<Value, Value>
+makeChipletGroupedIds(Location loc, OpBuilder b, Value workgroupIdX,
+                      Value workgroupIdY, Value workgroupCountX,
+                      Value workgroupCountY, unsigned chipletGroupTile) {
+  // Create one dimension ID for workgroup
```

**Comment:**
```suggestion
  // Create one dimension ID for workgroup.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:120`

```diff
@@ -68,6 +68,95 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// reoredering to make workgroup ids move slowly between chiplet groups
+static Value chipletAwareWorkgroupReordering(Location loc, OpBuilder b,
+                                             Value linearizedId,
+                                             Value workgroupCountX,
+                                             Value workgroupCountY,
+                                             int64_t numChipletsPerGroup) {
+  Value numChipletsVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, numChipletsPerGroup);
+  Value workgroupCount =
+      b.create<arith::MulIOp>(loc, workgroupCountX, workgroupCountY);
+  Value workgroupCountPerChiplet =
+      b.create<arith::DivUIOp>(loc, workgroupCount, numChipletsVal);
+  Value chipletId = b.create<arith::RemUIOp>(loc, linearizedId, numChipletsVal);
+  Value wgIdWithinChiplet =
+      b.create<arith::DivUIOp>(loc, linearizedId, numChipletsVal);
+  Value reorderedId = b.create<arith::AddIOp>(
+      loc, wgIdWithinChiplet,
+      b.create<arith::MulIOp>(loc, chipletId, workgroupCountPerChiplet));
+
+  // The following code is used to handle the remainder part
+
+  Value constOne = b.createOrFold<arith::ConstantIndexOp>(loc, 1);
+  Value lastWorkgroupId =
+      b.create<arith::SubIOp>(loc, workgroupCount, constOne);
+  Value modulatedLastWorkgroupId = b.create<arith::SubIOp>(
+      loc, lastWorkgroupId,
+      b.create<arith::RemUIOp>(loc, workgroupCount, numChipletsVal));
+  Value isGreaterThanFinalWorkgroupId = b.create<arith::CmpIOp>(
+      loc, arith::CmpIPredicate::ugt, linearizedId, modulatedLastWorkgroupId);
+  linearizedId = b.create<arith::SelectOp>(loc, isGreaterThanFinalWorkgroupId,
+                                           linearizedId, reorderedId);
+
+  return linearizedId;
+}
+
+// Chiplet-aware workgroup reordering strategy: reordering + super-grouping.
+// Step 1: Reorder the workgroup grid to move slowly between
+// chiplet groups (Function: chipletAwareWorkgroupReordering).
+// Step 2: Implement 'super-grouping' of workgroups before switching to the next
+// column.
+static std::pair<Value, Value>
+makeChipletGroupedIds(Location loc, OpBuilder b, Value workgroupIdX,
+                      Value workgroupIdY, Value workgroupCountX,
+                      Value workgroupCountY, unsigned chipletGroupTile) {
+  // Create one dimension ID for workgroup
+  Value linearized =
+      b.create<arith::MulIOp>(loc, workgroupIdY, workgroupCountX);
+  linearized = b.create<arith::AddIOp>(loc, linearized, workgroupIdX);
+
+  // This value is hardcoded for cdna3(mi300x)
```

**Comment:**
Should we plumb this through and add to the target description attribute? Do we have an issue for this?

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.td:280`

```diff
@@ -269,6 +269,27 @@ def IREEGPU_MmaScheduleAttr : AttrDef<IREEGPU_Dialect, "MMASchedule"> {
   }];
 }
 
+//===----------------------------------------------------------------------===//
+// Workgroup Reordering Attr
+
+def IREEGPU_WorkGroupReorderAttr: AttrDef<IREEGPU_Dialect, "WorkgroupReorderOptions">{
+  let mnemonic = "reorder_workgroups";
+  let cppNamespace = "::mlir::iree_compiler::IREE::GPU";
+
+  string description = [{
+    options for workgroup reordering strategies to improve L2 cache hit rate
```

**Comment:**
```suggestion
    Options for workgroup reordering strategies to improve L2 cache hit rate.
```

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp:95`

```diff
@@ -92,21 +92,31 @@ getPipelineOptions(FunctionOpInterface funcOp,
       // Get the workgroups reorder config and enable the workgroup reordering.
       Attribute reorderWorkgroupOption =
           config.get(LLVMGPUAttrNames::kReorderWorkgroups);
-      if (!isa<StringAttr>(reorderWorkgroupOption))
-        funcOp.emitOpError() << "'" << LLVMGPUAttrNames::kReorderWorkgroups
-                             << "' is expected to be a string attribute";
-      StringRef reorderStr = llvm::cast<StringAttr>(reorderWorkgroupOption);
-      if (reorderStr == "transpose") {
-        pipelineOptions.reorderStrategy = ReorderWorkgroupsStrategy::Transpose;
-      } else if (reorderStr == "swizzle") {
-        pipelineOptions.reorderStrategy = ReorderWorkgroupsStrategy::Swizzle;
-      } else {
-        if (reorderStr != "none")
-          funcOp.emitOpError()
-              << "Unknown " << LLVMGPUAttrNames::kReorderWorkgroups
-              << "value: " << reorderWorkgroupOption;
-        else
+      if (llvm::isa<IREE::GPU::WorkgroupReorderOptionsAttr>(
```

**Comment:**
nit: Do we need the `llvm::`?

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp:98`

```diff
@@ -92,21 +92,31 @@ getPipelineOptions(FunctionOpInterface funcOp,
       // Get the workgroups reorder config and enable the workgroup reordering.
       Attribute reorderWorkgroupOption =
           config.get(LLVMGPUAttrNames::kReorderWorkgroups);
-      if (!isa<StringAttr>(reorderWorkgroupOption))
-        funcOp.emitOpError() << "'" << LLVMGPUAttrNames::kReorderWorkgroups
-                             << "' is expected to be a string attribute";
-      StringRef reorderStr = llvm::cast<StringAttr>(reorderWorkgroupOption);
-      if (reorderStr == "transpose") {
-        pipelineOptions.reorderStrategy = ReorderWorkgroupsStrategy::Transpose;
-      } else if (reorderStr == "swizzle") {
-        pipelineOptions.reorderStrategy = ReorderWorkgroupsStrategy::Swizzle;
-      } else {
-        if (reorderStr != "none")
-          funcOp.emitOpError()
-              << "Unknown " << LLVMGPUAttrNames::kReorderWorkgroups
-              << "value: " << reorderWorkgroupOption;
-        else
+      if (llvm::isa<IREE::GPU::WorkgroupReorderOptionsAttr>(
+              reorderWorkgroupOption)) {
+        IREE::GPU::WorkgroupReorderOptionsAttr ReorderOption =
+            llvm::dyn_cast<IREE::GPU::WorkgroupReorderOptionsAttr>(
```

**Comment:**
also here

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/test/ROCDL/config_user_vector_distribute.mlir:90`

```diff
@@ -87,11 +87,11 @@ hal.executable public @main_0_dispatch_0 {
 
 // OPT-OUT:       #[[$TRANSLATION:.+]] = #iree_codegen.translation_info<LLVMGPUVectorDistribute workgroup_size = [128, 2, 1] subgroup_size = 64
 // OPT-OUT-SAME:    mma_schedule = #iree_gpu.mma_schedule<intrinsic = #iree_gpu.mma_layout<MFMA_F16_16x16x16_F32>,
-// OPT-OUT-SAME:    reorder_workgroups = "transpose"
+// OPT-OUT-SAME:    reorder_workgroups = #iree_gpu.reorder_workgroups<reorder_option = transpose>
```

**Comment:**
Should we add the (optional) parameter to the same attribute?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/Passes.td:210`

```diff
@@ -199,10 +199,15 @@ def ReorderWorkgroupsPass :
   let dependentDialects = ["::mlir::affine::AffineDialect"];
   let options = [
     Option<"strategy", "strategy", "std::string", /*default=*/"",
-           "Workgroup reordering strategy, one of: '' (none),  'transpose', 'swizzle'">,
-    Option<"logTile", "logTile", "unsigned",
+           "Workgroup reordering strategy, one of: '' (none),  'transpose', 'swizzle', 'chipletgroup'">,
+    Option<"logSwTile", "logSwTile", "unsigned",
             /*default=*/"0",
-           "The log2 of the tile size used for swizzling. (0: disabled, non-0: swizzling enabled)">,
+           "The log2 of the tile size used for swizzling. "
+           "(0: swizzling disabled, non-0: swizzling enabled)">,
+    Option<"logCgTile", "logCgTile", "unsigned",
+            /*default=*/"0",
+           "The log2 of the tile size used for chipletgroup. "
+           "(0: chipletgroup disabled, non-0: chipletgroup enabled)">,
```

**Comment:**
Let's not reuse the flag for swizzle tile size to enable / disable chiplet-based reordering.  Do we need it at all to control chiplet-aware reordering?

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:115`

```diff
@@ -68,6 +68,95 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// reoredering to make workgroup ids move slowly between chiplet groups
+static Value chipletAwareWorkgroupReordering(Location loc, OpBuilder b,
+                                             Value linearizedId,
+                                             Value workgroupCountX,
+                                             Value workgroupCountY,
+                                             int64_t numChipletsPerGroup) {
+  Value numChipletsVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, numChipletsPerGroup);
+  Value workgroupCount =
+      b.create<arith::MulIOp>(loc, workgroupCountX, workgroupCountY);
+  Value workgroupCountPerChiplet =
+      b.create<arith::DivUIOp>(loc, workgroupCount, numChipletsVal);
+  Value chipletId = b.create<arith::RemUIOp>(loc, linearizedId, numChipletsVal);
+  Value wgIdWithinChiplet =
+      b.create<arith::DivUIOp>(loc, linearizedId, numChipletsVal);
+  Value reorderedId = b.create<arith::AddIOp>(
+      loc, wgIdWithinChiplet,
+      b.create<arith::MulIOp>(loc, chipletId, workgroupCountPerChiplet));
+
+  // The following code is used to handle the remainder part
+
+  Value constOne = b.createOrFold<arith::ConstantIndexOp>(loc, 1);
+  Value lastWorkgroupId =
+      b.create<arith::SubIOp>(loc, workgroupCount, constOne);
+  Value modulatedLastWorkgroupId = b.create<arith::SubIOp>(
+      loc, lastWorkgroupId,
+      b.create<arith::RemUIOp>(loc, workgroupCount, numChipletsVal));
+  Value isGreaterThanFinalWorkgroupId = b.create<arith::CmpIOp>(
+      loc, arith::CmpIPredicate::ugt, linearizedId, modulatedLastWorkgroupId);
+  linearizedId = b.create<arith::SelectOp>(loc, isGreaterThanFinalWorkgroupId,
+                                           linearizedId, reorderedId);
+
+  return linearizedId;
+}
+
+// Chiplet-aware workgroup reordering strategy: reordering + super-grouping.
+// Step 1: Reorder the workgroup grid to move slowly between
+// chiplet groups (Function: chipletAwareWorkgroupReordering).
+// Step 2: Implement 'super-grouping' of workgroups before switching to the next
+// column.
+static std::pair<Value, Value>
+makeChipletGroupedIds(Location loc, OpBuilder b, Value workgroupIdX,
+                      Value workgroupIdY, Value workgroupCountX,
+                      Value workgroupCountY, unsigned chipletGroupTile) {
+  // Create one dimension ID for workgroup
```

**Comment:**
not addressed

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:160`

```diff
@@ -68,6 +68,117 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// Reoredering to make workgroup ids move slowly between chiplet groups.
+
+// The following example illustrates the concept behind this function:
+// Currently, the GPU launches workgroups in a round-robin fashion across
+// each XCD partition on the GPU.
+// Assume we have 16 workgroups and XCDPartitionsOnGPU is 4.
+// The default GPU schedule will launch workgroups {0, 1, 2, 3, ..., 15} in
+// the following round-robin fashion:
+// Partition 0: {0, 4, 8, 12}
+// Partition 1: {1, 5, 9, 13}
+// Partition 2: {2, 6, 10, 14}
+// Partition 3: {3, 7, 11, 15}
+
+// After reordering, the workgroup IDs are {0, 4, 8, 12, 1, ..., 15},
+// resulting in the round-robin launching fashion:
+// Partition 0: {0, 1, 2, 3}
+// Partition 1: {4, 5, 6, 7}
+// Partition 2: {8, 9, 10, 11}
+// Partition 3: {12, 13, 14, 15}
+
+// The return value is each workgroup's permuted Id
+// In the above example:
+// linearedId 0's permuted Id is still 0
+// linearedId 1's permiuted Id is 4
+static Value chipletAwareWorkgroupReordering(Location loc, OpBuilder b,
+                                             Value linearizedId,
+                                             Value workgroupCountX,
+                                             Value workgroupCountY,
+                                             int64_t XCDParitionsOnGPU) {
+  Value numChipletsVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, XCDParitionsOnGPU);
+  Value workgroupCount =
+      b.create<arith::MulIOp>(loc, workgroupCountX, workgroupCountY);
+  Value workgroupCountPerChiplet =
+      b.create<arith::DivUIOp>(loc, workgroupCount, numChipletsVal);
+  Value chipletId = b.create<arith::RemUIOp>(loc, linearizedId, numChipletsVal);
+  Value wgIdWithinChiplet =
+      b.create<arith::DivUIOp>(loc, linearizedId, numChipletsVal);
+  Value reorderedId = b.create<arith::AddIOp>(
+      loc, wgIdWithinChiplet,
+      b.create<arith::MulIOp>(loc, chipletId, workgroupCountPerChiplet));
+
+  // Handle the remainder part.
+  Value constOne = b.createOrFold<arith::ConstantIndexOp>(loc, 1);
+  Value lastWorkgroupId =
+      b.create<arith::SubIOp>(loc, workgroupCount, constOne);
+  Value modulatedLastWorkgroupId = b.create<arith::SubIOp>(
+      loc, lastWorkgroupId,
+      b.create<arith::RemUIOp>(loc, workgroupCount, numChipletsVal));
+  Value isGreaterThanFinalWorkgroupId = b.create<arith::CmpIOp>(
+      loc, arith::CmpIPredicate::ugt, linearizedId, modulatedLastWorkgroupId);
+  Value finalId = b.create<arith::SelectOp>(loc, isGreaterThanFinalWorkgroupId,
+                                            linearizedId, reorderedId);
+
+  return finalId;
+}
+
+// Chiplet-aware workgroup reordering strategy: reordering + super-grouping.
+// Step 1: Reorder the workgroup grid to move slowly between
+// chiplet groups (Function: chipletAwareWorkgroupReordering).
+// Step 2: Implement 'super-grouping' of workgroups before switching to the next
+// column.
+static std::pair<Value, Value>
+makeChipletGroupedIds(Location loc, OpBuilder b, Value workgroupIdX,
+                      Value workgroupIdY, Value workgroupCountX,
+                      Value workgroupCountY, unsigned chipletGroupTile,
+                      unsigned numXCDs) {
+  // Create one dimension ID for workgroup
+  Value linearized =
+      b.create<arith::MulIOp>(loc, workgroupIdY, workgroupCountX);
+  linearized = b.create<arith::AddIOp>(loc, linearized, workgroupIdX);
+
+  assert(numXCDs > 1);
+  // Map chiplets to perform a spatially local tile operation.
+  // Reorder the linearized ID such that every consecutive group of chiplets
+  // is the slowest-changing dimension in the grid.
+  // Emphircally found that two chiplets as a group has better locality
+  // throughout.
+  linearized = chipletAwareWorkgroupReordering(
+      loc, b, linearized, workgroupCountX, workgroupCountY, numXCDs / 2);
+
+  // Detailed explaination about the idea behind the below implementation:
+  // the L2 Cache Optimizations subsection in
+  // https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html#
+  // Emphircally, found rowGroupSize=16 for mi300x achieves good performance
+  unsigned rowGroupSize = chipletGroupTile;
+  Value rowGroupSizeVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, rowGroupSize);
+  // group every 16 workgroups along Y dimension
+  // Number of workgroups in the group
```

**Comment:**
nit: it's weird that the comments are interleaved with code mid-sentence
```suggestion
  unsigned rowGroupSize = chipletGroupTile;
  Value rowGroupSizeVal =
      b.createOrFold<arith::ConstantIndexOp>(loc, rowGroupSize);
  
  // Empirically, found rowGroupSize=16 for mi300x achieves good performance
  // group every 16 workgroups along Y dimension.
  
  // Number of workgroups in the group.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:92`

```diff
@@ -68,6 +68,119 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// Reoredering to make workgroup ids move slowly between chiplet groups.
+
+// The following example illustrates the concept behind this function:
+// Currently, the GPU launches workgroups in a round-robin fashion across
+// each XCD partition on the GPU.
+// Assume we have 16 workgroups and XCDPartitionsOnGPU is 4.
+// The default GPU schedule will launch workgroups {0, 1, 2, 3, ..., 15} in
+// the following round-robin fashion:
+// Partition 0: {0, 4, 8, 12}
+// Partition 1: {1, 5, 9, 13}
+// Partition 2: {2, 6, 10, 14}
+// Partition 3: {3, 7, 11, 15}
+
+// After reordering, the workgroup IDs are {0, 4, 8, 12, 1, ..., 15},
+// resulting in the round-robin launching fashion:
+// Partition 0: {0, 1, 2, 3}
+// Partition 1: {4, 5, 6, 7}
+// Partition 2: {8, 9, 10, 11}
+// Partition 3: {12, 13, 14, 15}
+
+// The return value is each workgroup's permuted Id
+// In the above example:
```

**Comment:**
```suggestion
// Returns permuted workgroup id (X and Y dimensions).
// In the above example:
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:73`

```diff
@@ -68,6 +68,119 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// Reoredering to make workgroup ids move slowly between chiplet groups.
+
+// The following example illustrates the concept behind this function:
```

**Comment:**
```suggestion
// Example:
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:78`

```diff
@@ -68,6 +68,119 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// Reoredering to make workgroup ids move slowly between chiplet groups.
+
+// The following example illustrates the concept behind this function:
+// Currently, the GPU launches workgroups in a round-robin fashion across
+// each XCD partition on the GPU.
+// Assume we have 16 workgroups and XCDPartitionsOnGPU is 4.
+// The default GPU schedule will launch workgroups {0, 1, 2, 3, ..., 15} in
+// the following round-robin fashion:
```

**Comment:**
```suggestion
// the following order:
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:85`

```diff
@@ -68,6 +68,119 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// Reoredering to make workgroup ids move slowly between chiplet groups.
+
+// The following example illustrates the concept behind this function:
+// Currently, the GPU launches workgroups in a round-robin fashion across
+// each XCD partition on the GPU.
+// Assume we have 16 workgroups and XCDPartitionsOnGPU is 4.
+// The default GPU schedule will launch workgroups {0, 1, 2, 3, ..., 15} in
+// the following round-robin fashion:
+// Partition 0: {0, 4, 8, 12}
+// Partition 1: {1, 5, 9, 13}
+// Partition 2: {2, 6, 10, 14}
+// Partition 3: {3, 7, 11, 15}
+
+// After reordering, the workgroup IDs are {0, 4, 8, 12, 1, ..., 15},
+// resulting in the round-robin launching fashion:
```

**Comment:**
```suggestion
// resulting in the launch order:
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:160`

```diff
@@ -68,6 +68,119 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// Reoredering to make workgroup ids move slowly between chiplet groups.
+
+// The following example illustrates the concept behind this function:
+// Currently, the GPU launches workgroups in a round-robin fashion across
+// each XCD partition on the GPU.
+// Assume we have 16 workgroups and XCDPartitionsOnGPU is 4.
+// The default GPU schedule will launch workgroups {0, 1, 2, 3, ..., 15} in
+// the following round-robin fashion:
+// Partition 0: {0, 4, 8, 12}
+// Partition 1: {1, 5, 9, 13}
+// Partition 2: {2, 6, 10, 14}
+// Partition 3: {3, 7, 11, 15}
+
+// After reordering, the workgroup IDs are {0, 4, 8, 12, 1, ..., 15},
+// resulting in the round-robin launching fashion:
+// Partition 0: {0, 1, 2, 3}
+// Partition 1: {4, 5, 6, 7}
+// Partition 2: {8, 9, 10, 11}
+// Partition 3: {12, 13, 14, 15}
+
+// The return value is each workgroup's permuted Id
+// In the above example:
+// linearedId 0's permuted Id is still 0
+// linearedId 1's permiuted Id is 4
+static Value chipletAwareWorkgroupReordering(Location loc, OpBuilder b,
+                                             Value linearizedId,
+                                             Value workgroupCountX,
+                                             Value workgroupCountY,
+                                             int64_t XCDParitionsOnGPU) {
+  Value numChipletsVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, XCDParitionsOnGPU);
+  Value workgroupCount =
+      b.create<arith::MulIOp>(loc, workgroupCountX, workgroupCountY);
+  Value workgroupCountPerChiplet =
+      b.create<arith::DivUIOp>(loc, workgroupCount, numChipletsVal);
+  Value chipletId = b.create<arith::RemUIOp>(loc, linearizedId, numChipletsVal);
+  Value wgIdWithinChiplet =
+      b.create<arith::DivUIOp>(loc, linearizedId, numChipletsVal);
+  Value reorderedId = b.create<arith::AddIOp>(
+      loc, wgIdWithinChiplet,
+      b.create<arith::MulIOp>(loc, chipletId, workgroupCountPerChiplet));
+
+  // Handle the remainder part.
+  Value constOne = b.createOrFold<arith::ConstantIndexOp>(loc, 1);
+  Value lastWorkgroupId =
+      b.create<arith::SubIOp>(loc, workgroupCount, constOne);
+  Value modulatedLastWorkgroupId = b.create<arith::SubIOp>(
+      loc, lastWorkgroupId,
+      b.create<arith::RemUIOp>(loc, workgroupCount, numChipletsVal));
+  Value isGreaterThanFinalWorkgroupId = b.create<arith::CmpIOp>(
+      loc, arith::CmpIPredicate::ugt, linearizedId, modulatedLastWorkgroupId);
+  Value finalId = b.create<arith::SelectOp>(loc, isGreaterThanFinalWorkgroupId,
+                                            linearizedId, reorderedId);
+
+  return finalId;
+}
+
+// Chiplet-aware workgroup reordering strategy: reordering + super-grouping.
+// Step 1: Reorder the workgroup grid to move slowly between
+// chiplet groups (Function: chipletAwareWorkgroupReordering).
+// Step 2: Implement 'super-grouping' of workgroups before switching to the next
+// column.
+static std::pair<Value, Value>
+makeChipletGroupedIds(Location loc, OpBuilder b, Value workgroupIdX,
+                      Value workgroupIdY, Value workgroupCountX,
+                      Value workgroupCountY, unsigned chipletGroupTile,
+                      unsigned numXCDs) {
+  // Create one dimension ID for workgroup.
+  Value linearized =
+      b.create<arith::MulIOp>(loc, workgroupIdY, workgroupCountX);
+  linearized = b.create<arith::AddIOp>(loc, linearized, workgroupIdX);
+
+  assert(numXCDs > 1);
+  // Map chiplets to perform a spatially local tile operation.
+  // Reorder the linearized ID such that every consecutive group of chiplets
+  // is the slowest-changing dimension in the grid.
+  // Emphircally found that two chiplets as a group has better locality
+  // throughout.
+  linearized = chipletAwareWorkgroupReordering(
+      loc, b, linearized, workgroupCountX, workgroupCountY, numXCDs / 2);
+
+  // Detailed explaination about the idea behind the below implementation:
+  // the L2 Cache Optimizations subsection in
+  // https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html#
+  unsigned rowGroupSize = chipletGroupTile;
+  Value rowGroupSizeVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, rowGroupSize);
+
+  // Emphircally, found rowGroupSize=16 for mi300x achieves good performance
+  // group every 16 workgroups along Y dimension.
```

**Comment:**
```suggestion
  // Empirically, found rowGroupSize=16 for MI300X achieves good performance
  // group every 16 workgroups along Y dimension.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:154`

```diff
@@ -68,6 +68,119 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// Reoredering to make workgroup ids move slowly between chiplet groups.
+
+// The following example illustrates the concept behind this function:
+// Currently, the GPU launches workgroups in a round-robin fashion across
+// each XCD partition on the GPU.
+// Assume we have 16 workgroups and XCDPartitionsOnGPU is 4.
+// The default GPU schedule will launch workgroups {0, 1, 2, 3, ..., 15} in
+// the following round-robin fashion:
+// Partition 0: {0, 4, 8, 12}
+// Partition 1: {1, 5, 9, 13}
+// Partition 2: {2, 6, 10, 14}
+// Partition 3: {3, 7, 11, 15}
+
+// After reordering, the workgroup IDs are {0, 4, 8, 12, 1, ..., 15},
+// resulting in the round-robin launching fashion:
+// Partition 0: {0, 1, 2, 3}
+// Partition 1: {4, 5, 6, 7}
+// Partition 2: {8, 9, 10, 11}
+// Partition 3: {12, 13, 14, 15}
+
+// The return value is each workgroup's permuted Id
+// In the above example:
+// linearedId 0's permuted Id is still 0
+// linearedId 1's permiuted Id is 4
+static Value chipletAwareWorkgroupReordering(Location loc, OpBuilder b,
+                                             Value linearizedId,
+                                             Value workgroupCountX,
+                                             Value workgroupCountY,
+                                             int64_t XCDParitionsOnGPU) {
+  Value numChipletsVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, XCDParitionsOnGPU);
+  Value workgroupCount =
+      b.create<arith::MulIOp>(loc, workgroupCountX, workgroupCountY);
+  Value workgroupCountPerChiplet =
+      b.create<arith::DivUIOp>(loc, workgroupCount, numChipletsVal);
+  Value chipletId = b.create<arith::RemUIOp>(loc, linearizedId, numChipletsVal);
+  Value wgIdWithinChiplet =
+      b.create<arith::DivUIOp>(loc, linearizedId, numChipletsVal);
+  Value reorderedId = b.create<arith::AddIOp>(
+      loc, wgIdWithinChiplet,
+      b.create<arith::MulIOp>(loc, chipletId, workgroupCountPerChiplet));
+
+  // Handle the remainder part.
+  Value constOne = b.createOrFold<arith::ConstantIndexOp>(loc, 1);
+  Value lastWorkgroupId =
+      b.create<arith::SubIOp>(loc, workgroupCount, constOne);
+  Value modulatedLastWorkgroupId = b.create<arith::SubIOp>(
+      loc, lastWorkgroupId,
+      b.create<arith::RemUIOp>(loc, workgroupCount, numChipletsVal));
+  Value isGreaterThanFinalWorkgroupId = b.create<arith::CmpIOp>(
+      loc, arith::CmpIPredicate::ugt, linearizedId, modulatedLastWorkgroupId);
+  Value finalId = b.create<arith::SelectOp>(loc, isGreaterThanFinalWorkgroupId,
+                                            linearizedId, reorderedId);
+
+  return finalId;
+}
+
+// Chiplet-aware workgroup reordering strategy: reordering + super-grouping.
+// Step 1: Reorder the workgroup grid to move slowly between
+// chiplet groups (Function: chipletAwareWorkgroupReordering).
+// Step 2: Implement 'super-grouping' of workgroups before switching to the next
+// column.
+static std::pair<Value, Value>
+makeChipletGroupedIds(Location loc, OpBuilder b, Value workgroupIdX,
+                      Value workgroupIdY, Value workgroupCountX,
+                      Value workgroupCountY, unsigned chipletGroupTile,
+                      unsigned numXCDs) {
+  // Create one dimension ID for workgroup.
+  Value linearized =
+      b.create<arith::MulIOp>(loc, workgroupIdY, workgroupCountX);
+  linearized = b.create<arith::AddIOp>(loc, linearized, workgroupIdX);
+
+  assert(numXCDs > 1);
+  // Map chiplets to perform a spatially local tile operation.
+  // Reorder the linearized ID such that every consecutive group of chiplets
+  // is the slowest-changing dimension in the grid.
+  // Emphircally found that two chiplets as a group has better locality
+  // throughout.
+  linearized = chipletAwareWorkgroupReordering(
+      loc, b, linearized, workgroupCountX, workgroupCountY, numXCDs / 2);
+
+  // Detailed explaination about the idea behind the below implementation:
+  // the L2 Cache Optimizations subsection in
+  // https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html#
```

**Comment:**
```suggestion
  // Map chiplets to perform a spatially local tile operation.
  // Reorder the linearized ID such that every consecutive group of chiplets
  // is the slowest-changing dimension in the grid.
  // Empirically found that two chiplets as a group has better locality
  // throughout.
  linearized = chipletAwareWorkgroupReordering(
      loc, b, linearized, workgroupCountX, workgroupCountY, numXCDs / 2);

  // Detailed explanation about the idea behind the below implementation:
  // the L2 Cache Optimizations subsection in
  // https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html#
```

---

**File:** `compiler/src/iree/compiler/Codegen/Dialect/GPU/IR/IREEGPUAttrs.td:281`

```diff
@@ -269,6 +269,27 @@ def IREEGPU_MmaScheduleAttr : AttrDef<IREEGPU_Dialect, "MMASchedule"> {
   }];
 }
 
+//===----------------------------------------------------------------------===//
+// Workgroup Reordering Attr
+
+def IREEGPU_WorkGroupReorderAttr: AttrDef<IREEGPU_Dialect, "WorkgroupReorderOptions">{
+  let mnemonic = "reorder_workgroups";
+  let cppNamespace = "::mlir::iree_compiler::IREE::GPU";
+
+  string description = [{
+    Options for workgroup reordering strategies to improve L2 cache hit rate
+    and thus provide performance improvement.
```

**Comment:**
```suggestion
    Options for workgroup reordering strategies to improve L2 cache hit rate.
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:147`

```diff
@@ -144,19 +144,19 @@ makeChipletGroupedIds(Location loc, OpBuilder b, Value workgroupIdX,
   // Map chiplets to perform a spatially local tile operation.
   // Reorder the linearized ID such that every consecutive group of chiplets
   // is the slowest-changing dimension in the grid.
-  // Emphircally found that two chiplets as a group has better locality
+  // Emphirically found that two chiplets as a group has better locality
```

**Comment:**
```suggestion
  // Empirically found that two chiplets as a group has better locality
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:159`

```diff
@@ -144,19 +144,19 @@ makeChipletGroupedIds(Location loc, OpBuilder b, Value workgroupIdX,
   // Map chiplets to perform a spatially local tile operation.
   // Reorder the linearized ID such that every consecutive group of chiplets
   // is the slowest-changing dimension in the grid.
-  // Emphircally found that two chiplets as a group has better locality
+  // Emphirically found that two chiplets as a group has better locality
   // throughout.
   linearized = chipletAwareWorkgroupReordering(
       loc, b, linearized, workgroupCountX, workgroupCountY, numXCDs / 2);
 
-  // Detailed explaination about the idea behind the below implementation:
+  // Detailed explanation about the idea behind the below implementation:
   // the L2 Cache Optimizations subsection in
   // https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html#
   unsigned rowGroupSize = chipletGroupTile;
   Value rowGroupSizeVal =
       b.createOrFold<arith::ConstantIndexOp>(loc, rowGroupSize);
 
-  // Emphircally, found rowGroupSize=16 for mi300x achieves good performance
+  // Emphirically, found rowGroupSize=16 for MI300X achieves good performance
```

**Comment:**
```suggestion
  // Empirically, found rowGroupSize=16 for MI300X achieves good performance
```

---

**File:** `compiler/src/iree/compiler/Codegen/Common/GPU/WorkgroupReordering.cpp:110`

```diff
@@ -68,6 +68,95 @@ makeSwizzledIds(Location loc, OpBuilder b, Value workgroupIdX,
   return {swizzledIdX, swizzledIdY};
 }
 
+// reoredering to make workgroup ids move slowly between chiplet groups
+static Value chipletAwareWorkgroupReordering(Location loc, OpBuilder b,
+                                             Value linearizedId,
+                                             Value workgroupCountX,
+                                             Value workgroupCountY,
+                                             int64_t numChipletsPerGroup) {
+  Value numChipletsVal =
+      b.createOrFold<arith::ConstantIndexOp>(loc, numChipletsPerGroup);
+  Value workgroupCount =
+      b.create<arith::MulIOp>(loc, workgroupCountX, workgroupCountY);
+  Value workgroupCountPerChiplet =
+      b.create<arith::DivUIOp>(loc, workgroupCount, numChipletsVal);
+  Value chipletId = b.create<arith::RemUIOp>(loc, linearizedId, numChipletsVal);
+  Value wgIdWithinChiplet =
+      b.create<arith::DivUIOp>(loc, linearizedId, numChipletsVal);
+  Value reorderedId = b.create<arith::AddIOp>(
+      loc, wgIdWithinChiplet,
+      b.create<arith::MulIOp>(loc, chipletId, workgroupCountPerChiplet));
+
+  // The following code is used to handle the remainder part
+
+  Value constOne = b.createOrFold<arith::ConstantIndexOp>(loc, 1);
+  Value lastWorkgroupId =
+      b.create<arith::SubIOp>(loc, workgroupCount, constOne);
+  Value modulatedLastWorkgroupId = b.create<arith::SubIOp>(
+      loc, lastWorkgroupId,
+      b.create<arith::RemUIOp>(loc, workgroupCount, numChipletsVal));
+  Value isGreaterThanFinalWorkgroupId = b.create<arith::CmpIOp>(
+      loc, arith::CmpIPredicate::ugt, linearizedId, modulatedLastWorkgroupId);
+  linearizedId = b.create<arith::SelectOp>(loc, isGreaterThanFinalWorkgroupId,
+                                           linearizedId, reorderedId);
+
+  return linearizedId;
+}
+
+// Chiplet-aware workgroup reordering strategy: reordering + super-grouping.
+// Step 1: Reorder the workgroup grid to move slowly between
+// chiplet groups (Function: chipletAwareWorkgroupReordering).
+// Step 2: Implement 'super-grouping' of workgroups before switching to the next
+// column.
```

**Comment:**
Not addressed.

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.cpp:223`

```diff
@@ -206,6 +217,12 @@ static ReorderWorkgroupsStrategy getReorderWorkgroupsStrategy(
   return option.value_or(clReorderWorkgroupsStrategy);
 }
 
+// Reconciles log2 of the workgroup reordering tile size based on the pipeline
+// `option` and the CLI flag.
+static unsigned
+getReorderWorkgroupsLogTileSize(const std::optional<int64_t> &option) {
```

**Comment:**
```suggestion
getReorderWorkgroupsLogTileSize(std::optional<int64_t> option) {
```

This is a very small type, we can pass by value

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.cpp:224`

```diff
@@ -206,6 +217,12 @@ static ReorderWorkgroupsStrategy getReorderWorkgroupsStrategy(
   return option.value_or(clReorderWorkgroupsStrategy);
 }
 
+// Reconciles log2 of the workgroup reordering tile size based on the pipeline
+// `option` and the CLI flag.
+static unsigned
+getReorderWorkgroupsLogTileSize(const std::optional<int64_t> &option) {
+  return (unsigned)option.value_or(clReorderWorkgroupsLogTile);
```

**Comment:**
```suggestion
  int64_t logTile = option.value_or(clReorderWorkgroupsLogTile);
  assert(logTile >= 0);
  return static_cast<unsigned>(logTile);
```

---


---


## [PR #17645](https://github.com/iree-org/iree/pull/17645): Enable Workgroup Reordering Based on Translation Info Config Entries

### Review Summary

**CHANGES_REQUESTED** (2024-06-11)


**COMMENTED** (2024-06-12)


**COMMENTED** (2024-06-12)


**COMMENTED** (2024-06-12)

LGTM % nits. I will give it a try before approving.


**COMMENTED** (2024-06-12)


**APPROVED** (2024-06-12)

LGTM. I can see this improves performance on SDXL convs. Thanks for all the fixes.


### Code Comments

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp:101`

```diff
@@ -86,8 +86,26 @@ getPipelineOptions(FunctionOpInterface funcOp,
   if (DictionaryAttr config = translationInfo.getConfiguration()) {
     if (config.contains(LLVMGPUAttrNames::kNoReduceSharedMemoryBankConflicts))
       pipelineOptions.enableReduceSharedMemoryBankConflicts = false;
-    if (config.contains(LLVMGPUAttrNames::kNoReorderWorkgroups))
-      pipelineOptions.enableReorderWorkgroups = false;
+    if (config.contains(LLVMGPUAttrNames::kReorderWorkgroups)) {
+      pipelineOptions.enableReorderWorkgroups = true;
+      // Get the workgroups reorder config and enable the workgroup reordering
+      Attribute reorderGroupOption =
+          config.get(LLVMGPUAttrNames::kReorderWorkgroups);
+      assert(mlir::isa<mlir::StringAttr>(reorderGroupOption) &&
+             "reorder strategy should be a StringAttr");
+      StringRef reorderStr =
+          llvm::cast<StringAttr>(reorderGroupOption).getValue();
+      if (reorderStr == "transpose" || reorderStr == "Transpose") {
+        pipelineOptions.reorderOption =
+            LLVMGPUPipelineOptions::reorderWorkGroupOption::Transpose;
+      } else if (reorderStr == "swizzle" || reorderStr == "Swizzle") {
```

**Comment:**
I don't think we need to support both variants -- lowercase is fine.

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp:105`

```diff
@@ -86,8 +86,26 @@ getPipelineOptions(FunctionOpInterface funcOp,
   if (DictionaryAttr config = translationInfo.getConfiguration()) {
     if (config.contains(LLVMGPUAttrNames::kNoReduceSharedMemoryBankConflicts))
       pipelineOptions.enableReduceSharedMemoryBankConflicts = false;
-    if (config.contains(LLVMGPUAttrNames::kNoReorderWorkgroups))
-      pipelineOptions.enableReorderWorkgroups = false;
+    if (config.contains(LLVMGPUAttrNames::kReorderWorkgroups)) {
+      pipelineOptions.enableReorderWorkgroups = true;
+      // Get the workgroups reorder config and enable the workgroup reordering
+      Attribute reorderGroupOption =
+          config.get(LLVMGPUAttrNames::kReorderWorkgroups);
+      assert(mlir::isa<mlir::StringAttr>(reorderGroupOption) &&
+             "reorder strategy should be a StringAttr");
+      StringRef reorderStr =
+          llvm::cast<StringAttr>(reorderGroupOption).getValue();
+      if (reorderStr == "transpose" || reorderStr == "Transpose") {
+        pipelineOptions.reorderOption =
+            LLVMGPUPipelineOptions::reorderWorkGroupOption::Transpose;
+      } else if (reorderStr == "swizzle" || reorderStr == "Swizzle") {
+        pipelineOptions.reorderOption =
+            LLVMGPUPipelineOptions::reorderWorkGroupOption::Swizzle;
+      } else {
+        pipelineOptions.reorderOption =
```

**Comment:**
Here, should we check that the string is `none`? This is to diagnose cases when the value is misspelt.

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.cpp:81`

```diff
@@ -77,7 +77,8 @@ static llvm::cl::opt<bool> clLLVMGPUEnablePrefetch(
 
 llvm::raw_ostream &operator<<(llvm::raw_ostream &os,
                               const LLVMGPUPipelineOptions &options) {
-  return os << "{" << "enableReduceSharedMemoryBankConflicts = "
+  return os << "{"
+            << "enableReduceSharedMemoryBankConflicts = "
```

**Comment:**
Why is this changed?

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.h:34`

```diff
@@ -25,15 +25,19 @@ namespace mlir::iree_compiler {
 /// attribute. These are used to override default pass heuristics at the
 /// function granularity.
 namespace LLVMGPUAttrNames {
-inline constexpr StringLiteral kNoReorderWorkgroups = "no_reorder_workgroups";
+inline constexpr StringLiteral kReorderWorkgroups = "reorder_workgroups";
 inline constexpr StringLiteral kNoReduceSharedMemoryBankConflicts =
     "no_reduce_shared_memory_bank_conflicts";
 } //  namespace LLVMGPUAttrNames
 
 struct LLVMGPUPipelineOptions {
+  enum reorderWorkGroupOption { None, Transpose, Swizzle };
```

**Comment:**
```suggestion
  enum ReorderWorkgroupsOption { None, Transpose, Swizzle };
```

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.h:37`

```diff
@@ -25,15 +25,19 @@ namespace mlir::iree_compiler {
 /// attribute. These are used to override default pass heuristics at the
 /// function granularity.
 namespace LLVMGPUAttrNames {
-inline constexpr StringLiteral kNoReorderWorkgroups = "no_reorder_workgroups";
+inline constexpr StringLiteral kReorderWorkgroups = "reorder_workgroups";
 inline constexpr StringLiteral kNoReduceSharedMemoryBankConflicts =
     "no_reduce_shared_memory_bank_conflicts";
 } //  namespace LLVMGPUAttrNames
 
 struct LLVMGPUPipelineOptions {
+  enum reorderWorkGroupOption { None, Transpose, Swizzle };
+
   bool enableReduceSharedMemoryBankConflicts = true;
-  bool enableReorderWorkgroups = true;
+  bool enableReorderWorkgroups = false;
```

**Comment:**
We don't need this anymore

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.cpp:337`

```diff
@@ -322,10 +323,24 @@ void addGPUMatmulSimtPassPipeline(OpPassManager &funcPassManager,
   }
 
   if (options.enableReorderWorkgroups) {
+    ReorderWorkgrupsStrategy reorderStrategy;
+    if (options.reorderOption ==
+        LLVMGPUPipelineOptions::reorderWorkGroupOption::Transpose)
+      reorderStrategy = ReorderWorkgrupsStrategy::Transpose;
+    else if (options.reorderOption ==
+             LLVMGPUPipelineOptions::reorderWorkGroupOption::Swizzle)
+      reorderStrategy = ReorderWorkgrupsStrategy::Swizzle;
+    else
+      reorderStrategy = ReorderWorkgrupsStrategy::None;
+    funcPassManager.addPass(createReorderWorkgroups(
+        reorderStrategy, clReorderWorkgroupsLogSwizzleTile,
+        canReorderWorkgroups));
```

**Comment:**
This should probably go to a helper function

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.cpp:83`

```diff
@@ -77,7 +77,8 @@ static llvm::cl::opt<bool> clLLVMGPUEnablePrefetch(
 
 llvm::raw_ostream &operator<<(llvm::raw_ostream &os,
                               const LLVMGPUPipelineOptions &options) {
-  return os << "{" << "enableReduceSharedMemoryBankConflicts = "
+  return os << "{"
+            << "enableReduceSharedMemoryBankConflicts = "
             << options.enableReduceSharedMemoryBankConflicts
             << ", enableReorderWorkgroups = " << options.enableReorderWorkgroups
```

**Comment:**
The enum value is missing, we should also print it.

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.cpp:326`

```diff
@@ -322,10 +323,24 @@ void addGPUMatmulSimtPassPipeline(OpPassManager &funcPassManager,
   }
 
   if (options.enableReorderWorkgroups) {
+    ReorderWorkgrupsStrategy reorderStrategy;
```

**Comment:**
Can we use the same enum type and assign it directly?

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.h:34`

```diff
@@ -25,15 +25,19 @@ namespace mlir::iree_compiler {
 /// attribute. These are used to override default pass heuristics at the
 /// function granularity.
 namespace LLVMGPUAttrNames {
-inline constexpr StringLiteral kNoReorderWorkgroups = "no_reorder_workgroups";
+inline constexpr StringLiteral kReorderWorkgroups = "reorder_workgroups";
 inline constexpr StringLiteral kNoReduceSharedMemoryBankConflicts =
     "no_reduce_shared_memory_bank_conflicts";
 } //  namespace LLVMGPUAttrNames
 
 struct LLVMGPUPipelineOptions {
+  enum reorderWorkGroupOption { None, Transpose, Swizzle };
```

**Comment:**
We should be careful so that we can distinguish the case when this disables workgroup reordering (when enabled globally) and when it's not set. I think we can use `std::optional<ReorderWorkgroupsStrategy>`

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/test/ROCDL/config_user_vector_distribute.mlir:74`

```diff
@@ -71,7 +71,7 @@ hal.executable public @main_0_dispatch_0 {
 
 // CHECK:       #[[$TRANSLATION:.+]] = #iree_codegen.translation_info<LLVMGPUVectorDistribute workgroup_size = [128, 2, 1] subgroup_size = 64
 // CHECK-SAME:    mma_schedule = #iree_gpu.mma_schedule<intrinsic = #iree_gpu.mma_layout<MFMA_F16_16x16x16_F32>,
-// CHECK-SAME:    no_reorder_workgroups
+// CHECK-SAME:    reorder_workgroups = "transpose"
```

**Comment:**
I don't think this tests what it appears to since the attribute matches the global default. IMO we should maintain a test that disables globally-enables reordering (through the CLI flag) and then add a new test that enabled reordering when disabled globally.

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp:102`

```diff
@@ -87,23 +87,20 @@ getPipelineOptions(FunctionOpInterface funcOp,
     if (config.contains(LLVMGPUAttrNames::kNoReduceSharedMemoryBankConflicts))
       pipelineOptions.enableReduceSharedMemoryBankConflicts = false;
     if (config.contains(LLVMGPUAttrNames::kReorderWorkgroups)) {
-      pipelineOptions.enableReorderWorkgroups = true;
       // Get the workgroups reorder config and enable the workgroup reordering
       Attribute reorderGroupOption =
           config.get(LLVMGPUAttrNames::kReorderWorkgroups);
       assert(mlir::isa<mlir::StringAttr>(reorderGroupOption) &&
              "reorder strategy should be a StringAttr");
       StringRef reorderStr =
           llvm::cast<StringAttr>(reorderGroupOption).getValue();
-      if (reorderStr == "transpose" || reorderStr == "Transpose") {
-        pipelineOptions.reorderOption =
-            LLVMGPUPipelineOptions::reorderWorkGroupOption::Transpose;
-      } else if (reorderStr == "swizzle" || reorderStr == "Swizzle") {
-        pipelineOptions.reorderOption =
-            LLVMGPUPipelineOptions::reorderWorkGroupOption::Swizzle;
+      if (reorderStr == "transpose") {
+        pipelineOptions.reorderStrategy = ReorderWorkgrupsStrategy::Transpose;
+      } else if (reorderStr == "swizzle") {
+        pipelineOptions.reorderStrategy = ReorderWorkgrupsStrategy::Swizzle;
       } else {
-        pipelineOptions.reorderOption =
-            LLVMGPUPipelineOptions::reorderWorkGroupOption::None;
+        assert(reorderStr == "none" && "Unhandled reorder option");
```

**Comment:**
This should be an op error IMO -- the compiler shouldn't crash on invalid IR.

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.cpp:91`

```diff
@@ -77,9 +77,28 @@ static llvm::cl::opt<bool> clLLVMGPUEnablePrefetch(
 
 llvm::raw_ostream &operator<<(llvm::raw_ostream &os,
                               const LLVMGPUPipelineOptions &options) {
+  if (options.reorderStrategy) {
+    StringRef reorderStr;
+    if (options.reorderStrategy.value() == ReorderWorkgrupsStrategy::Transpose)
+      reorderStr = "transpose";
+    else if (options.reorderStrategy.value() ==
+             ReorderWorkgrupsStrategy::Swizzle)
+      reorderStr = "swizzle";
+    else {
+      assert(options.reorderStrategy.value() ==
+                 ReorderWorkgrupsStrategy::None &&
+             "Unhandled reorder option");
+      reorderStr = "none";
+    }
+
+    return os << "{" << "enableReduceSharedMemoryBankConflicts = "
+              << options.enableReduceSharedMemoryBankConflicts
+              << ", ReorderWorkgroupsStrategy = " << reorderStr
+              << ", enableUkernels = " << options.enableUkernels << "}";
+  }
```

**Comment:**
We should always print the strategy here. If the value is `std::nullopt` we can print something like `<not set>`

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.h:35`

```diff
@@ -25,15 +26,19 @@ namespace mlir::iree_compiler {
 /// attribute. These are used to override default pass heuristics at the
 /// function granularity.
 namespace LLVMGPUAttrNames {
-inline constexpr StringLiteral kNoReorderWorkgroups = "no_reorder_workgroups";
+inline constexpr StringLiteral kReorderWorkgroups = "reorder_workgroups";
 inline constexpr StringLiteral kNoReduceSharedMemoryBankConflicts =
     "no_reduce_shared_memory_bank_conflicts";
 } //  namespace LLVMGPUAttrNames
 
 struct LLVMGPUPipelineOptions {
+  //   enum reorderWorkGroupsOption { None, Transpose, Swizzle };
```

**Comment:**
Remove this

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.h:40`

```diff
@@ -25,15 +26,19 @@ namespace mlir::iree_compiler {
 /// attribute. These are used to override default pass heuristics at the
 /// function granularity.
 namespace LLVMGPUAttrNames {
-inline constexpr StringLiteral kNoReorderWorkgroups = "no_reorder_workgroups";
+inline constexpr StringLiteral kReorderWorkgroups = "reorder_workgroups";
 inline constexpr StringLiteral kNoReduceSharedMemoryBankConflicts =
     "no_reduce_shared_memory_bank_conflicts";
 } //  namespace LLVMGPUAttrNames
 
 struct LLVMGPUPipelineOptions {
+  //   enum reorderWorkGroupsOption { None, Transpose, Swizzle };
+
   bool enableReduceSharedMemoryBankConflicts = true;
-  bool enableReorderWorkgroups = true;
   bool enableUkernels = false;
+
+  //   reorderWorkGroupOption reorderOption = None;
```

**Comment:**
Remove this

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp:93`

```diff
@@ -86,8 +86,23 @@ getPipelineOptions(FunctionOpInterface funcOp,
   if (DictionaryAttr config = translationInfo.getConfiguration()) {
     if (config.contains(LLVMGPUAttrNames::kNoReduceSharedMemoryBankConflicts))
       pipelineOptions.enableReduceSharedMemoryBankConflicts = false;
-    if (config.contains(LLVMGPUAttrNames::kNoReorderWorkgroups))
-      pipelineOptions.enableReorderWorkgroups = false;
+    if (config.contains(LLVMGPUAttrNames::kReorderWorkgroups)) {
+      // Get the workgroups reorder config and enable the workgroup reordering
+      Attribute reorderGroupOption =
+          config.get(LLVMGPUAttrNames::kReorderWorkgroups);
+      assert(mlir::isa<mlir::StringAttr>(reorderGroupOption) &&
```

**Comment:**
```suggestion
      assert(isa<StringAttr>(reorderGroupOption) &&
```

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp:96`

```diff
@@ -86,8 +86,23 @@ getPipelineOptions(FunctionOpInterface funcOp,
   if (DictionaryAttr config = translationInfo.getConfiguration()) {
     if (config.contains(LLVMGPUAttrNames::kNoReduceSharedMemoryBankConflicts))
       pipelineOptions.enableReduceSharedMemoryBankConflicts = false;
-    if (config.contains(LLVMGPUAttrNames::kNoReorderWorkgroups))
-      pipelineOptions.enableReorderWorkgroups = false;
+    if (config.contains(LLVMGPUAttrNames::kReorderWorkgroups)) {
+      // Get the workgroups reorder config and enable the workgroup reordering
+      Attribute reorderGroupOption =
+          config.get(LLVMGPUAttrNames::kReorderWorkgroups);
+      assert(mlir::isa<mlir::StringAttr>(reorderGroupOption) &&
+             "reorder strategy should be a StringAttr");
+      StringRef reorderStr =
+          llvm::cast<StringAttr>(reorderGroupOption).getValue();
```

**Comment:**
```suggestion
         cast<StringAttr>(reorderGroupOption).getValue();
```

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.cpp:82`

```diff
@@ -77,9 +77,28 @@ static llvm::cl::opt<bool> clLLVMGPUEnablePrefetch(
 
 llvm::raw_ostream &operator<<(llvm::raw_ostream &os,
                               const LLVMGPUPipelineOptions &options) {
+  if (options.reorderStrategy) {
+    StringRef reorderStr;
+    if (options.reorderStrategy.value() == ReorderWorkgrupsStrategy::Transpose)
```

**Comment:**
nit: you can compare directly with `==`
```suggestion
    if (options.reorderStrategy == ReorderWorkgrupsStrategy::Transpose)
```

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/Passes.cpp:88`

```diff
@@ -77,9 +77,28 @@ static llvm::cl::opt<bool> clLLVMGPUEnablePrefetch(
 
 llvm::raw_ostream &operator<<(llvm::raw_ostream &os,
                               const LLVMGPUPipelineOptions &options) {
+  if (options.reorderStrategy) {
+    StringRef reorderStr;
+    if (options.reorderStrategy.value() == ReorderWorkgrupsStrategy::Transpose)
+      reorderStr = "transpose";
+    else if (options.reorderStrategy.value() ==
+             ReorderWorkgrupsStrategy::Swizzle)
+      reorderStr = "swizzle";
+    else {
+      assert(options.reorderStrategy.value() ==
```

**Comment:**
nit: If one of the if-else branches has braces around it, the other ones should have them too

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/test/ROCDL/config_user_vector_distribute.mlir:5`

```diff
@@ -2,11 +2,20 @@
 // RUN:   --iree-codegen-reorder-workgroups-strategy=transpose \
 // RUN:   --pass-pipeline="builtin.module(hal.executable(hal.executable.variant(builtin.module(iree-llvmgpu-select-lowering-strategy, func.func(iree-llvmgpu-lower-executable-target)))))" %s | FileCheck %s
 
+// A new test that disables the global CLI flag (--iree-codegen-reorder-workgroups-strategy) and checks that applying reorder_workgroups = "transpose" enables workgroup reordering.
```

**Comment:**
```suggestion
// Check that applying `reorder_workgroups = "transpose"` enables workgroup reordering.
```

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/test/ROCDL/config_user_vector_distribute.mlir:8`

```diff
@@ -2,11 +2,20 @@
 // RUN:   --iree-codegen-reorder-workgroups-strategy=transpose \
 // RUN:   --pass-pipeline="builtin.module(hal.executable(hal.executable.variant(builtin.module(iree-llvmgpu-select-lowering-strategy, func.func(iree-llvmgpu-lower-executable-target)))))" %s | FileCheck %s
 
+// A new test that disables the global CLI flag (--iree-codegen-reorder-workgroups-strategy) and checks that applying reorder_workgroups = "transpose" enables workgroup reordering.
+
+// RUN: iree-opt --split-input-file --iree-gpu-test-target=gfx942 --iree-codegen-llvmgpu-use-vector-distribution \
+// RUN:   --pass-pipeline="builtin.module(hal.executable(hal.executable.variant(builtin.module(iree-llvmgpu-select-lowering-strategy, func.func(iree-llvmgpu-lower-executable-target)))))" %s | FileCheck %s --check-prefix=RWP
```

**Comment:**
What does RWP stand for?

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/test/ROCDL/config_user_vector_distribute.mlir:100`

```diff
@@ -90,14 +110,24 @@ hal.executable public @main_0_dispatch_0 {
       // CHECK-LABEL: func.func @main_0_dispatch_0_matmul_transpose_b
       // CHECK:         memref.alloc() : memref<128x36xf16, #gpu.address_space<workgroup>>
       // CHECK:         memref.alloc() : memref<128x36xf16, #gpu.address_space<workgroup>>
-      // CHECK-DAG:     hal.interface.workgroup.id[1] : index
-      // CHECK-DAG:     hal.interface.workgroup.id[0] : index
-      // CHECK-NEXT:    scf.for
+      // CHECK-DAG:     %[[WG_Y:.+]] = hal.interface.workgroup.id[1] : index
+      // CHECK-DAG:     %[[WG_X:.+]] = hal.interface.workgroup.id[0] : index
+      // CHECK-DAG:     arith.muli %[[WG_Y]], %{{.+}} : index
+      // CHECK-DAG:     arith.addi %{{.+}}, %[[WG_X]] : index
+      // CHECK:         scf.for
 
+      // RWP-LABEL: func.func @main_0_dispatch_0_matmul_transpose_b
+      // RWP:         memref.alloc() : memref<128x36xf16, #gpu.address_space<workgroup>>
+      // RWP:         memref.alloc() : memref<128x36xf16, #gpu.address_space<workgroup>>
+      // RWP-DAG:     %[[WG_Y:.+]] = hal.interface.workgroup.id[1] : index
+      // RWP-DAG:     %[[WG_X:.+]] = hal.interface.workgroup.id[0] : index
+      // RWP-DAG:     arith.muli %[[WG_Y]], %{{.+}} : index
+      // RWP-DAG:     arith.addi %{{.+}}, %[[WG_X]] : index
+      // RWP:         scf.for  
       func.func @main_0_dispatch_0_matmul_transpose_b_2048x10240x1280_f16xf16xf32()
         attributes {translation_info = #iree_codegen.translation_info<LLVMGPUVectorDistribute workgroup_size = [128, 2, 1] subgroup_size = 64, {
           mma_schedule = #iree_gpu.mma_schedule<intrinsic = #iree_gpu.mma_layout<MFMA_F16_16x16x16_F32>, subgroup_m_count = 2, subgroup_n_count = 2>,
-          no_reorder_workgroups  // Disable the 'reorderWorkgroups' pass.
```

**Comment:**
We should maintain a test where global reordering is disabled through this attribute

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp:96`

```diff
@@ -86,8 +86,23 @@ getPipelineOptions(FunctionOpInterface funcOp,
   if (DictionaryAttr config = translationInfo.getConfiguration()) {
     if (config.contains(LLVMGPUAttrNames::kNoReduceSharedMemoryBankConflicts))
       pipelineOptions.enableReduceSharedMemoryBankConflicts = false;
-    if (config.contains(LLVMGPUAttrNames::kNoReorderWorkgroups))
-      pipelineOptions.enableReorderWorkgroups = false;
+    if (config.contains(LLVMGPUAttrNames::kReorderWorkgroups)) {
+      // Get the workgroups reorder config and enable the workgroup reordering
+      Attribute reorderGroupOption =
+          config.get(LLVMGPUAttrNames::kReorderWorkgroups);
+      assert(mlir::isa<mlir::StringAttr>(reorderGroupOption) &&
+             "reorder strategy should be a StringAttr");
+      StringRef reorderStr =
+          llvm::cast<StringAttr>(reorderGroupOption).getValue();
```

**Comment:**
not resolved

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp:103`

```diff
@@ -86,8 +86,25 @@ getPipelineOptions(FunctionOpInterface funcOp,
   if (DictionaryAttr config = translationInfo.getConfiguration()) {
     if (config.contains(LLVMGPUAttrNames::kNoReduceSharedMemoryBankConflicts))
       pipelineOptions.enableReduceSharedMemoryBankConflicts = false;
-    if (config.contains(LLVMGPUAttrNames::kNoReorderWorkgroups))
-      pipelineOptions.enableReorderWorkgroups = false;
+    if (config.contains(LLVMGPUAttrNames::kReorderWorkgroups)) {
+      // Get the workgroups reorder config and enable the workgroup reordering
+      Attribute reorderGroupOption =
+          config.get(LLVMGPUAttrNames::kReorderWorkgroups);
+      assert(isa<StringAttr>(reorderGroupOption) &&
+             "reorder strategy should be a StringAttr");
+      StringRef reorderStr =
+          llvm::cast<StringAttr>(reorderGroupOption).getValue();
+      if (reorderStr == "transpose") {
+        pipelineOptions.reorderStrategy = ReorderWorkgrupsStrategy::Transpose;
+      } else if (reorderStr == "swizzle") {
+        pipelineOptions.reorderStrategy = ReorderWorkgrupsStrategy::Swizzle;
+      } else {
+        if (reorderStr != "none")
+          funcOp.emitOpError("Unhandled reorder option");
```

**Comment:**
I don't remember the exact syntax, but this error can be made more helpful and precise:
```suggestion
          funcOp.emitOpError() << "Unknown " << LLVMGPUAttrNames::kReorderWorkgroups << "value: " << reorderGroupOpton;
```

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp:91`

```diff
@@ -86,8 +86,25 @@ getPipelineOptions(FunctionOpInterface funcOp,
   if (DictionaryAttr config = translationInfo.getConfiguration()) {
     if (config.contains(LLVMGPUAttrNames::kNoReduceSharedMemoryBankConflicts))
       pipelineOptions.enableReduceSharedMemoryBankConflicts = false;
-    if (config.contains(LLVMGPUAttrNames::kNoReorderWorkgroups))
-      pipelineOptions.enableReorderWorkgroups = false;
+    if (config.contains(LLVMGPUAttrNames::kReorderWorkgroups)) {
+      // Get the workgroups reorder config and enable the workgroup reordering
+      Attribute reorderGroupOption =
```

**Comment:**
```suggestion
      Attribute reorderWorkgroupOption =
```

---

**File:** `compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp:90`

```diff
@@ -86,8 +86,25 @@ getPipelineOptions(FunctionOpInterface funcOp,
   if (DictionaryAttr config = translationInfo.getConfiguration()) {
     if (config.contains(LLVMGPUAttrNames::kNoReduceSharedMemoryBankConflicts))
       pipelineOptions.enableReduceSharedMemoryBankConflicts = false;
-    if (config.contains(LLVMGPUAttrNames::kNoReorderWorkgroups))
-      pipelineOptions.enableReorderWorkgroups = false;
+    if (config.contains(LLVMGPUAttrNames::kReorderWorkgroups)) {
+      // Get the workgroups reorder config and enable the workgroup reordering
```

**Comment:**
```suggestion
      // Get the workgroups reorder config and enable the workgroup reordering.
```

---


---


## [PR #17539](https://github.com/iree-org/iree/pull/17539): Add support for Conv2D in new filter layout (Fhwc) : (NchwFchw => NhwcFhwc)

### Review Summary

**COMMENTED** (2024-06-05)


### Code Comments

**File:** `compiler/src/iree/compiler/Preprocessing/Common/ConvertConvToChannelsLast.cpp:486`

```diff
@@ -450,34 +476,53 @@ namespace {
 
 struct ConvertLinalgConvNchwFchw : OpRewritePattern<linalg::Conv2DNchwFchwOp> {
   using OpRewritePattern::OpRewritePattern;
-  ConvertLinalgConvNchwFchw(MLIRContext *context, PatternBenefit benefit = 2)
-      : OpRewritePattern<linalg::Conv2DNchwFchwOp>(context, benefit) {}
+  ConvertLinalgConvNchwFchw(MLIRContext *context, bool enableFHWC = false,
+                            PatternBenefit benefit = 2)
+      : OpRewritePattern<linalg::Conv2DNchwFchwOp>(context, benefit),
+        enableFHWCFilter(enableFHWC) {}
 
   LogicalResult matchAndRewrite(linalg::Conv2DNchwFchwOp convOp,
                                 PatternRewriter &rewriter) const override {
-    return transposeConvLikeLinalgOp(
-        rewriter, convOp, /*tilingFactor=*/-1,
-        namedConvBuilderFn<linalg::Conv2DNchwFchwOp, linalg::Conv2DNhwcHwcfOp>);
+    auto strides = convOp.getStrides();
```

**Comment:**
Don't use auto when the type is not obvious based on the RHS only.

---

**File:** `compiler/src/iree/compiler/Preprocessing/Common/ConvertConvToChannelsLast.cpp:495`

```diff
@@ -450,34 +476,53 @@ namespace {
 
 struct ConvertLinalgConvNchwFchw : OpRewritePattern<linalg::Conv2DNchwFchwOp> {
   using OpRewritePattern::OpRewritePattern;
-  ConvertLinalgConvNchwFchw(MLIRContext *context, PatternBenefit benefit = 2)
-      : OpRewritePattern<linalg::Conv2DNchwFchwOp>(context, benefit) {}
+  ConvertLinalgConvNchwFchw(MLIRContext *context, bool enableFHWC = false,
+                            PatternBenefit benefit = 2)
+      : OpRewritePattern<linalg::Conv2DNchwFchwOp>(context, benefit),
+        enableFHWCFilter(enableFHWC) {}
 
   LogicalResult matchAndRewrite(linalg::Conv2DNchwFchwOp convOp,
                                 PatternRewriter &rewriter) const override {
-    return transposeConvLikeLinalgOp(
-        rewriter, convOp, /*tilingFactor=*/-1,
-        namedConvBuilderFn<linalg::Conv2DNchwFchwOp, linalg::Conv2DNhwcHwcfOp>);
+    auto strides = convOp.getStrides();
+    bool hasAllOneStrides =
+        strides.isSplat() && strides.getSplatValue<int64_t>() == 1;
+    // Only enable this new filter layout when all strides are 1.
+    if (enableFHWCFilter && hasAllOneStrides) {
+      return transposeConvLikeLinalgOp(
+          rewriter, convOp, /*tilingFactor=*/-1, enableFHWCFilter,
+          namedConvBuilderFn<linalg::Conv2DNchwFchwOp,
+                             linalg::Conv2DNhwcFhwcOp>);
+    } else {
```

**Comment:**
no else after return: https://www.llvm.org/docs/CodingStandards.html#don-t-use-else-after-a-return

---


---
